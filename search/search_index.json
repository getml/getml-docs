{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"getML Documentation Welcome to the getML technical documentation. This document is written for data scientists who want to use the getML software suite for their projects. For general information about getML visit getml.com . For a collection of demo notebooks, visit getml-demo . You can also contact us for any questions or inquiries. Note Some components of getML have been open sourced as part of getML community edition . You may have a look at community vs enterprise edition table to see the highlights of both the editions. getML in one minute getML is an innovative tool for the end-to-end automation of data science projects. It covers everything from convenient data loading procedures to the deployment of trained models. Most notably, getML includes advanced algorithms for automated feature engineering (feature learning) on relational data and time series. Feature engineering on relational data is defined as the creation of a flat table by merging and aggregating data. It is sometimes also referred to as data wrangling . Feature engineering is necessary if your data is distributed over more than one data table. Automated feature engineering Saves up to 90% of the time spent on a data science project Increases the prediction accuracy over manual feature engineering Andrew Ng, Professor at Stanford University and Co-founder of Google Brain described manual feature engineering as follows: Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering. The main purpose of getML is to automate this \"difficult, time-consuming\" process as much as possible. getML comes with a high-performance engine written in C++ and an intuitive Python API . Completing a data science project with getML consists of seven simple steps. import getml getml . engine . launch () getml . engine . set_project ( 'one_minute_to_getml' ) Load the data into the engine population = getml . data . DataFrame . from_csv ( 'data_population.csv' , name = 'population_table' ) peripheral = getml . data . DataFrame . from_csv ( 'data_peripheral.csv' , name = 'peripheral_table' ) 2. Annotate the data population . set_role ( 'target' , getml . data . role . target ) population . set_role ( 'join_key' , getml . data . role . join_key ) ... 3. Define the data model dm = getml . data . DataModel ( population . to_placeholder ( \"POPULATION\" )) dm . add ( peripheral . to_placeholder ( \"PERIPHERAL\" )) dm . POPULATION . join ( dm . PERIPHERAL , on = \"join_key\" , ) 4. Train the feature learning algorithm and the predictor pipe = getml . pipeline . Pipeline ( data_model = dm , feature_learners = getml . feature_learning . FastProp () predictors = getml . predictors . LinearRegression () ) pipe . fit ( population = population , peripheral = [ peripheral ] ) 5. Evaluate pipe . score ( population = population_unseen , peripheral = [ peripheral_unseen ] ) 6. Predict pipe . predict ( population = population_unseen , peripheral = [ peripheral_unseen ] ) 7. Deploy # Allow the pipeline to respond to HTTP requests pipe . deploy ( True ) Check out the rest of this documentation to find out how getML achieves top performance on real-world data science projects with many tables and complex data schemes. How to use this guide If you want to get started with getML right away, we recommend to follow the installation instructions and then go through the getting started guide . If you are looking for more detailed information, other sections of this documentation are more suitable. There are three major parts: Tutorials The tutorials section contains examples of how to use getML in real-world projects. All tutorials are based on public data sets so that you can follow along. If you are looking for an intuitive access to getML, the tutorials section is the right place to go. Also, the code examples are explicitly intended to be used as a template for your own projects. User guide The user guide explains all conceptional details behind getML in depth. It can serve as a reference guide for experienced users but it's also suitable for first day users who want to get a deeper understanding of how getML works. Each chapter in the user guide represents one step of a typical data science project. API documentation The API documentation covers everything related to the Python interface to the getML engine. Each module comes with a dedicated section that contains concrete code examples. You can also check out our other resources getML homepage","title":"getML Documentation"},{"location":"#getml-documentation","text":"Welcome to the getML technical documentation. This document is written for data scientists who want to use the getML software suite for their projects. For general information about getML visit getml.com . For a collection of demo notebooks, visit getml-demo . You can also contact us for any questions or inquiries. Note Some components of getML have been open sourced as part of getML community edition . You may have a look at community vs enterprise edition table to see the highlights of both the editions.","title":"getML Documentation"},{"location":"#getml-in-one-minute","text":"getML is an innovative tool for the end-to-end automation of data science projects. It covers everything from convenient data loading procedures to the deployment of trained models. Most notably, getML includes advanced algorithms for automated feature engineering (feature learning) on relational data and time series. Feature engineering on relational data is defined as the creation of a flat table by merging and aggregating data. It is sometimes also referred to as data wrangling . Feature engineering is necessary if your data is distributed over more than one data table. Automated feature engineering Saves up to 90% of the time spent on a data science project Increases the prediction accuracy over manual feature engineering Andrew Ng, Professor at Stanford University and Co-founder of Google Brain described manual feature engineering as follows: Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering. The main purpose of getML is to automate this \"difficult, time-consuming\" process as much as possible. getML comes with a high-performance engine written in C++ and an intuitive Python API . Completing a data science project with getML consists of seven simple steps. import getml getml . engine . launch () getml . engine . set_project ( 'one_minute_to_getml' ) Load the data into the engine population = getml . data . DataFrame . from_csv ( 'data_population.csv' , name = 'population_table' ) peripheral = getml . data . DataFrame . from_csv ( 'data_peripheral.csv' , name = 'peripheral_table' ) 2. Annotate the data population . set_role ( 'target' , getml . data . role . target ) population . set_role ( 'join_key' , getml . data . role . join_key ) ... 3. Define the data model dm = getml . data . DataModel ( population . to_placeholder ( \"POPULATION\" )) dm . add ( peripheral . to_placeholder ( \"PERIPHERAL\" )) dm . POPULATION . join ( dm . PERIPHERAL , on = \"join_key\" , ) 4. Train the feature learning algorithm and the predictor pipe = getml . pipeline . Pipeline ( data_model = dm , feature_learners = getml . feature_learning . FastProp () predictors = getml . predictors . LinearRegression () ) pipe . fit ( population = population , peripheral = [ peripheral ] ) 5. Evaluate pipe . score ( population = population_unseen , peripheral = [ peripheral_unseen ] ) 6. Predict pipe . predict ( population = population_unseen , peripheral = [ peripheral_unseen ] ) 7. Deploy # Allow the pipeline to respond to HTTP requests pipe . deploy ( True ) Check out the rest of this documentation to find out how getML achieves top performance on real-world data science projects with many tables and complex data schemes.","title":"getML in one minute"},{"location":"#how-to-use-this-guide","text":"If you want to get started with getML right away, we recommend to follow the installation instructions and then go through the getting started guide . If you are looking for more detailed information, other sections of this documentation are more suitable. There are three major parts: Tutorials The tutorials section contains examples of how to use getML in real-world projects. All tutorials are based on public data sets so that you can follow along. If you are looking for an intuitive access to getML, the tutorials section is the right place to go. Also, the code examples are explicitly intended to be used as a template for your own projects. User guide The user guide explains all conceptional details behind getML in depth. It can serve as a reference guide for experienced users but it's also suitable for first day users who want to get a deeper understanding of how getML works. Each chapter in the user guide represents one step of a typical data science project. API documentation The API documentation covers everything related to the Python interface to the getML engine. Each module comes with a dedicated section that contains concrete code examples. You can also check out our other resources getML homepage","title":"How to use this guide"},{"location":"user_guide/annotating_data/annotating_data/","text":"Annotating data After you have imported your data into the getML engine, there is one more step to undertake before you can start learning features: You need to assign a role to each column. Why is that? First, the general structure of the individual data frames is needed to construct the relational data model . This is done by assigning the roles join key and time stamp . The former defines the columns that are used to join different data frames, the latter ensures that only rows in a reasonable time frame are taken into account (otherwise there might be data leaks). Second, you need to tell the feature learning algorithm how to interpret the individual columns for it to construct sophisticated features. That is why we need the roles numerical , categorical , and target . You can also assign units to each column in a Data Frame. This chapter contains detailed information on the individual roles and units . In short When building the data model , you should keep the following things in mind: Every DataFrame in a data model needs to have at least one column ( columns ) with the role join key . The role time stamp has to be used to prevent data leaks (refer to data model time series for details). When learning features , please keep the following things in mind: Only columns with roles of categorical , numerical , and time stamp will be used by the feature learning algorithm for aggregations or conditions, unless you explicitly tell it to aggregate target columns as well (refer to allow_lagged_target in join() ). Columns are only compared with each other if they have the same unit . If you want to make sure that a column is only used for comparison, you can set comparison_only (refer to annotating units ). Time stamps are automatically set to comparison_only . Roles Roles determine if and how columns are handled during the construction of the data model and how they are interpreted by the feature learning algorithm . The following roles are available in getML: Role Class Included in FL algorithm categorical StringColumn yes numerical FloatColumn yes text StringColumn yes time_stamp FloatColumn yes join_key StringColumn no target FloatColumn not by default unused_float FloatColumn no unused_string StringColumn no When constructing a DataFrame via the class methods from_csv , from_pandas , from_db , and from_json , all columns will have either the role unused float or unused string . Unused columns will be ignored by the feature learning and machine learning (ML) algorithms. import pandas as pd data_df = dict ( animal = [ \"hawk\" , \"parrot\" , \"goose\" ], votes = [ 12341 , 5127 , 65311 ], weight = [ 12.14 , 12.6 , 11.92 ], animal_id = [ 123 , 512 , 671 ], date = [ \"2019-05-02\" , \"2019-02-28\" , \"2018-12-24\" ] ) pandas_df = pd . DataFrame ( data = data_df ) getml_df = getml . data . DataFrame . from_pandas ( pandas_df , name = 'animal elections' ) getml_df # Output: # | votes | weight | animal_id | animal | date | # | unused float | unused float | unused float | unused string | unused string | # ------------------------------------------------------------------------------ # | 12341 | 12.14 | 123 | hawk | 2019-05-02 | # | 5127 | 12.6 | 512 | parrot | 2019-02-28 | # | 65311 | 11.92 | 671 | goose | 2018-12-24 | getml_df . set_role ([ 'animal_id' ], getml . data . roles . join_key ) getml_df . set_role ([ 'animal' ], getml . data . roles . categorical ) getml_df . set_role ([ 'votes' , 'weight' ], getml . data . roles . numerical ) getml_df . set_role ([ 'date' ], getml . data . roles . time_stamp ) getml_df # Updated DataFrame structure will be displayed here. To make use of the imported data, you have to tell getML how you intend to use each column by assigning a role ( roles ). This is done by using the set_role method of the DataFrame . Each column must have exactly one role. If you wish to use a column in two different roles, you have to add it twice and assign each copy a different role. getml_df . set_role ([ 'animal_id' ], getml . data . roles . join_key ) getml_df . set_role ([ 'animal' ], getml . data . roles . categorical ) getml_df . set_role ([ 'votes' , 'weight' ], getml . data . roles . numerical ) getml_df . set_role ([ 'date' ], getml . data . roles . time_stamp ) getml_df # Output: # | date | animal_id | animal | votes | weight | # | time stamp | join key | categorical | numerical | numerical | # --------------------------------------------------------------------------------- # | 2019-05-02T00:00:00.000000Z | 123 | hawk | 12341 | 12.14 | # | 2019-02-28T00:00:00.000000Z | 512 | parrot | 5127 | 12.6 | # | 2018-12-24T00:00:00.000000Z | 671 | goose | 65311 | 11.92 | When assigning new roles to existing columns, you might notice that some of these calls are completed in an instance while others might take a considerable amount of time. What's happening here? A column's role also determines its type. When you set a new role, an implicit type conversion might take place. A note on reproducibility and efficiency When building a stable pipeline you want to deploy in a productive environment, the flexible default behavior of the import interface might be more of an obstacle. For instance, CSV files are not type-safe. A column that was interpreted as a float column for one set of files might be interpreted as a string column for a different set of files. This obviously has implications for the stability of your pipeline. Therefore, it might be a good idea to hard-code column roles. In the getML Python API, you can bypass the default deduction of the role of each column by providing a dictionary mapping each column name to a role in the import interface. roles = { \"categorical\" : [ \"colname1\" , \"colname2\" ], \"target\" : [ \"colname3\" ]} df_expd = getml . data . DataFrame . from_csv ( fnames = [ \"file1.csv\" , \"file2.csv\" ], name = \"MY DATA FRAME\" , sep = ';' , quotechar = '\"' , roles = roles , ignore = True ) If the ignore argument is set to True , any columns missing in the dictionary won't be imported at all. If you feel that writing the roles by hand is too tedious, you can use dry : If you call the import interface while setting the dry argument to True , no data is read. Instead, the default roles of all columns will be returned as a dictionary. You can store, alter, and hard-code this dictionary into your stable pipeline. roles = getml . data . DataFrame . from_csv ( fnames = [ \"file1.csv\" , \"file2.csv\" ], name = \"MY DATA FRAME\" , sep = ';' , quotechar = '\"' , dry = True ) Even if your data source is type safe, setting roles is still a good idea because it is also more efficient. Using set_role() creates a deep copy of the original column and might perform an implicit type conversion. If you already know where you want your data to end up, it might be a good idea to set roles in advance. Join key Join keys are required to establish a relation between two DataFrame objects. Please refer to the data models for details. The content of this column is allowed to contain NULL values. NULL values won't be matched to anything, not even to NULL values in other join keys. columns of this role will not be aggregated by the feature learning algorithm or used for conditions. Time stamp This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins. In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default: WHERE time_stamp > some_fixed_date Instead, time stamps will always be compared to other time stamps: WHERE time_stamp1 - time_stamp2 > some_value This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample. When assigning the role time stamp to a column that is currently a StringColumn , you need to specify the format of this string. You can do so by using the time_formats argument of set_role() . You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are %w - abbreviated weekday (Mon, Tue, ...) %W - full weekday (Monday, Tuesday, ...) %b - abbreviated month (Jan, Feb, ...) %B - full month (January, February, ...) %d - zero-padded day of month (01 .. 31) %e - day of month (1 .. 31) %f - space-padded day of month ( 1 .. 31) %m - zero-padded month (01 .. 12) %n - month (1 .. 12) %o - space-padded month ( 1 .. 12) %y - year without century (70) %Y - year with century (1970) %H - hour (00 .. 23) %h - hour (00 .. 12) %a - am/pm %A - AM/PM %M - minute (00 .. 59) %S - second (00 .. 59) %s - seconds and microseconds (equivalent to %S.%F) %i - millisecond (000 .. 999) %c - centisecond (0 .. 9) %F - fractional seconds/microseconds (000000 - 999999) %z - time zone differential in ISO 8601 format (Z or +NN.NN) %Z - time zone differential in RFC format (GMT or +NNNN) %% - percent sign If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL. data_df = dict ( date1 = [ getml . data . time . days ( 365 ), getml . data . time . days ( 366 ), getml . data . time . days ( 367 )], date2 = [ '1971-01-01' , '1971-01-02' , '1971-01-03' ], date3 = [ '1|1|71' , '1|2|71' , '1|3|71' ], ) df = getml . data . DataFrame . from_dict ( data_df , name = 'dates' ) df . set_role ([ 'date1' , 'date2' , 'date3' ], getml . data . roles . time_stamp , time_formats = [ '%Y-%m- %d ' , '%n| %e |%y' ]) df # | date1 | date2 | date3 | # | time stamp | time stamp | time stamp | # ------------------------------------------------------------------------------------------- # | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | # | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | # | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | Note getML time stamps are actually floats expressing the number of seconds since UNIX time (1970-01-01T00:00:00). Target The associated columns contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to allow_lagged_target in join() ). However, they are such an important part of the analysis that the population table is required to contain at least one of them (refer to data model tables ). The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be NULL . Numerical This role tells the getML engine to include the associated FloatColumn during the feature learning. It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch. Categorical This role tells the getML engine to include the associated StringColumn during feature learning. It should be used for all data with no inherent ordering, even if the categories are encoded as integers instead of strings. Text getML provides the role text to annotate free form text fields within relational data structures. getML deals with columns of role text through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the TextFieldSplitter preprocessor. For more information on getML's handling of text fields, refer to the Preprocessing section Unused_float Marks a FloatColumn as unused. The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. Unused_string Marks a StringColumn as unused. The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. Units By default, all columns of role categorical or numerical will only be compared to fixed values. ... WHERE numerical_column > some_value OR categorical_column == 'some string' ... If you want the feature learning algorithms to compare these columns with each other (like in the snippet below), you have to explicitly set a unit. ... WHERE numerical_column1 - numerical_column2 > some_value OR categorical_column1 != categorical_column2 ... Using set_unit() you can set the unit of a column to an arbitrary, non-empty string. If it matches the string of another column, both of them will be compared by the getML engine. Please note that a column can not have more than one unit. There are occasions where only a pairwise comparison of columns but not a comparison with fixed values is useful. To cope with this problem, you can set the comparison_only flag in set_unit() . Note Note that time stamps are used for comparison only by default. The feature learning algorithm will not compare them to a fixed date, because it is very unlikely that such a feature would perform well out-of-sample.","title":"Annotating data"},{"location":"user_guide/annotating_data/annotating_data/#annotating-data_1","text":"After you have imported your data into the getML engine, there is one more step to undertake before you can start learning features: You need to assign a role to each column. Why is that? First, the general structure of the individual data frames is needed to construct the relational data model . This is done by assigning the roles join key and time stamp . The former defines the columns that are used to join different data frames, the latter ensures that only rows in a reasonable time frame are taken into account (otherwise there might be data leaks). Second, you need to tell the feature learning algorithm how to interpret the individual columns for it to construct sophisticated features. That is why we need the roles numerical , categorical , and target . You can also assign units to each column in a Data Frame. This chapter contains detailed information on the individual roles and units .","title":"Annotating data"},{"location":"user_guide/annotating_data/annotating_data/#in-short","text":"When building the data model , you should keep the following things in mind: Every DataFrame in a data model needs to have at least one column ( columns ) with the role join key . The role time stamp has to be used to prevent data leaks (refer to data model time series for details). When learning features , please keep the following things in mind: Only columns with roles of categorical , numerical , and time stamp will be used by the feature learning algorithm for aggregations or conditions, unless you explicitly tell it to aggregate target columns as well (refer to allow_lagged_target in join() ). Columns are only compared with each other if they have the same unit . If you want to make sure that a column is only used for comparison, you can set comparison_only (refer to annotating units ). Time stamps are automatically set to comparison_only .","title":"In short"},{"location":"user_guide/annotating_data/annotating_data/#roles","text":"Roles determine if and how columns are handled during the construction of the data model and how they are interpreted by the feature learning algorithm . The following roles are available in getML: Role Class Included in FL algorithm categorical StringColumn yes numerical FloatColumn yes text StringColumn yes time_stamp FloatColumn yes join_key StringColumn no target FloatColumn not by default unused_float FloatColumn no unused_string StringColumn no When constructing a DataFrame via the class methods from_csv , from_pandas , from_db , and from_json , all columns will have either the role unused float or unused string . Unused columns will be ignored by the feature learning and machine learning (ML) algorithms. import pandas as pd data_df = dict ( animal = [ \"hawk\" , \"parrot\" , \"goose\" ], votes = [ 12341 , 5127 , 65311 ], weight = [ 12.14 , 12.6 , 11.92 ], animal_id = [ 123 , 512 , 671 ], date = [ \"2019-05-02\" , \"2019-02-28\" , \"2018-12-24\" ] ) pandas_df = pd . DataFrame ( data = data_df ) getml_df = getml . data . DataFrame . from_pandas ( pandas_df , name = 'animal elections' ) getml_df # Output: # | votes | weight | animal_id | animal | date | # | unused float | unused float | unused float | unused string | unused string | # ------------------------------------------------------------------------------ # | 12341 | 12.14 | 123 | hawk | 2019-05-02 | # | 5127 | 12.6 | 512 | parrot | 2019-02-28 | # | 65311 | 11.92 | 671 | goose | 2018-12-24 | getml_df . set_role ([ 'animal_id' ], getml . data . roles . join_key ) getml_df . set_role ([ 'animal' ], getml . data . roles . categorical ) getml_df . set_role ([ 'votes' , 'weight' ], getml . data . roles . numerical ) getml_df . set_role ([ 'date' ], getml . data . roles . time_stamp ) getml_df # Updated DataFrame structure will be displayed here. To make use of the imported data, you have to tell getML how you intend to use each column by assigning a role ( roles ). This is done by using the set_role method of the DataFrame . Each column must have exactly one role. If you wish to use a column in two different roles, you have to add it twice and assign each copy a different role. getml_df . set_role ([ 'animal_id' ], getml . data . roles . join_key ) getml_df . set_role ([ 'animal' ], getml . data . roles . categorical ) getml_df . set_role ([ 'votes' , 'weight' ], getml . data . roles . numerical ) getml_df . set_role ([ 'date' ], getml . data . roles . time_stamp ) getml_df # Output: # | date | animal_id | animal | votes | weight | # | time stamp | join key | categorical | numerical | numerical | # --------------------------------------------------------------------------------- # | 2019-05-02T00:00:00.000000Z | 123 | hawk | 12341 | 12.14 | # | 2019-02-28T00:00:00.000000Z | 512 | parrot | 5127 | 12.6 | # | 2018-12-24T00:00:00.000000Z | 671 | goose | 65311 | 11.92 | When assigning new roles to existing columns, you might notice that some of these calls are completed in an instance while others might take a considerable amount of time. What's happening here? A column's role also determines its type. When you set a new role, an implicit type conversion might take place.","title":"Roles"},{"location":"user_guide/annotating_data/annotating_data/#a-note-on-reproducibility-and-efficiency","text":"When building a stable pipeline you want to deploy in a productive environment, the flexible default behavior of the import interface might be more of an obstacle. For instance, CSV files are not type-safe. A column that was interpreted as a float column for one set of files might be interpreted as a string column for a different set of files. This obviously has implications for the stability of your pipeline. Therefore, it might be a good idea to hard-code column roles. In the getML Python API, you can bypass the default deduction of the role of each column by providing a dictionary mapping each column name to a role in the import interface. roles = { \"categorical\" : [ \"colname1\" , \"colname2\" ], \"target\" : [ \"colname3\" ]} df_expd = getml . data . DataFrame . from_csv ( fnames = [ \"file1.csv\" , \"file2.csv\" ], name = \"MY DATA FRAME\" , sep = ';' , quotechar = '\"' , roles = roles , ignore = True ) If the ignore argument is set to True , any columns missing in the dictionary won't be imported at all. If you feel that writing the roles by hand is too tedious, you can use dry : If you call the import interface while setting the dry argument to True , no data is read. Instead, the default roles of all columns will be returned as a dictionary. You can store, alter, and hard-code this dictionary into your stable pipeline. roles = getml . data . DataFrame . from_csv ( fnames = [ \"file1.csv\" , \"file2.csv\" ], name = \"MY DATA FRAME\" , sep = ';' , quotechar = '\"' , dry = True ) Even if your data source is type safe, setting roles is still a good idea because it is also more efficient. Using set_role() creates a deep copy of the original column and might perform an implicit type conversion. If you already know where you want your data to end up, it might be a good idea to set roles in advance.","title":"A note on reproducibility and efficiency"},{"location":"user_guide/annotating_data/annotating_data/#join-key","text":"Join keys are required to establish a relation between two DataFrame objects. Please refer to the data models for details. The content of this column is allowed to contain NULL values. NULL values won't be matched to anything, not even to NULL values in other join keys. columns of this role will not be aggregated by the feature learning algorithm or used for conditions.","title":"Join key"},{"location":"user_guide/annotating_data/annotating_data/#time-stamp","text":"This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins. In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default: WHERE time_stamp > some_fixed_date Instead, time stamps will always be compared to other time stamps: WHERE time_stamp1 - time_stamp2 > some_value This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample. When assigning the role time stamp to a column that is currently a StringColumn , you need to specify the format of this string. You can do so by using the time_formats argument of set_role() . You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are %w - abbreviated weekday (Mon, Tue, ...) %W - full weekday (Monday, Tuesday, ...) %b - abbreviated month (Jan, Feb, ...) %B - full month (January, February, ...) %d - zero-padded day of month (01 .. 31) %e - day of month (1 .. 31) %f - space-padded day of month ( 1 .. 31) %m - zero-padded month (01 .. 12) %n - month (1 .. 12) %o - space-padded month ( 1 .. 12) %y - year without century (70) %Y - year with century (1970) %H - hour (00 .. 23) %h - hour (00 .. 12) %a - am/pm %A - AM/PM %M - minute (00 .. 59) %S - second (00 .. 59) %s - seconds and microseconds (equivalent to %S.%F) %i - millisecond (000 .. 999) %c - centisecond (0 .. 9) %F - fractional seconds/microseconds (000000 - 999999) %z - time zone differential in ISO 8601 format (Z or +NN.NN) %Z - time zone differential in RFC format (GMT or +NNNN) %% - percent sign If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL. data_df = dict ( date1 = [ getml . data . time . days ( 365 ), getml . data . time . days ( 366 ), getml . data . time . days ( 367 )], date2 = [ '1971-01-01' , '1971-01-02' , '1971-01-03' ], date3 = [ '1|1|71' , '1|2|71' , '1|3|71' ], ) df = getml . data . DataFrame . from_dict ( data_df , name = 'dates' ) df . set_role ([ 'date1' , 'date2' , 'date3' ], getml . data . roles . time_stamp , time_formats = [ '%Y-%m- %d ' , '%n| %e |%y' ]) df # | date1 | date2 | date3 | # | time stamp | time stamp | time stamp | # ------------------------------------------------------------------------------------------- # | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | # | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | # | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | Note getML time stamps are actually floats expressing the number of seconds since UNIX time (1970-01-01T00:00:00).","title":"Time stamp"},{"location":"user_guide/annotating_data/annotating_data/#target","text":"The associated columns contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to allow_lagged_target in join() ). However, they are such an important part of the analysis that the population table is required to contain at least one of them (refer to data model tables ). The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be NULL .","title":"Target"},{"location":"user_guide/annotating_data/annotating_data/#numerical","text":"This role tells the getML engine to include the associated FloatColumn during the feature learning. It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch.","title":"Numerical"},{"location":"user_guide/annotating_data/annotating_data/#categorical","text":"This role tells the getML engine to include the associated StringColumn during feature learning. It should be used for all data with no inherent ordering, even if the categories are encoded as integers instead of strings.","title":"Categorical"},{"location":"user_guide/annotating_data/annotating_data/#text","text":"getML provides the role text to annotate free form text fields within relational data structures. getML deals with columns of role text through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the TextFieldSplitter preprocessor. For more information on getML's handling of text fields, refer to the Preprocessing section","title":"Text"},{"location":"user_guide/annotating_data/annotating_data/#unused_float","text":"Marks a FloatColumn as unused. The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors.","title":"Unused_float"},{"location":"user_guide/annotating_data/annotating_data/#unused_string","text":"Marks a StringColumn as unused. The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors.","title":"Unused_string"},{"location":"user_guide/annotating_data/annotating_data/#units","text":"By default, all columns of role categorical or numerical will only be compared to fixed values. ... WHERE numerical_column > some_value OR categorical_column == 'some string' ... If you want the feature learning algorithms to compare these columns with each other (like in the snippet below), you have to explicitly set a unit. ... WHERE numerical_column1 - numerical_column2 > some_value OR categorical_column1 != categorical_column2 ... Using set_unit() you can set the unit of a column to an arbitrary, non-empty string. If it matches the string of another column, both of them will be compared by the getML engine. Please note that a column can not have more than one unit. There are occasions where only a pairwise comparison of columns but not a comparison with fixed values is useful. To cope with this problem, you can set the comparison_only flag in set_unit() . Note Note that time stamps are used for comparison only by default. The feature learning algorithm will not compare them to a fixed date, because it is very unlikely that such a feature would perform well out-of-sample.","title":"Units"},{"location":"user_guide/data_model/data_model/","text":"Data model Defining the data model is a crucial step before training one of getML's Pipeline s. You typically deal with this step after having imported your data and specified the roles of each column. When working with getML, the raw data usually comes in the form of relational data. That means the information relevant for a prediction is spread over several tables. The data model is the definition of the relationships between all of them. Most relational machine learning problems can be represented in the form of a star schema, in which case you can use the StarSchema abstraction. If your data set is a time series, you can use the TimeSeries abstraction. Tables When defining the data model, we distinguish between a population table and one or more peripheral tables. In the context of this tutorial, we will use the term \"table\" as a catch-all for DataFrame s and View s. The population table The population table is the main table of the analysis. It defines the statistical population of your machine learning problem and contains the target variable(s), which we want to predict. Furthermore, the table usually also contains one or more columns with the role join_key . These are keys used to establish a relationship \u2013 also called joins \u2013 with one or more peripheral tables. The example below contains the population table of a customer churn analysis. The target variable is churn \u2013 whether a person stops using the services and products of a company. It also contains the information whether or not a given customer has churned after a certain reference date. The join key customer_id is used to establish relations with a peripheral table . Additionally, the date the customer joined the company is contained in the column date_joined , which we have assigned the role time_stamp . Peripheral tables Peripheral tables contain additional information relevant for the prediction of the target variable in the population table . Each of them is related to the latter (or another peripheral table, refer to the snowflake schema ) via a join_key . The images below represent two peripheral tables that could be used for our customer churn analysis. One table represents complaints a customer made with a certain agent, and the other represents the transactions the customer made using their account. Placeholders In getML, Placeholder s are used to construct the DataModel . They are abstract representations of DataFrame s or View s and the relationships among each other, but do not contain any data themselves. The idea behind the placeholder concept is that they allow constructing an abstract data model without any reference to an actual data set. This data model serves as input for the Pipeline . Later on, the feature_learning algorithms can be trained and applied on any data set that follows this data model. More information on how to construct placeholders and build a data model can be found in the API documentation for Placeholder and DataModel . Joins Joins are used to establish relationships between placeholders. To join two placeholders, the data frames used to derive them should both have at least one join_key . The joining itself is done using the join() method. All columns corresponding to time stamps have to be given the role join_key , and one of them in both the population and peripheral table is usually passed to the join() method. This approach ensures that no information from the future is considered during training by including only those rows of the peripheral table in the join operation for which the time stamp of the corresponding row in the population table is either the same or more recent. Data schemata After creating placeholders for all data frames in an analysis, we are ready to create the actual data schema. A data schema is a certain way of assembling population and peripheral tables. The star schema The StarSchema is the simplest way of establishing relations between the population and the peripheral tables, sufficient for the majority of data science projects. In the star schema, the population table is surrounded by any number of peripheral tables, all joined via a certain join key. However, no joins between peripheral tables are allowed. Because this is a very popular schema in many machine learning problems on relational data, getML contains a special class for these sorts of problems: StarSchema . The population table and two peripheral tables introduced in Tables can be arranged in a star schema like this: The snowflake schema In some cases, the star schema is not enough to represent the complexity of a data set. This is where the snowflake schema comes in: In a snowflake schema, peripheral tables can have peripheral tables of their own. Assume that in the customer churn analysis shown earlier, there is an additional table containing information about the calls a certain agent made in customer service. It can be joined to the COMPLAINTS table using the key agent_id . To model snowflake schemata, you need to use the DataModel and Container classes. Time series Time series can be handled by a self-join. In addition, some extra parameters and considerations are required when building features based on time stamps. Self-joining a single table If you are dealing with a classical (multivariate) time series and all your data is contained in a single table, all the concepts covered so far still apply. You just have to perform a so-called self-join by providing your table as both the population and peripheral table and join them. The process works as follows: Whenever a row in the population table - a single measurement - is taken, it will be combined with all the content of the peripheral table - the same time series - for which the time stamps are smaller than the one in the line we picked. You can also use the TimeSeries abstraction, which abstracts away the self-join. In this case, you do not have to think about self-joins too much. Horizon and Memory Crucial concepts of time series analysis are horizon and memory. In the context of getML's time series analysis, horizon is defined as a point forecast. That means the prediction of the target variable at the point as far in the future as defined by the horizon. Memory, on the other hand is the time duration into the past, that is considered when making a prediction. The memory is used to define the time window of data entering the model between the past and now . The horizon defines the point in the future that the predictions is being made for. time_stamps and on Two parameters in the time series signature determine how the self join is carried out. The time_stamps parameter defines what column is the underlying time dimension to which memory and horizon are applied to. The chosen column must also be of role time_stamp . on simply provides an extra handle to control, what subset of the data is part of any given time series. For example if you have a time series of sales data, you might want to only consider the sales data of a certain product category. In this case you would specify the on parameter to be the column containing the product category. Tip If you assign a column to the on parameter, then this column will not enter the model as a predictor. If you have reason to believe that this column is relevant to the model (i.e. the actual product category), duplicate that column in advance and assign the duplicate to the on parameter. (see class method add() ) Lagged Target and horizon Another useful parameter in time series analysis is lagged_target . This boolean controls whether the target variable is used as a predictor. Including the target variable as a predictor can be useful in time series analysis, when at time of prediction, the target variable up until and including now is known. In turn, this means lagged target variables are only permissible if the target variable is predicted for some when in the future. That is, the horizon must be assigned a positive value. Features based on time stamps The getML engine is able to automatically generate features based on aggregations over time windows. Both the length of the time window and the aggregation itself will be determined by the feature learning algorithm. The only requirement is to provide the temporal resolution your time series is sampled with in the delta_t parameter in any feature learning algorithm.","title":"Data model"},{"location":"user_guide/data_model/data_model/#data-model_1","text":"Defining the data model is a crucial step before training one of getML's Pipeline s. You typically deal with this step after having imported your data and specified the roles of each column. When working with getML, the raw data usually comes in the form of relational data. That means the information relevant for a prediction is spread over several tables. The data model is the definition of the relationships between all of them. Most relational machine learning problems can be represented in the form of a star schema, in which case you can use the StarSchema abstraction. If your data set is a time series, you can use the TimeSeries abstraction.","title":"Data model"},{"location":"user_guide/data_model/data_model/#tables","text":"When defining the data model, we distinguish between a population table and one or more peripheral tables. In the context of this tutorial, we will use the term \"table\" as a catch-all for DataFrame s and View s.","title":"Tables"},{"location":"user_guide/data_model/data_model/#the-population-table","text":"The population table is the main table of the analysis. It defines the statistical population of your machine learning problem and contains the target variable(s), which we want to predict. Furthermore, the table usually also contains one or more columns with the role join_key . These are keys used to establish a relationship \u2013 also called joins \u2013 with one or more peripheral tables. The example below contains the population table of a customer churn analysis. The target variable is churn \u2013 whether a person stops using the services and products of a company. It also contains the information whether or not a given customer has churned after a certain reference date. The join key customer_id is used to establish relations with a peripheral table . Additionally, the date the customer joined the company is contained in the column date_joined , which we have assigned the role time_stamp .","title":"The population table"},{"location":"user_guide/data_model/data_model/#peripheral-tables","text":"Peripheral tables contain additional information relevant for the prediction of the target variable in the population table . Each of them is related to the latter (or another peripheral table, refer to the snowflake schema ) via a join_key . The images below represent two peripheral tables that could be used for our customer churn analysis. One table represents complaints a customer made with a certain agent, and the other represents the transactions the customer made using their account.","title":"Peripheral tables"},{"location":"user_guide/data_model/data_model/#placeholders","text":"In getML, Placeholder s are used to construct the DataModel . They are abstract representations of DataFrame s or View s and the relationships among each other, but do not contain any data themselves. The idea behind the placeholder concept is that they allow constructing an abstract data model without any reference to an actual data set. This data model serves as input for the Pipeline . Later on, the feature_learning algorithms can be trained and applied on any data set that follows this data model. More information on how to construct placeholders and build a data model can be found in the API documentation for Placeholder and DataModel .","title":"Placeholders"},{"location":"user_guide/data_model/data_model/#joins","text":"Joins are used to establish relationships between placeholders. To join two placeholders, the data frames used to derive them should both have at least one join_key . The joining itself is done using the join() method. All columns corresponding to time stamps have to be given the role join_key , and one of them in both the population and peripheral table is usually passed to the join() method. This approach ensures that no information from the future is considered during training by including only those rows of the peripheral table in the join operation for which the time stamp of the corresponding row in the population table is either the same or more recent.","title":"Joins"},{"location":"user_guide/data_model/data_model/#data-schemata","text":"After creating placeholders for all data frames in an analysis, we are ready to create the actual data schema. A data schema is a certain way of assembling population and peripheral tables.","title":"Data schemata"},{"location":"user_guide/data_model/data_model/#the-star-schema","text":"The StarSchema is the simplest way of establishing relations between the population and the peripheral tables, sufficient for the majority of data science projects. In the star schema, the population table is surrounded by any number of peripheral tables, all joined via a certain join key. However, no joins between peripheral tables are allowed. Because this is a very popular schema in many machine learning problems on relational data, getML contains a special class for these sorts of problems: StarSchema . The population table and two peripheral tables introduced in Tables can be arranged in a star schema like this:","title":"The star schema"},{"location":"user_guide/data_model/data_model/#the-snowflake-schema","text":"In some cases, the star schema is not enough to represent the complexity of a data set. This is where the snowflake schema comes in: In a snowflake schema, peripheral tables can have peripheral tables of their own. Assume that in the customer churn analysis shown earlier, there is an additional table containing information about the calls a certain agent made in customer service. It can be joined to the COMPLAINTS table using the key agent_id . To model snowflake schemata, you need to use the DataModel and Container classes.","title":"The snowflake schema"},{"location":"user_guide/data_model/data_model/#time-series","text":"Time series can be handled by a self-join. In addition, some extra parameters and considerations are required when building features based on time stamps.","title":"Time series"},{"location":"user_guide/data_model/data_model/#self-joining-a-single-table","text":"If you are dealing with a classical (multivariate) time series and all your data is contained in a single table, all the concepts covered so far still apply. You just have to perform a so-called self-join by providing your table as both the population and peripheral table and join them. The process works as follows: Whenever a row in the population table - a single measurement - is taken, it will be combined with all the content of the peripheral table - the same time series - for which the time stamps are smaller than the one in the line we picked. You can also use the TimeSeries abstraction, which abstracts away the self-join. In this case, you do not have to think about self-joins too much.","title":"Self-joining a single table"},{"location":"user_guide/data_model/data_model/#horizon-and-memory","text":"Crucial concepts of time series analysis are horizon and memory. In the context of getML's time series analysis, horizon is defined as a point forecast. That means the prediction of the target variable at the point as far in the future as defined by the horizon. Memory, on the other hand is the time duration into the past, that is considered when making a prediction. The memory is used to define the time window of data entering the model between the past and now . The horizon defines the point in the future that the predictions is being made for.","title":"Horizon and Memory"},{"location":"user_guide/data_model/data_model/#time_stamps-and-on","text":"Two parameters in the time series signature determine how the self join is carried out. The time_stamps parameter defines what column is the underlying time dimension to which memory and horizon are applied to. The chosen column must also be of role time_stamp . on simply provides an extra handle to control, what subset of the data is part of any given time series. For example if you have a time series of sales data, you might want to only consider the sales data of a certain product category. In this case you would specify the on parameter to be the column containing the product category. Tip If you assign a column to the on parameter, then this column will not enter the model as a predictor. If you have reason to believe that this column is relevant to the model (i.e. the actual product category), duplicate that column in advance and assign the duplicate to the on parameter. (see class method add() )","title":"time_stamps and on"},{"location":"user_guide/data_model/data_model/#lagged-target-and-horizon","text":"Another useful parameter in time series analysis is lagged_target . This boolean controls whether the target variable is used as a predictor. Including the target variable as a predictor can be useful in time series analysis, when at time of prediction, the target variable up until and including now is known. In turn, this means lagged target variables are only permissible if the target variable is predicted for some when in the future. That is, the horizon must be assigned a positive value.","title":"Lagged Target and horizon"},{"location":"user_guide/data_model/data_model/#features-based-on-time-stamps","text":"The getML engine is able to automatically generate features based on aggregations over time windows. Both the length of the time window and the aggregation itself will be determined by the feature learning algorithm. The only requirement is to provide the temporal resolution your time series is sampled with in the delta_t parameter in any feature learning algorithm.","title":"Features based on time stamps"},{"location":"user_guide/deployment/deployment/","text":"Deployment The results of the feature learning and the prediction can be retrieved in different ways and formats. Transpiling pipelines Using SQLCode.save() , you can transpile Pipelines to SQL code, which can be used without any proprietary components. Returning Python objects Using the Pipeline.transform and Pipeline.predict methods of a trained Pipeline , you can access both the features and the predictions as numpy.ndarray via the Python API. Writing into a database You can also write both features and prediction results back into a new table of the connected database by providing the table_name argument in the Pipeline.transform and Pipeline.predict methods. Please refer to the unified import interface for information on how to connect to a database. Responding to a HTTP POST request The getML suite contains HTTP endpoints to post new data via a JSON string and retrieve either the resulting features or the predictions. Batch prediction Batch prediction pipelines are the most common way of productionizing machine learning pipelines on relational data. These pipelines are usually set to run regularly (once a month, once a week, once a day...) to create a batch of predictions on the newest data. They are typically inserted into a Docker container and scheduled using tools like Jenkins and/or Airflow . If you are looking for a pure Python, 100% open-source way to productionize getML's Pipeline s, you can transpile all the features into sqlite3 code. sqlite3 is part of the Python standard library, and you can use getML's 100% open source and pure Python sqlite3 which provides some useful extra functionality not included in Python's standard library. HTTP Endpoints As soon as you have trained a pipeline, whitelisted it for external access using its deploy method, and configured the getML monitor for remote access , you can transform new data into features or make predictions on them using these endpoints: Transform endpoint: http://localhost:1709/transform/PIPELINE_NAME Predict endpoint: http://localhost:1709/predict/PIPELINE_NAME To each of them, you must send a POST request containing the new data as a JSON string in a specific request format . Note For testing and developing purposes, you can also use the HTTP port of the monitor to query the endpoints. Note that this is only possible within the same host. The corresponding syntax is http://localhost:1709/predict/PIPELINE_NAME Request Format In all POST requests to the endpoints, a JSON string with the following syntax has to be provided in the body: { \"peripheral\" : [{ \"column_1\" : [], \"column_2\" : [] },{ \"column_1\" : [], \"column_2\" : [] }], \"population\" : { \"column_1\" : [], \"column_2\" : [] } } It has to have exactly two keys in the top level called population and peripheral . These will contain the new input data. The order of the columns is irrelevant. They will be matched according to their names. However, the order of the individual peripheral tables is very important and has to exactly match the order the corresponding Placeholder have been provided in the constructor of pipeline . In our example above, we could post a JSON string like this: { \"peripheral\" : [{ \"column_01\" : [ 2.4 , 3.0 , 1.2 , 1.4 , 2.2 ], \"join_key\" : [ \"0\" , \"0\" , \"0\" , \"0\" , \"0\" ], \"time_stamp\" : [ 0.1 , 0.2 , 0.3 , 0.4 , 0.8 ] }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } } Time stamp formats in requests You might have noticed that the time stamps in the example above have been passed as numerical values and not as their string representations shown in the beginning. Both ways are supported by the getML monitor. But if you choose to pass the string representation, you also have to specify the particular format in order for the getML engine to interpret your data properly. { \"peripheral\" : [{ \"column_01\" : [ 2.4 , 3.0 , 1.2 , 1.4 , 2.2 ], \"join_key\" : [ \"0\" , \"0\" , \"0\" , \"0\" , \"0\" ], \"time_stamp\" : [ \"2010-01-01 00:15:00\" , \"2010-01-01 08:00:00\" , \"2010-01-01 09:30:00\" , \"2010-01-01 13:00:00\" , \"2010-01-01 23:35:00\" ] }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ \"2010-01-01 12:30:00\" , \"2010-01-01 23:30:00\" ] }, \"timeFormats\" : [ \"%Y-%m-%d %H:%M:%S\" ] } All special characters available for specifying the format of the time stamps are listed and described in e.g. read_csv() . Using an existing DataFrame You can also use a DataFrame that already exists on the getML engine: { \"peripheral\" : [{ \"df\" : \"peripheral_table\" }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } } Using data from a database You can also read the data from the connected database (see unified import interface ) by passing an arbitrary query to the query key: { \"peripheral\" : [{ \"query\" : \"SELECT * FROM PERIPHERAL WHERE join_key = '0';\" }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } } Transform Endpoint The transform endpoint returns the generated features. http://localhost:1709/transform/PIPELINE_NAME Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool curl , which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility. curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\ http://localhost:1709/transform/PIPELINE_NAME Predict Endpoint When using getML as an end-to-end data science pipeline, you can use the predict endpoint to upload new, unseen data and receive the resulting predictions as a response via HTTP. http://localhost:1709/predict/PIPELINE_NAME Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool curl , which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility. curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\ http://localhost:1709/predict/PIPELINE_NAME","title":"Deployment"},{"location":"user_guide/deployment/deployment/#deployment","text":"The results of the feature learning and the prediction can be retrieved in different ways and formats.","title":"Deployment"},{"location":"user_guide/deployment/deployment/#transpiling-pipelines","text":"Using SQLCode.save() , you can transpile Pipelines to SQL code, which can be used without any proprietary components.","title":"Transpiling pipelines"},{"location":"user_guide/deployment/deployment/#returning-python-objects","text":"Using the Pipeline.transform and Pipeline.predict methods of a trained Pipeline , you can access both the features and the predictions as numpy.ndarray via the Python API.","title":"Returning Python objects"},{"location":"user_guide/deployment/deployment/#writing-into-a-database","text":"You can also write both features and prediction results back into a new table of the connected database by providing the table_name argument in the Pipeline.transform and Pipeline.predict methods. Please refer to the unified import interface for information on how to connect to a database.","title":"Writing into a database"},{"location":"user_guide/deployment/deployment/#responding-to-a-http-post-request","text":"The getML suite contains HTTP endpoints to post new data via a JSON string and retrieve either the resulting features or the predictions.","title":"Responding to a HTTP POST request"},{"location":"user_guide/deployment/deployment/#batch-prediction","text":"Batch prediction pipelines are the most common way of productionizing machine learning pipelines on relational data. These pipelines are usually set to run regularly (once a month, once a week, once a day...) to create a batch of predictions on the newest data. They are typically inserted into a Docker container and scheduled using tools like Jenkins and/or Airflow . If you are looking for a pure Python, 100% open-source way to productionize getML's Pipeline s, you can transpile all the features into sqlite3 code. sqlite3 is part of the Python standard library, and you can use getML's 100% open source and pure Python sqlite3 which provides some useful extra functionality not included in Python's standard library.","title":"Batch prediction"},{"location":"user_guide/deployment/deployment/#http-endpoints","text":"As soon as you have trained a pipeline, whitelisted it for external access using its deploy method, and configured the getML monitor for remote access , you can transform new data into features or make predictions on them using these endpoints: Transform endpoint: http://localhost:1709/transform/PIPELINE_NAME Predict endpoint: http://localhost:1709/predict/PIPELINE_NAME To each of them, you must send a POST request containing the new data as a JSON string in a specific request format . Note For testing and developing purposes, you can also use the HTTP port of the monitor to query the endpoints. Note that this is only possible within the same host. The corresponding syntax is http://localhost:1709/predict/PIPELINE_NAME","title":"HTTP Endpoints"},{"location":"user_guide/deployment/deployment/#request-format","text":"In all POST requests to the endpoints, a JSON string with the following syntax has to be provided in the body: { \"peripheral\" : [{ \"column_1\" : [], \"column_2\" : [] },{ \"column_1\" : [], \"column_2\" : [] }], \"population\" : { \"column_1\" : [], \"column_2\" : [] } } It has to have exactly two keys in the top level called population and peripheral . These will contain the new input data. The order of the columns is irrelevant. They will be matched according to their names. However, the order of the individual peripheral tables is very important and has to exactly match the order the corresponding Placeholder have been provided in the constructor of pipeline . In our example above, we could post a JSON string like this: { \"peripheral\" : [{ \"column_01\" : [ 2.4 , 3.0 , 1.2 , 1.4 , 2.2 ], \"join_key\" : [ \"0\" , \"0\" , \"0\" , \"0\" , \"0\" ], \"time_stamp\" : [ 0.1 , 0.2 , 0.3 , 0.4 , 0.8 ] }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } }","title":"Request Format"},{"location":"user_guide/deployment/deployment/#time-stamp-formats-in-requests","text":"You might have noticed that the time stamps in the example above have been passed as numerical values and not as their string representations shown in the beginning. Both ways are supported by the getML monitor. But if you choose to pass the string representation, you also have to specify the particular format in order for the getML engine to interpret your data properly. { \"peripheral\" : [{ \"column_01\" : [ 2.4 , 3.0 , 1.2 , 1.4 , 2.2 ], \"join_key\" : [ \"0\" , \"0\" , \"0\" , \"0\" , \"0\" ], \"time_stamp\" : [ \"2010-01-01 00:15:00\" , \"2010-01-01 08:00:00\" , \"2010-01-01 09:30:00\" , \"2010-01-01 13:00:00\" , \"2010-01-01 23:35:00\" ] }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ \"2010-01-01 12:30:00\" , \"2010-01-01 23:30:00\" ] }, \"timeFormats\" : [ \"%Y-%m-%d %H:%M:%S\" ] } All special characters available for specifying the format of the time stamps are listed and described in e.g. read_csv() .","title":"Time stamp formats in requests"},{"location":"user_guide/deployment/deployment/#using-an-existing-dataframe","text":"You can also use a DataFrame that already exists on the getML engine: { \"peripheral\" : [{ \"df\" : \"peripheral_table\" }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } }","title":"Using an existing DataFrame"},{"location":"user_guide/deployment/deployment/#using-data-from-a-database","text":"You can also read the data from the connected database (see unified import interface ) by passing an arbitrary query to the query key: { \"peripheral\" : [{ \"query\" : \"SELECT * FROM PERIPHERAL WHERE join_key = '0';\" }], \"population\" : { \"column_01\" : [ 2.2 , 3.2 ], \"join_key\" : [ \"0\" , \"0\" ], \"time_stamp\" : [ 0.65 , 0.81 ] } }","title":"Using data from a database"},{"location":"user_guide/deployment/deployment/#transform-endpoint","text":"The transform endpoint returns the generated features. http://localhost:1709/transform/PIPELINE_NAME Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool curl , which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility. curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\ http://localhost:1709/transform/PIPELINE_NAME","title":"Transform Endpoint"},{"location":"user_guide/deployment/deployment/#predict-endpoint","text":"When using getML as an end-to-end data science pipeline, you can use the predict endpoint to upload new, unseen data and receive the resulting predictions as a response via HTTP. http://localhost:1709/predict/PIPELINE_NAME Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool curl , which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility. curl --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\ http://localhost:1709/predict/PIPELINE_NAME","title":"Predict Endpoint"},{"location":"user_guide/feature_engineering/feature_engineering/","text":"Feature engineering The deep learning revolution has enabled automated feature engineering for images and sound data. Yet, for relational data and classical time series analysis, feature engineering is still done by hand or using very simple brute force methods. Our mission is to change that. The automation of feature engineering on relational data and time series is at the heart of the getML software suite. There are other libraries that implement feature engineering tools on top of frameworks like data.tables in R, pandas in Python, or Apache Spark . In essence, they all use a brute force approach: Generate a large number of features, then use some sort of feature selection routine to pick a small subselection of them. getML has chosen another path: Our highly efficient feature learning algorithms produce features that are far more advanced than what manual feature engineering could achieve or what could be accomplished using simple brute force approaches. Definition Feature engineering is the process of constructing variables, so-called features, from a dataset. These features are used as the input for machine learning algorithms. In most real-world datasets, the raw data is spread over multiple tables and the task is to bring these tables together and construct features based on their relationships. These features are stored in a flat feature table. In other words, feature engineering is the operation of merging and aggregating a relational data model into a flat (feature) table. From an academic point of view, most machine learning algorithms used nowadays can be classified as propositional learners. The process of creating flat attribute-value representations from relational data through simple rules or aggregation functions therefore is called propositionalization. Usually, feature engineering is done manually, by using brute force approaches or domain knowledge. This process is sometimes also referred to as data wrangling . In any case it is a tedious, time-consuming, and error-prone process. Manual feature engineering is often done by writing scripts in languages like Python, R, or SQL. Note Unfortunately, the term feature engineering is ambiguous. More often than not, feature engineering is meant to describe numerical transformations or encoding techniques on a single table. The definition used above assumes that the raw data comes in relational form, which is true for almost all real-world data sets. Feature learning vs. propositionalization We follow academia and classify techniques that use simple, merely unconditional transformations (like aggregations) to construct flat (attribute-value) representations as propositionalization approaches , while we classify algorithms which directly learn from relational data structures as feature learning. Here, we pick up a term coined in the deep learning context, where complex relationships are equally learned from raw input data. getML provides a framework capable of automatically extracting useful and meaningful features from a relational data model by finding the best merge and aggregate operations. In fact, the relationships between the target and the original data is learned through one of getML's feature learning algorithms . Design principles The general procedure for feature learning on relational data and time series using getML looks like this: The only required input is a relational data schema . In particular, there needs to be some sort of target variable(s) , which shall be predicted. For time series, the schema would typically be a self-join . In addition to this general information on the data schema, the intended usage of the variables has to be provided by setting the roles of the corresponding columns. How to setup a data scheme is described in data model . Features are often of the type (illustrated in pseudo SQL-like syntax): COUNT the number of ` transactions ` within the last X ` days ` where \\(X\\) is some sort of fixed numerical value. getML's algorithms do identify appropriate values for \\(X\\) automatically and there is no need for you to provide them by hand. Features can also take the form of: COUNT the number of ` transactions ` for which the ` transaction type ` is \u2018 type_1 \u2019 OR \u2018 type_2 \u2019 OR \u2019 type_3 \u2019 OR \u2026 getML's algorithms also find appropriate conditions based on categorical data without any input from the user. The feature learning algorithms can handle combinations of conditions too. So, features of the form: SOME_AGGREGATION ( over some column ) WHERE ( condition_1 AND condition_2 ) OR ( condition_3 AND condition_4 ) OR condition_5 will be engineered automatically as well. Again, no input from the user is required. To increase transparency relating to the created features, they can be expressed in SQL code. Even though automatically generated features will always be less intuitive than hand-crafted ones and could be quite complex, we want the user to get an understanding of what is going on. Algorithms getML contains four powerful feature learning algorithms: FastProp , Multirel , Relboost and RelMT . FastProp FastProp is getML's take on propositionalization. It is a fast and efficient implementation utilizing aggregations-based operations, which transform a relational data structure to a flat table. FastProp allows for the really fast generation of a substantial number of features based on simple (unconditional) aggregations. A typical FastProp feature looks like this: CREATE TABLE FEATURE_1 AS SELECT MAX ( t2 . column ) AS feature_1 , t1 . rowid AS \"rownum\" FROM \"population\" t1 LEFT JOIN \"peripheral\" t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp ORDER BY t2 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; You may notice that such a feature looks pretty similar to the Multirel feature below. And indeed, FastProp shares some of its aggregations with Multirel. FastProp features, however, are usually much simpler because they lack the complex conditions learned by getML's other algorithms (the WHERE statement in the SQL representation). FastProp is an excellent choice in an exploration phase of a data science project and delivers decent results out of the box in many cases. It is recommended that you combine FastProp with mappings . Multirel Simply speaking, Multirel is a more efficient variation of Multi-relational Decision Tree Learning (MRDTL). The core idea is to minimize redundancies in the original algorithm by incremental updates. We then combined our improved version of MRDTL with ensemble learning methods. MRDTL is a strain of academic literature that was particularly popular in the early 2000s. It is based on a greedy, tree-like approach: Define some sort of objective function that evaluates the quality of your feature as it relates to the target variable(s). Pick an aggregation and some column to be aggregated. Try out different conditions. Keep the one that generates the greatest improvement of your objective. Repeat until no improvement can be found or some sort of stopping criterion is reached. The reason this approach has never really taken off outside of academia is that an efficient implementation is far from trivial. Most papers on MRDTL implement the algorithm on top of an existing relational database system, like MySQL. The main problem with trying to implement something like this on top of an existing database is that it requires many redundant operations. Consider a feature like: COUNT the number of ` transactions ` in the last X ` days ` As we iterate through different values for the threshold \\(X\\) , we are forced to repeat the same operations on the same data over and over again. Tasks like this bring traditional database engines to their knees. The core idea of getML's Multirel algorithm is to minimize redundancies through incremental updates . To allow for incremental updates and maximal efficiency, we developed a database engine from scratch in C++. When we evaluate a feature like: COUNT the number of ` transactions ` in the last 90 ` days ` and COUNT the number of ` transactions ` in the last 91 ` days ` very little changes in between. Multirel only recalculates what has changed and keeps everything else untouched. Therefore, it needs two ingredients that can be incrementally updated: An objective function and the actual aggregation(s). Our first ingredient is an objective function that must be suited for incremental updates. When we move from 90 to 91 days, presumably only very few lines in the population table actually change. We do not need to recalculate the entire table. In practice, most commonly used objective functions are fine and this is not much of a limitation. However, there are some, like rank correlation, that cannot be used. The second ingredient, the aggregations, must allow for incremental updates too. This part is a bit harder, so let us elaborate: Let\u2019s say we have a match between the population table that contains our targets and another table (or a self-join). This match happens to be between the two thresholds 90 and 91 days. As we move from 90 to 91 days, we have to update our aggregation for that match. For maximum efficiency, this needs also to be done incrementally. That means we do not want to recalculate the entire aggregation for all matches that it aggregates - instead just for the one match in between the two thresholds. We want to also support the AND and OR combinations of conditions. Therefore, it is possible that a match was not included in the aggregation before, but becomes part of it as we move the threshold. It is also possible that the match was included in the aggregation, but now it isn\u2019t anymore. For an aggregation like Count , incremental updates are straight-forward. If the match was not included, but now it is, then increment by 1. If was included, but it isn\u2019t anymore, then decrement by 1. Things are more tricky for aggregations like Max , Median , or CountDistinct . For instance, whereas incrementing Max is easy, decrementing it is hard. If the match used to be included and is in fact the maximum value, we now have to find the next biggest match. And we have to find it quickly - ideally iterating through a set of thresholds should take linear time in the number of matches. To make it even more complicated, some cross-joins might result in a lot of matches, so any data structures that have non-trivial memory overhead are a no-go. Everything so far has shed light on how we train one feature. But in practice, we want more than one. So, how do we do that? Since we are using a tree-based algorithm anyway, we are able to harness the power of ensemble learning algorithms that have been shown to work very well with non-relational decision trees, namely bagging and gradient boosting. With bagging, we just sample randomly from our population table. We train a feature on that sample and then pick a different random sample to train the next feature. With gradient boosting, we calculate the pseudo-residuals of our previously trained features. We then train features that predict these pseudo-residuals. This procedure guarantees that new features are targeted and compensate the weaknesses of older ones. Transpiled to SQL, a typical feature generated by Multirel looks like this: CREATE TABLE FEATURE_1 AS SELECT COUNT ( * ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE ( ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 ) ) AND t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; Further information can be found in the API documentation for Multirel . Relboost Relboost is a generalization of the gradient boosting algorithm. More specifically, it generalizes the xgboost implementation to relational learning. The main difference between Relboost and Multirel is that Multirel aggregates columns, whereas Relboost aggregates learnable weights . Relboost addresses a problem with Multirel that is related to computational complexity theory: In Multirel, every column can be aggregated and/or used for generating a condition. That means that the number of possible features is \\(\\mathcal{O}(n^2)\\) in the number of columns in the original tables. As a result, having twice as many columns will lead to a search space that is four times as large (in reality, it is a bit more complicated than that, but the basic point is true). Any computer scientist or applied mathematician will tell you that \\(\\mathcal{O}(n^2)\\) is a problem. If you have tables with many columns, it might turn out to be a problem. Of course, this issue is not specific to Multirel: It is a very fundamental problem that you would also have, if you were to write your features by hand or use brute force. Relboost offers a way out of this dilemma: Because Relboost aggregates learnable weights and columns will only be used for conditions, but not for aggregation. So, now the search space is \\(\\mathcal{O}(n)\\) in the number of columns in the original tables - much better. This might seem very theoretical, but it has considerable implications: From our experience with real-world data in various projects, we know that Relboost usually outperforms Multirel in terms of predictive accuracy and training time. However, these advantages come at a price: First, the features generated by Relboost are less intuitive. They are further away from what you might write by hand, even though they can still be expressed as SQL code. Second, it is more difficult to apply Relboost to multiple targets , because Relboost has to learn separate rules and weights for each target. Expressed as SQL code, a typical feature generated by Relboost looks like this: CREATE TABLE FEATURE_1 AS SELECT SUM ( CASE WHEN ( t1 . time_stamp - t2 . time_stamp > 0 . 499624 ) THEN 0 . 0 WHEN ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 OR t1 . time_stamp IS NULL OR t2 . time_stamp IS NULL ) THEN 1 . 0 ELSE NULL END ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; Further information can be found in the API documentation for Relboost . RelMT RelMT is a generalization of linear model trees to relational data. Linear model trees are decision trees with a linear model at each leaf, resulting in a hybrid model that combines the strengths of linear models (like interpretability or the ability to capture linear relationships) with those of tree-based algorithms (like good performance or the ability to capture nonlinear relationships). RelMT features are particularly well-suited for time-series applications because time series often carry autoregressive structures, which can be approximated well by linear models. Think that this month's revenue can usually be modeled particularly well as a (linear) function of last month's revenue and so on. Purely tree-based models often struggle to learn such relationships because they have to fit a piecewise-constant model by predicting the average of all observations associated with each leaf. Thus, it can require a vast amount of splits to approximate a linear relationship. Here is a typical RelMT feature: CREATE TABLE FEATURE_1 AS SELECT SUM ( CASE WHEN ( t1 . time_stamp - t2 . time_stamp > 0 . 499624 ) THEN COALESCE ( t1 . time_stamp - julianday ( '1970-01-01' ) - 17202 . 004 , 0 . 0 ) * - 122 . 121 + COALESCE ( t2 . column - 3301 . 156 , 0 . 0 ) * - 0 . 003 WHEN ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 OR t1 . time_stamp IS NULL OR t2 . time_stamp IS NULL ) THEN COALESCE ( t1 . time_stamp - julianday ( '1970-01-01' ) - 17202 . 004 , 0 . 0 ) * 3 . 654 + COALESCE ( t2 . column - 3301 . 156 , 0 . 0 ) * - 1 . 824 + - 8 . 720 ELSE NULL END ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; RelMT features share some characteristics with Relboost features: Compare the example feature to the Relboost feature above. Both algorithms generate splits based on a combination of conditions (the WHEN part of the CASE WHEN statement above). But while Relboost learns weights for its leaves (the THEN part of the CASE WHEN statement), RelMT learns a linear model, allowing for linear combinations between columns from the population table and columns of a certain peripheral table.","title":"Feature engineering"},{"location":"user_guide/feature_engineering/feature_engineering/#feature-engineering_1","text":"The deep learning revolution has enabled automated feature engineering for images and sound data. Yet, for relational data and classical time series analysis, feature engineering is still done by hand or using very simple brute force methods. Our mission is to change that. The automation of feature engineering on relational data and time series is at the heart of the getML software suite. There are other libraries that implement feature engineering tools on top of frameworks like data.tables in R, pandas in Python, or Apache Spark . In essence, they all use a brute force approach: Generate a large number of features, then use some sort of feature selection routine to pick a small subselection of them. getML has chosen another path: Our highly efficient feature learning algorithms produce features that are far more advanced than what manual feature engineering could achieve or what could be accomplished using simple brute force approaches.","title":"Feature engineering"},{"location":"user_guide/feature_engineering/feature_engineering/#definition","text":"Feature engineering is the process of constructing variables, so-called features, from a dataset. These features are used as the input for machine learning algorithms. In most real-world datasets, the raw data is spread over multiple tables and the task is to bring these tables together and construct features based on their relationships. These features are stored in a flat feature table. In other words, feature engineering is the operation of merging and aggregating a relational data model into a flat (feature) table. From an academic point of view, most machine learning algorithms used nowadays can be classified as propositional learners. The process of creating flat attribute-value representations from relational data through simple rules or aggregation functions therefore is called propositionalization. Usually, feature engineering is done manually, by using brute force approaches or domain knowledge. This process is sometimes also referred to as data wrangling . In any case it is a tedious, time-consuming, and error-prone process. Manual feature engineering is often done by writing scripts in languages like Python, R, or SQL. Note Unfortunately, the term feature engineering is ambiguous. More often than not, feature engineering is meant to describe numerical transformations or encoding techniques on a single table. The definition used above assumes that the raw data comes in relational form, which is true for almost all real-world data sets.","title":"Definition"},{"location":"user_guide/feature_engineering/feature_engineering/#feature-learning-vs-propositionalization","text":"We follow academia and classify techniques that use simple, merely unconditional transformations (like aggregations) to construct flat (attribute-value) representations as propositionalization approaches , while we classify algorithms which directly learn from relational data structures as feature learning. Here, we pick up a term coined in the deep learning context, where complex relationships are equally learned from raw input data. getML provides a framework capable of automatically extracting useful and meaningful features from a relational data model by finding the best merge and aggregate operations. In fact, the relationships between the target and the original data is learned through one of getML's feature learning algorithms .","title":"Feature learning vs. propositionalization"},{"location":"user_guide/feature_engineering/feature_engineering/#design-principles","text":"The general procedure for feature learning on relational data and time series using getML looks like this: The only required input is a relational data schema . In particular, there needs to be some sort of target variable(s) , which shall be predicted. For time series, the schema would typically be a self-join . In addition to this general information on the data schema, the intended usage of the variables has to be provided by setting the roles of the corresponding columns. How to setup a data scheme is described in data model . Features are often of the type (illustrated in pseudo SQL-like syntax): COUNT the number of ` transactions ` within the last X ` days ` where \\(X\\) is some sort of fixed numerical value. getML's algorithms do identify appropriate values for \\(X\\) automatically and there is no need for you to provide them by hand. Features can also take the form of: COUNT the number of ` transactions ` for which the ` transaction type ` is \u2018 type_1 \u2019 OR \u2018 type_2 \u2019 OR \u2019 type_3 \u2019 OR \u2026 getML's algorithms also find appropriate conditions based on categorical data without any input from the user. The feature learning algorithms can handle combinations of conditions too. So, features of the form: SOME_AGGREGATION ( over some column ) WHERE ( condition_1 AND condition_2 ) OR ( condition_3 AND condition_4 ) OR condition_5 will be engineered automatically as well. Again, no input from the user is required. To increase transparency relating to the created features, they can be expressed in SQL code. Even though automatically generated features will always be less intuitive than hand-crafted ones and could be quite complex, we want the user to get an understanding of what is going on.","title":"Design principles"},{"location":"user_guide/feature_engineering/feature_engineering/#algorithms","text":"getML contains four powerful feature learning algorithms: FastProp , Multirel , Relboost and RelMT .","title":"Algorithms"},{"location":"user_guide/feature_engineering/feature_engineering/#fastprop","text":"FastProp is getML's take on propositionalization. It is a fast and efficient implementation utilizing aggregations-based operations, which transform a relational data structure to a flat table. FastProp allows for the really fast generation of a substantial number of features based on simple (unconditional) aggregations. A typical FastProp feature looks like this: CREATE TABLE FEATURE_1 AS SELECT MAX ( t2 . column ) AS feature_1 , t1 . rowid AS \"rownum\" FROM \"population\" t1 LEFT JOIN \"peripheral\" t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp ORDER BY t2 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; You may notice that such a feature looks pretty similar to the Multirel feature below. And indeed, FastProp shares some of its aggregations with Multirel. FastProp features, however, are usually much simpler because they lack the complex conditions learned by getML's other algorithms (the WHERE statement in the SQL representation). FastProp is an excellent choice in an exploration phase of a data science project and delivers decent results out of the box in many cases. It is recommended that you combine FastProp with mappings .","title":"FastProp"},{"location":"user_guide/feature_engineering/feature_engineering/#multirel","text":"Simply speaking, Multirel is a more efficient variation of Multi-relational Decision Tree Learning (MRDTL). The core idea is to minimize redundancies in the original algorithm by incremental updates. We then combined our improved version of MRDTL with ensemble learning methods. MRDTL is a strain of academic literature that was particularly popular in the early 2000s. It is based on a greedy, tree-like approach: Define some sort of objective function that evaluates the quality of your feature as it relates to the target variable(s). Pick an aggregation and some column to be aggregated. Try out different conditions. Keep the one that generates the greatest improvement of your objective. Repeat until no improvement can be found or some sort of stopping criterion is reached. The reason this approach has never really taken off outside of academia is that an efficient implementation is far from trivial. Most papers on MRDTL implement the algorithm on top of an existing relational database system, like MySQL. The main problem with trying to implement something like this on top of an existing database is that it requires many redundant operations. Consider a feature like: COUNT the number of ` transactions ` in the last X ` days ` As we iterate through different values for the threshold \\(X\\) , we are forced to repeat the same operations on the same data over and over again. Tasks like this bring traditional database engines to their knees. The core idea of getML's Multirel algorithm is to minimize redundancies through incremental updates . To allow for incremental updates and maximal efficiency, we developed a database engine from scratch in C++. When we evaluate a feature like: COUNT the number of ` transactions ` in the last 90 ` days ` and COUNT the number of ` transactions ` in the last 91 ` days ` very little changes in between. Multirel only recalculates what has changed and keeps everything else untouched. Therefore, it needs two ingredients that can be incrementally updated: An objective function and the actual aggregation(s). Our first ingredient is an objective function that must be suited for incremental updates. When we move from 90 to 91 days, presumably only very few lines in the population table actually change. We do not need to recalculate the entire table. In practice, most commonly used objective functions are fine and this is not much of a limitation. However, there are some, like rank correlation, that cannot be used. The second ingredient, the aggregations, must allow for incremental updates too. This part is a bit harder, so let us elaborate: Let\u2019s say we have a match between the population table that contains our targets and another table (or a self-join). This match happens to be between the two thresholds 90 and 91 days. As we move from 90 to 91 days, we have to update our aggregation for that match. For maximum efficiency, this needs also to be done incrementally. That means we do not want to recalculate the entire aggregation for all matches that it aggregates - instead just for the one match in between the two thresholds. We want to also support the AND and OR combinations of conditions. Therefore, it is possible that a match was not included in the aggregation before, but becomes part of it as we move the threshold. It is also possible that the match was included in the aggregation, but now it isn\u2019t anymore. For an aggregation like Count , incremental updates are straight-forward. If the match was not included, but now it is, then increment by 1. If was included, but it isn\u2019t anymore, then decrement by 1. Things are more tricky for aggregations like Max , Median , or CountDistinct . For instance, whereas incrementing Max is easy, decrementing it is hard. If the match used to be included and is in fact the maximum value, we now have to find the next biggest match. And we have to find it quickly - ideally iterating through a set of thresholds should take linear time in the number of matches. To make it even more complicated, some cross-joins might result in a lot of matches, so any data structures that have non-trivial memory overhead are a no-go. Everything so far has shed light on how we train one feature. But in practice, we want more than one. So, how do we do that? Since we are using a tree-based algorithm anyway, we are able to harness the power of ensemble learning algorithms that have been shown to work very well with non-relational decision trees, namely bagging and gradient boosting. With bagging, we just sample randomly from our population table. We train a feature on that sample and then pick a different random sample to train the next feature. With gradient boosting, we calculate the pseudo-residuals of our previously trained features. We then train features that predict these pseudo-residuals. This procedure guarantees that new features are targeted and compensate the weaknesses of older ones. Transpiled to SQL, a typical feature generated by Multirel looks like this: CREATE TABLE FEATURE_1 AS SELECT COUNT ( * ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE ( ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 ) ) AND t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; Further information can be found in the API documentation for Multirel .","title":"Multirel"},{"location":"user_guide/feature_engineering/feature_engineering/#relboost","text":"Relboost is a generalization of the gradient boosting algorithm. More specifically, it generalizes the xgboost implementation to relational learning. The main difference between Relboost and Multirel is that Multirel aggregates columns, whereas Relboost aggregates learnable weights . Relboost addresses a problem with Multirel that is related to computational complexity theory: In Multirel, every column can be aggregated and/or used for generating a condition. That means that the number of possible features is \\(\\mathcal{O}(n^2)\\) in the number of columns in the original tables. As a result, having twice as many columns will lead to a search space that is four times as large (in reality, it is a bit more complicated than that, but the basic point is true). Any computer scientist or applied mathematician will tell you that \\(\\mathcal{O}(n^2)\\) is a problem. If you have tables with many columns, it might turn out to be a problem. Of course, this issue is not specific to Multirel: It is a very fundamental problem that you would also have, if you were to write your features by hand or use brute force. Relboost offers a way out of this dilemma: Because Relboost aggregates learnable weights and columns will only be used for conditions, but not for aggregation. So, now the search space is \\(\\mathcal{O}(n)\\) in the number of columns in the original tables - much better. This might seem very theoretical, but it has considerable implications: From our experience with real-world data in various projects, we know that Relboost usually outperforms Multirel in terms of predictive accuracy and training time. However, these advantages come at a price: First, the features generated by Relboost are less intuitive. They are further away from what you might write by hand, even though they can still be expressed as SQL code. Second, it is more difficult to apply Relboost to multiple targets , because Relboost has to learn separate rules and weights for each target. Expressed as SQL code, a typical feature generated by Relboost looks like this: CREATE TABLE FEATURE_1 AS SELECT SUM ( CASE WHEN ( t1 . time_stamp - t2 . time_stamp > 0 . 499624 ) THEN 0 . 0 WHEN ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 OR t1 . time_stamp IS NULL OR t2 . time_stamp IS NULL ) THEN 1 . 0 ELSE NULL END ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; Further information can be found in the API documentation for Relboost .","title":"Relboost"},{"location":"user_guide/feature_engineering/feature_engineering/#relmt","text":"RelMT is a generalization of linear model trees to relational data. Linear model trees are decision trees with a linear model at each leaf, resulting in a hybrid model that combines the strengths of linear models (like interpretability or the ability to capture linear relationships) with those of tree-based algorithms (like good performance or the ability to capture nonlinear relationships). RelMT features are particularly well-suited for time-series applications because time series often carry autoregressive structures, which can be approximated well by linear models. Think that this month's revenue can usually be modeled particularly well as a (linear) function of last month's revenue and so on. Purely tree-based models often struggle to learn such relationships because they have to fit a piecewise-constant model by predicting the average of all observations associated with each leaf. Thus, it can require a vast amount of splits to approximate a linear relationship. Here is a typical RelMT feature: CREATE TABLE FEATURE_1 AS SELECT SUM ( CASE WHEN ( t1 . time_stamp - t2 . time_stamp > 0 . 499624 ) THEN COALESCE ( t1 . time_stamp - julianday ( '1970-01-01' ) - 17202 . 004 , 0 . 0 ) * - 122 . 121 + COALESCE ( t2 . column - 3301 . 156 , 0 . 0 ) * - 0 . 003 WHEN ( t1 . time_stamp - t2 . time_stamp <= 0 . 499624 OR t1 . time_stamp IS NULL OR t2 . time_stamp IS NULL ) THEN COALESCE ( t1 . time_stamp - julianday ( '1970-01-01' ) - 17202 . 004 , 0 . 0 ) * 3 . 654 + COALESCE ( t2 . column - 3301 . 156 , 0 . 0 ) * - 1 . 824 + - 8 . 720 ELSE NULL END ) AS feature_1 , t1 . join_key , t1 . time_stamp FROM ( SELECT * , ROW_NUMBER () OVER ( ORDER BY join_key , time_stamp ASC ) AS rownum FROM POPULATION ) t1 LEFT JOIN PERIPHERAL t2 ON t1 . join_key = t2 . join_key WHERE t2 . time_stamp <= t1 . time_stamp GROUP BY t1 . rownum , t1 . join_key , t1 . time_stamp ; RelMT features share some characteristics with Relboost features: Compare the example feature to the Relboost feature above. Both algorithms generate splits based on a combination of conditions (the WHEN part of the CASE WHEN statement above). But while Relboost learns weights for its leaves (the THEN part of the CASE WHEN statement), RelMT learns a linear model, allowing for linear combinations between columns from the population table and columns of a certain peripheral table.","title":"RelMT"},{"location":"user_guide/getml_suite/engine/","text":"The getML engine A verbose title about foobar The getML engine is a standalone program written in C++ that does the actual work of feature engineering and prediction. Starting the engine The engine can be started using the dedicated launcher icon or by using the getML command line interface (CLI). For more information, check out the installation instructions for your operating system. Shutting down the engine There are several ways to shut down the getML engine: Click the 'Shutdown' tab in the sidebar of the monitor Press Ctrl-C (if started via the command line) Run the getML command-line interface (CLI) (see installation ) using the -stop option macOS only: Right-click the getML icon in the status bar and click 'Quit' (if started via the launchpad) Logging The engine keeps a log about what it is currently doing. The easiest way to view the log is to click the 'Log' tab in the sidebar of the getML monitor. The engine will also output its log to the command line when it is started using the command-line interface.","title":"Engine"},{"location":"user_guide/getml_suite/engine/#the-getml-engine","text":"","title":"The getML engine"},{"location":"user_guide/getml_suite/engine/#a-verbose-title-about-foobar","text":"The getML engine is a standalone program written in C++ that does the actual work of feature engineering and prediction.","title":"A verbose title about foobar"},{"location":"user_guide/getml_suite/engine/#starting-the-engine","text":"The engine can be started using the dedicated launcher icon or by using the getML command line interface (CLI). For more information, check out the installation instructions for your operating system.","title":"Starting the engine"},{"location":"user_guide/getml_suite/engine/#shutting-down-the-engine","text":"There are several ways to shut down the getML engine: Click the 'Shutdown' tab in the sidebar of the monitor Press Ctrl-C (if started via the command line) Run the getML command-line interface (CLI) (see installation ) using the -stop option macOS only: Right-click the getML icon in the status bar and click 'Quit' (if started via the launchpad)","title":"Shutting down the engine"},{"location":"user_guide/getml_suite/engine/#logging","text":"The engine keeps a log about what it is currently doing. The easiest way to view the log is to click the 'Log' tab in the sidebar of the getML monitor. The engine will also output its log to the command line when it is started using the command-line interface.","title":"Logging"},{"location":"user_guide/getml_suite/getml_suite/","text":"The getML suite The getML software consists of three fundamental components: Engine Monitor Python API The getML engine is written in C++ and is the heart of the getML suite. It holds all data, is responsible for the feature engineering and the machine learning (ML) part, and does all the heavy lifting. You can control the engine using the getML Python API . The API provides handlers to the objects in the engine and all functionalities necessary to do an end-to-end data science project. To help you explore the various data sets and ML models built during your analysis, we provide the getML monitor . The monitor is written in Go. In addition to visualization, it lets you handle the login and the account management for the getML suite. To get started with the getML, head over to the installation instructions.","title":"The getML suite"},{"location":"user_guide/getml_suite/getml_suite/#the-getml-suite","text":"The getML software consists of three fundamental components: Engine Monitor Python API The getML engine is written in C++ and is the heart of the getML suite. It holds all data, is responsible for the feature engineering and the machine learning (ML) part, and does all the heavy lifting. You can control the engine using the getML Python API . The API provides handlers to the objects in the engine and all functionalities necessary to do an end-to-end data science project. To help you explore the various data sets and ML models built during your analysis, we provide the getML monitor . The monitor is written in Go. In addition to visualization, it lets you handle the login and the account management for the getML suite. To get started with the getML, head over to the installation instructions.","title":"The getML suite"},{"location":"user_guide/getml_suite/monitor/","text":"The getML monitor The getML monitor contains information on the data imported into the engine as well as the trained pipelines and their performance. It is written in Go and compiled into a binary that is separate from the getML engine. Accessing the monitor The monitor is always started on the same machine as the engine. The engine and the monitor use sockets to communicate. The monitor opens an HTTP port - 1709 by default - for you to access it via your favorite internet browser. Entering the following address into the navigation bar will point your browser to the monitor: http://localhost:1709 The HTTP port can only be accessed from within the host the getML suite is running on. The main purpose of the monitor is to help you with your data science project by providing visual feedback. Tip If you experience any issues opening the monitor, try any of these steps: Manually shutdown the engine and restart it: getml.engine.shutdown() and getml.engine.launch() Kill the associated background process in the terminal and restart the engine Close all tabs and windows in which the monitor ran previously and try again","title":"Monitor"},{"location":"user_guide/getml_suite/monitor/#the-getml-monitor","text":"The getML monitor contains information on the data imported into the engine as well as the trained pipelines and their performance. It is written in Go and compiled into a binary that is separate from the getML engine.","title":"The getML monitor"},{"location":"user_guide/getml_suite/monitor/#accessing-the-monitor","text":"The monitor is always started on the same machine as the engine. The engine and the monitor use sockets to communicate. The monitor opens an HTTP port - 1709 by default - for you to access it via your favorite internet browser. Entering the following address into the navigation bar will point your browser to the monitor: http://localhost:1709 The HTTP port can only be accessed from within the host the getML suite is running on. The main purpose of the monitor is to help you with your data science project by providing visual feedback. Tip If you experience any issues opening the monitor, try any of these steps: Manually shutdown the engine and restart it: getml.engine.shutdown() and getml.engine.launch() Kill the associated background process in the terminal and restart the engine Close all tabs and windows in which the monitor ran previously and try again","title":"Accessing the monitor"},{"location":"user_guide/getml_suite/python_api/","text":"The getML Python API The getML Python API is shipped along with the matching version of the getML engine and monitor in the file you can download from getml.com (see Installation ). The most important thing you have to keep in mind when working with the Python API is this: All classes in the Python API are just handles to objects living in the getML engine. In addition, two basic requirements need to be fulfilled to successfully use the API: You need a running getML engine (on the same host as your Python session) (see starting the engine ) You need to set a project in the getML engine using getml.engine.set_project . import getml getml . engine . set_project ( 'test' ) This section provides some general information about the API and how it interacts with the engine. For an in-depth read about its individual classes, check out the Python API documentation . Connecting to the getML engine The getML Python API automatically connects to the engine with every command you execute. It establishes a socket connection to the engine through a port inferred by the time you connect to a project (or create a new project). Session management You can set the current project (see Managing projects ) using set_project() . If no project matches the supplied name, a new one will be created. To get a list of all available projects in your engine, you can use list_projects() and to remove an entire project, you can use delete_project() . Lifecycles and synchronization between engine and API The most important objects are the following: Data frames ( DataFrame ), which act as a container for all your data. Pipelines ( Pipeline ), which hold the trained states of the algorithms. Lifecycle of a DataFrame You can create a DataFrame by calling one of the class methods: from_csv() , from_db() , from_json() , or from_pandas() . These create a data frame object in the getML engine, import the provided data, and return a handler to the object as a DataFrame in the Python API (see Importing data ). When you apply any method, like add() , the changes will be automatically reflected in both the engine and Python. Under the hood, the Python API sends a command to create a new column to the getML engine. The moment the engine is done, it informs the Python API and the latter triggers the refresh() method to update the Python handler. Data frames are never saved automatically and never loaded automatically . All unsaved changes to a DataFrame will be lost when restarting the engine. To save a DataFrame , use save() . You can also use batch operations like save_all() and load_all() to save or load all data frames associated with the current project. The DataFrames container for the current project in memory can be accessed through getml.project.data_frames . To access the container, holding all of a project's data frames: getml . project . data_frames To save all data frames in memory to the project folder: getml . project . data_frames . save_all () You can subset the container to access single DataFrame instances. You can then call all available methods on those instances. For example, to store a single data frame to disk: getml . project . data_frames [ 0 ] . save () If a DataFrame called NAME_OF_THE_DATA_FRAME is already available in memory, load_data_frame() will return a handle to that data frame. If no such DataFrame is held in memory, the function will try to load the data frame from disk and then return a handle. If that is unsuccessful, an exception is thrown. If you want to force the API to load the version stored on disk over the one held in memory, you can use the load() method as follows: df = getml . data . DataFrame ( NAME_OF_THE_DATA_FRAME ) . load () Lifecycle of a Pipeline The lifecycle of a Pipeline is straightforward since the getML engine automatically saves all changes made to a pipeline and automatically loads all pipelines contained in a project. Using the constructors, the individual pipelines are created within the Python API, where they are represented as a set of hyperparameters. The actual weights of the machine learning algorithms are only stored in the getML engine and never transferred to the Python API. When applying any method, like fit() , the changes will be automatically reflected in both the engine and the Python API. When using set_project() to load an existing project, all pipelines contained in that project will be automatically loaded into memory. You can get an overview of all pipelines associated with the current project by accessing the Pipelines container, accessible through getml.project.pipelines . In order to create a corresponding handle in the Python API, you can use load() : pipe = getml . pipeline . load ( NAME_OF_THE_PIPELINE ) The function list_pipelines() lists all available pipelines within a project.","title":"Python api"},{"location":"user_guide/getml_suite/python_api/#the-getml-python-api","text":"The getML Python API is shipped along with the matching version of the getML engine and monitor in the file you can download from getml.com (see Installation ). The most important thing you have to keep in mind when working with the Python API is this: All classes in the Python API are just handles to objects living in the getML engine. In addition, two basic requirements need to be fulfilled to successfully use the API: You need a running getML engine (on the same host as your Python session) (see starting the engine ) You need to set a project in the getML engine using getml.engine.set_project . import getml getml . engine . set_project ( 'test' ) This section provides some general information about the API and how it interacts with the engine. For an in-depth read about its individual classes, check out the Python API documentation .","title":"The getML Python API"},{"location":"user_guide/getml_suite/python_api/#connecting-to-the-getml-engine","text":"The getML Python API automatically connects to the engine with every command you execute. It establishes a socket connection to the engine through a port inferred by the time you connect to a project (or create a new project).","title":"Connecting to the getML engine"},{"location":"user_guide/getml_suite/python_api/#session-management","text":"You can set the current project (see Managing projects ) using set_project() . If no project matches the supplied name, a new one will be created. To get a list of all available projects in your engine, you can use list_projects() and to remove an entire project, you can use delete_project() .","title":"Session management"},{"location":"user_guide/getml_suite/python_api/#lifecycles-and-synchronization-between-engine-and-api","text":"The most important objects are the following: Data frames ( DataFrame ), which act as a container for all your data. Pipelines ( Pipeline ), which hold the trained states of the algorithms.","title":"Lifecycles and synchronization between engine and API"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-dataframe","text":"You can create a DataFrame by calling one of the class methods: from_csv() , from_db() , from_json() , or from_pandas() . These create a data frame object in the getML engine, import the provided data, and return a handler to the object as a DataFrame in the Python API (see Importing data ). When you apply any method, like add() , the changes will be automatically reflected in both the engine and Python. Under the hood, the Python API sends a command to create a new column to the getML engine. The moment the engine is done, it informs the Python API and the latter triggers the refresh() method to update the Python handler. Data frames are never saved automatically and never loaded automatically . All unsaved changes to a DataFrame will be lost when restarting the engine. To save a DataFrame , use save() . You can also use batch operations like save_all() and load_all() to save or load all data frames associated with the current project. The DataFrames container for the current project in memory can be accessed through getml.project.data_frames . To access the container, holding all of a project's data frames: getml . project . data_frames To save all data frames in memory to the project folder: getml . project . data_frames . save_all () You can subset the container to access single DataFrame instances. You can then call all available methods on those instances. For example, to store a single data frame to disk: getml . project . data_frames [ 0 ] . save () If a DataFrame called NAME_OF_THE_DATA_FRAME is already available in memory, load_data_frame() will return a handle to that data frame. If no such DataFrame is held in memory, the function will try to load the data frame from disk and then return a handle. If that is unsuccessful, an exception is thrown. If you want to force the API to load the version stored on disk over the one held in memory, you can use the load() method as follows: df = getml . data . DataFrame ( NAME_OF_THE_DATA_FRAME ) . load ()","title":"Lifecycle of a DataFrame"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-pipeline","text":"The lifecycle of a Pipeline is straightforward since the getML engine automatically saves all changes made to a pipeline and automatically loads all pipelines contained in a project. Using the constructors, the individual pipelines are created within the Python API, where they are represented as a set of hyperparameters. The actual weights of the machine learning algorithms are only stored in the getML engine and never transferred to the Python API. When applying any method, like fit() , the changes will be automatically reflected in both the engine and the Python API. When using set_project() to load an existing project, all pipelines contained in that project will be automatically loaded into memory. You can get an overview of all pipelines associated with the current project by accessing the Pipelines container, accessible through getml.project.pipelines . In order to create a corresponding handle in the Python API, you can use load() : pipe = getml . pipeline . load ( NAME_OF_THE_PIPELINE ) The function list_pipelines() lists all available pipelines within a project.","title":"Lifecycle of a Pipeline"},{"location":"user_guide/hyperopt/hyperopt/","text":"Hyperparameter optimization In the sections on feature engineering and predicting , we learned how to train both the feature learning algorithm and the machine learning algorithm used for prediction in the getML engine. However, there are lots of parameters involved. Multirel , Relboost , RelMT , FastProp , LinearRegression , LogisticRegression , XGBoostClassifier , and XGBoostRegressor all have their own settings. That is why you might want to use hyperparameter optimization. The most relevant parameters of these classes can be chosen to constitute individual dimensions of a parameter space. For each parameter, a lower and upper bound has to be provided and the hyperparameter optimization will search the space within these bounds. This will be done iteratively by drawing a specific parameter combination, overwriting the corresponding parameters in a base pipeline, and then fitting and scoring it. The algorithm used to draw from the parameter space is represented by the different classes of hyperopt . While RandomSearch and LatinHypercubeSearch are purely statistical approaches, GaussianHyperparameterSearch uses prior knowledge obtained from evaluations of previous parameter combinations to select the next one. Tuning routines The easiest way to conduct a hyperparameter optimization in getML are the tuning routines tune_feature_learners() and tune_predictors() . They roughly work as follows: They begin with a base pipeline, in which the default parameters for the feature learner or the predictor are used. They then proceed by optimizing 2 or 3 parameters at a time using a GaussianHyperparameterSearch . If the best pipeline outperforms the base pipeline, the best pipeline becomes the new base pipeline. Taking the base pipeline from the previous steps, the tuning routine then optimizes the next 2 or 3 hyperparameters. If the best pipeline from that hyperparameter optimization outperforms the current base pipeline, that pipeline becomes the new base pipeline. These steps are then repeated for more hyperparameters. The following tables list the tuning recipes and hyperparameter subspaces for each step: Tuning recipes for predictors Predictor Stage Hyperparameter Subspace LinearRegression ; LogisticRegression 1 (base parameters) reg_lambda [1E-11, 100] learning_rate [0.5, 0.99] XGBoostClassifier ; XGBoostRegressor 1 (base parameters) learning_rate [0.05, 0.3] 2 (tree parameters) max_depth [1, 15] min_child_weights [1, 6] gamma [0, 5] 3 (sampling parameters) colsample_bytree [0.75, 0.9] subsample [0.75, 0.9] 4 (regularization parameters) reg_alpha [0, 5] reg_lambda [0, 10] Tuning recipes for feature learners Feature Learner Stage Hyperparameter Subspace FastProp 1 (base parameters) num_features [50, 500] n_most_frequent [0, 20] Multirel 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] Relboost 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] RelMT 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_depth [1, 8] min_num_samples [1, 500] 3 (regularization parameters) reg_lambda [0, 0.0001] The advantage of the tuning routines is that they provide a convenient out-of-the-box experience for hyperparameter tuning. For most use cases, it is sufficient to tune the XGBoost predictor. More advanced users can rely on the more low-level hyperparameter optimization routines. Random search A RandomSearch draws random hyperparameter sets from the hyperparameter space. Latin hypercube search A LatinHypercubeSearch draws almost random hyperparameter sets from the hyperparameter space, but ensures that they are sufficiently different from each other. Gaussian hyperparameter search A GaussianHyperparameterSearch works like this: It begins with a burn-in phase, usually about 70% to 90% of all iterations. During that burn-in phase, the hyperparameter space is sampled more or less at random, using either a random search or a latin hypercube search. You can control this phase using ratio_iter and surrogate_burn_in_algorithm . Once enough information has been collected, it fits a Gaussian process on the hyperparameters with the score we want to maximize or minimize as the predicted variable. Note that the Gaussian process has hyperparameters itself, which are also optimized. You can control this phase using gaussian_kernel , gaussian_optimization_algorithm , gaussian_optimization_burn_in_algorithm , and gaussian_optimization_burn_ins . It then uses the Gaussian process to predict the expected information (EI). The EI is a measure of how much additional information it might get from evaluating a particular point in the hyperparameter space. The expected information is to be maximized. The point in the hyperparameter space with the maximum expected information is the next point that is actually evaluated (meaning a new pipeline with these hyperparameters is trained). You can control this phase using optimization_algorithm , optimization_burn_ins , and optimization_burn_in_algorithm . In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists: At first, it picks random hyperparameter combinations. Once it has gained a better understanding of the hyperparameter space, it starts evaluating hyperparameter combinations that are particularly interesting.","title":"Hyperopt"},{"location":"user_guide/hyperopt/hyperopt/#hyperparameter-optimization","text":"In the sections on feature engineering and predicting , we learned how to train both the feature learning algorithm and the machine learning algorithm used for prediction in the getML engine. However, there are lots of parameters involved. Multirel , Relboost , RelMT , FastProp , LinearRegression , LogisticRegression , XGBoostClassifier , and XGBoostRegressor all have their own settings. That is why you might want to use hyperparameter optimization. The most relevant parameters of these classes can be chosen to constitute individual dimensions of a parameter space. For each parameter, a lower and upper bound has to be provided and the hyperparameter optimization will search the space within these bounds. This will be done iteratively by drawing a specific parameter combination, overwriting the corresponding parameters in a base pipeline, and then fitting and scoring it. The algorithm used to draw from the parameter space is represented by the different classes of hyperopt . While RandomSearch and LatinHypercubeSearch are purely statistical approaches, GaussianHyperparameterSearch uses prior knowledge obtained from evaluations of previous parameter combinations to select the next one.","title":"Hyperparameter optimization"},{"location":"user_guide/hyperopt/hyperopt/#tuning-routines","text":"The easiest way to conduct a hyperparameter optimization in getML are the tuning routines tune_feature_learners() and tune_predictors() . They roughly work as follows: They begin with a base pipeline, in which the default parameters for the feature learner or the predictor are used. They then proceed by optimizing 2 or 3 parameters at a time using a GaussianHyperparameterSearch . If the best pipeline outperforms the base pipeline, the best pipeline becomes the new base pipeline. Taking the base pipeline from the previous steps, the tuning routine then optimizes the next 2 or 3 hyperparameters. If the best pipeline from that hyperparameter optimization outperforms the current base pipeline, that pipeline becomes the new base pipeline. These steps are then repeated for more hyperparameters. The following tables list the tuning recipes and hyperparameter subspaces for each step:","title":"Tuning routines"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-predictors","text":"Predictor Stage Hyperparameter Subspace LinearRegression ; LogisticRegression 1 (base parameters) reg_lambda [1E-11, 100] learning_rate [0.5, 0.99] XGBoostClassifier ; XGBoostRegressor 1 (base parameters) learning_rate [0.05, 0.3] 2 (tree parameters) max_depth [1, 15] min_child_weights [1, 6] gamma [0, 5] 3 (sampling parameters) colsample_bytree [0.75, 0.9] subsample [0.75, 0.9] 4 (regularization parameters) reg_alpha [0, 5] reg_lambda [0, 10]","title":"Tuning recipes for predictors"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-feature-learners","text":"Feature Learner Stage Hyperparameter Subspace FastProp 1 (base parameters) num_features [50, 500] n_most_frequent [0, 20] Multirel 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] Relboost 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] RelMT 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_depth [1, 8] min_num_samples [1, 500] 3 (regularization parameters) reg_lambda [0, 0.0001] The advantage of the tuning routines is that they provide a convenient out-of-the-box experience for hyperparameter tuning. For most use cases, it is sufficient to tune the XGBoost predictor. More advanced users can rely on the more low-level hyperparameter optimization routines.","title":"Tuning recipes for feature learners"},{"location":"user_guide/hyperopt/hyperopt/#random-search","text":"A RandomSearch draws random hyperparameter sets from the hyperparameter space.","title":"Random search"},{"location":"user_guide/hyperopt/hyperopt/#latin-hypercube-search","text":"A LatinHypercubeSearch draws almost random hyperparameter sets from the hyperparameter space, but ensures that they are sufficiently different from each other.","title":"Latin hypercube search"},{"location":"user_guide/hyperopt/hyperopt/#gaussian-hyperparameter-search","text":"A GaussianHyperparameterSearch works like this: It begins with a burn-in phase, usually about 70% to 90% of all iterations. During that burn-in phase, the hyperparameter space is sampled more or less at random, using either a random search or a latin hypercube search. You can control this phase using ratio_iter and surrogate_burn_in_algorithm . Once enough information has been collected, it fits a Gaussian process on the hyperparameters with the score we want to maximize or minimize as the predicted variable. Note that the Gaussian process has hyperparameters itself, which are also optimized. You can control this phase using gaussian_kernel , gaussian_optimization_algorithm , gaussian_optimization_burn_in_algorithm , and gaussian_optimization_burn_ins . It then uses the Gaussian process to predict the expected information (EI). The EI is a measure of how much additional information it might get from evaluating a particular point in the hyperparameter space. The expected information is to be maximized. The point in the hyperparameter space with the maximum expected information is the next point that is actually evaluated (meaning a new pipeline with these hyperparameters is trained). You can control this phase using optimization_algorithm , optimization_burn_ins , and optimization_burn_in_algorithm . In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists: At first, it picks random hyperparameter combinations. Once it has gained a better understanding of the hyperparameter space, it starts evaluating hyperparameter combinations that are particularly interesting.","title":"Gaussian hyperparameter search"},{"location":"user_guide/importing_data/csv_interface/","text":"CSV interface The fastest way to import data into the getML engine is to read it directly from CSV files. Import from CSV Using the from_csv() class method, you can create a new DataFrame based on a table stored in the provided file(s). The read_csv() method will replace the content of the current DataFrame instance or append further rows. Export to CSV In addition to reading data from a CSV file, you can also write an existing DataFrame back into one using to_csv() .","title":"Csv interface"},{"location":"user_guide/importing_data/csv_interface/#csv-interface","text":"The fastest way to import data into the getML engine is to read it directly from CSV files.","title":"CSV interface"},{"location":"user_guide/importing_data/csv_interface/#import-from-csv","text":"Using the from_csv() class method, you can create a new DataFrame based on a table stored in the provided file(s). The read_csv() method will replace the content of the current DataFrame instance or append further rows.","title":"Import from CSV"},{"location":"user_guide/importing_data/csv_interface/#export-to-csv","text":"In addition to reading data from a CSV file, you can also write an existing DataFrame back into one using to_csv() .","title":"Export to CSV"},{"location":"user_guide/importing_data/greenplum_interface/","text":"Greenplum interface Greenplum is an open source database system maintained by Pivotal Software, Inc. It can be connected to the getML engine using the function connect_greenplum() . But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line. Import from Greenplum By selecting an existing table of your database in the from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export to Greenplum You can also write your results back into the Greenplum database. By providing a name for the destination table in Pipeline.transform() , the features generated from your raw data will be written back. Passing it into Pipeline.predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Greenplum interface"},{"location":"user_guide/importing_data/greenplum_interface/#greenplum-interface","text":"Greenplum is an open source database system maintained by Pivotal Software, Inc. It can be connected to the getML engine using the function connect_greenplum() . But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.","title":"Greenplum interface"},{"location":"user_guide/importing_data/greenplum_interface/#import-from-greenplum","text":"By selecting an existing table of your database in the from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import from Greenplum"},{"location":"user_guide/importing_data/greenplum_interface/#export-to-greenplum","text":"You can also write your results back into the Greenplum database. By providing a name for the destination table in Pipeline.transform() , the features generated from your raw data will be written back. Passing it into Pipeline.predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export to Greenplum"},{"location":"user_guide/importing_data/importing_data/","text":"Importing data Before being able to analyze and process your data using the getML software, you have to import it into the engine. At the end of this step, you will have your data in data frame objects in the getML engine and will be ready to annotate them . Note If you have imported your data into the engine before and want to restore it, refer to Lifecycle of DataFrame Unified import interface The getML Python API provides a unified import interface requiring similar arguments and resulting in the same output format, regardless of the data source. You can use one of the dedicated from_csv() , from_pandas() , from_db() , and from_json() class methods to construct a data frame object in the getML engine, fill it with the provided data, and retrieve a DataFrame handle in the Python API. If you already have a data frame object in place, you can use the read_csv() , read_pandas() , read_db() , or read_json() methods of the corresponding DataFrame handle to either replace its content with new data or append to it. All those functions also have their counterparts for exporting called to_csv() , to_pandas() , to_db() , and to_json() . The particularities of the individual formats will be covered in the following sections: CSV interface Pandas interface JSON interface SQLite3 interface MySQL interface MariaDB interface PostgreSQL interface Greenplum interface ODBC interface Data Frames The resulting DataFrame instance in the Python API represents a handle to the corresponding data frame object in the getML engine. The mapping between the two is done based on the name of the object, which has to be unique. Similarly, the names of the columns are required to be unique within the data frame they are associated with. Handling of NULL values Unfortunately, data sources often contain missing or corrupt data - also called NULL values. getML is able to work with missing values except for the target variable , which must not contain any NULL values (because having NULL targets does not make any sense). Please refer to the section on join keys for details about their handling during the construction of the data model. During import, a NULL value is automatically inserted at all occurrences of the strings \"nan\", \"None\", \"NA\", or an empty string as well as at all occurrences of None and NaN .","title":"Importing data"},{"location":"user_guide/importing_data/importing_data/#importing-data_1","text":"Before being able to analyze and process your data using the getML software, you have to import it into the engine. At the end of this step, you will have your data in data frame objects in the getML engine and will be ready to annotate them . Note If you have imported your data into the engine before and want to restore it, refer to Lifecycle of DataFrame","title":"Importing data"},{"location":"user_guide/importing_data/importing_data/#unified-import-interface","text":"The getML Python API provides a unified import interface requiring similar arguments and resulting in the same output format, regardless of the data source. You can use one of the dedicated from_csv() , from_pandas() , from_db() , and from_json() class methods to construct a data frame object in the getML engine, fill it with the provided data, and retrieve a DataFrame handle in the Python API. If you already have a data frame object in place, you can use the read_csv() , read_pandas() , read_db() , or read_json() methods of the corresponding DataFrame handle to either replace its content with new data or append to it. All those functions also have their counterparts for exporting called to_csv() , to_pandas() , to_db() , and to_json() . The particularities of the individual formats will be covered in the following sections: CSV interface Pandas interface JSON interface SQLite3 interface MySQL interface MariaDB interface PostgreSQL interface Greenplum interface ODBC interface","title":"Unified import interface"},{"location":"user_guide/importing_data/importing_data/#data-frames","text":"The resulting DataFrame instance in the Python API represents a handle to the corresponding data frame object in the getML engine. The mapping between the two is done based on the name of the object, which has to be unique. Similarly, the names of the columns are required to be unique within the data frame they are associated with.","title":"Data Frames"},{"location":"user_guide/importing_data/importing_data/#handling-of-null-values","text":"Unfortunately, data sources often contain missing or corrupt data - also called NULL values. getML is able to work with missing values except for the target variable , which must not contain any NULL values (because having NULL targets does not make any sense). Please refer to the section on join keys for details about their handling during the construction of the data model. During import, a NULL value is automatically inserted at all occurrences of the strings \"nan\", \"None\", \"NA\", or an empty string as well as at all occurrences of None and NaN .","title":"Handling of NULL values"},{"location":"user_guide/importing_data/json_interface/","text":"JSON interface The another convenient but slow way to import data into the getML engine via its Python API. Import from JSON Using the from_json() class method, you can create a new DataFrame based on a JSON string. The read_json() method will replace the content of the current DataFrame instance or append further rows. Export to JSON In addition to reading data from a JSON string, you can also convert an existing DataFrame into one using to_json() .","title":"Json interface"},{"location":"user_guide/importing_data/json_interface/#json-interface","text":"The another convenient but slow way to import data into the getML engine via its Python API.","title":"JSON interface"},{"location":"user_guide/importing_data/json_interface/#import-from-json","text":"Using the from_json() class method, you can create a new DataFrame based on a JSON string. The read_json() method will replace the content of the current DataFrame instance or append further rows.","title":"Import from JSON"},{"location":"user_guide/importing_data/json_interface/#export-to-json","text":"In addition to reading data from a JSON string, you can also convert an existing DataFrame into one using to_json() .","title":"Export to JSON"},{"location":"user_guide/importing_data/mariadb_interface/","text":"MariaDB interface MariaDB is a popular open source fork of MySQL. It can be connected to the getML engine using the function connect_mariadb() . But first, make sure your database is running, you have the corresponding hostname, port as well as your username and password ready, and you can reach it from your command line. If you are unsure which port or socket your MariaDB is running on, you can start the mysql command line interface $ mysql and use the following queries to get the required insights. MariaDB [( none )] > SELECT @@ port ; MariaDB [( none )] > SELECT @@ socket ; Import from MariaDB By selecting an existing table of your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export to MariaDB You can also write your results back into the MariaDB database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Mariadb interface"},{"location":"user_guide/importing_data/mariadb_interface/#mariadb-interface","text":"MariaDB is a popular open source fork of MySQL. It can be connected to the getML engine using the function connect_mariadb() . But first, make sure your database is running, you have the corresponding hostname, port as well as your username and password ready, and you can reach it from your command line. If you are unsure which port or socket your MariaDB is running on, you can start the mysql command line interface $ mysql and use the following queries to get the required insights. MariaDB [( none )] > SELECT @@ port ; MariaDB [( none )] > SELECT @@ socket ;","title":"MariaDB interface"},{"location":"user_guide/importing_data/mariadb_interface/#import-from-mariadb","text":"By selecting an existing table of your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import from MariaDB"},{"location":"user_guide/importing_data/mariadb_interface/#export-to-mariadb","text":"You can also write your results back into the MariaDB database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export to MariaDB"},{"location":"user_guide/importing_data/mysql_interface/","text":"MySQL interface MySQL is one of the most popular databases in use today. It can be connected to the getML engine using the function connect_mysql() . But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line. If you are unsure which port or socket your MySQL is running on, you can start the mysql command line interface $ mysql and use the following queries to get the required insights. > SELECT @@ port ; > SELECT @@ socket ; Import from MySQL By selecting an existing table of your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export to MySQL You can also write your results back into the MySQL database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Mysql interface"},{"location":"user_guide/importing_data/mysql_interface/#mysql-interface","text":"MySQL is one of the most popular databases in use today. It can be connected to the getML engine using the function connect_mysql() . But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line. If you are unsure which port or socket your MySQL is running on, you can start the mysql command line interface $ mysql and use the following queries to get the required insights. > SELECT @@ port ; > SELECT @@ socket ;","title":"MySQL interface"},{"location":"user_guide/importing_data/mysql_interface/#import-from-mysql","text":"By selecting an existing table of your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import from MySQL"},{"location":"user_guide/importing_data/mysql_interface/#export-to-mysql","text":"You can also write your results back into the MySQL database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export to MySQL"},{"location":"user_guide/importing_data/odbc_interface/","text":"ODBC interface ODBC (Open Database Connectivity) is an API specification for connecting software programming language to a database, developed by Microsoft. In a nutshell, it works like this: Any database of relevance has an ODBC driver that translates calls from the ODBC API into a format the database can understand, returning the query results in a format understood by the ODBC API. To connect getML or other software to a database using ODBC, you first need to install the ODBC driver provided by your database vendor. In theory, ODBC drivers should translate queries from the SQL 99 standard into the SQL dialect, but this is often ignored in practice. Also, not all ODBC drivers support all ODBC calls. At getML, native APIs are preferred for connecting to relational databases. ODBC is used when native APIs are not feasible due to licensing or other restrictions, especially for connecting to proprietary databases like Oracle, Microsoft SQL Server, or IBM DB2. ODBC is pre-installed on modern Windows operating systems, while Linux and macOS can use open-source implementations like unixODBC and iODBC, with getML using unixODBC. An example: Microsoft SQL Server To connect to Microsoft SQL Server using ODBC: If you do not have a Microsoft SQL Server instance, you can download a trial or development version. Download the ODBC driver for SQL Server . Configure the ODBC driver. Many drivers provide customized scripts for this, so manual configuration might not be necessary. For Linux and macOS, create a .odbc.ini file in your home directory with the following contents: [ANY-NAME-YOU-WANT] Driver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.5.so.2.1 Server = 123.45.67.890 Port = 1433 User = YOUR-USERNAME Password = YOUR-PASSWORD Database = YOUR-DATABASE Language = us_english NeedODBCTypesOnly = 1 On Docker , you can make appropriate changes to the Dockerfile and then rerun ./setup.sh or bash setup.sh . You will need to set the following parameters: The first line is the server name or data source name . You can use this name to tell getML that this is the server you want to connect to. The Driver is the location of the ODBC driver you have just downloaded. The location or file name might be different on your system. The Server is the IP address of the server. If the server is on the same machine as getML, just write \"localhost\". The Port is the port on which to connect the server. The default port for SQL Server is 1433. User and Password are the user name and password that allow access to the server. The Database is the database inside the server you want to connect to. You can now connect getML to the database: getml . database . connect_odbc ( server_name = \"ANY-NAME-YOU-WANT\" , user = \"YOUR-USERNAME\" , password = \"YOUR-PASSWORD\" , escape_chars = \"[]\" ) Important: Always pass escape_chars Earlier we mentioned that ODBC drivers are supposed to translate standard SQL queries into the specific SQL dialects, but this requirement is often ignored. A typical issue is escape characters , needed when the names of your schemas, tables, or columns are SQL keywords, like the loans dataset containing a table named ORDER . To avoid this problem, you can envelop the schema, table, and column names in escape characters . SELECT \"some_column\" FROM \"SOME_SCHEMA\" . \"SOME_TABLE\" ; getML always uses escape characters for its automatically generated queries. The SQL standard requires that the quotation mark (\") be used as the escape character. However, many SQL dialects do not follow this requirement, e.g., SQL Server uses \"[]\": SELECT [ some_column ] FROM [ SOME_SCHEMA ].[ SOME_TABLE ]; MySQL and MariaDB work like this: SELECT ` some_column ` FROM ` SOME_SCHEMA ` . ` SOME_TABLE ` ; To avoid frustration, determine your server's escape characters and explicitly pass them to connect_odbc() . Import data using ODBC By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export data using ODBC You can also write your results back into the PostgreSQL database. When you provide a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Odbc interface"},{"location":"user_guide/importing_data/odbc_interface/#odbc-interface","text":"ODBC (Open Database Connectivity) is an API specification for connecting software programming language to a database, developed by Microsoft. In a nutshell, it works like this: Any database of relevance has an ODBC driver that translates calls from the ODBC API into a format the database can understand, returning the query results in a format understood by the ODBC API. To connect getML or other software to a database using ODBC, you first need to install the ODBC driver provided by your database vendor. In theory, ODBC drivers should translate queries from the SQL 99 standard into the SQL dialect, but this is often ignored in practice. Also, not all ODBC drivers support all ODBC calls. At getML, native APIs are preferred for connecting to relational databases. ODBC is used when native APIs are not feasible due to licensing or other restrictions, especially for connecting to proprietary databases like Oracle, Microsoft SQL Server, or IBM DB2. ODBC is pre-installed on modern Windows operating systems, while Linux and macOS can use open-source implementations like unixODBC and iODBC, with getML using unixODBC.","title":"ODBC interface"},{"location":"user_guide/importing_data/odbc_interface/#an-example-microsoft-sql-server","text":"To connect to Microsoft SQL Server using ODBC: If you do not have a Microsoft SQL Server instance, you can download a trial or development version. Download the ODBC driver for SQL Server . Configure the ODBC driver. Many drivers provide customized scripts for this, so manual configuration might not be necessary. For Linux and macOS, create a .odbc.ini file in your home directory with the following contents: [ANY-NAME-YOU-WANT] Driver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.5.so.2.1 Server = 123.45.67.890 Port = 1433 User = YOUR-USERNAME Password = YOUR-PASSWORD Database = YOUR-DATABASE Language = us_english NeedODBCTypesOnly = 1 On Docker , you can make appropriate changes to the Dockerfile and then rerun ./setup.sh or bash setup.sh . You will need to set the following parameters: The first line is the server name or data source name . You can use this name to tell getML that this is the server you want to connect to. The Driver is the location of the ODBC driver you have just downloaded. The location or file name might be different on your system. The Server is the IP address of the server. If the server is on the same machine as getML, just write \"localhost\". The Port is the port on which to connect the server. The default port for SQL Server is 1433. User and Password are the user name and password that allow access to the server. The Database is the database inside the server you want to connect to. You can now connect getML to the database: getml . database . connect_odbc ( server_name = \"ANY-NAME-YOU-WANT\" , user = \"YOUR-USERNAME\" , password = \"YOUR-PASSWORD\" , escape_chars = \"[]\" )","title":"An example: Microsoft SQL Server"},{"location":"user_guide/importing_data/odbc_interface/#important-always-pass-escape_chars","text":"Earlier we mentioned that ODBC drivers are supposed to translate standard SQL queries into the specific SQL dialects, but this requirement is often ignored. A typical issue is escape characters , needed when the names of your schemas, tables, or columns are SQL keywords, like the loans dataset containing a table named ORDER . To avoid this problem, you can envelop the schema, table, and column names in escape characters . SELECT \"some_column\" FROM \"SOME_SCHEMA\" . \"SOME_TABLE\" ; getML always uses escape characters for its automatically generated queries. The SQL standard requires that the quotation mark (\") be used as the escape character. However, many SQL dialects do not follow this requirement, e.g., SQL Server uses \"[]\": SELECT [ some_column ] FROM [ SOME_SCHEMA ].[ SOME_TABLE ]; MySQL and MariaDB work like this: SELECT ` some_column ` FROM ` SOME_SCHEMA ` . ` SOME_TABLE ` ; To avoid frustration, determine your server's escape characters and explicitly pass them to connect_odbc() .","title":"Important: Always pass escape_chars"},{"location":"user_guide/importing_data/odbc_interface/#import-data-using-odbc","text":"By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import data using ODBC"},{"location":"user_guide/importing_data/odbc_interface/#export-data-using-odbc","text":"You can also write your results back into the PostgreSQL database. When you provide a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export data using ODBC"},{"location":"user_guide/importing_data/pandas_interface/","text":"Pandas interface Pandas is one of the key packages used in most data science projects done in Python. The associated import interface is one of the slowest, but you can harness the good data exploration and manipulation capabilities of this Python package. Import from Pandas Using the DataFrame.from_pandas() class method, you can create a new DataFrame based on the provided pandas.DataFrame . The read_pandas() method will replace the content of the current DataFrame instance or append further rows. Export to Pandas In addition to reading data from a pandas.DataFrame , you can also write an existing DataFrame back into a pandas.DataFrame using DataFrame.to_pandas() . Due to the way data is stored within the getML engine, the dtypes of the original pandas.DataFrame cannot be restored properly and there might be inconsistencies in the order of microseconds being introduced into timestamps.","title":"Pandas interface"},{"location":"user_guide/importing_data/pandas_interface/#pandas-interface","text":"Pandas is one of the key packages used in most data science projects done in Python. The associated import interface is one of the slowest, but you can harness the good data exploration and manipulation capabilities of this Python package.","title":"Pandas interface"},{"location":"user_guide/importing_data/pandas_interface/#import-from-pandas","text":"Using the DataFrame.from_pandas() class method, you can create a new DataFrame based on the provided pandas.DataFrame . The read_pandas() method will replace the content of the current DataFrame instance or append further rows.","title":"Import from Pandas"},{"location":"user_guide/importing_data/pandas_interface/#export-to-pandas","text":"In addition to reading data from a pandas.DataFrame , you can also write an existing DataFrame back into a pandas.DataFrame using DataFrame.to_pandas() . Due to the way data is stored within the getML engine, the dtypes of the original pandas.DataFrame cannot be restored properly and there might be inconsistencies in the order of microseconds being introduced into timestamps.","title":"Export to Pandas"},{"location":"user_guide/importing_data/postgres_interface/","text":"PostgreSQL interface PostgreSQL is a powerful and well-established open source database system. It can be connected to the getML engine using the function connect_postgres() . Make sure your database is running, you have the corresponding hostname, port, user name, and password ready, and you can reach it from your command line. Import from PostgreSQL By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export to PostgreSQL You can also write your results back into the PostgreSQL database. If you provide a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Postgres interface"},{"location":"user_guide/importing_data/postgres_interface/#postgresql-interface","text":"PostgreSQL is a powerful and well-established open source database system. It can be connected to the getML engine using the function connect_postgres() . Make sure your database is running, you have the corresponding hostname, port, user name, and password ready, and you can reach it from your command line.","title":"PostgreSQL interface"},{"location":"user_guide/importing_data/postgres_interface/#import-from-postgresql","text":"By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import from PostgreSQL"},{"location":"user_guide/importing_data/postgres_interface/#export-to-postgresql","text":"You can also write your results back into the PostgreSQL database. If you provide a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export to PostgreSQL"},{"location":"user_guide/importing_data/sqlite3_interface/","text":"SQLite3 interface SQLite3 is a popular in-memory database. It is faster than classical relational databases like PostgreSQL but less stable under massive parallel access. It requires all contained datasets to be loaded into memory, which might use up too much RAM, especially for large datasets. As with all other databases in the unified import interface of the getML Python API, you first need to connect to it using connect_sqlite3() . Import from SQLite3 By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query. Export to SQLite3 You can also write your results back into the SQLite3 database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Sqlite3 interface"},{"location":"user_guide/importing_data/sqlite3_interface/#sqlite3-interface","text":"SQLite3 is a popular in-memory database. It is faster than classical relational databases like PostgreSQL but less stable under massive parallel access. It requires all contained datasets to be loaded into memory, which might use up too much RAM, especially for large datasets. As with all other databases in the unified import interface of the getML Python API, you first need to connect to it using connect_sqlite3() .","title":"SQLite3 interface"},{"location":"user_guide/importing_data/sqlite3_interface/#import-from-sqlite3","text":"By selecting an existing table from your database in the DataFrame.from_db() class method, you can create a new DataFrame containing all its data. Alternatively, you can use the read_db() and read_query() methods to replace the content of the current DataFrame instance or append further rows based on either a table or a specific query.","title":"Import from SQLite3"},{"location":"user_guide/importing_data/sqlite3_interface/#export-to-sqlite3","text":"You can also write your results back into the SQLite3 database. By providing a name for the destination table in transform() , the features generated from your raw data will be written back. Passing it into predict() generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.","title":"Export to SQLite3"},{"location":"user_guide/predicting/predicting/","text":"Predicting Now that you know how to engineer a flat table of features , you are ready to make predictions of the target variable(s) . Using getML getML comes with four built-in machine learning predictors: LinearRegression LogisticRegression XGBoostClassifier XGBoostRegressor Using one of them in your analysis is very simple. Just pass one as the predictor argument to either Pipeline on initialization. As a list, more than one predictor can be passed to the pipeline. feature_learner1 = getml . feature_learners . Relboost () feature_learner2 = getml . feature_learners . Multirel () predictor = getml . predictors . XGBoostRegressor () pipe = getml . pipeline . Pipeline ( data_model = data_model , peripheral = peripheral_placeholder , feature_learners = [ feature_learner1 , feature_learner2 ], predictors = predictor , ) When you call fit() on a pipeline, the entire pipeline will be trained. Note The time estimation for training a pipeline is a rough estimate. Occasionally, the training time can be significantly longer than the estimate. But the pipeline never silently crashes. Given enough time, computations always finish. Note that Pipeline comes with dependency tracking. That means it can figure out on its own what has changed and what needs to be trained again. feature_learner1 = getml . feature_learners . Relboost () feature_learner2 = getml . feature_learners . Multirel () predictor = getml . predictors . XGBoostRegressor () pipe = getml . pipeline . Pipeline ( data_model = data_model , population = population_placeholder , peripheral = peripheral_placeholder , feature_learners = [ feature_learner1 , feature_learner2 ], predictors = predictor ) pipe . fit ( ... ) pipe . predictors [ 0 ] . n_estimators = 50 # Only the predictor has changed, # so only the predictor will be refitted. pipe . fit ( ... ) To score the performance of your prediction on a test data set, the getML models come with a score() method. The available metrics are documented in scores . To use a trained model, including both the trained features and the predictor, to make predictions on new, unseen data, call the predict() method of your model. Using external software In our experience, the most relevant contribution to making accurate predictions are the generated features. Before trying to tweak your analysis by using sophisticated prediction algorithms and tuning their hyperparameters, we recommend tuning the hyperparameters of your Multirel or Relboost instead. You can do so either by hand or using getML's automated hyperparameter optimization . If you wish to use external predictors, you can transform new data, which is compliant with your relational data model, to a flat feature table using the transform() method of your pipeline.","title":"Predicting"},{"location":"user_guide/predicting/predicting/#predicting_1","text":"Now that you know how to engineer a flat table of features , you are ready to make predictions of the target variable(s) .","title":"Predicting"},{"location":"user_guide/predicting/predicting/#using-getml","text":"getML comes with four built-in machine learning predictors: LinearRegression LogisticRegression XGBoostClassifier XGBoostRegressor Using one of them in your analysis is very simple. Just pass one as the predictor argument to either Pipeline on initialization. As a list, more than one predictor can be passed to the pipeline. feature_learner1 = getml . feature_learners . Relboost () feature_learner2 = getml . feature_learners . Multirel () predictor = getml . predictors . XGBoostRegressor () pipe = getml . pipeline . Pipeline ( data_model = data_model , peripheral = peripheral_placeholder , feature_learners = [ feature_learner1 , feature_learner2 ], predictors = predictor , ) When you call fit() on a pipeline, the entire pipeline will be trained. Note The time estimation for training a pipeline is a rough estimate. Occasionally, the training time can be significantly longer than the estimate. But the pipeline never silently crashes. Given enough time, computations always finish. Note that Pipeline comes with dependency tracking. That means it can figure out on its own what has changed and what needs to be trained again. feature_learner1 = getml . feature_learners . Relboost () feature_learner2 = getml . feature_learners . Multirel () predictor = getml . predictors . XGBoostRegressor () pipe = getml . pipeline . Pipeline ( data_model = data_model , population = population_placeholder , peripheral = peripheral_placeholder , feature_learners = [ feature_learner1 , feature_learner2 ], predictors = predictor ) pipe . fit ( ... ) pipe . predictors [ 0 ] . n_estimators = 50 # Only the predictor has changed, # so only the predictor will be refitted. pipe . fit ( ... ) To score the performance of your prediction on a test data set, the getML models come with a score() method. The available metrics are documented in scores . To use a trained model, including both the trained features and the predictor, to make predictions on new, unseen data, call the predict() method of your model.","title":"Using getML"},{"location":"user_guide/predicting/predicting/#using-external-software","text":"In our experience, the most relevant contribution to making accurate predictions are the generated features. Before trying to tweak your analysis by using sophisticated prediction algorithms and tuning their hyperparameters, we recommend tuning the hyperparameters of your Multirel or Relboost instead. You can do so either by hand or using getML's automated hyperparameter optimization . If you wish to use external predictors, you can transform new data, which is compliant with your relational data model, to a flat feature table using the transform() method of your pipeline.","title":"Using external software"},{"location":"user_guide/preprocessing/preprocessing/","text":"Preprocessing As preprocessing, we categorize operations on data frames that are not directly related to the relational data model. While feature learning and propositionalization deal with relational data structures and result in a single-table representation thereof, we categorize all operations that work on single tables as preprocessing. This includes numerical transformations, encoding techniques, or alternative representations. getML's preprocessors allow you to extract domains from email addresses ( EmailDomain ), impute missing values ( Imputation ), map categorical columns to a continuous representation ( Mapping ), extract seasonal components from time stamps ( Seasonal ), extract sub strings from string-based columns ( Substring ) and split up text columns ( TextFieldSplitter ). Preprocessing operations in getML are very efficient and happen really fast. In fact, most of the time you won't even notice the presence of a preprocessor in your pipeline. getML's preprocessors operate on an abstract level without polluting your original data, are evaluated lazily and their set-up requires minimal effort. Here is a small example that shows the Seasonal preprocessor in action. import getml getml . project . switch ( \"seasonal\" ) traffic = getml . datasets . load_interstate94 () # traffic explicitly holds seasonal components (hour, day, month, ...) # extracted from column ds; we copy traffic and delete all those components traffic2 = traffic . drop ([ \"hour\" , \"weekday\" , \"day\" , \"month\" , \"year\" ]) start_test = getml . data . time . datetime ( 2018 , 3 , 14 ) split = getml . data . split . time ( population = traffic , test = start_test , time_stamp = \"ds\" , ) time_series1 = getml . data . TimeSeries ( population = traffic , split = split , time_stamps = \"ds\" , horizon = getml . data . time . hours ( 1 ), memory = getml . data . time . days ( 7 ), lagged_targets = True , ) time_series2 = getml . data . TimeSeries ( population = traffic2 , split = split , time_stamps = \"ds\" , horizon = getml . data . time . hours ( 1 ), memory = getml . data . time . days ( 7 ), lagged_targets = True , ) fast_prop = getml . feature_learning . FastProp ( loss_function = getml . feature_learning . loss_function . SquareLoss ) pipe1 = getml . pipeline . Pipeline ( data_model = time_series1 . data_model , feature_learners = [ fast_prop ], predictors = [ getml . predictors . XGBoostRegressor ()] ) pipe2 = getml . pipeline . Pipeline ( data_model = time_series2 . data_model , preprocessors = [ getml . preprocessors . Seasonal ()], feature_learners = [ fast_prop ], predictors = [ getml . predictors . XGBoostRegressor ()] ) # pipe1 includes no preprocessor but receives the data frame with the components pipe1 . fit ( time_series1 . train ) # pipe2 includes the preprocessor; receives data w/o components pipe2 . fit ( time_series2 . train ) month_based1 = pipe1 . features . filter ( lambda feat : \"month\" in feat . sql ) month_based2 = pipe2 . features . filter ( lambda feat : \"COUNT( DISTINCT t2. \\\" strftime('%m'\" in feat . sql ) print ( month_based1 [ 1 ] . sql ) # Output: # DROP TABLE IF EXISTS \"FEATURE_1_10\"; # # CREATE TABLE \"FEATURE_1_10\" AS # SELECT COUNT( t2.\"month\" ) - COUNT( DISTINCT t2.\"month\" ) AS \"feature_1_10\", # t1.rowid AS \"rownum\" # FROM \"POPULATION__STAGING_TABLE_1\" t1 # LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2 # ON 1 = 1 # WHERE t2.\"ds, '+1.000000 hours'\" <= t1.\"ds\" # AND ( t2.\"ds, '+7.041667 days'\" > t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL ) # GROUP BY t1.rowid; print ( month_based2 [ 0 ] . sql ) # Output: # DROP TABLE IF EXISTS \"FEATURE_1_5\"; # # CREATE TABLE \"FEATURE_1_5\" AS # SELECT COUNT( t2.\"strftime('%m', ds )\" ) - COUNT( DISTINCT t2.\"strftime('%m', ds )\" ) AS \"feature_1_5\", # t1.rowid AS \"rownum\" # FROM \"POPULATION__STAGING_TABLE_1\" t1 # LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2 # ON 1 = 1 # WHERE t2.\"ds, '+1.000000 hours'\" <= t1.\"ds\" # AND ( t2.\"ds, '+7.041667 days'\" > t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL ) # GROUP BY t1.rowid; If you compare both of the features above, you will notice they are exactly the same: COUNT - COUNT(DISTINCT) on the month component conditional on the time-based restrictions introduced through memory and horizon. Pipelines can include more than one preprocessor. While most of getML's preprocessors are straightforward, two of them deserve a more detailed introduction: Mapping and TextFieldSplitter . Mappings Mapping s are an alternative representation for categorical columns, text columns, and (quasi-categorical) discrete-numerical columns. Each discrete value (category) of a categorical column is mapped to a continuous spectrum by calculating the average target value for the subset of all rows belonging to the respective category. For columns from peripheral tables, the average target values are propagated back by traversing the relational structure. Mappings are a simple and interpretable alternative representation for categorical data. By introducing a continuous representation, mappings allow getML's feature learning algorithms to apply arbitrary aggregations to categorical columns. Further, mappings enable huge gains in efficiency when learning patterns from categorical data. You can control the extent mappings are utilized by specifying the minimum number of matching rows required for categories that constitutes a mapping through the min_freq parameter. Here is an example mapping from the CORA notebook : DROP TABLE IF EXISTS \"CATEGORICAL_MAPPING_1_1_1\" ; CREATE TABLE \"CATEGORICAL_MAPPING_1_1_1\" ( key TEXT NOT NULL PRIMARY KEY , value NUMERIC ); INSERT INTO \"CATEGORICAL_MAPPING_1_1_1\" ( key , value ) VALUES ( 'Case_Based' , 0 . 7109826589595376 ), ( 'Rule_Learning' , 0 . 07368421052631578 ), ( 'Reinforcement_Learning' , 0 . 0576923076923077 ), ( 'Theory' , 0 . 0547945205479452 ), ( 'Genetic_Algorithms' , 0 . 03157894736842105 ), ( 'Neural_Networks' , 0 . 02088772845953003 ), ( 'Probabilistic_Methods' , 0 . 01293103448275862 ); Inspecting the actual values, it's highly likely, that this mapping stems from a feature learned by a sub learner targeting the label \"Case_Based\". In addition to the trivial case, we can see that the next closed neighboring category is the \"Rule_Learning\" category, to which 7.3 % of the papers citing the target papers are categorized. Handling of free form text getML provides the role text to annotate free form text fields within relational data structures. Learning from text columns works as follows: First, for each of the text columns, a vocabulary is built by taking into account the feature learner's text mining specific hyperparameter vocab_size . If a text field contains words that belong to the vocabulary, getML deals with columns of role text through one of two approaches: Text fields can either can be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the TextFieldSplitter preprocessor. Opting for the second approach is as easy as adding the TextFieldSplitter to the list of preprocessors on your Pipeline . The resulting bag of words can be viewed as another one-to-many relationship within our data model where each row holding a text field is related to n peripheral rows (n is the number of words in the text field). Consider the following example, where the text field is split into a relational bag of words. One row of a table with a text field rownum text field 52 The quick brown fox jumps over the lazy dog The (implicit) peripheral table that results from splitting rownum words 52 the 52 quick 52 brown 52 fox 52 jumps 52 over 52 the 52 lazy 52 dog As text fields now present another relation, getML's feature learning algorithms are able to learn structural logic from text fields' contents by applying aggregations over the resulting bag of words itself ( COUNT WHERE words IN ('quick', 'jumps') ). Further, by utilizing mappings , any aggregation applicable to a (mapped) categorical column can be applied to bag-of-words mappings as well. Note that the splitting of text fields can be computationally expensive. If performance suffers too much, you may resort to the default behavior by removing the TextFieldSplitter from your Pipeline .","title":"Preprocessing"},{"location":"user_guide/preprocessing/preprocessing/#preprocessing_1","text":"As preprocessing, we categorize operations on data frames that are not directly related to the relational data model. While feature learning and propositionalization deal with relational data structures and result in a single-table representation thereof, we categorize all operations that work on single tables as preprocessing. This includes numerical transformations, encoding techniques, or alternative representations. getML's preprocessors allow you to extract domains from email addresses ( EmailDomain ), impute missing values ( Imputation ), map categorical columns to a continuous representation ( Mapping ), extract seasonal components from time stamps ( Seasonal ), extract sub strings from string-based columns ( Substring ) and split up text columns ( TextFieldSplitter ). Preprocessing operations in getML are very efficient and happen really fast. In fact, most of the time you won't even notice the presence of a preprocessor in your pipeline. getML's preprocessors operate on an abstract level without polluting your original data, are evaluated lazily and their set-up requires minimal effort. Here is a small example that shows the Seasonal preprocessor in action. import getml getml . project . switch ( \"seasonal\" ) traffic = getml . datasets . load_interstate94 () # traffic explicitly holds seasonal components (hour, day, month, ...) # extracted from column ds; we copy traffic and delete all those components traffic2 = traffic . drop ([ \"hour\" , \"weekday\" , \"day\" , \"month\" , \"year\" ]) start_test = getml . data . time . datetime ( 2018 , 3 , 14 ) split = getml . data . split . time ( population = traffic , test = start_test , time_stamp = \"ds\" , ) time_series1 = getml . data . TimeSeries ( population = traffic , split = split , time_stamps = \"ds\" , horizon = getml . data . time . hours ( 1 ), memory = getml . data . time . days ( 7 ), lagged_targets = True , ) time_series2 = getml . data . TimeSeries ( population = traffic2 , split = split , time_stamps = \"ds\" , horizon = getml . data . time . hours ( 1 ), memory = getml . data . time . days ( 7 ), lagged_targets = True , ) fast_prop = getml . feature_learning . FastProp ( loss_function = getml . feature_learning . loss_function . SquareLoss ) pipe1 = getml . pipeline . Pipeline ( data_model = time_series1 . data_model , feature_learners = [ fast_prop ], predictors = [ getml . predictors . XGBoostRegressor ()] ) pipe2 = getml . pipeline . Pipeline ( data_model = time_series2 . data_model , preprocessors = [ getml . preprocessors . Seasonal ()], feature_learners = [ fast_prop ], predictors = [ getml . predictors . XGBoostRegressor ()] ) # pipe1 includes no preprocessor but receives the data frame with the components pipe1 . fit ( time_series1 . train ) # pipe2 includes the preprocessor; receives data w/o components pipe2 . fit ( time_series2 . train ) month_based1 = pipe1 . features . filter ( lambda feat : \"month\" in feat . sql ) month_based2 = pipe2 . features . filter ( lambda feat : \"COUNT( DISTINCT t2. \\\" strftime('%m'\" in feat . sql ) print ( month_based1 [ 1 ] . sql ) # Output: # DROP TABLE IF EXISTS \"FEATURE_1_10\"; # # CREATE TABLE \"FEATURE_1_10\" AS # SELECT COUNT( t2.\"month\" ) - COUNT( DISTINCT t2.\"month\" ) AS \"feature_1_10\", # t1.rowid AS \"rownum\" # FROM \"POPULATION__STAGING_TABLE_1\" t1 # LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2 # ON 1 = 1 # WHERE t2.\"ds, '+1.000000 hours'\" <= t1.\"ds\" # AND ( t2.\"ds, '+7.041667 days'\" > t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL ) # GROUP BY t1.rowid; print ( month_based2 [ 0 ] . sql ) # Output: # DROP TABLE IF EXISTS \"FEATURE_1_5\"; # # CREATE TABLE \"FEATURE_1_5\" AS # SELECT COUNT( t2.\"strftime('%m', ds )\" ) - COUNT( DISTINCT t2.\"strftime('%m', ds )\" ) AS \"feature_1_5\", # t1.rowid AS \"rownum\" # FROM \"POPULATION__STAGING_TABLE_1\" t1 # LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2 # ON 1 = 1 # WHERE t2.\"ds, '+1.000000 hours'\" <= t1.\"ds\" # AND ( t2.\"ds, '+7.041667 days'\" > t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL ) # GROUP BY t1.rowid; If you compare both of the features above, you will notice they are exactly the same: COUNT - COUNT(DISTINCT) on the month component conditional on the time-based restrictions introduced through memory and horizon. Pipelines can include more than one preprocessor. While most of getML's preprocessors are straightforward, two of them deserve a more detailed introduction: Mapping and TextFieldSplitter .","title":"Preprocessing"},{"location":"user_guide/preprocessing/preprocessing/#mappings","text":"Mapping s are an alternative representation for categorical columns, text columns, and (quasi-categorical) discrete-numerical columns. Each discrete value (category) of a categorical column is mapped to a continuous spectrum by calculating the average target value for the subset of all rows belonging to the respective category. For columns from peripheral tables, the average target values are propagated back by traversing the relational structure. Mappings are a simple and interpretable alternative representation for categorical data. By introducing a continuous representation, mappings allow getML's feature learning algorithms to apply arbitrary aggregations to categorical columns. Further, mappings enable huge gains in efficiency when learning patterns from categorical data. You can control the extent mappings are utilized by specifying the minimum number of matching rows required for categories that constitutes a mapping through the min_freq parameter. Here is an example mapping from the CORA notebook : DROP TABLE IF EXISTS \"CATEGORICAL_MAPPING_1_1_1\" ; CREATE TABLE \"CATEGORICAL_MAPPING_1_1_1\" ( key TEXT NOT NULL PRIMARY KEY , value NUMERIC ); INSERT INTO \"CATEGORICAL_MAPPING_1_1_1\" ( key , value ) VALUES ( 'Case_Based' , 0 . 7109826589595376 ), ( 'Rule_Learning' , 0 . 07368421052631578 ), ( 'Reinforcement_Learning' , 0 . 0576923076923077 ), ( 'Theory' , 0 . 0547945205479452 ), ( 'Genetic_Algorithms' , 0 . 03157894736842105 ), ( 'Neural_Networks' , 0 . 02088772845953003 ), ( 'Probabilistic_Methods' , 0 . 01293103448275862 ); Inspecting the actual values, it's highly likely, that this mapping stems from a feature learned by a sub learner targeting the label \"Case_Based\". In addition to the trivial case, we can see that the next closed neighboring category is the \"Rule_Learning\" category, to which 7.3 % of the papers citing the target papers are categorized.","title":"Mappings"},{"location":"user_guide/preprocessing/preprocessing/#handling-of-free-form-text","text":"getML provides the role text to annotate free form text fields within relational data structures. Learning from text columns works as follows: First, for each of the text columns, a vocabulary is built by taking into account the feature learner's text mining specific hyperparameter vocab_size . If a text field contains words that belong to the vocabulary, getML deals with columns of role text through one of two approaches: Text fields can either can be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the TextFieldSplitter preprocessor. Opting for the second approach is as easy as adding the TextFieldSplitter to the list of preprocessors on your Pipeline . The resulting bag of words can be viewed as another one-to-many relationship within our data model where each row holding a text field is related to n peripheral rows (n is the number of words in the text field). Consider the following example, where the text field is split into a relational bag of words.","title":"Handling of free form text"},{"location":"user_guide/preprocessing/preprocessing/#one-row-of-a-table-with-a-text-field","text":"rownum text field 52 The quick brown fox jumps over the lazy dog","title":"One row of a table with a text field"},{"location":"user_guide/preprocessing/preprocessing/#the-implicit-peripheral-table-that-results-from-splitting","text":"rownum words 52 the 52 quick 52 brown 52 fox 52 jumps 52 over 52 the 52 lazy 52 dog As text fields now present another relation, getML's feature learning algorithms are able to learn structural logic from text fields' contents by applying aggregations over the resulting bag of words itself ( COUNT WHERE words IN ('quick', 'jumps') ). Further, by utilizing mappings , any aggregation applicable to a (mapped) categorical column can be applied to bag-of-words mappings as well. Note that the splitting of text fields can be computationally expensive. If performance suffers too much, you may resort to the default behavior by removing the TextFieldSplitter from your Pipeline .","title":"The (implicit) peripheral table that results from splitting"},{"location":"user_guide/project_management/project_management/","text":"Managing projects When working with getML, all data is bundled into projects. getML's projects are managed through the getml.project module. The relationship between projects and engine processes Each project is tied to a specific instance of the getML engine running as a global process (independent from your python session). In this way, it is possible to share one getML instance with multiple users to work on different projects. When switching projects through getml.project.switch() , the python API spawns a new process and establishes a connection to this process, while the currently loaded project remains in memory and its process is delegated to the background (until you explicitly suspend() the project). You can also work on multiple projects simultaneously from different python sessions. This comes in particularly handy if you use Jupyter Lab to open multiple notebooks and manage multiple python kernels simultaneously. To load an existing project or create a new one, you can do so from the 'Projects' view in the monitor or use the API ( getml.engine.set_project() ). If you want to shut down the engine process associated with the current project, you can call getml.project.suspend() . When you suspend the project, the memory of the engine is flushed and all unsaved changes to the data frames are lost (see lifecycles and synchronization between engine and API for details). All pipelines of the new project are automatically loaded into memory. You can retrieve all of your project's pipelines through getml.project.pipelines . Projects can be deleted by clicking the trash can icon in the 'Projects' tab of the getML monitor or by calling getml.engine.delete_project() (to delete a project by name) or getml.project.delete() (to suspend and delete the project currently loaded). Managing data using projects Every project has its own folder in ~/.getML/getml-VERSION/projects (for Linux and macOS) in which all of its data and pipelines are stored. On Windows, the projects folder is in the same location as getML.exe . These folders can be easily shared between different instances of getML; even between different operating systems. However, individual pipelines or data frames cannot be simply copied to another project folder \u2013 they are tied to the project. Projects can be bundled and exported/imported. Using the project module to manage your project The getml.project module is the entry point to your projects. From here, you can: query project-specific data ( getml.project.pipelines , getml.project.data_frames , getml.project.hyperopts ), manage the state of the current project ( getml.project.delete() , getml.project.restart() , getml.project.switch() , getml.project.suspend() ), and import projects from or export projects as a .getml bundle to disk ( getml.project.load() , getml.project.save() ).","title":"Project management"},{"location":"user_guide/project_management/project_management/#managing-projects","text":"When working with getML, all data is bundled into projects. getML's projects are managed through the getml.project module.","title":"Managing projects"},{"location":"user_guide/project_management/project_management/#the-relationship-between-projects-and-engine-processes","text":"Each project is tied to a specific instance of the getML engine running as a global process (independent from your python session). In this way, it is possible to share one getML instance with multiple users to work on different projects. When switching projects through getml.project.switch() , the python API spawns a new process and establishes a connection to this process, while the currently loaded project remains in memory and its process is delegated to the background (until you explicitly suspend() the project). You can also work on multiple projects simultaneously from different python sessions. This comes in particularly handy if you use Jupyter Lab to open multiple notebooks and manage multiple python kernels simultaneously. To load an existing project or create a new one, you can do so from the 'Projects' view in the monitor or use the API ( getml.engine.set_project() ). If you want to shut down the engine process associated with the current project, you can call getml.project.suspend() . When you suspend the project, the memory of the engine is flushed and all unsaved changes to the data frames are lost (see lifecycles and synchronization between engine and API for details). All pipelines of the new project are automatically loaded into memory. You can retrieve all of your project's pipelines through getml.project.pipelines . Projects can be deleted by clicking the trash can icon in the 'Projects' tab of the getML monitor or by calling getml.engine.delete_project() (to delete a project by name) or getml.project.delete() (to suspend and delete the project currently loaded).","title":"The relationship between projects and engine processes"},{"location":"user_guide/project_management/project_management/#managing-data-using-projects","text":"Every project has its own folder in ~/.getML/getml-VERSION/projects (for Linux and macOS) in which all of its data and pipelines are stored. On Windows, the projects folder is in the same location as getML.exe . These folders can be easily shared between different instances of getML; even between different operating systems. However, individual pipelines or data frames cannot be simply copied to another project folder \u2013 they are tied to the project. Projects can be bundled and exported/imported.","title":"Managing data using projects"},{"location":"user_guide/project_management/project_management/#using-the-project-module-to-manage-your-project","text":"The getml.project module is the entry point to your projects. From here, you can: query project-specific data ( getml.project.pipelines , getml.project.data_frames , getml.project.hyperopts ), manage the state of the current project ( getml.project.delete() , getml.project.restart() , getml.project.switch() , getml.project.suspend() ), and import projects from or export projects as a .getml bundle to disk ( getml.project.load() , getml.project.save() ).","title":"Using the project module to manage your project"}]}