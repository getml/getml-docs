{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"getML Documentation","text":"<p>Welcome to the getML technical documentation. This document is written for data scientists who want to use the getML software suite for their projects. For general information about getML visit getml.com. For a collection of demo notebooks, visit getml-demo. You can also contact us for any questions or inquiries.</p> <p>Note</p> <p>Some components of getML have been open sourced as part of getML community edition.  You may have a look at  community vs enterprise edition table to see the highlights of both the editions. </p>"},{"location":"#getml-in-one-minute","title":"getML in one minute","text":"<p>getML is an innovative tool for the end-to-end automation of data science projects. It covers everything from convenient data loading procedures  to the deployment of trained models. </p> <p>Most notably, getML includes advanced algorithms for automated feature engineering (feature learning) on relational data and time series. Feature engineering on relational data is defined as the creation of a  flat table by merging and aggregating data. It is sometimes also referred to as data wrangling. Feature engineering is necessary if your data is distributed over more than one data table.</p> <p>Automated feature engineering</p> <ul> <li>Saves up to 90% of the time spent on a data science project</li> <li>Increases the prediction accuracy over manual feature engineering </li> </ul> <p>Andrew Ng, Professor at Stanford University and Co-founder of Google Brain described manual feature engineering as follows:</p> <p>Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.</p> <p>The main purpose of getML is to automate this \"difficult, time-consuming\" process as much as possible.</p> <p>getML comes with a high-performance engine written in C++ and an intuitive Python API. Completing a data science project with getML consists of seven simple steps.</p> <pre><code>import getml\n\ngetml.engine.launch()\ngetml.engine.set_project('one_minute_to_getml')\n</code></pre> <ol> <li>Load the data into the engine</li> </ol> <p><pre><code>population = getml.data.DataFrame.from_csv('data_population.csv',\n            name='population_table')\nperipheral = getml.data.DataFrame.from_csv('data_peripheral.csv',\n            name='peripheral_table')\n</code></pre> 2. Annotate the data</p> <p><pre><code>population.set_role('target', getml.data.role.target)\npopulation.set_role('join_key', getml.data.role.join_key)\n...\n</code></pre> 3. Define the data model</p> <p><pre><code>dm = getml.data.DataModel(population.to_placeholder(\"POPULATION\"))\ndm.add(peripheral.to_placeholder(\"PERIPHERAL\"))\ndm.POPULATION.join(\n   dm.PERIPHERAL,\n   on=\"join_key\",\n)\n</code></pre> 4. Train the feature learning algorithm and the predictor</p> <p><pre><code>pipe = getml.pipeline.Pipeline(\n    data_model=dm,\n    feature_learners=getml.feature_learning.FastProp()\n    predictors=getml.predictors.LinearRegression()\n)\n\npipe.fit(\n    population=population,\n    peripheral=[peripheral]\n)\n</code></pre> 5. Evaluate</p> <p><pre><code>pipe.score(\n    population=population_unseen,\n    peripheral=[peripheral_unseen]\n)\n</code></pre> 6. Predict   </p> <p><pre><code>pipe.predict(\n    population=population_unseen,\n    peripheral=[peripheral_unseen]\n)\n</code></pre> 7. Deploy</p> <p><pre><code># Allow the pipeline to respond to HTTP requests\npipe.deploy(True)\n</code></pre> Check out the rest of this documentation to find out how getML achieves top performance on real-world data science projects with many tables and complex data schemes.</p>"},{"location":"#how-to-use-this-guide","title":"How to use this guide","text":"<p>If you want to get started with getML right away, we recommend to follow the installation instructions and then go through the getting started guide. </p> <p>If you are looking for more detailed information, other sections of this documentation are more suitable. There are three major parts: </p> <p>Tutorials</p> <p>The tutorials section contains examples of how to use getML in    real-world projects. All tutorials are based on public data sets    so that   you can follow along. If you are looking for an intuitive access to   getML, the tutorials section is the right place to go. Also, the   code examples are explicitly intended to be used as a template for   your own projects.  </p> <p>User guide</p> <p>The user guide explains all conceptional details behind getML in   depth. It can serve as a reference guide for experienced users but it's also   suitable for first day users who want to get a deeper understanding   of how getML works. Each chapter in the   user guide represents one step of a typical data science project.</p> <p>API documentation</p> <p>The API documentation covers everything related to the Python   interface to the getML engine. Each module comes with a dedicated   section that contains concrete code examples.</p> <p>You can also check out our other resources</p> <p>getML homepage</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>communication</li> <li>constants</li> <li>cross_validation</li> <li>data<ul> <li>access</li> <li>columns<ul> <li>aggregation</li> <li>collect_footer_data</li> <li>column</li> <li>columns</li> <li>constants</li> <li>format</li> <li>from_value</li> <li>get_scalar</li> <li>last_change</li> <li>last_change_from_col</li> <li>length</li> <li>length_property</li> <li>make_iter</li> <li>parse</li> <li>random</li> <li>repr</li> <li>repr_html</li> <li>subroles</li> <li>to_arrow</li> <li>to_numpy</li> <li>unique</li> <li>unit</li> </ul> </li> <li>concat</li> <li>container</li> <li>data_frame</li> <li>data_model</li> <li>diagram</li> <li>helpers</li> <li>helpers2</li> <li>load_container</li> <li>placeholder</li> <li>relationship</li> <li>roles</li> <li>roles_obj</li> <li>split<ul> <li>concat</li> <li>random</li> <li>time</li> </ul> </li> <li>staging</li> <li>star_schema</li> <li>subroles<ul> <li>exclude</li> <li>include</li> <li>only</li> </ul> </li> <li>subset</li> <li>time</li> <li>time_series</li> <li>view</li> </ul> </li> <li>database<ul> <li>connect_bigquery</li> <li>connect_greenplum</li> <li>connect_hana</li> <li>connect_mariadb</li> <li>connect_mysql</li> <li>connect_odbc</li> <li>connect_postgres</li> <li>connect_sqlite3</li> <li>connection</li> <li>copy_table</li> <li>drop_table</li> <li>execute</li> <li>get</li> <li>get_colnames</li> <li>helpers</li> <li>list_connections</li> <li>list_tables</li> <li>read_csv</li> <li>read_s3</li> <li>sniff_csv</li> <li>sniff_s3</li> </ul> </li> <li>datasets<ul> <li>base</li> <li>samples_generator</li> </ul> </li> <li>engine<ul> <li>helpers</li> <li>launch</li> </ul> </li> <li>feature_learning<ul> <li>aggregations</li> <li>fastboost</li> <li>fastprop</li> <li>feature_learner</li> <li>loss_functions</li> <li>multirel</li> <li>relboost</li> <li>relmt</li> <li>validation</li> </ul> </li> <li>helpers</li> <li>hyperopt<ul> <li>burn_in</li> <li>helpers</li> <li>hyperopt</li> <li>kernels</li> <li>load_hyperopt</li> <li>optimization</li> <li>tuning</li> <li>validation</li> </ul> </li> <li>log</li> <li>pipeline<ul> <li>column</li> <li>columns</li> <li>dialect</li> <li>feature</li> <li>features</li> <li>helpers</li> <li>helpers2</li> <li>issues</li> <li>metadata</li> <li>metrics</li> <li>pipeline</li> <li>plots</li> <li>score</li> <li>scores_container</li> <li>sql_code</li> <li>sql_string</li> <li>table</li> <li>tables</li> <li>tags</li> </ul> </li> <li>predictors<ul> <li>linear_regression</li> <li>logistic_regression</li> <li>predictor</li> <li>scale_gbm_classifier</li> <li>scale_gbm_regressor</li> <li>xgboost_classifier</li> <li>xgboost_regressor</li> </ul> </li> <li>preprocessors<ul> <li>category_trimmer</li> <li>email_domain</li> <li>imputation</li> <li>mapping</li> <li>preprocessor</li> <li>seasonal</li> <li>substring</li> <li>text_field_splitter</li> <li>validate</li> </ul> </li> <li>progress_bar</li> <li>project<ul> <li>attrs</li> <li>containers<ul> <li>data_frames</li> <li>hyperopts</li> <li>pipelines</li> </ul> </li> </ul> </li> <li>spark</li> <li>sqlite3<ul> <li>connect</li> <li>contains</li> <li>count_above_mean</li> <li>count_below_mean</li> <li>count_distinct_over_count</li> <li>email_domain</li> <li>ewma</li> <li>ewma_trend</li> <li>execute</li> <li>first</li> <li>get_word</li> <li>helpers</li> <li>kurtosis</li> <li>last</li> <li>median</li> <li>mode</li> <li>num_max</li> <li>num_min</li> <li>num_words</li> <li>quantiles</li> <li>read_csv</li> <li>read_list</li> <li>read_pandas</li> <li>skew</li> <li>sniff_csv</li> <li>sniff_pandas</li> <li>split_text_field</li> <li>stddev</li> <li>time_since_first_maximum</li> <li>time_since_first_minimum</li> <li>time_since_last_maximum</li> <li>time_since_last_minimum</li> <li>to_list</li> <li>to_pandas</li> <li>trend</li> <li>var</li> <li>variation_coefficient</li> </ul> </li> <li>utilities<ul> <li>formatting<ul> <li>cell_formatter</li> <li>column_formatter</li> <li>data_frame_formatter</li> <li>ellipsis</li> <li>formatter</li> <li>helpers</li> <li>signature_formatter</li> <li>view_formatter</li> </ul> </li> <li>templates</li> </ul> </li> <li>version</li> </ul>"},{"location":"reference/communication/","title":"Communication","text":"<p>Handles the communication for the getML library</p>"},{"location":"reference/communication/#getml.communication.port","title":"<code>port = 1708</code>  <code>module-attribute</code>","text":"<p>The port of the getML engine. The port is automatically set according to the process spawned when setting a project. The monitor automatically looks up a free port (starting from 1708). Setting the port here has no effect.</p>"},{"location":"reference/communication/#getml.communication.tcp_port","title":"<code>tcp_port = 1711</code>  <code>module-attribute</code>","text":"<p>The TCP port of the getML monitor.</p>"},{"location":"reference/communication/#getml.communication.engine_exception_handler","title":"<code>engine_exception_handler(msg, fallback='')</code>","text":"<p>Looks at the error message thrown by the engine and decides whether to throw a corresponding Exception using the same string or altering the message first.</p> <p>In either way, this function will always throw some sort of Exception.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Error message returned by the getML engine.</p> required <code>fallback</code> <code>str</code> <p>If not empty, the default Exception will carry this string.</p> <code>''</code> Source code in <code>getml/communication.py</code> <pre><code>def engine_exception_handler(msg: str, fallback=\"\"):\n    \"\"\"Looks at the error message thrown by the engine and decides whether\n    to throw a corresponding Exception using the same string or\n    altering the message first.\n\n    In either way, this function will always throw some sort of Exception.\n\n    Args:\n        msg (str): Error message returned by the getML engine.\n        fallback (str):\n            If not empty, the default Exception will carry this\n            string.\n    \"\"\"\n\n    if not fallback:\n        fallback = msg\n\n    raise IOError(fallback)\n</code></pre>"},{"location":"reference/communication/#getml.communication.is_monitor_alive","title":"<code>is_monitor_alive()</code>","text":"<p>Checks if the getML monitor is running.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the getML monitor is running and ready to accept commands and <code>False</code> otherwise.</p> Source code in <code>getml/communication.py</code> <pre><code>def is_monitor_alive() -&gt; bool:\n    \"\"\"Checks if the getML monitor is running.\n\n    Returns:\n        `True` if the getML monitor is running and ready to accept commands and `False` otherwise.\n    \"\"\"\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\n    try:\n        sock.connect((\"localhost\", tcp_port))\n    except ConnectionRefusedError:\n        return False\n\n    sock.close()\n\n    return True\n</code></pre>"},{"location":"reference/communication/#getml.communication.log","title":"<code>log(sock)</code>","text":"<p>Prints all the logs received by the socket.</p> Source code in <code>getml/communication.py</code> <pre><code>def log(sock: socket.socket):\n    \"\"\"\n    Prints all the logs received by the socket.\n    \"\"\"\n\n    pbar: Optional[_ProgressBar] = None\n\n    while True:\n        msg = recv_string(sock)\n\n        if msg[:5] != \"log: \":\n            if pbar is not None:\n                pbar.close()\n            return msg\n\n        msg = msg[5:]\n\n        if \"Progress: \" in msg:\n            msg = msg.split(\"Progress: \")[1].split(\"%\")[0]\n            progress = int(msg)\n            if pbar is not None:\n                pbar.show(progress)\n            continue\n\n        if pbar is not None:\n            pbar.close()\n        pbar = _ProgressBar(description=msg)\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_bytestring","title":"<code>recv_bytestring(sock)</code>","text":"<p>Receives a bytestring from the getml engine.</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_bytestring(sock: socket.socket):\n    \"\"\"\n    Receives a bytestring from the getml engine.\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    size_str = recv_data(sock, np.nbytes[np.uint64])\n\n    size = (\n        np.frombuffer(size_str, dtype=np.uint64).byteswap()[0]\n        if sys.byteorder == \"little\"\n        else np.frombuffer(size_str, dtype=np.uint64)[0]\n    )\n\n    return recv_data(sock, size)\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_data","title":"<code>recv_data(sock, size)</code>","text":"<p>Receives data (of any type) sent by the getml engine.</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_data(sock: socket.socket, size: Union[numbers.Real, int]):\n    \"\"\"Receives data (of any type) sent by the getml engine.\"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    if not isinstance(size, (numbers.Real, int)):\n        raise TypeError(\"'size' must be a real number.\")\n\n    data = []\n\n    bytes_received = np.uint64(0)\n\n    max_chunk_size = np.uint64(2048)\n\n    while bytes_received &lt; np.uint64(int(size)):\n        current_chunk_size = int(min(size - bytes_received, max_chunk_size))\n\n        chunk = sock.recv(current_chunk_size)\n\n        if not chunk:\n            raise IOError(\n                \"\"\"The getML engine died unexpectedly.\n                    If this wasn't done on purpose, please get in contact\n                    with our support or file a bug report.\"\"\"\n            )\n\n        data.append(chunk)\n\n        bytes_received += np.uint64(len(chunk))\n\n    return \"\".encode().join(data)\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_float_matrix","title":"<code>recv_float_matrix(sock)</code>","text":"<p>Receives a matrix (type np.float64) from the getml engine.</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_float_matrix(sock: socket.socket):\n    \"\"\"\n    Receives a matrix (type np.float64) from the getml engine.\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    # By default, numeric data sent over the socket is big endian,\n    # also referred to as network-byte-order!\n    if sys.byteorder == \"little\":\n        shape_str = recv_data(sock, np.nbytes[np.int32] * 2)\n\n        shape = np.frombuffer(shape_str, dtype=np.int32).byteswap().astype(np.uint64)\n\n        size = shape[0] * shape[1] * np.uint64(np.nbytes[np.float64])\n\n        matrix = recv_data(sock, size)\n\n        matrix = np.frombuffer(matrix, dtype=np.float64).byteswap()\n\n        matrix = matrix.reshape(shape[0], shape[1])\n\n    else:\n        shape_str = recv_data(sock, np.nbytes[np.int32] * 2)\n\n        shape = np.frombuffer(shape_str, dtype=np.int32).astype(np.uint64)\n\n        size = shape[0] * shape[1] * np.uint64(np.nbytes[np.float64])\n\n        matrix = recv_data(sock, size)\n\n        matrix = np.frombuffer(matrix, dtype=np.float64)\n\n        matrix = matrix.reshape(shape[0], shape[1])\n\n    return matrix\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_issues","title":"<code>recv_issues(sock)</code>","text":"<p>Receives a set of warnings to raise from the getml engine.</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_issues(sock: socket.socket) -&gt; List[_Issue]:\n    \"\"\"\n    Receives a set of warnings to raise from the getml engine.\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    json_str = recv_string(sock)\n\n    if json_str[0] != \"{\":\n        raise ValueError(json_str)\n\n    json_obj = json.loads(json_str)\n\n    all_warnings = json_obj[\"warnings_\"]\n\n    return [\n        _Issue(\n            message=w[\"message_\"], label=w[\"label_\"], warning_type=w[\"warning_type_\"]\n        )\n        for w in all_warnings\n    ]\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_string","title":"<code>recv_string(sock)</code>","text":"<p>Receives a string from the getml engine (an actual string, not a bytestring).</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_string(sock: socket.socket):\n    \"\"\"\n    Receives a string from the getml engine\n    (an actual string, not a bytestring).\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    # By default, numeric data sent over the socket is big endian,\n    # also referred to as network-byte-order!\n    if sys.byteorder == \"little\":\n        size_str = recv_data(sock, np.nbytes[np.int32])\n\n        size = np.frombuffer(size_str, dtype=np.int32).byteswap()[0]\n\n    else:\n        size_str = recv_data(sock, np.nbytes[np.int32])\n\n        size = np.frombuffer(size_str, dtype=np.int32)[0]\n\n    data = recv_data(sock, size)\n\n    return data.decode(errors=\"ignore\")\n</code></pre>"},{"location":"reference/communication/#getml.communication.recv_string_column","title":"<code>recv_string_column(sock)</code>","text":"<p>Receives a column of type string from the getml engine</p> Source code in <code>getml/communication.py</code> <pre><code>def recv_string_column(sock: socket.socket):\n    \"\"\"\n    Receives a column of type string from the getml engine\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    nbytes_str = recv_data(sock, np.nbytes[np.uint64])\n\n    if sys.byteorder == \"little\":\n        nbytes = np.frombuffer(nbytes_str, dtype=np.uint64).byteswap()[0]\n    else:\n        nbytes = np.frombuffer(nbytes_str, dtype=np.uint64)[0]\n\n    col = (\n        recv_data(sock, nbytes).decode(errors=\"ignore\").split(GETML_SEP)\n        if nbytes != 0\n        else []\n    )\n\n    col = np.asarray(col, dtype=object)\n\n    return col.reshape(len(col), 1)\n</code></pre>"},{"location":"reference/communication/#getml.communication.send","title":"<code>send(cmd)</code>","text":"<p>Sends a command to the getml engine and closes the established connection.</p> <p>Creates a socket and sends a command to the getML engine using the module-wide variable <code>port</code>.</p> <p>A message (string) from the <code>socket.socket</code> will be received using <code>recv_string</code> and the socket will be closed. If the message is \"Success!\", everything works properly. Else, an Exception will be thrown containing the message.</p> <p>In case another message is supposed to be sent by the engine, <code>send_and_get_socket</code> has to be used and the calling function must handle the message itself!</p> <p>Please be very careful when altering the routing/calling behavior of the socket communication! The engine is quite sensitive and might freeze.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>dict</code> <p>A dictionary specifying the command the engine is supposed to execute. It must contain at least two string values with the corresponding keys being named \"name_\" and \"type_\".</p> required Source code in <code>getml/communication.py</code> <pre><code>def send(cmd: Dict[str, Any]):\n    \"\"\"Sends a command to the getml engine and closes the established\n    connection.\n\n    Creates a socket and sends a command to the getML engine using the\n    module-wide variable [`port`][getml.communication.port].\n\n    A message (string) from the `socket.socket` will be\n    received using [`recv_string`][getml.communication.recv_string] and the socket will\n    be closed. If the message is \"Success!\", everything works\n    properly. Else, an Exception will be thrown containing the\n    message.\n\n    In case another message is supposed to be sent by the engine,\n    [`send_and_get_socket`][getml.communication.send_and_get_socket] has to be used\n    and the calling function must handle the message itself!\n\n    Please be very careful when altering the routing/calling behavior\n    of the socket communication! The engine is quite sensitive and might freeze.\n\n    Args:\n        cmd (dict): A dictionary specifying the command the engine is\n            supposed to execute. It _must_ contain at least two string\n            values with the corresponding keys being named \"name_\" and\n            \"type_\".\n    \"\"\"\n\n    if not isinstance(cmd, dict):\n        raise TypeError(\"'cmd' must be a dict.\")\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", port))\n    except ConnectionRefusedError:\n        raise ConnectionRefusedError(_make_error_msg())\n\n    # Use a custom encoder which knows how to handle the classes\n    # defined in the getml package.\n    msg = json.dumps(cmd, cls=_GetmlEncoder)\n\n    send_string(sock, msg)\n\n    # The calling function does not want to further use the\n    # socket. Therefore, it will be checked here whether the\n    # request was successful and closed.\n    msg = recv_string(sock)\n\n    sock.close()\n\n    if msg != \"Success!\":\n        engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/communication/#getml.communication.send_and_get_socket","title":"<code>send_and_get_socket(cmd)</code>","text":"<p>Sends a command to the getml engine and returns the established connection.</p> <p>Creates a socket and sends a command to the getML engine using the module-wide variable <code>port</code>.</p> <p>The function will return the socket it opened and the calling function is free to receive whatever data is desires over it. But the latter also has to take care of closing the socket afterwards itself!</p> <p>Please be very careful when altering the routing/calling behavior of the socket communication! The engine is quite sensitive and might freeze. Especially implemented handling of socket sessions (their passing from function to function) must not be altered or separated in distinct calls to the <code>send</code> function! Some commands have to be sent via the same socket or the engine will not be able to handle them and might block.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>dict</code> <p>A dictionary specifying the command the engine is supposed to execute. It must contain at least two string values with the corresponding keys being named \"name_\" and \"type_\".\"</p> required <p>Returns:</p> Type Description <code>socket</code> <p>A socket which, using the Python API, can communicate with the getML engine.</p> Source code in <code>getml/communication.py</code> <pre><code>def send_and_get_socket(cmd: Dict[str, Any]) -&gt; socket.socket:\n    \"\"\"Sends a command to the getml engine and returns the established\n    connection.\n\n    Creates a socket and sends a command to the getML engine using the\n    module-wide variable [`port`][getml.communication.port].\n\n    The function will return the socket it opened and the calling\n    function is free to receive whatever data is desires over it. But\n    the latter also has to take care of closing the socket afterwards\n    itself!\n\n    Please be very careful when altering the routing/calling behavior\n    of the socket communication! The engine is quite sensitive and might freeze.\n    Especially implemented handling of\n    socket sessions (their passing from function to function) must not\n    be altered or separated in distinct calls to the\n    [`send`][getml.communication.send] function! Some commands have\n    to be sent via the same socket or the engine will not be able to\n    handle them and might block.\n\n    Args:\n        cmd (dict): A dictionary specifying the command the engine is\n            supposed to execute. It _must_ contain at least two string\n            values with the corresponding keys being named \"name_\" and\n            \"type_\".\"\n\n    Returns:\n        A socket which, using the Python API, can communicate with the getML engine.\n    \"\"\"\n\n    if not isinstance(cmd, dict):\n        raise TypeError(\"'cmd' must be a dict.\")\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", port))\n    except ConnectionRefusedError:\n        raise ConnectionRefusedError(_make_error_msg())\n\n    # Use a custom encoder which knows how to handle the classes\n    # defined in the getml package.\n    msg = json.dumps(cmd, cls=_GetmlEncoder)\n\n    send_string(sock, msg)\n\n    return sock\n</code></pre>"},{"location":"reference/communication/#getml.communication.send_string","title":"<code>send_string(sock, string)</code>","text":"<p>Sends a string to the getml engine (an actual string, not a bytestring).</p> Source code in <code>getml/communication.py</code> <pre><code>def send_string(sock: socket.socket, string: str):\n    \"\"\"\n    Sends a string to the getml engine\n    (an actual string, not a bytestring).\n    \"\"\"\n\n    if not isinstance(sock, socket.socket):\n        raise TypeError(\"'sock' must be a socket.\")\n\n    if not isinstance(string, str):\n        raise TypeError(\"'string' must be a str.\")\n\n    encoded = string.encode(\"utf-8\")\n\n    size = len(encoded)\n\n    # By default, numeric data sent over the socket is big endian,\n    # also referred to as network-byte-order!\n    if sys.byteorder == \"little\":\n        sock.sendall(np.asarray([size]).astype(np.int32).byteswap().tobytes())\n    else:\n        sock.sendall(np.asarray([size]).astype(np.int32).tobytes())\n\n    sock.sendall(encoded)\n</code></pre>"},{"location":"reference/constants/","title":"Constants","text":"<p>Contains various constants for getML</p>"},{"location":"reference/constants/#getml.constants.COMPARISON_ONLY","title":"<code>COMPARISON_ONLY = ', comparison only'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.JOIN_KEY_SEP","title":"<code>JOIN_KEY_SEP = '$GETML_JOIN_KEY_SEP'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.MULTIPLE_JOIN_KEYS_BEGIN","title":"<code>MULTIPLE_JOIN_KEYS_BEGIN = '$GETML_MULTIPLE_JOIN_KEYS_BEGIN'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.MULTIPLE_JOIN_KEYS_END","title":"<code>MULTIPLE_JOIN_KEYS_END = '$GETML_MULTIPLE_JOIN_KEYS_END'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.NO_JOIN_KEY","title":"<code>NO_JOIN_KEY = '$GETML_NO_JOIN_KEY'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.ROWID","title":"<code>ROWID = 'rowid'</code>  <code>module-attribute</code>","text":""},{"location":"reference/constants/#getml.constants.TIME_FORMATS","title":"<code>TIME_FORMATS = ['%Y-%m-%dT%H:%M:%s%z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Y-%m-%dT%H:%M:%S.%F', '%Y-%m-%dT%H:%M:%S.%i', '%Y-%m-%dT%H:%M:%S.%c', '%Y-%m-%d %H:%M:%S.%F', '%Y-%m-%d %H:%M:%S.%i', '%Y-%m-%d %H:%M:%S.%c', '%Y-%m-%d %H:%M:%s%z']</code>  <code>module-attribute</code>","text":"<p>The default time stamp formats to be used.</p> <p>Whenever a time stamp is parsed from a string, the getML engine tries different time stamp formats.</p> <p>The procedure works as follows: The engine tries to parse the string using the first format. If that fails, it will use the second format etc.</p> <p>If none of the time stamp formats work, then it tries to parse the string as a floating point value, which it will interpret as the number of seconds since UNIX time (January 1, 1970).</p> <p>If that fails as well, the value is set to NULL.</p>"},{"location":"reference/constants/#getml.constants.TIME_STAMP","title":"<code>TIME_STAMP = 'time stamp'</code>  <code>module-attribute</code>","text":""},{"location":"reference/cross_validation/","title":"Cross validation","text":"<p>Conducts a cross validation.</p>"},{"location":"reference/cross_validation/#getml.cross_validation.cross_validation","title":"<code>cross_validation(pipeline, population, peripheral, n_folds=10, seed=5849)</code>","text":"<p>Conducts a cross validation.</p> Source code in <code>getml/cross_validation.py</code> <pre><code>def cross_validation(pipeline, population, peripheral, n_folds=10, seed=5849):\n    \"\"\"\n    Conducts a cross validation.\n    \"\"\"\n\n    if not isinstance(pipeline, Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline.\")\n\n    if not isinstance(population, (DataFrame, View)):\n        raise TypeError(\"'pipeline' must be a pipeline.\")\n\n    if not isinstance(n_folds, int):\n        raise TypeError(\"'n_folds' must be an integer.\")\n\n    if n_folds &lt; 2:\n        raise ValueError(\"'n_folds' must be &gt;= 2.\")\n\n    # peripheral will be typechecked by pipeline .fit(...).\n    # No need to duplicate code.\n\n    kwargs = {\n        \"fold \" + str(i + 1).zfill(len(str(n_folds))): (1.0 / float(n_folds))\n        for i in range(n_folds)\n    }\n\n    split = random(seed=seed, train=0.0, test=0.0, validation=0.0, **kwargs)\n\n    scores_objects = []\n\n    for fold in list(kwargs.keys()):\n        print(fold + \":\")\n        print()\n        train_set = population[split != fold].with_name(\n            \"cross validation: train - \" + fold\n        )\n        validation_set = population[split == fold].with_name(\n            \"cross validation: validation - \" + fold\n        )\n        pipe = deepcopy(pipeline)\n        pipe.tags += [\"cross validation\", fold]\n        pipe.fit(train_set, peripheral)\n        scores_objects.append(pipe.score(validation_set, peripheral))\n        print()\n\n    scores_data = [d for obj in scores_objects for d in obj.data]\n\n    return Scores(data=scores_data, latest=scores_objects[-1]._latest).sort(\n        key=lambda score: score.set_used, descending=False\n    )\n</code></pre>"},{"location":"reference/helpers/","title":"Helpers","text":"<p>Collection of helper classes that are relevant for many submodules.</p>"},{"location":"reference/log/","title":"Log","text":"<p>Determines the logging level of the getML library.</p> <p>Logs generated by getML are either on the level WARNING or INFO.</p>"},{"location":"reference/progress_bar/","title":"Progress bar","text":"<p>Displays progress in bar form.</p>"},{"location":"reference/spark/","title":"Spark","text":"<p>This module is useful for productionizing pipelines on Apache Spark.</p>"},{"location":"reference/spark/#getml.spark.execute","title":"<code>execute(spark, fname)</code>","text":"<p>Executes an SQL script or several SQL scripts on Spark.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The spark session.</p> required <code>fname</code> <code>str</code> <p>The names of the SQL script or a folder containing SQL scripts. If you decide to pass a folder, the SQL scripts must have the ending '.sql'.</p> required Source code in <code>getml/spark.py</code> <pre><code>def execute(spark, fname):\n    \"\"\"\n    Executes an SQL script or several SQL scripts on Spark.\n\n    Args:\n        spark (pyspark.sql.session.SparkSession):\n            The spark session.\n\n        fname (str):\n            The names of the SQL script or a folder containing SQL scripts.\n            If you decide to pass a folder, the SQL scripts must have the ending '.sql'.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    # ------------------------------------------------------------\n\n    if os.path.isdir(fname):\n        scripts = _retrieve_scripts(fname, \".sql\")\n        for script in scripts:\n            execute(spark, script)\n        return\n\n    # ------------------------------------------------------------\n\n    _log(\"Executing \" + fname + \"...\")\n\n    with open(fname, \"rt\", encoding=\"utf-8\") as sql_files:\n        queries = sql_files.read().split(\";\")\n\n    for query in queries:\n        if query.strip():\n            spark.sql(query)\n</code></pre>"},{"location":"reference/version/","title":"Version","text":""},{"location":"reference/data/__init__/","title":"init","text":"<p>Contains functionalities for importing, handling, and retrieving data from the getML engine.</p> <p>All data relevant for the getML suite has to be present in the getML engine. Its Python API itself does not store any of the data used for training or prediction. Instead, it provides a handler class for the data frame objects in the getML engine, the <code>DataFrame</code>. Either using this overall handler for the underlying data set or the individual <code>columns</code> it is composed of, one can both import and retrieve data from the engine as well as performing operations on them. In addition to the data frame objects, the engine also uses an abstract and lightweight version of the underlying data model, which is represented by the <code>Placeholder</code>.</p> <p>In general, working with data within the getML suite is organized in three different steps.</p> <ul> <li>Importing the data into the getML engine .</li> <li>Annotating the data by assigning   <code>roles</code> to the individual <code>columns</code></li> <li>Constructing the data model by deriving   <code>Placeholder</code> from the data and joining them to   represent the data schema.</li> </ul> Example <p>Creating a new data frame object in the getML engine and importing data is done by one of the class methods <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p>In this example we chose to directly load data from a public database in the internet. But, firstly, we have to connect the getML engine to the database (see MySQL interface in the user guide for further details).</p> <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"financial\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\",\n    time_formats=['%Y/%m/%d']\n)\n</code></pre> <p>Using the established connection, we can tell the engine to construct a new data frame object called <code>df_loan</code>, fill it with the data of <code>loan</code> table contained in the MySQL database, and return a <code>DataFrame</code> handler associated with it.</p> <p><pre><code>loan = getml.DataFrame.from_db('loan', 'df_loan')\n\nprint(loan)\n</code></pre> <pre><code>| loan_id      | account_id   | amount       | duration     | date          | payments      | status        |\n| unused float | unused float | unused float | unused float | unused string | unused string | unused string |\n-------------------------------------------------------------------------------------------------------------\n| 4959         | 2            | 80952        | 24           | 1994-01-05    | 3373.00       | A             |\n| 4961         | 19           | 30276        | 12           | 1996-04-29    | 2523.00       | B             |\n| 4962         | 25           | 30276        | 12           | 1997-12-08    | 2523.00       | A             |\n| 4967         | 37           | 318480       | 60           | 1998-10-14    | 5308.00       | D             |\n| 4968         | 38           | 110736       | 48           | 1998-04-19    | 2307.00       | C             |\n</code></pre> In order to construct the data model and for the feature learning algorithm to get the most out of your data, you have to assign roles to columns using the <code>set_role</code> method (see Annotating data for details).</p> <p>(For demonstration purposes, we assign <code>payments</code> the <code>target role</code>. In reality, you would want to forecast the defaulting behaviour, which is encoded in the <code>status</code> column. See the loans notebook.)</p> <p><pre><code>loan.set_role([\"duration\", \"amount\"], getml.data.roles.numerical)\nloan.set_role([\"loan_id\", \"account_id\"], getml.data.roles.join_key)\nloan.set_role(\"date\", getml.data.roles.time_stamp)\nloan.set_role([\"payments\"], getml.data.roles.target)\n\nprint(loan)\n</code></pre> <pre><code>| date                        | loan_id  | account_id | payments  | duration  | amount    | status        |\n| time stamp                  | join key | join key   | target    | numerical | numerical | unused string |\n-----------------------------------------------------------------------------------------------------------\n| 1994-01-05T00:00:00.000000Z | 4959     | 2          | 3373      | 24        | 80952     | A             |\n| 1996-04-29T00:00:00.000000Z | 4961     | 19         | 2523      | 12        | 30276     | B             |\n| 1997-12-08T00:00:00.000000Z | 4962     | 25         | 2523      | 12        | 30276     | A             |\n| 1998-10-14T00:00:00.000000Z | 4967     | 37         | 5308      | 60        | 318480    | D             |\n| 1998-04-19T00:00:00.000000Z | 4968     | 38         | 2307      | 48        | 110736    | C             |\n</code></pre> Finally, we are able to construct the data model by deriving <code>Placeholder</code> from each <code>DataFrame</code> and establishing relations between them using the <code>join</code> method.</p> <pre><code># But, first, we need a second data set to build a data model.\ntrans = getml.DataFrame.from_db(\n    'trans', 'df_trans',\n    roles = {getml.data.roles.numerical: [\"amount\", \"balance\"],\n             getml.data.roles.categorical: [\"type\", \"bank\", \"k_symbol\",\n                                            \"account\", \"operation\"],\n             getml.data.roles.join_key: [\"account_id\"],\n             getml.data.roles.time_stamp: [\"date\"]\n    }\n)\n\nph_loan = loan.to_placeholder()\nph_trans = trans.to_placeholder()\n\nph_loan.join(ph_trans, on=\"account_id\",\n            time_stamps=\"date\")\n</code></pre> <p>The data model contained in <code>ph_loan</code> can now be used to construct a <code>Pipeline</code>.</p>"},{"location":"reference/data/__init__/#getml.data.Container","title":"<code>Container</code>","text":"<p>A container holds the actual data in the form of a <code>DataFrame</code> or a <code>View</code>.</p> <p>The purpose of a container is twofold:</p> <ul> <li> <p>Assigning concrete data to an abstract <code>DataModel</code>.</p> </li> <li> <p>Storing data and allowing you to reproduce previous results.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code>s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>train</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>validation</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>test</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>kwargs</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in <code>Subset</code>s other than the predefined train, validation and test subsets. You can call these subsets anything you want to, and you can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\ncontainer = getml.data.Container(my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(container.my_subset)\n</code></pre></p> <code>{}</code> Example <p>A <code>DataModel</code> only contains abstract data. When we fit a pipeline, we need to assign concrete data.</p> <p>This example is taken from the loans notebook . Note that in the notebook the high level <code>StarSchema</code> implementation is used. For demonstration purposes we are proceeding now with the low level implementation.</p> <p><pre><code># The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# We now have abstract placeholders on something\n# called \"population\", \"meta\", \"order\" and \"trans\".\n# But how do we assign concrete data? By using\n# a container.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. Their aliases need\n# to match the names of the placeholders in the\n# data model.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# When we call 'train', the container\n# will return the train set and the\n# peripheral tables.\nmy_pipeline.fit(container.train)\n\n# Same for 'test'\nmy_pipeline.score(container.test)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\ncontainer = getml.data.Container(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# container.train and container.test\n# work just like above.\n</code></pre> <p>Containers can also be used for storage and reproducing your results. A recommended pattern is to assign 'baseline roles' to your data frames and then using a <code>View</code> to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which container you have used.</p> Source code in <code>getml/data/container.py</code> <pre><code>class Container:\n    \"\"\"\n    A container holds the actual data in the form of a [`DataFrame`][getml.DataFrame] or a [`View`][getml.data.View].\n\n    The purpose of a container is twofold:\n\n    - Assigning concrete data to an abstract [`DataModel`][getml.data.DataModel].\n\n    - Storing data and allowing you to reproduce previous results.\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`add`][getml.data.Container.add].\n\n        split ([`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset]s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        train ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *train*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        validation ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *validation*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        test ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *test*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        kwargs ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in [`Subset`][getml.data.Subset]s\n            other than the predefined *train*, *validation* and *test* subsets.\n            You can call these subsets anything you want to, and you can access them\n            just like *train*, *validation* and *test*.\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n            Example:\n                ```python\n                # Pass the subset.\n                container = getml.data.Container(my_subset=my_data_frame)\n\n                # You can access the subset just like train,\n                # validation or test\n                my_pipeline.fit(container.my_subset)\n                ```\n\n    Example:\n        A [`DataModel`][getml.data.DataModel] only contains abstract data. When we\n        fit a pipeline, we need to assign concrete data.\n\n        This example is taken from the\n        [loans notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n        Note that in the notebook the high level [`StarSchema`][getml.data.StarSchema] implementation is used. For\n        demonstration purposes we are proceeding now with the low level implementation.\n\n        ```python\n        # The abstract data model is constructed\n        # using the DataModel class. A data model\n        # does not contain any actual data. It just\n        # defines the abstract relational structure.\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        dm.add(getml.data.to_placeholder(\n            meta=meta,\n            order=order,\n            trans=trans)\n        )\n\n        dm.population.join(\n            dm.trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        dm.population.join(\n            dm.order,\n            on=\"account_id\",\n        )\n\n        dm.population.join(\n            dm.meta,\n            on=\"account_id\",\n        )\n\n        # We now have abstract placeholders on something\n        # called \"population\", \"meta\", \"order\" and \"trans\".\n        # But how do we assign concrete data? By using\n        # a container.\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views. Their aliases need\n        # to match the names of the placeholders in the\n        # data model.\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # Freezing makes the container immutable.\n        # This is not required, but often a good idea.\n        container.freeze()\n\n        # When we call 'train', the container\n        # will return the train set and the\n        # peripheral tables.\n        my_pipeline.fit(container.train)\n\n        # Same for 'test'\n        my_pipeline.score(container.test)\n        ```\n        If you don't already have a train and test set,\n        you can use a function from the\n        [`split`][getml.data.split] module.\n\n        ```python\n\n        split = getml.data.split.random(\n            train=0.8, test=0.2)\n\n        container = getml.data.Container(\n            population=population_all,\n            split=split,\n        )\n\n        # The remaining code is the same as in\n        # the example above. In particular,\n        # container.train and container.test\n        # work just like above.\n        ```\n\n        Containers can also be used for storage and reproducing your\n        results.\n        A recommended pattern is to assign 'baseline roles' to your data frames\n        and then using a [`View`][getml.data.View] to tweak them:\n\n        ```python\n\n        # Assign baseline roles\n        data_frame.set_role([\"jk\"], getml.data.roles.join_key)\n        data_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        data_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\n        data_frame.set_role([\"col5\"], getml.data.roles.target)\n\n        # Make the data frame immutable, so in-place operations are\n        # no longer possible.\n        data_frame.freeze()\n\n        # Save the data frame.\n        data_frame.save()\n\n        # I suspect that col1 leads to overfitting, so I will drop it.\n        view = data_frame.drop([\"col1\"])\n\n        # Insert the view into a container.\n        container = getml.data.Container(...)\n        container.add(some_alias=view)\n        container.save()\n        ```\n\n        The advantage of using such a pattern is that it enables you to\n        always completely retrace your entire pipeline without creating\n        deep copies of the data frames whenever you have made a small\n        change like the one in our example. Note that the pipeline will\n        record which container you have used.\n    \"\"\"\n\n    def __init__(\n        self,\n        population=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        train=None,\n        validation=None,\n        test=None,\n        **kwargs,\n    ):\n\n        if population is not None and not isinstance(population, (DataFrame, View)):\n            raise TypeError(\n                \"'population' must be a getml.DataFrame or a getml.data.View, got \"\n                + type(population).__name__\n                + \".\"\n            )\n\n        if peripheral is not None and not _is_typed_dict(\n            peripheral, str, [DataFrame, View]\n        ):\n            raise TypeError(\n                \"'peripheral' must be a dict \"\n                + \"of getml.DataFrames or getml.data.Views.\"\n            )\n\n        if split is not None and not isinstance(\n            split, (StringColumn, StringColumnView)\n        ):\n            raise TypeError(\n                \"'split' must be StringColumn or a StringColumnView, got \"\n                + type(split).__name__\n                + \".\"\n            )\n\n        if not isinstance(deep_copy, bool):\n            raise TypeError(\n                \"'deep_copy' must be a bool, got \" + type(split).__name__ + \".\"\n            )\n\n        exclusive = (population is not None) ^ (\n            len(_make_subsets_from_kwargs(train, validation, test, **kwargs)) != 0\n        )\n\n        if not exclusive:\n            raise ValueError(\n                \"'population' and 'train', 'validation', 'test' as well as \"\n                + \"other subsets signified by kwargs are mutually exclusive. \"\n                + \"You have to pass \"\n                + \"either 'population' or some subsets, but you cannot pass both.\"\n            )\n\n        if population is None and split is not None:\n            raise ValueError(\n                \"'split's are used for splitting population DataFrames.\"\n                \"Hence, if you supply 'split', you also have to supply \"\n                \"a population.\"\n            )\n\n        if population is not None and split is None:\n            logger.warning(\n                \"You have passed a population table without passing 'split'. \"\n                \"You can access the entire set to pass to your pipeline \"\n                \"using the .full attribute.\"\n            )\n            split = from_value(\"full\")\n\n        self._id = _make_id()\n\n        self._population = population\n        self._peripheral = peripheral or {}\n        self._split = split\n        self._deep_copy = deep_copy\n\n        self._subsets = (\n            _make_subsets_from_split(population, split)\n            if split is not None\n            else _make_subsets_from_kwargs(train, validation, test, **kwargs)\n        )\n\n        if split is None and not _is_typed_dict(self._subsets, str, [DataFrame, View]):\n            raise TypeError(\n                \"'train', 'validation', 'test' and all other subsets must be either a \"\n                \"getml.DataFrame or a getml.data.View.\"\n            )\n\n        if deep_copy:\n            self._population = _deep_copy(self._population, self._id)\n            self._peripheral = {\n                k: _deep_copy(v, self._id) for (k, v) in self._peripheral.items()\n            }\n            self._subsets = {\n                k: _deep_copy(v, self._id) for (k, v) in self._subsets.items()\n            }\n\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n        self._frozen_time = None\n\n    def __dir__(self):\n        attrs = dir(type(self)) + self._ipython_key_completion()\n        attrs = [x for x in attrs if x.isidentifier()]\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        if \"_\" + key in self.__dict__:\n            return self.__dict__[\"_\" + key]\n\n        if key in self.__dict__[\"_subsets\"]:\n            if self.__dict__[\"_deep_copy\"] and self._frozen_time is None:\n                raise ValueError(\n                    cleandoc(\n                        f\"\"\"\n                        If you set deep_copy=True, you must call\n                        {type(self).__name__}.freeze() before you can extract data. The\n                        idea of deep_copy is to ensure that you can always retrace and\n                        reproduce your results. That is why the container\n                        needs to be immutable before it can be used.\n                        \"\"\"\n                    )\n                )\n\n            last_change = _get_last_change(\n                self.__dict__[\"_population\"],\n                self.__dict__[\"_peripheral\"],\n                self.__dict__[\"_subsets\"],\n            )\n\n            if self.__dict__[\"_last_change\"] != last_change:\n                logger.warning(\n                    cleandoc(\n                        f\"\"\"\n                        Some of the data frames added to the {type(self).__name__} have\n                        been modified after they were added.  This might lead to\n                        unexpected results. To avoid these sort of problems, you can set\n                        deep_copy=True when creating the {type(self).__name__}.\n                        \"\"\"\n                    )\n                )\n\n            return Subset(\n                container_id=self.__dict__[\"_id\"],\n                population=self.__dict__[\"_subsets\"][key].with_name(key),\n                peripheral=self.__dict__[\"_peripheral\"],\n            )\n\n        if key in self.__dict__[\"_peripheral\"]:\n            return self.__dict__[\"_peripheral\"][key]\n\n        raise KeyError(f\"{type(self).__name__} holds no data with name {key!r}.\")\n\n    def __repr__(self):\n        pop, perph = self._format()\n\n        template = cleandoc(\n            \"\"\"\n            population\n            {pop}\n\n            peripheral\n            {perph}\n            \"\"\"\n        )\n\n        return template.format(pop=pop._render_string(), perph=perph._render_string())\n\n    def _repr_html_(self):\n        pop, perph = self._format()\n\n        template = cleandoc(\n            \"\"\"\n            &lt;div style='margin-top: 15px;'&gt;\n            &lt;div style='float: left; margin-right: 50px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;population&lt;/div&gt;\n                {pop}\n            &lt;/div&gt;\n            &lt;div style='float: left;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;peripheral&lt;/div&gt;\n                {perph}\n            &lt;/div&gt;\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return template.format(pop=pop._render_html(), perph=perph._render_html())\n\n    def _format(self):\n        headers_pop = [[\"subset\", \"name\", \"rows\", \"type\"]]\n        rows_pop = [\n            [key, subset.name, subset.nrows(), type(subset).__name__]\n            for key, subset in self.subsets.items()  # pytype: disable=attribute-error\n        ]\n\n        headers_perph = [[\"name\", \"rows\", \"type\"]]\n\n        rows_perph = [\n            [perph.name, perph.nrows(), type(perph).__name__]\n            for perph in self.peripheral.values()  # pytype: disable=attribute-error\n        ]\n\n        names = [\n            perph.name\n            for perph in self.peripheral.values()  # pytype: disable=attribute-error\n        ]\n        aliases = list(self.peripheral.keys())  # pytype: disable=attribute-error\n\n        if any(alias not in names for alias in aliases):\n            headers_perph[0].insert(0, \"alias\")\n\n            for alias, row in zip(aliases, rows_perph):\n                row.insert(0, alias)\n\n        return _Formatter(headers=headers_pop, rows=rows_pop), _Formatter(\n            headers=headers_perph, rows=rows_perph\n        )\n\n    def _getml_deserialize(self):\n        cmd = {k[1:] + \"_\": v for (k, v) in self.__dict__.items() if v is not None}\n\n        if self._population is not None:\n            cmd[\"population_\"] = self._population._getml_deserialize()\n\n        if self._split is not None:\n            cmd[\n                \"split_\"\n            ] = self._split._getml_deserialize()  # pytype: disable=attribute-error\n\n        cmd[\"peripheral_\"] = {\n            k: v._getml_deserialize() for (k, v) in self._peripheral.items()\n        }\n\n        cmd[\"subsets_\"] = {\n            k: v._getml_deserialize() for (k, v) in self._subsets.items()\n        }\n\n        return cmd\n\n    def _ipython_key_completion(self):\n        attrs = [v[1:] for v in list(vars(self))]\n        attrs.extend(self._peripheral)\n        if not self._deep_copy or self._frozen_time is not None:\n            attrs.extend(self._subsets)\n        return attrs\n\n    def __setattr__(self, name, value):\n        if not name or name[0] != \"_\":\n            raise ValueError(\"Attempting a write operation on read-only data.\")\n        vars(self)[name] = value\n\n    def add(self, *args, **kwargs):\n        \"\"\"\n        Adds new peripheral data frames or views.\n        \"\"\"\n        wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n        if wrong_type:\n            raise TypeError(\n                \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n            )\n\n        wrong_type = [\n            k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n        ]\n\n        if wrong_type:\n            raise TypeError(\n                \"You must pass getml.DataFrames or getml.data.Views, \"\n                f\"but the following arguments were neither: {wrong_type!r}.\"\n            )\n\n        kwargs = {**{item.name: item for item in args}, **kwargs}\n\n        if self._frozen_time is not None:\n            raise ValueError(\n                f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n            )\n\n        if self._deep_copy:\n            kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n        self._peripheral = {**self._peripheral, **kwargs}\n\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n    def freeze(self):\n        \"\"\"\n        Freezes the container, so that changes are no longer possible.\n\n        This is required before you can extract data when `deep_copy=True`. The idea of\n        `deep_copy` is to ensure that you can always retrace and reproduce your results.\n        That is why the container needs to be immutable before it can be\n        used.\n        \"\"\"\n        self.sync()\n        self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    def save(self):\n        \"\"\"\n        Saves the Container to disk.\n        \"\"\"\n\n        cmd = dict()\n        cmd[\"type_\"] = \"DataContainer.save\"\n        cmd[\"name_\"] = self._id\n\n        cmd[\"container_\"] = self._getml_deserialize()\n\n        comm.send(cmd)\n\n    def sync(self):\n        \"\"\"\n        Synchronizes the last change with the data to avoid warnings that the data\n        has been changed.\n\n        This is only a problem when `deep_copy=False`.\n        \"\"\"\n        if self._frozen_time is not None:\n            raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n    def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"\n        TODO\n        \"\"\"\n        subsets = (\n            {name: df.to_pandas() for name, df in self._subsets.items()}\n            if self._subsets\n            else {}\n        )\n        peripherals = (\n            {name: df.to_pandas() for name, df in self.peripheral.items()}\n            if self.peripheral\n            else {}\n        )\n        if subsets or peripherals:\n            return {**subsets, **peripherals}\n\n        raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Container.add","title":"<code>add(*args, **kwargs)</code>","text":"<p>Adds new peripheral data frames or views.</p> Source code in <code>getml/data/container.py</code> <pre><code>def add(self, *args, **kwargs):\n    \"\"\"\n    Adds new peripheral data frames or views.\n    \"\"\"\n    wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n    if wrong_type:\n        raise TypeError(\n            \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n        )\n\n    wrong_type = [\n        k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n    ]\n\n    if wrong_type:\n        raise TypeError(\n            \"You must pass getml.DataFrames or getml.data.Views, \"\n            f\"but the following arguments were neither: {wrong_type!r}.\"\n        )\n\n    kwargs = {**{item.name: item for item in args}, **kwargs}\n\n    if self._frozen_time is not None:\n        raise ValueError(\n            f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n        )\n\n    if self._deep_copy:\n        kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n    self._peripheral = {**self._peripheral, **kwargs}\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Container.freeze","title":"<code>freeze()</code>","text":"<p>Freezes the container, so that changes are no longer possible.</p> <p>This is required before you can extract data when <code>deep_copy=True</code>. The idea of <code>deep_copy</code> is to ensure that you can always retrace and reproduce your results. That is why the container needs to be immutable before it can be used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def freeze(self):\n    \"\"\"\n    Freezes the container, so that changes are no longer possible.\n\n    This is required before you can extract data when `deep_copy=True`. The idea of\n    `deep_copy` is to ensure that you can always retrace and reproduce your results.\n    That is why the container needs to be immutable before it can be\n    used.\n    \"\"\"\n    self.sync()\n    self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Container.save","title":"<code>save()</code>","text":"<p>Saves the Container to disk.</p> Source code in <code>getml/data/container.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves the Container to disk.\n    \"\"\"\n\n    cmd = dict()\n    cmd[\"type_\"] = \"DataContainer.save\"\n    cmd[\"name_\"] = self._id\n\n    cmd[\"container_\"] = self._getml_deserialize()\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Container.sync","title":"<code>sync()</code>","text":"<p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/container.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    if self._frozen_time is not None:\n        raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Container.to_pandas","title":"<code>to_pandas()</code>","text":"<p>TODO</p> Source code in <code>getml/data/container.py</code> <pre><code>def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    TODO\n    \"\"\"\n    subsets = (\n        {name: df.to_pandas() for name, df in self._subsets.items()}\n        if self._subsets\n        else {}\n    )\n    peripherals = (\n        {name: df.to_pandas() for name, df in self.peripheral.items()}\n        if self.peripheral\n        else {}\n    )\n    if subsets or peripherals:\n        return {**subsets, **peripherals}\n\n    raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame","title":"<code>DataFrame</code>","text":"<p>Handler for the data stored in the getML engine.</p> <p>The <code>DataFrame</code> class represents a data frame object in the getML engine but does not contain any actual data itself. To create such a data frame object, fill it with data via the Python API, and to retrieve a handler for it, you can use one of the <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code> class methods. The Importing Data section in the user guide explains the particularities of each of those flavors of the unified import interface.</p> <p>If the data frame object is already present in the engine - either in memory as a temporary object or on disk when <code>save</code> was called earlier -, the <code>load_data_frame</code> function will create a new handler without altering the underlying data. For more information about the lifecycle of the data in the getML engine and its synchronization with the Python API please see the corresponding User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier used to link the handler with the underlying data frame object in the engine.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> Example <p>Creating a new data frame object in the getML engine and importing data is done by one the class functions <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p><pre><code>random = numpy.random.RandomState(7263)\n\ntable = pandas.DataFrame()\ntable['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\ntable['join_key'] = numpy.arange(1000)\ntable['time_stamp'] = random.rand(1000)\ntable['target'] = random.rand(1000)\n\ndf_table = getml.DataFrame.from_pandas(table, name = 'table')\n</code></pre> In addition to creating a new data frame object in the getML engine and filling it with all the content of <code>table</code>, the <code>from_pandas</code> function also returns a <code>DataFrame</code> handler to the underlying data.</p> <p>You don't have to create the data frame objects anew for each session. You can use their <code>save</code> method to write them to disk, the <code>list_data_frames</code> function to list all available objects in the engine, and <code>load_data_frame</code> to create a <code>DataFrame</code> handler for a data set already present in the getML engine (see User Guide for details).</p> <pre><code>df_table.save()\n\ngetml.data.list_data_frames()\n\ndf_table_reloaded = getml.data.load_data_frame('table')\n</code></pre> Note <p>Although the Python API does not store the actual data itself, you can use the <code>to_csv</code>, <code>to_db</code>, <code>to_json</code>, and <code>to_pandas</code> methods to retrieve them.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>class DataFrame:\n    \"\"\"Handler for the data stored in the getML engine.\n\n    The [`DataFrame`][getml.DataFrame] class represents a data frame\n    object in the getML engine but does not contain any actual data\n    itself. To create such a data frame object, fill it with data via\n    the Python API, and to retrieve a handler for it, you can use one\n    of the [`from_csv`][getml.DataFrame.from_csv],\n    [`from_db`][getml.DataFrame.from_db],\n    [`from_json`][getml.DataFrame.from_json], or\n    [`from_pandas`][getml.DataFrame.from_pandas] class methods. The\n    [Importing Data][importing-data] section in the user guide explains the\n    particularities of each of those flavors of the unified\n    import interface.\n\n    If the data frame object is already present in the engine -\n    either in memory as a temporary object or on disk when\n    [`save`][getml.DataFrame.save] was called earlier -, the\n    [`load_data_frame`][getml.data.load_data_frame] function will create a new\n    handler without altering the underlying data. For more information\n    about the lifecycle of the data in the getML engine and its\n    synchronization with the Python API please see the\n    corresponding [User Guide][python-api-lifecycles].\n\n    Args:\n        name (str):\n            Unique identifier used to link the handler with\n            the underlying data frame object in the engine.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n    Example:\n        Creating a new data frame object in the getML engine and importing\n        data is done by one the class functions\n        [`from_csv`][getml.DataFrame.from_csv],\n        [`from_db`][getml.DataFrame.from_db],\n        [`from_json`][getml.DataFrame.from_json], or\n        [`from_pandas`][getml.DataFrame.from_pandas].\n\n        ```python\n        random = numpy.random.RandomState(7263)\n\n        table = pandas.DataFrame()\n        table['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\n        table['join_key'] = numpy.arange(1000)\n        table['time_stamp'] = random.rand(1000)\n        table['target'] = random.rand(1000)\n\n        df_table = getml.DataFrame.from_pandas(table, name = 'table')\n        ```\n        In addition to creating a new data frame object in the getML\n        engine and filling it with all the content of `table`, the\n        [`from_pandas`][getml.DataFrame.from_pandas] function also\n        returns a [`DataFrame`][getml.DataFrame] handler to the\n        underlying data.\n\n        You don't have to create the data frame objects anew for each\n        session. You can use their [`save`][getml.DataFrame.save]\n        method to write them to disk, the\n        [`list_data_frames`][getml.data.list_data_frames] function to list all\n        available objects in the engine, and\n        [`load_data_frame`][getml.data.load_data_frame] to create a\n        [`DataFrame`][getml.DataFrame] handler for a data set already\n        present in the getML engine (see\n        [User Guide][python-api] for details).\n\n        ```python\n        df_table.save()\n\n        getml.data.list_data_frames()\n\n        df_table_reloaded = getml.data.load_data_frame('table')\n        ```\n\n    Note:\n        Although the Python API does not store the actual data itself,\n        you can use the [`to_csv`][getml.DataFrame.to_csv],\n        [`to_db`][getml.DataFrame.to_db],\n        [`to_json`][getml.DataFrame.to_json], and\n        [`to_pandas`][getml.DataFrame.to_pandas] methods to retrieve\n        them.\n\n    \"\"\"\n\n    _categorical_roles = roles_._categorical_roles\n    _numerical_roles = roles_._numerical_roles\n    _possible_keys = _categorical_roles + _numerical_roles\n\n    def __init__(self, name, roles=None):\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        vars(self)[\"name\"] = name\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        roles = roles or dict()\n\n        if not isinstance(roles, dict):\n            raise TypeError(\"'roles' must be dict or a getml.data.Roles object\")\n\n        for key, val in roles.items():\n            if key not in self._possible_keys:\n                msg = \"'{}' is not a proper role and will be ignored\\n\"\n                msg += \"Possible roles are: {}\"\n                raise ValueError(msg.format(key, self._possible_keys))\n            if not _is_typed_list(val, str):\n                raise TypeError(\n                    \"'{}' must be None, an empty list, or a list of str.\".format(key)\n                )\n\n        # ------------------------------------------------------------\n\n        join_keys = roles.get(\"join_key\", [])\n        time_stamps = roles.get(\"time_stamp\", [])\n        categorical = roles.get(\"categorical\", [])\n        numerical = roles.get(\"numerical\", [])\n        targets = roles.get(\"target\", [])\n        text = roles.get(\"text\", [])\n        unused_floats = roles.get(\"unused_float\", [])\n        unused_strings = roles.get(\"unused_string\", [])\n\n        # ------------------------------------------------------------\n\n        vars(self)[\"_categorical_columns\"] = [\n            StringColumn(name=cname, role=roles_.categorical, df_name=self.name)\n            for cname in categorical\n        ]\n\n        vars(self)[\"_join_key_columns\"] = [\n            StringColumn(name=cname, role=roles_.join_key, df_name=self.name)\n            for cname in join_keys\n        ]\n\n        vars(self)[\"_numerical_columns\"] = [\n            FloatColumn(name=cname, role=roles_.numerical, df_name=self.name)\n            for cname in numerical\n        ]\n\n        vars(self)[\"_target_columns\"] = [\n            FloatColumn(name=cname, role=roles_.target, df_name=self.name)\n            for cname in targets\n        ]\n\n        vars(self)[\"_text_columns\"] = [\n            StringColumn(name=cname, role=roles_.text, df_name=self.name)\n            for cname in text\n        ]\n\n        vars(self)[\"_time_stamp_columns\"] = [\n            FloatColumn(name=cname, role=roles_.time_stamp, df_name=self.name)\n            for cname in time_stamps\n        ]\n\n        vars(self)[\"_unused_float_columns\"] = [\n            FloatColumn(name=cname, role=roles_.unused_float, df_name=self.name)\n            for cname in unused_floats\n        ]\n\n        vars(self)[\"_unused_string_columns\"] = [\n            StringColumn(name=cname, role=roles_.unused_string, df_name=self.name)\n            for cname in unused_strings\n        ]\n\n        # ------------------------------------------------------------\n\n        self._check_duplicates()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _columns(self):\n        return (\n            vars(self)[\"_categorical_columns\"]\n            + vars(self)[\"_join_key_columns\"]\n            + vars(self)[\"_numerical_columns\"]\n            + vars(self)[\"_target_columns\"]\n            + vars(self)[\"_text_columns\"]\n            + vars(self)[\"_time_stamp_columns\"]\n            + vars(self)[\"_unused_float_columns\"]\n            + vars(self)[\"_unused_string_columns\"]\n        )\n\n    # ----------------------------------------------------------------\n\n    def _delete(self, mem_only: bool = False):\n        \"\"\"Deletes the data frame from the getML engine.\n\n        If called with the `mem_only` option set to True, the data\n        frame corresponding to the handler represented by the current\n        instance can be reloaded using the\n        [`load`][getml.DataFrame.load] method.\n\n        Args:\n            mem_only (bool, optional):\n                If True, the data frame will not be deleted\n                permanently, but just from memory (RAM).\n        \"\"\"\n\n        if not isinstance(mem_only, bool):\n            raise TypeError(\"'mem_only' must be of type bool\")\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.delete\"\n        cmd[\"name_\"] = self.name\n        cmd[\"mem_only_\"] = mem_only\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def __delitem__(self, colname: str):\n        self._drop(colname)\n\n    # ----------------------------------------------------------------\n\n    def __eq__(self, other):\n        if not isinstance(other, DataFrame):\n            raise TypeError(\n                \"A DataFrame can only be compared to another getml.DataFrame\"\n            )\n\n        # ------------------------------------------------------------\n\n        for kkey in self.__dict__:\n            if kkey not in other.__dict__:\n                return False\n\n            # Take special care when comparing numbers.\n            if isinstance(self.__dict__[kkey], numbers.Real):\n                if not np.isclose(self.__dict__[kkey], other.__dict__[kkey]):\n                    return False\n\n            elif self.__dict__[kkey] != other.__dict__[kkey]:\n                return False\n\n        # ------------------------------------------------------------\n\n        return True\n\n    # ----------------------------------------------------------------\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            return super().__getattribute__(name)\n\n    # ------------------------------------------------------------\n\n    def __getitem__(self, name):\n        if isinstance(\n            name,\n            (numbers.Integral, slice, BooleanColumnView, FloatColumn, FloatColumnView),\n        ):\n            return self.where(index=name)\n\n        if isinstance(name, list):\n            not_in_colnames = set(name) - set(self.colnames)\n            if not_in_colnames:\n                raise KeyError(f\"{list(not_in_colnames)} not found.\")\n            dropped = [col for col in self.colnames if col not in name]\n            return View(base=self, dropped=dropped)\n\n        col = _get_column(name, self._columns)\n\n        if col is not None:\n            return col\n\n        raise KeyError(\"Column named '\" + name + \"' not found.\")\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame\"\n        cmd[\"name_\"] = self.name\n\n        return cmd\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return self.nrows()\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if not _exists_in_memory(self.name):\n            return _empty_data_frame()\n\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_string(footer=footer)\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        return self.to_html()\n\n    # ------------------------------------------------------------\n\n    def __setattr__(self, name, value):\n        if name in vars(self):\n            vars(self)[name] = value\n        else:\n            self.add(value, name)\n\n    # ------------------------------------------------------------\n\n    def __setitem__(self, name, col):\n        self.add(col, name)\n\n    # ------------------------------------------------------------\n\n    def _add_categorical_column(self, col, name, role, subroles, unit):\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.add_categorical_column\"\n        cmd[\"name_\"] = name\n\n        cmd[\"col_\"] = col.cmd\n        cmd[\"df_name_\"] = self.name\n        cmd[\"role_\"] = role\n        cmd[\"subroles_\"] = subroles\n        cmd[\"unit_\"] = unit\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.recv_issues(sock)\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _add_column(self, col, name, role, subroles, unit):\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.add_column\"\n        cmd[\"name_\"] = name\n\n        cmd[\"col_\"] = col.cmd\n        cmd[\"df_name_\"] = self.name\n        cmd[\"role_\"] = role\n        cmd[\"subroles_\"] = subroles\n        cmd[\"unit_\"] = unit\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.recv_issues(sock)\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _add_numpy_array(self, numpy_array, name, role, subroles, unit):\n        if len(numpy_array.shape) != 1:\n            raise TypeError(\n                \"\"\"numpy.ndarray needs to be one-dimensional!\n                Maybe you can call .ravel().\"\"\"\n            )\n\n        if role is None:\n            temp_df = pd.DataFrame()\n            temp_df[\"column\"] = numpy_array\n            if _is_numerical_type(temp_df.dtypes[0]):\n                role = roles_.unused_float\n            else:\n                role = roles_.unused_string\n\n        col = (\n            FloatColumn(name=name, role=role, df_name=self.name)\n            if role in self._numerical_roles\n            else StringColumn(name=name, role=role, df_name=self.name)\n        )\n\n        _send_numpy_array(col, numpy_array)\n\n        if subroles:\n            self._set_subroles(name, subroles, append=False)\n\n        if unit:\n            self._set_unit(name, unit)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _check_duplicates(self) -&gt; None:\n        all_colnames: List[str] = []\n\n        all_colnames = _check_if_exists(self._categorical_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._join_key_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._numerical_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._target_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._text_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._time_stamp_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._unused_names, all_colnames)\n\n    # ------------------------------------------------------------\n\n    def _check_plausibility(self, data_frame):\n        self._check_duplicates()\n\n        for col in self._categorical_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._join_key_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._numerical_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._target_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._text_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._time_stamp_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._unused_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n    # ------------------------------------------------------------\n\n    def _collect_footer_data(self):\n        footer = namedtuple(\n            \"footer\", [\"n_rows\", \"n_cols\", \"memory_usage\", \"name\", \"type\", \"url\"]\n        )\n\n        return footer(\n            n_rows=self.nrows(),\n            n_cols=self.ncols(),\n            memory_usage=self.memory_usage,\n            name=self.name,\n            type=\"getml.DataFrame\",\n            url=self._monitor_url,\n        )\n\n    # ------------------------------------------------------------\n\n    def _close(self, sock):\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.close\"\n        cmd[\"name_\"] = self.name\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    # ------------------------------------------------------------\n\n    def _drop(self, colname: str):\n        if not isinstance(colname, str):\n            raise TypeError(\"'colname' must be either a string.\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.remove_column\"\n        cmd[\"name_\"] = colname\n\n        cmd[\"df_name_\"] = self.name\n\n        comm.send(cmd)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _format(self):\n        formatted = _DataFrameFormatter(self)\n        return formatted\n\n    # ------------------------------------------------------------\n\n    def _set_role(self, name, role, time_formats):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        subroles = col.subroles\n\n        unit = TIME_STAMP + COMPARISON_ONLY if role == roles_.time_stamp else col.unit\n\n        self.add(\n            col,\n            name=name,\n            role=role,\n            subroles=subroles,\n            unit=unit,\n            time_formats=time_formats,\n        )\n\n    # ------------------------------------------------------------\n\n    def _set_subroles(self, name: str, subroles: List[str], append: bool):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd.update(col.cmd)\n\n        cmd[\"type_\"] += \".set_subroles\"\n\n        cmd[\"subroles_\"] = list(set(col.subroles + subroles)) if append else subroles\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def _set_unit(self, name: str, unit: str):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd.update(col.cmd)\n\n        cmd[\"type_\"] += \".set_unit\"\n\n        cmd[\"unit_\"] = unit\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def add(self, col, name, role=None, subroles=None, unit=\"\", time_formats=None):\n        \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n        Args:\n            col ([`column`][getml.column] or `numpy.ndarray`):\n                The column or numpy.ndarray to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from `getml.data.roles`.\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n        \"\"\"\n\n        if isinstance(col, np.ndarray):\n            self._add_numpy_array(col, name, role, subroles, unit)\n            return\n\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n\n        is_string = isinstance(col, (StringColumnView, StringColumn))\n\n        if is_string:\n            self._add_categorical_column(col, name, role, subroles, unit)\n        else:\n            self._add_column(col, name, role, subroles, unit)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _categorical_names(self):\n        return [col.name for col in self._categorical_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def colnames(self):\n        \"\"\"\n        List of the names of all columns.\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return (\n            self._time_stamp_names\n            + self._join_key_names\n            + self._target_names\n            + self._categorical_names\n            + self._numerical_names\n            + self._text_names\n            + self._unused_names\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self):\n        \"\"\"\n        Alias for [`colnames`][getml.DataFrame.colnames].\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return self.colnames\n\n    # ------------------------------------------------------------\n\n    def copy(self, name: str) -&gt; \"DataFrame\":\n        \"\"\"\n        Creates a deep copy of the data frame under a new name.\n\n        Args:\n            name (str):\n                The name of the new data frame.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                A handle to the deep copy.\n        \"\"\"\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be a string.\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.concat\"\n        cmd[\"name_\"] = name\n\n        cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n        comm.send(cmd)\n\n        return DataFrame(name=name).refresh()\n\n    # ------------------------------------------------------------\n\n    def delete(self):\n        \"\"\"\n        Permanently deletes the data frame. `delete` first unloads the data frame\n        from memory and then deletes it from disk.\n        \"\"\"\n        # ------------------------------------------------------------\n\n        self._delete()\n\n    # ------------------------------------------------------------\n\n    def drop(self, cols):\n        \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if not _is_typed_list(names, str):\n            raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n        return View(base=self, dropped=names)\n\n    # ------------------------------------------------------------\n\n    def freeze(self):\n        \"\"\"Freezes the data frame.\n\n        After you have frozen the data frame, the data frame is immutable\n        and in-place operations are no longer possible. However, you can\n        still create views. In other words, operations like\n        [`set_role`][getml.DataFrame.set_role] are no longer possible,\n        but operations like [`with_role`][getml.DataFrame.with_role] are.\n        \"\"\"\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.freeze\"\n        cmd[\"name_\"] = self.name\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_arrow(cls, table, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from an Arrow Table.\n\n        This is one of the fastest way to get data into the\n        getML engine.\n\n        Args:\n            table (pyarrow.Table):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(table, pa.Table):\n            raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_arrow(table)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_arrow(table=table, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_csv(\n        cls,\n        fnames,\n        name,\n        num_lines_sniffed=1000,\n        num_lines_read=0,\n        quotechar='\"',\n        sep=\",\",\n        skip=0,\n        colnames=None,\n        roles=None,\n        ignore=False,\n        dry=False,\n        verbose=True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from CSV files.\n\n        The getML engine will construct a data\n        frame object in the engine, fill it with the data read from\n        the CSV file(s), and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            fnames (List[str]):\n                CSV file paths to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            num_lines_sniffed (int, optional):\n                Number of lines analyzed by the sniffer.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional): The first line of a CSV file\n                usually contains the column names. When this is not the case,\n                you need to explicitly pass them.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n            verbose (bool, optional):\n                If True, when fnames are urls, the filenames are\n                printed to stdout during the download.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Note:\n            It is assumed that the first line of each CSV file\n            contains a header with the column names.\n\n        Example:\n            Let's assume you have two CSV files - *file1.csv* and\n            *file2.csv* - in the current working directory. You can\n            import their data into the getML engine using.\n            ```python\n            df_expd = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"'\n                )\n\n            # However, the CSV format lacks type safety. If you want to\n            # build a reliable pipeline, it is a good idea\n            # to hard-code the roles:\n\n            roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n            df_expd = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"',\n                roles=roles\n                )\n\n            # If you think that typing out all the roles by hand is too\n            # cumbersome, you can use a dry run:\n\n            roles = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"',\n                dry=True\n            )\n            ```\n\n            This will return the roles dictionary it would have used. You\n            can now hard-code this.\n\n        \"\"\"\n\n        if not isinstance(fnames, list):\n            fnames = [fnames]\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(num_lines_sniffed, numbers.Real):\n            raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be str.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        fnames = _retrieve_urls(fnames, verbose=verbose)\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_csv(\n                fnames=fnames,\n                num_lines_sniffed=int(num_lines_sniffed),\n                quotechar=quotechar,\n                sep=sep,\n                skip=int(skip),\n                colnames=colnames,\n            )\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_csv(\n            fnames=fnames,\n            append=False,\n            quotechar=quotechar,\n            sep=sep,\n            num_lines_read=num_lines_read,\n            skip=skip,\n            colnames=colnames,\n        )\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_db(\n        cls, table_name, name=None, roles=None, ignore=False, dry=False, conn=None\n    ):\n        \"\"\"Create a DataFrame from a table in a database.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from table `table_name` in the connected\n        database (see [`database`][getml.database]), and return a\n        corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            table_name (str):\n                Name of the table to be read.\n\n            name (str):\n                Name of the data frame to be created. If not passed,\n                then the *table_name* will be used.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection, the engine\n                will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Example:\n            ```python\n            getml.database.connect_mysql(\n                host=\"db.relational-data.org\",\n                port=3306,\n                dbname=\"financial\",\n                user=\"guest\",\n                password=\"relational\"\n            )\n\n            loan = getml.DataFrame.from_db(\n                table_name='loan', name='data_frame_loan')\n            ```\n        \"\"\"\n\n        # -------------------------------------------\n\n        name = name or table_name\n\n        # -------------------------------------------\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\n                \"'roles' must be a getml.data.Roles object, a dict or None.\"\n            )\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # -------------------------------------------\n\n        conn = conn or database.Connection()\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_db(table_name, conn)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_dict(\n        cls, data: Dict[str, List[Any]], name: str, roles=None, ignore=False, dry=False\n    ):\n        \"\"\"Create a new DataFrame from a dict\n\n        Args:\n            data (dict):\n                The dict containing the data.\n                The data should be in the following format:\n                ```python\n                data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n                ```\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n        \"\"\"\n\n        if not isinstance(data, dict):\n            raise TypeError(\"'data' must be dict.\")\n\n        return cls.from_arrow(\n            table=pa.Table.from_pydict(data),\n            name=name,\n            roles=roles,\n            ignore=ignore,\n            dry=dry,\n        )\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_json(cls, json_str, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a new DataFrame from a JSON string.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the JSON string, and return a\n        corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            json_str (str):\n                The JSON string containing the data.\n                The json_str should be in the following format:\n                ```python\n                json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n                ```\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n        Returns:\n            [`DataFrame`][getml.data.DataFrame]: Handler of the underlying data.\n\n        \"\"\"\n\n        if not isinstance(json_str, str):\n            raise TypeError(\"'json_str' must be str.\")\n\n        return cls.from_dict(\n            data=json.loads(json_str),\n            name=name,\n            roles=roles,\n            ignore=ignore,\n            dry=dry,\n        )\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_pandas(cls, pandas_df, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the `pandas.DataFrame`, and\n        return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            pandas_df (pandas.DataFrame):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                 roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                          getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(pandas_df, pd.DataFrame):\n            raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        pandas_df_modified = _modify_pandas_columns(pandas_df)\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_pandas(pandas_df_modified)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_pandas(pandas_df=pandas_df_modified, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_parquet(cls, fname, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from parquet files.\n\n        This is one of the fastest way to get data into the\n        getML engine.\n\n        Args:\n            fname (str):\n                The path of the parquet file to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_parquet(fname)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_parquet(fname=fname, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_pyspark(cls, spark_df, name, roles=None, ignore=False, dry=False) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the `pyspark.sql.DataFrame`, and\n        return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            spark_df (pyspark.sql.DataFrame):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            head = spark_df.limit(2).toPandas()\n\n            sniffed_roles = _sniff_pandas(head)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_pyspark(spark_df=spark_df, append=False)\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_s3(\n        cls,\n        bucket: str,\n        keys: List[str],\n        region: str,\n        name: str,\n        num_lines_sniffed=1000,\n        num_lines_read=0,\n        sep=\",\",\n        skip=0,\n        colnames=None,\n        roles=None,\n        ignore=False,\n        dry=False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n        This classmethod will construct a data\n        frame object in the engine, fill it with the data read from\n        the CSV file(s), and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            keys (List[str]):\n                The list of keys (files in the bucket) to be read.\n\n            region (str):\n                The region in which the bucket is located.\n\n            name (str):\n                Name of the data frame to be created.\n\n            num_lines_sniffed (int, optional):\n                Number of lines analyzed by the sniffer.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names. When this is not the case,\n                you need to explicitly pass them.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Example:\n            Let's assume you have two CSV files - *file1.csv* and\n            *file2.csv* - in the bucket. You can\n            import their data into the getML engine using the following\n            commands:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            data_frame_expd = data.DataFrame.from_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY DATA FRAME\",\n                sep=';'\n            )\n            ```\n\n            You can also set the access credential as environment variables\n            before you launch the getML engine.\n\n            Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n            for further information on overriding the CSV sniffer for greater\n            type safety.\n\n        Note:\n            Not supported in the getML community edition.\n        \"\"\"\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be str.\")\n\n        if not _is_non_empty_typed_list(keys, str):\n            raise TypeError(\"'keys' must be either a string or a list of str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(num_lines_sniffed, numbers.Real):\n            raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_s3(\n                bucket=bucket,\n                keys=keys,\n                region=region,\n                num_lines_sniffed=int(num_lines_sniffed),\n                sep=sep,\n                skip=int(skip),\n                colnames=colnames,\n            )\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_s3(\n            bucket=bucket,\n            keys=keys,\n            region=region,\n            append=False,\n            sep=sep,\n            num_lines_read=int(num_lines_read),\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_view(\n        cls,\n        view,\n        name,\n        dry=False,\n    ):\n        \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n        This classmethod will construct a data\n        frame object in the engine, fill it with the data read from\n        the [`View`][getml.data.View], and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            view ([`View`][getml.data.View]):\n                The view from which we want to read the data.\n\n            name (str):\n                Name of the data frame to be created.\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n\n        \"\"\"\n        # ------------------------------------------------------------\n\n        if not isinstance(view, View):\n            raise TypeError(\"'view' must be getml.data.View.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        if dry:\n            return view.roles\n\n        data_frame = cls(name)\n\n        # ------------------------------------------------------------\n\n        return data_frame.read_view(view=view, append=False)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _join_key_names(self):\n        return [col.name for col in self._join_key_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def last_change(self) -&gt; str:\n        \"\"\"\n        A string describing the last time this data frame has been changed.\n        \"\"\"\n        return _last_change(self.name)\n\n    # ------------------------------------------------------------\n\n    def load(self) -&gt; \"DataFrame\":\n        \"\"\"Loads saved data from disk.\n\n        The data frame object holding the same name as the current\n        [`DataFrame`][getml.DataFrame] instance will be loaded from\n        disk into the getML engine and updates the current handler\n        using [`refresh`][getml.DataFrame.refresh].\n\n        Example:\n            First, we have to create and import data sets.\n            ```python\n            d, _ = getml.datasets.make_numerical(population_name = 'test')\n            getml.data.list_data_frames()\n            ```\n\n            In the output of [`list_data_frames`][getml.data.list_data_frames] we\n            can find our underlying data frame object 'test' listed\n            under the 'in_memory' key (it was created and imported by\n            [`make_numerical`][getml.datasets.make_numerical]). This means the\n            getML engine does only hold it in memory (RAM) yet, and we\n            still have to [`save`][getml.DataFrame.save] it to\n            disk in order to [`load`][getml.DataFrame.load] it\n            again or to prevent any loss of information between\n            different sessions.\n            ```python\n            d.save()\n            getml.data.list_data_frames()\n            d2 = getml.DataFrame(name = 'test').load()\n            ```\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        Note:\n            When invoking [`load`][getml.DataFrame.load] all\n            changes of the underlying data frame object that took\n            place after the last call to the\n            [`save`][getml.DataFrame.save] method will be\n            lost. Thus, this method  enables you to undo changes\n            applied to the [`DataFrame`][getml.DataFrame].\n            ```python\n            d, _ = getml.datasets.make_numerical()\n            d.save()\n\n            # Accidental change we want to undo\n            d.rm('column_01')\n\n            d.load()\n            ```\n            If [`save`][getml.DataFrame.save] hasn't been called\n            on the current instance yet, or it wasn't stored to disk in\n            a previous session, [`load`][getml.DataFrame.load]\n            will throw an exception\n\n                File or directory '../projects/X/data/Y/' not found!\n\n            Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n            offers an easier way of creating\n            [`DataFrame`][getml.DataFrame] handlers to data in the\n            getML engine.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.load\"\n        cmd[\"name_\"] = self.name\n        comm.send(cmd)\n        return self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def memory_usage(self):\n        \"\"\"\n        Convenience wrapper that returns the memory usage in MB.\n        \"\"\"\n        return self.nbytes() / 1e06\n\n    # ------------------------------------------------------------\n\n    @property\n    def _monitor_url(self) -&gt; Optional[str]:\n        \"\"\"\n        A link to the data frame in the getML monitor.\n        \"\"\"\n        url = comm._monitor_url()\n        return (\n            url + \"getdataframe/\" + comm._get_project_name() + \"/\" + self.name + \"/\"\n            if url\n            else None\n        )\n\n    # ------------------------------------------------------------\n\n    def nbytes(self):\n        \"\"\"Size of the data stored in the underlying data frame in the getML\n        engine.\n\n        Returns:\n            numpy.uint64:\n                Size of the underlying object in bytes.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.nbytes\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                sock.close()\n                comm.engine_exception_handler(msg)\n            nbytes = comm.recv_string(sock)\n\n        return np.uint64(nbytes)\n\n    # ------------------------------------------------------------\n\n    def ncols(self):\n        \"\"\"\n        Number of columns in the current instance.\n\n        Returns:\n            int:\n                Overall number of columns\n        \"\"\"\n        return len(self.colnames)\n\n    # ------------------------------------------------------------\n\n    def nrows(self):\n        \"\"\"\n        Number of rows in the current instance.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.nrows\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                sock.close()\n                comm.engine_exception_handler(msg)\n            nrows = comm.recv_string(sock)\n\n        return int(nrows)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _numerical_names(self):\n        return [col.name for col in self._numerical_columns]\n\n    # --------------------------------------------------------------------------\n\n    def read_arrow(self, table, append=False):\n        \"\"\"Uploads a `pyarrow.Table`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `table`.\n\n        Args:\n            table (pyarrow.Table):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Current instance.\n\n        Note:\n            For columns containing `pandas.Timestamp` there can\n            be small inconsistencies in the order of microseconds\n            when sending the data to the getML engine. This is due to\n            the way the underlying information is stored.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(table, pa.Table):\n            raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_pandas(...).\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_arrow\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"append_\"] = append\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        with comm.send_and_get_socket(cmd) as sock:\n            with sock.makefile(mode=\"wb\") as sink:\n                batches = table.to_batches()\n                with pa.ipc.new_stream(sink, table.schema) as writer:\n                    for batch in batches:\n                        writer.write_batch(batch)\n            msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        return self.refresh()\n\n    # --------------------------------------------------------------------------\n\n    def read_csv(\n        self,\n        fnames,\n        append=False,\n        quotechar='\"',\n        sep=\",\",\n        num_lines_read=0,\n        skip=0,\n        colnames=None,\n        time_formats=None,\n        verbose=True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read CSV files.\n\n        It is assumed that the first line of each CSV file contains a\n        header with the column names.\n\n        Args:\n            fnames (List[str]):\n                CSV file paths to be read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names.\n                When this is not the case, you need to explicitly pass them.\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n            verbose (bool, optional):\n                If True, when `fnames` are urls, the filenames are printed to\n                stdout during the download.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if not isinstance(fnames, list):\n            fnames = [fnames]\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be str.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_csv(...).\"\"\"\n            )\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\n                \"\"\"'fnames' must be a list containing at\n                least one path to a CSV file\"\"\"\n            )\n\n        fnames_ = _retrieve_urls(fnames, verbose)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fnames_\"] = fnames_\n\n        cmd[\"append_\"] = append\n        cmd[\"num_lines_read_\"] = num_lines_read\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"skip_\"] = skip\n        cmd[\"time_formats_\"] = time_formats\n\n        if colnames is not None:\n            cmd[\"colnames_\"] = colnames\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_json(self, json_str, append=False, time_formats=None):\n        \"\"\"Fill from JSON\n\n        Fills the data frame with data from a JSON string.\n\n        Args:\n\n            json_str (str):\n                The JSON string containing the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                `json_str` be appended or replace the existing data?\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        Note:\n            This does not support NaN values. If you want support for NaN,\n            use [`from_json`][getml.DataFrame.from_json] instead.\n\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_json(...).\"\"\"\n            )\n\n        if not isinstance(json_str, str):\n            raise TypeError(\"'json_str' must be of type str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be of type bool\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\n                \"\"\"'time_formats' must be a list of strings\n                containing at least one time format\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.from_json\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n        cmd[\"time_formats_\"] = time_formats\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.send_string(sock, json_str)\n            msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_parquet(\n        self,\n        fname: str,\n        append: bool = False,\n        verbose: bool = True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read a parquet file.\n\n        Args:\n            fname (str):\n                The filepath of the parquet file to be read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n        \"\"\"\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than\n                zero columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_parquet(...).\"\"\"\n            )\n\n        fname_ = _retrieve_urls([fname], verbose)[0]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_parquet\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fname_\"] = fname_\n        cmd[\"append_\"] = append\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_s3(\n        self,\n        bucket: str,\n        keys: List[str],\n        region: str,\n        append: bool = False,\n        sep: str = \",\",\n        num_lines_read: int = 0,\n        skip: int = 0,\n        colnames: Optional[List[str]] = None,\n        time_formats: Optional[List[str]] = None,\n    ):\n        \"\"\"Read CSV files from an S3 bucket.\n\n        It is assumed that the first line of each CSV file contains a\n        header with the column names.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            keys (List[str]):\n                The list of keys (files in the bucket) to be read.\n\n            region (str):\n                The region in which the bucket is located.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names.\n                When this is not the case, you need to explicitly pass them.\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        Note:\n            Not supported in the getML community edition.\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be str.\")\n\n        if not _is_non_empty_typed_list(keys, str):\n            raise TypeError(\"'keys' must be either a string or a list of str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_s3(...).\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"append_\"] = append\n        cmd[\"bucket_\"] = bucket\n        cmd[\"keys_\"] = keys\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"time_formats_\"] = time_formats\n        cmd[\"num_lines_read_\"] = num_lines_read\n        cmd[\"skip_\"] = skip\n\n        if colnames is not None:\n            cmd[\"colnames_\"] = colnames\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def read_view(\n        self,\n        view: View,\n        append: bool = False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read the data from a [`View`][getml.data.View].\n\n        Args:\n            view ([`View`][getml.data.View]):\n                The view to read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        \"\"\"\n\n        if not isinstance(view, View):\n            raise TypeError(\"'view' must be getml.data.View.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        view.check()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_view\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = view._getml_deserialize()\n\n        cmd[\"append_\"] = append\n\n        comm.send(cmd)\n\n        return self.refresh()\n\n    # --------------------------------------------------------------------------\n\n    def read_db(self, table_name: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n        \"\"\"\n        Fill from Database.\n\n        The DataFrame will be filled from a table in the database.\n\n        Args:\n            table_name (str):\n                Table from which we want to retrieve the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                `table_name` be appended or replace the existing data?\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n        \"\"\"\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_db(...).\"\"\"\n            )\n\n        conn = conn or database.Connection()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_db\"\n        cmd[\"name_\"] = self.name\n        cmd[\"table_name_\"] = table_name\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Uploads a `pandas.DataFrame`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `pandas_df`.\n\n        Args:\n            pandas_df (pandas.DataFrame):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n        Note:\n            For columns containing `pandas.Timestamp` there can\n            occur small inconsistencies in the order of microseconds\n            when sending the data to the getML engine. This is due to\n            the way the underlying information is stored.\n        \"\"\"\n\n        if not isinstance(pandas_df, pd.DataFrame):\n            raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_pandas(...).\"\"\"\n            )\n\n        table = pa.Table.from_pandas(_modify_pandas_columns(pandas_df))\n\n        return self.read_arrow(table, append=append)\n\n    # --------------------------------------------------------------------------\n\n    def read_pyspark(self, spark_df, append: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `pandas_df`.\n\n        Args:\n            spark_df (pyspark.sql.DataFrame):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n        \"\"\"\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        temp_dir = _retrieve_temp_dir()\n        os.makedirs(temp_dir, exist_ok=True)\n        path = os.path.join(temp_dir, self.name)\n        spark_df.write.mode(\"overwrite\").parquet(path)\n\n        filepaths = [\n            os.path.join(path, filepath)\n            for filepath in os.listdir(path)\n            if filepath[-8:] == \".parquet\"\n        ]\n\n        for i, filepath in enumerate(filepaths):\n            self.read_parquet(filepath, append or i &gt; 0)\n\n        shutil.rmtree(path)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_query(self, query: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n        \"\"\"Fill from query\n\n        Fills the data frame with data from a table in the database.\n\n        Args:\n            query (str):\n                The query used to retrieve the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n        \"\"\"\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_db(...).\"\"\"\n            )\n\n        if not isinstance(query, str):\n            raise TypeError(\"'query' must be of type str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be of type bool\")\n\n        conn = conn or database.Connection()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_query\"\n        cmd[\"name_\"] = self.name\n        cmd[\"query_\"] = query\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def refresh(self) -&gt; \"DataFrame\":\n        \"\"\"Aligns meta-information of the current instance with the\n        corresponding data frame in the getML engine.\n\n        This method can be used to avoid encoding conflicts. Note that\n        [`load`][getml.DataFrame.load] as well as several other\n        methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.refresh\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n        if msg[0] != \"{\":\n            comm.engine_exception_handler(msg)\n\n        roles = json.loads(msg)\n\n        self.__init__(name=self.name, roles=roles)  # type: ignore\n\n        return self\n\n    # ------------------------------------------------------------\n\n    @property\n    def roles(self):\n        \"\"\"\n        The roles of the columns included\n        in this DataFrame.\n        \"\"\"\n        return Roles(\n            categorical=self._categorical_names,\n            join_key=self._join_key_names,\n            numerical=self._numerical_names,\n            target=self._target_names,\n            text=self._text_names,\n            time_stamp=self._time_stamp_names,\n            unused_float=self._unused_float_names,\n            unused_string=self._unused_string_names,\n        )\n\n    # ------------------------------------------------------------\n\n    def remove_subroles(self, cols):\n        \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n        Args:\n            columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        for name in names:\n            self._set_subroles(name, subroles=[], append=False)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def remove_unit(self, cols):\n        \"\"\"Removes the unit from one or more columns.\n\n        Args:\n            columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        for name in names:\n            self._set_unit(name, \"\")\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def rowid(self):\n        \"\"\"\n        The rowids for this data frame.\n        \"\"\"\n        return rowid()[: self.nrows()]\n\n    # ------------------------------------------------------------\n\n    def save(self) -&gt; \"DataFrame\":\n        \"\"\"Writes the underlying data in the getML engine to disk.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                The current instance.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.save\"\n        cmd[\"name_\"] = self.name\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def set_role(self, cols, role, time_formats=None):\n        \"\"\"Assigns a new role to one or more columns.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names of the columns.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n\n        Example:\n            ```python\n            data_df = dict(\n                animal=[\"hawk\", \"parrot\", \"goose\"],\n                votes=[12341, 5127, 65311],\n                date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n            df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n            df.set_role(['animal'], getml.data.roles.categorical)\n            df.set_role(['votes'], getml.data.roles.numerical)\n            df.set_role(\n                ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n            df\n            ```\n            ```\n            | date                        | animal      | votes     |\n            | time stamp                  | categorical | numerical |\n            ---------------------------------------------------------\n            | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n            | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n            | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n            ```\n        \"\"\"\n        # ------------------------------------------------------------\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        # ------------------------------------------------------------\n\n        names = _handle_cols(cols)\n\n        if not isinstance(role, str):\n            raise TypeError(\"'role' must be str.\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        # ------------------------------------------------------------\n\n        for nname in names:\n            if nname not in self.colnames:\n                raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n        if role not in self._possible_keys:\n            raise ValueError(\n                \"'role' must be one of the following values: \"\n                + str(self._possible_keys)\n            )\n\n        # ------------------------------------------------------------\n\n        for name in names:\n            if self[name].role != role:\n                self._set_role(name, role, time_formats)\n\n        # ------------------------------------------------------------\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def set_subroles(self, cols, subroles, append=True):\n        \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n                Must be from [`subroles`][getml.data.subroles].\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if isinstance(subroles, str):\n            subroles = [subroles]\n\n        if not _is_non_empty_typed_list(subroles, str):\n            raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be a bool.\")\n\n        for name in names:\n            self._set_subroles(name, subroles, append)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def set_unit(self, cols, unit, comparison_only=False):\n        \"\"\"Assigns a new unit to one or more columns.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if not isinstance(unit, str):\n            raise TypeError(\"Parameter 'unit' must be a str.\")\n\n        if comparison_only:\n            unit += COMPARISON_ONLY\n\n        for name in names:\n            self._set_unit(name, unit)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def shape(self):\n        \"\"\"\n        A tuple containing the number of rows and columns of\n        the DataFrame.\n        \"\"\"\n        self.refresh()\n        return (self.nrows(), self.ncols())\n\n    # ------------------------------------------------------------\n\n    @property\n    def _target_names(self):\n        return [col.name for col in self._target_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _text_names(self):\n        return [col.name for col in self._text_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _time_stamp_names(self):\n        return [col.name for col in self._time_stamp_columns]\n\n    # ----------------------------------------------------------------\n\n    def to_arrow(self):\n        \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyarrow.Table`.\n\n        Returns:\n            pyarrow.Table:\n                Pyarrow equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self)\n\n    # ------------------------------------------------------------\n\n    def to_csv(\n        self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file.\n\n        Args:\n            fname (str):\n                The name of the CSV file.\n                The ending \".csv\" and an optional batch number will\n                be added automatically.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        fname_ = os.path.abspath(fname)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fname_\"] = fname_\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_db(self, table_name, conn=None):\n        \"\"\"Writes the underlying data into a newly created table in the\n        database.\n\n        Args:\n            table_name (str):\n                Name of the table to be created.\n\n                If a table of that name already exists, it will be\n                replaced.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n        \"\"\"\n\n        conn = conn or database.Connection()\n\n        self.refresh()\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        cmd = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_db\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"table_name_\"] = table_name\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n    # ----------------------------------------------------------------\n\n    def to_html(self, max_rows=10):\n        \"\"\"\n        Represents the data frame in HTML format, optimized for an\n        iPython notebook.\n\n        Args:\n            max_rows (int):\n                The maximum number of rows to be displayed.\n        \"\"\"\n\n        if not _exists_in_memory(self.name):\n            return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n        formatted = self._format()\n        formatted.max_rows = max_rows\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_html(footer=footer)\n\n    # ------------------------------------------------------------\n\n    def to_json(self):\n        \"\"\"Creates a JSON string from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a JSON string.\n        \"\"\"\n        return self.to_pandas().to_json()\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self):\n        \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        `pandas.DataFrame`.\n\n        Returns:\n            pandas.DataFrame:\n                Pandas equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_arrow(self).to_pandas()\n\n    # ------------------------------------------------------------\n\n    def to_parquet(self, fname, compression=\"snappy\"):\n        \"\"\"\n        Writes the underlying data into a newly created parquet file.\n\n        Args:\n            fname (str):\n                The name of the parquet file.\n                The ending \".parquet\" will be added automatically.\n\n            compression (str):\n                The compression format to use.\n                Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n        \"\"\"\n        _to_parquet(self, fname, compression)\n\n    # ----------------------------------------------------------------\n\n    def to_placeholder(self, name=None):\n        \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n        current [`DataFrame`][getml.DataFrame].\n\n        Args:\n            name (str, optional):\n                The name of the placeholder. If no\n                name is passed, then the name of the placeholder will\n                be identical to the name of the current data frame.\n\n        Returns:\n            [`Placeholder`][getml.data.Placeholder]:\n                A placeholder with the same name as this data frame.\n\n\n        \"\"\"\n        self.refresh()\n        return Placeholder(name=name or self.name, roles=self.roles)\n\n    # ----------------------------------------------------------------\n\n    def to_pyspark(self, spark, name=None):\n        \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyspark.sql.DataFrame`.\n\n        Args:\n            spark (pyspark.sql.SparkSession):\n                The pyspark session in which you want to\n                create the data frame.\n\n            name (str or None):\n                The name of the temporary view to be created on top\n                of the `pyspark.sql.DataFrame`,\n                with which it can be referred to\n                in Spark SQL (refer to\n                `pyspark.sql.DataFrame.createOrReplaceTempView`).\n                If none is passed, then the name of this\n                [`DataFrame`][getml.DataFrame] will be used.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Pyspark equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_pyspark(self, name, spark)\n\n    # ------------------------------------------------------------\n\n    def to_s3(self, bucket: str, key: str, region: str, sep=\",\", batch_size=50000):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file\n        located in an S3 bucket.\n        Note:\n            Note that S3 is not supported on Windows.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            key (str):\n                The key in the S3 bucket in which you want to\n                write the output. The ending \".csv\" and an optional\n                batch number will be added automatically.\n\n            region (str):\n                The region in which the bucket is located.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n\n        Example:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            your_df.to_s3(\n                bucket=\"your-bucket-name\",\n                key=\"filename-on-s3\",\n                region=\"us-east-2\",\n                sep=';'\n            )\n            ```\n\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be of type str\")\n\n        if not isinstance(key, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"bucket_\"] = bucket\n        cmd[\"key_\"] = key\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_float_names(self):\n        return [col.name for col in self._unused_float_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_names(self):\n        return self._unused_float_names + self._unused_string_names\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_string_names(self):\n        return [col.name for col in self._unused_string_columns]\n\n    # ------------------------------------------------------------\n\n    def unload(self):\n        \"\"\"\n        Unloads the data frame from memory.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        self._delete(mem_only=True)\n\n    # ------------------------------------------------------------\n\n    def where(\n        self,\n        index: Union[\n            numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n        ],\n    ) -&gt; View:\n        \"\"\"Extract a subset of rows.\n\n        Creates a new [`View`][getml.data.View] as a\n        subselection of the current instance.\n\n        Args:\n            index (int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n                Indicates the rows you want to select.\n\n        Example:\n            Generate example data:\n            ```python\n            data = dict(\n                fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n                price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n                join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n            fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n            roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n            fruits\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 0        | banana      | 2.4       |\n            | 1        | apple       | 3         |\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            | 3        | melon       | 3.4       |\n            | 3        | pineapple   | 3.4       |\n            ```\n            Apply where condition. This creates a new DataFrame called \"cherries\":\n\n            ```python\n            cherries = fruits.where(\n                fruits[\"fruit\"] == \"cherry\")\n\n            cherries\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            ```\n\n        \"\"\"\n        if isinstance(index, numbers.Integral):\n            index = index if int(index) &gt; 0 else len(self) + index\n            selector = arange(int(index), int(index) + 1)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, slice):\n            start, stop, _ = _make_default_slice(index, len(self))\n            selector = arange(start, stop, index.step)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n            return View(base=self, subselection=index)\n\n        raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n\n    # ------------------------------------------------------------\n\n    def with_column(\n        self, col, name, role=None, subroles=None, unit=\"\", time_formats=None\n    ):\n        \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n        Args:\n            col ([`columns`][getml.columns]):\n                The column to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from `getml.data.roles`.\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        \"\"\"\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n        return View(\n            base=self,\n            added={\n                \"col_\": col,\n                \"name_\": name,\n                \"role_\": role,\n                \"subroles_\": subroles,\n                \"unit_\": unit,\n            },\n        )\n\n    # ------------------------------------------------------------\n\n    def with_name(self, name):\n        \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n        Args:\n            name (str):\n                The name of the new view.\n        \"\"\"\n        return View(base=self, name=name)\n\n    # ------------------------------------------------------------\n\n    def with_role(self, cols, role, time_formats=None):\n        \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n        The difference between [`with_role`][getml.DataFrame.with_role] and\n        [`set_role`][getml.DataFrame.set_role] is that\n        [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n        evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret time\n        format string: `annotating_roles_time_stamp`. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to\n                parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n        \"\"\"\n        return _with_role(self, cols, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    def with_subroles(self, cols, subroles, append=True):\n        \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n        The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n        [`set_subroles`][getml.DataFrame.set_subroles] is that\n        [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n        evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n        return _with_subroles(self, cols, subroles, append)\n\n    # ------------------------------------------------------------\n\n    def with_unit(self, cols, unit, comparison_only=False):\n        \"\"\"Returns a view that contains a new unit on one or more columns.\n\n        The difference between [`with_unit`][getml.DataFrame.with_unit] and\n        [`set_unit`][getml.DataFrame.set_unit] is that\n        [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n        evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n\n                If True, this will also set the\n                [`compare`][getml.data.subroles.only.compare] subrole.  The feature\n                learning algorithms and the feature selectors will interpret this\n                accordingly.\n        \"\"\"\n        return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.colnames","title":"<code>colnames</code>  <code>property</code>","text":"<p>List of the names of all columns.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Alias for <code>colnames</code>.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.last_change","title":"<code>last_change: str</code>  <code>property</code>","text":"<p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.memory_usage","title":"<code>memory_usage</code>  <code>property</code>","text":"<p>Convenience wrapper that returns the memory usage in MB.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.roles","title":"<code>roles</code>  <code>property</code>","text":"<p>The roles of the columns included in this DataFrame.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.rowid","title":"<code>rowid</code>  <code>property</code>","text":"<p>The rowids for this data frame.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>A tuple containing the number of rows and columns of the DataFrame.</p>"},{"location":"reference/data/__init__/#getml.data.DataFrame.add","title":"<code>add(col, name, role=None, subroles=None, unit='', time_formats=None)</code>","text":"<p>Adds a column to the current <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`column`][getml.column] or `numpy.ndarray`</code> <p>The column or numpy.ndarray to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>getml.data.roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def add(self, col, name, role=None, subroles=None, unit=\"\", time_formats=None):\n    \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        col ([`column`][getml.column] or `numpy.ndarray`):\n            The column or numpy.ndarray to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from `getml.data.roles`.\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n\n    if isinstance(col, np.ndarray):\n        self._add_numpy_array(col, name, role, subroles, unit)\n        return\n\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n\n    is_string = isinstance(col, (StringColumnView, StringColumn))\n\n    if is_string:\n        self._add_categorical_column(col, name, role, subroles, unit)\n    else:\n        self._add_column(col, name, role, subroles, unit)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.copy","title":"<code>copy(name)</code>","text":"<p>Creates a deep copy of the data frame under a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new data frame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: A handle to the deep copy.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def copy(self, name: str) -&gt; \"DataFrame\":\n    \"\"\"\n    Creates a deep copy of the data frame under a new name.\n\n    Args:\n        name (str):\n            The name of the new data frame.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            A handle to the deep copy.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.delete","title":"<code>delete()</code>","text":"<p>Permanently deletes the data frame. <code>delete</code> first unloads the data frame from memory and then deletes it from disk.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Permanently deletes the data frame. `delete` first unloads the data frame\n    from memory and then deletes it from disk.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    self._delete()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.drop","title":"<code>drop(cols)</code>","text":"<p>Returns a new <code>View</code> that has one or several columns removed.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def drop(self, cols):\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not _is_typed_list(names, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=names)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.freeze","title":"<code>freeze()</code>","text":"<p>Freezes the data frame.</p> <p>After you have frozen the data frame, the data frame is immutable and in-place operations are no longer possible. However, you can still create views. In other words, operations like <code>set_role</code> are no longer possible, but operations like <code>with_role</code> are.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def freeze(self):\n    \"\"\"Freezes the data frame.\n\n    After you have frozen the data frame, the data frame is immutable\n    and in-place operations are no longer possible. However, you can\n    still create views. In other words, operations like\n    [`set_role`][getml.DataFrame.set_role] are no longer possible,\n    but operations like [`with_role`][getml.DataFrame.with_role] are.\n    \"\"\"\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.freeze\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_arrow","title":"<code>from_arrow(table, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from an Arrow Table.</p> <p>This is one of the fastest way to get data into the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_arrow(cls, table, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from an Arrow Table.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        table (pyarrow.Table):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_arrow(table)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_arrow(table=table, append=False)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_csv","title":"<code>from_csv(fnames, name, num_lines_sniffed=1000, num_lines_read=0, quotechar='\"', sep=',', skip=0, colnames=None, roles=None, ignore=False, dry=False, verbose=True)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from CSV files.</p> <p>The getML engine will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>CSV file paths to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, when fnames are urls, the filenames are printed to stdout during the download.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Note <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the current working directory. You can import their data into the getML engine using. <pre><code>df_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"'\n    )\n\n# However, the CSV format lacks type safety. If you want to\n# build a reliable pipeline, it is a good idea\n# to hard-code the roles:\n\nroles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n    )\n\n# If you think that typing out all the roles by hand is too\n# cumbersome, you can use a dry run:\n\nroles = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True\n)\n</code></pre></p> <p>This will return the roles dictionary it would have used. You can now hard-code this.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    fnames,\n    name,\n    num_lines_sniffed=1000,\n    num_lines_read=0,\n    quotechar='\"',\n    sep=\",\",\n    skip=0,\n    colnames=None,\n    roles=None,\n    ignore=False,\n    dry=False,\n    verbose=True,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files.\n\n    The getML engine will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        fnames (List[str]):\n            CSV file paths to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional): The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        verbose (bool, optional):\n            If True, when fnames are urls, the filenames are\n            printed to stdout during the download.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Note:\n        It is assumed that the first line of each CSV file\n        contains a header with the column names.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the current working directory. You can\n        import their data into the getML engine using.\n        ```python\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"'\n            )\n\n        # However, the CSV format lacks type safety. If you want to\n        # build a reliable pipeline, it is a good idea\n        # to hard-code the roles:\n\n        roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n            )\n\n        # If you think that typing out all the roles by hand is too\n        # cumbersome, you can use a dry run:\n\n        roles = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            dry=True\n        )\n        ```\n\n        This will return the roles dictionary it would have used. You\n        can now hard-code this.\n\n    \"\"\"\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    fnames = _retrieve_urls(fnames, verbose=verbose)\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_csv(\n            fnames=fnames,\n            num_lines_sniffed=int(num_lines_sniffed),\n            quotechar=quotechar,\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_csv(\n        fnames=fnames,\n        append=False,\n        quotechar=quotechar,\n        sep=sep,\n        num_lines_read=num_lines_read,\n        skip=skip,\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_db","title":"<code>from_db(table_name, name=None, roles=None, ignore=False, dry=False, conn=None)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a table in a database.</p> <p>It will construct a data frame object in the engine, fill it with the data read from table <code>table_name</code> in the connected database (see <code>database</code>), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created. If not passed, then the table_name will be used.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Example <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    port=3306,\n    dbname=\"financial\",\n    user=\"guest\",\n    password=\"relational\"\n)\n\nloan = getml.DataFrame.from_db(\n    table_name='loan', name='data_frame_loan')\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_db(\n    cls, table_name, name=None, roles=None, ignore=False, dry=False, conn=None\n):\n    \"\"\"Create a DataFrame from a table in a database.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from table `table_name` in the connected\n    database (see [`database`][getml.database]), and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        table_name (str):\n            Name of the table to be read.\n\n        name (str):\n            Name of the data frame to be created. If not passed,\n            then the *table_name* will be used.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection, the engine\n            will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Example:\n        ```python\n        getml.database.connect_mysql(\n            host=\"db.relational-data.org\",\n            port=3306,\n            dbname=\"financial\",\n            user=\"guest\",\n            password=\"relational\"\n        )\n\n        loan = getml.DataFrame.from_db(\n            table_name='loan', name='data_frame_loan')\n        ```\n    \"\"\"\n\n    # -------------------------------------------\n\n    name = name or table_name\n\n    # -------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\n            \"'roles' must be a getml.data.Roles object, a dict or None.\"\n        )\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # -------------------------------------------\n\n    conn = conn or database.Connection()\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_db(table_name, conn)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_dict","title":"<code>from_dict(data, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a new DataFrame from a dict</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dict containing the data. The data should be in the following format: <pre><code>data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n</code></pre></p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, data: Dict[str, List[Any]], name: str, roles=None, ignore=False, dry=False\n):\n    \"\"\"Create a new DataFrame from a dict\n\n    Args:\n        data (dict):\n            The dict containing the data.\n            The data should be in the following format:\n            ```python\n            data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n            ```\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(data, dict):\n        raise TypeError(\"'data' must be dict.\")\n\n    return cls.from_arrow(\n        table=pa.Table.from_pydict(data),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_json","title":"<code>from_json(json_str, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a new DataFrame from a JSON string.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the JSON string, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The JSON string containing the data. The json_str should be in the following format: <pre><code>json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n</code></pre></p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a new DataFrame from a JSON string.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the JSON string, and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        json_str (str):\n            The JSON string containing the data.\n            The json_str should be in the following format:\n            ```python\n            json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n            ```\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n    Returns:\n        [`DataFrame`][getml.data.DataFrame]: Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be str.\")\n\n    return cls.from_dict(\n        data=json.loads(json_str),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_pandas","title":"<code>from_pandas(pandas_df, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>pandas.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pandas.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>pandas_df</code> <code>DataFrame</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code> roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n          getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pandas(cls, pandas_df, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pandas.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        pandas_df (pandas.DataFrame):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n             roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                      getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    pandas_df_modified = _modify_pandas_columns(pandas_df)\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_pandas(pandas_df_modified)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pandas(pandas_df=pandas_df_modified, append=False)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_parquet","title":"<code>from_parquet(fname, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from parquet files.</p> <p>This is one of the fastest way to get data into the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The path of the parquet file to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_parquet(cls, fname, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from parquet files.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        fname (str):\n            The path of the parquet file to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_parquet(fname)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_parquet(fname=fname, append=False)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_pyspark","title":"<code>from_pyspark(spark_df, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>pyspark.sql.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pyspark.sql.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>DataFrame</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre></p> <p>Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pyspark(cls, spark_df, name, roles=None, ignore=False, dry=False) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pyspark.sql.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        spark_df (pyspark.sql.DataFrame):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        head = spark_df.limit(2).toPandas()\n\n        sniffed_roles = _sniff_pandas(head)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pyspark(spark_df=spark_df, append=False)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_s3","title":"<code>from_s3(bucket, keys, region, name, num_lines_sniffed=1000, num_lines_read=0, sep=',', skip=0, colnames=None, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from CSV files located in an S3 bucket.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\ndata_frame_expd = data.DataFrame.from_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY DATA FRAME\",\n    sep=';'\n)\n</code></pre></p> <p>You can also set the access credential as environment variables before you launch the getML engine.</p> <p>Also refer to the documentation on <code>from_csv</code> for further information on overriding the CSV sniffer for greater type safety.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_s3(\n    cls,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    name: str,\n    num_lines_sniffed=1000,\n    num_lines_read=0,\n    sep=\",\",\n    skip=0,\n    colnames=None,\n    roles=None,\n    ignore=False,\n    dry=False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        name (str):\n            Name of the data frame to be created.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        data_frame_expd = data.DataFrame.from_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY DATA FRAME\",\n            sep=';'\n        )\n        ```\n\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n        Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n        for further information on overriding the CSV sniffer for greater\n        type safety.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_s3(\n            bucket=bucket,\n            keys=keys,\n            region=region,\n            num_lines_sniffed=int(num_lines_sniffed),\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_s3(\n        bucket=bucket,\n        keys=keys,\n        region=region,\n        append=False,\n        sep=sep,\n        num_lines_read=int(num_lines_read),\n        skip=int(skip),\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.from_view","title":"<code>from_view(view, name, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>View</code>.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the <code>View</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>[`View`][getml.data.View]</code> <p>The view from which we want to read the data.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_view(\n    cls,\n    view,\n    name,\n    dry=False,\n):\n    \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the [`View`][getml.data.View], and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        view ([`View`][getml.data.View]):\n            The view from which we want to read the data.\n\n        name (str):\n            Name of the data frame to be created.\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if dry:\n        return view.roles\n\n    data_frame = cls(name)\n\n    # ------------------------------------------------------------\n\n    return data_frame.read_view(view=view, append=False)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.load","title":"<code>load()</code>","text":"<p>Loads saved data from disk.</p> <p>The data frame object holding the same name as the current <code>DataFrame</code> instance will be loaded from disk into the getML engine and updates the current handler using <code>refresh</code>.</p> Example <p>First, we have to create and import data sets. <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\ngetml.data.list_data_frames()\n</code></pre></p> <p>In the output of <code>list_data_frames</code> we can find our underlying data frame object 'test' listed under the 'in_memory' key (it was created and imported by <code>make_numerical</code>). This means the getML engine does only hold it in memory (RAM) yet, and we still have to <code>save</code> it to disk in order to <code>load</code> it again or to prevent any loss of information between different sessions. <pre><code>d.save()\ngetml.data.list_data_frames()\nd2 = getml.DataFrame(name = 'test').load()\n</code></pre></p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Updated handle the underlying data frame in the getML engine.</p> Note <p>When invoking <code>load</code> all changes of the underlying data frame object that took place after the last call to the <code>save</code> method will be lost. Thus, this method  enables you to undo changes applied to the <code>DataFrame</code>. <pre><code>d, _ = getml.datasets.make_numerical()\nd.save()\n\n# Accidental change we want to undo\nd.rm('column_01')\n\nd.load()\n</code></pre> If <code>save</code> hasn't been called on the current instance yet, or it wasn't stored to disk in a previous session, <code>load</code> will throw an exception</p> <pre><code>File or directory '../projects/X/data/Y/' not found!\n</code></pre> <p>Alternatively, <code>load_data_frame</code> offers an easier way of creating <code>DataFrame</code> handlers to data in the getML engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def load(self) -&gt; \"DataFrame\":\n    \"\"\"Loads saved data from disk.\n\n    The data frame object holding the same name as the current\n    [`DataFrame`][getml.DataFrame] instance will be loaded from\n    disk into the getML engine and updates the current handler\n    using [`refresh`][getml.DataFrame.refresh].\n\n    Example:\n        First, we have to create and import data sets.\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        getml.data.list_data_frames()\n        ```\n\n        In the output of [`list_data_frames`][getml.data.list_data_frames] we\n        can find our underlying data frame object 'test' listed\n        under the 'in_memory' key (it was created and imported by\n        [`make_numerical`][getml.datasets.make_numerical]). This means the\n        getML engine does only hold it in memory (RAM) yet, and we\n        still have to [`save`][getml.DataFrame.save] it to\n        disk in order to [`load`][getml.DataFrame.load] it\n        again or to prevent any loss of information between\n        different sessions.\n        ```python\n        d.save()\n        getml.data.list_data_frames()\n        d2 = getml.DataFrame(name = 'test').load()\n        ```\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    Note:\n        When invoking [`load`][getml.DataFrame.load] all\n        changes of the underlying data frame object that took\n        place after the last call to the\n        [`save`][getml.DataFrame.save] method will be\n        lost. Thus, this method  enables you to undo changes\n        applied to the [`DataFrame`][getml.DataFrame].\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        d.save()\n\n        # Accidental change we want to undo\n        d.rm('column_01')\n\n        d.load()\n        ```\n        If [`save`][getml.DataFrame.save] hasn't been called\n        on the current instance yet, or it wasn't stored to disk in\n        a previous session, [`load`][getml.DataFrame.load]\n        will throw an exception\n\n            File or directory '../projects/X/data/Y/' not found!\n\n        Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n        offers an easier way of creating\n        [`DataFrame`][getml.DataFrame] handlers to data in the\n        getML engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.load\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.nbytes","title":"<code>nbytes()</code>","text":"<p>Size of the data stored in the underlying data frame in the getML engine.</p> <p>Returns:</p> Type Description <p>numpy.uint64: Size of the underlying object in bytes.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nbytes(self):\n    \"\"\"Size of the data stored in the underlying data frame in the getML\n    engine.\n\n    Returns:\n        numpy.uint64:\n            Size of the underlying object in bytes.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nbytes\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nbytes = comm.recv_string(sock)\n\n    return np.uint64(nbytes)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.ncols","title":"<code>ncols()</code>","text":"<p>Number of columns in the current instance.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def ncols(self):\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n        int:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.nrows","title":"<code>nrows()</code>","text":"<p>Number of rows in the current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nrows(self):\n    \"\"\"\n    Number of rows in the current instance.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nrows\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nrows = comm.recv_string(sock)\n\n    return int(nrows)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_arrow","title":"<code>read_arrow(table, append=False)</code>","text":"<p>Uploads a <code>pyarrow.Table</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Current instance.</p> Note <p>For columns containing <code>pandas.Timestamp</code> there can be small inconsistencies in the order of microseconds when sending the data to the getML engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_arrow(self, table, append=False):\n    \"\"\"Uploads a `pyarrow.Table`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `table`.\n\n    Args:\n        table (pyarrow.Table):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Current instance.\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        be small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_arrow\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    with comm.send_and_get_socket(cmd) as sock:\n        with sock.makefile(mode=\"wb\") as sink:\n            batches = table.to_batches()\n            with pa.ipc.new_stream(sink, table.schema) as writer:\n                for batch in batches:\n                    writer.write_batch(batch)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_csv","title":"<code>read_csv(fnames, append=False, quotechar='\"', sep=',', num_lines_read=0, skip=0, colnames=None, time_formats=None, verbose=True)</code>","text":"<p>Read CSV files.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>CSV file paths to be read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_csv(\n    self,\n    fnames,\n    append=False,\n    quotechar='\"',\n    sep=\",\",\n    num_lines_read=0,\n    skip=0,\n    colnames=None,\n    time_formats=None,\n    verbose=True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read CSV files.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        fnames (List[str]):\n            CSV file paths to be read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        verbose (bool, optional):\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_csv(...).\"\"\"\n        )\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\n            \"\"\"'fnames' must be a list containing at\n            least one path to a CSV file\"\"\"\n        )\n\n    fnames_ = _retrieve_urls(fnames, verbose)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fnames_\"] = fnames_\n\n    cmd[\"append_\"] = append\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"time_formats_\"] = time_formats\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_db","title":"<code>read_db(table_name, append=False, conn=None)</code>","text":"<p>Fill from Database.</p> <p>The DataFrame will be filled from a table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table from which we want to retrieve the data.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of <code>table_name</code> be appended or replace the existing data?</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_db(self, table_name: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n    \"\"\"\n    Fill from Database.\n\n    The DataFrame will be filled from a table in the database.\n\n    Args:\n        table_name (str):\n            Table from which we want to retrieve the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `table_name` be appended or replace the existing data?\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_db\"\n    cmd[\"name_\"] = self.name\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_json","title":"<code>read_json(json_str, append=False, time_formats=None)</code>","text":"<p>Fill from JSON</p> <p>Fills the data frame with data from a JSON string.</p> <p>Args:</p> <pre><code>json_str (str):\n    The JSON string containing the data.\n\nappend (bool, optional):\n    If a data frame object holding the same ``name`` is\n    already present in the getML, should the content of\n    `json_str` be appended or replace the existing data?\n\ntime_formats (List[str], optional):\n    The list of formats tried when parsing time stamps.\n    The formats are allowed to contain the following\n    special characters:\n\n    * %w - abbreviated weekday (Mon, Tue, ...)\n    * %W - full weekday (Monday, Tuesday, ...)\n    * %b - abbreviated month (Jan, Feb, ...)\n    * %B - full month (January, February, ...)\n    * %d - zero-padded day of month (01 .. 31)\n    * %e - day of month (1 .. 31)\n    * %f - space-padded day of month ( 1 .. 31)\n    * %m - zero-padded month (01 .. 12)\n    * %n - month (1 .. 12)\n    * %o - space-padded month ( 1 .. 12)\n    * %y - year without century (70)\n    * %Y - year with century (1970)\n    * %H - hour (00 .. 23)\n    * %h - hour (00 .. 12)\n    * %a - am/pm\n    * %A - AM/PM\n    * %M - minute (00 .. 59)\n    * %S - second (00 .. 59)\n    * %s - seconds and microseconds (equivalent to %S.%F)\n    * %i - millisecond (000 .. 999)\n    * %c - centisecond (0 .. 9)\n    * %F - fractional seconds/microseconds (000000 - 999999)\n    * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n    * %Z - time zone differential in RFC format (GMT or +NNNN)\n    * %% - percent sign\n</code></pre> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Note <p>This does not support NaN values. If you want support for NaN, use <code>from_json</code> instead.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_json(self, json_str, append=False, time_formats=None):\n    \"\"\"Fill from JSON\n\n    Fills the data frame with data from a JSON string.\n\n    Args:\n\n        json_str (str):\n            The JSON string containing the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `json_str` be appended or replace the existing data?\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    Note:\n        This does not support NaN values. If you want support for NaN,\n        use [`from_json`][getml.DataFrame.from_json] instead.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_json(...).\"\"\"\n        )\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\n            \"\"\"'time_formats' must be a list of strings\n            containing at least one time format\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.from_json\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n    cmd[\"time_formats_\"] = time_formats\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, json_str)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_pandas","title":"<code>read_pandas(pandas_df, append=False)</code>","text":"<p>Uploads a <code>pandas.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pandas_df</code> <code>DataFrame</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> Note <p>For columns containing <code>pandas.Timestamp</code> there can occur small inconsistencies in the order of microseconds when sending the data to the getML engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pandas.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        pandas_df (pandas.DataFrame):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        occur small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    table = pa.Table.from_pandas(_modify_pandas_columns(pandas_df))\n\n    return self.read_arrow(table, append=append)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_parquet","title":"<code>read_parquet(fname, append=False, verbose=True)</code>","text":"<p>Read a parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The filepath of the parquet file to be read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_parquet(\n    self,\n    fname: str,\n    append: bool = False,\n    verbose: bool = True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read a parquet file.\n\n    Args:\n        fname (str):\n            The filepath of the parquet file to be read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n    \"\"\"\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than\n            zero columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_parquet(...).\"\"\"\n        )\n\n    fname_ = _retrieve_urls([fname], verbose)[0]\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_parquet\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_pyspark","title":"<code>read_pyspark(spark_df, append=False)</code>","text":"<p>Uploads a <code>pyspark.sql.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>DataFrame</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pyspark(self, spark_df, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        spark_df (pyspark.sql.DataFrame):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n    \"\"\"\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    temp_dir = _retrieve_temp_dir()\n    os.makedirs(temp_dir, exist_ok=True)\n    path = os.path.join(temp_dir, self.name)\n    spark_df.write.mode(\"overwrite\").parquet(path)\n\n    filepaths = [\n        os.path.join(path, filepath)\n        for filepath in os.listdir(path)\n        if filepath[-8:] == \".parquet\"\n    ]\n\n    for i, filepath in enumerate(filepaths):\n        self.read_parquet(filepath, append or i &gt; 0)\n\n    shutil.rmtree(path)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_query","title":"<code>read_query(query, append=False, conn=None)</code>","text":"<p>Fill from query</p> <p>Fills the data frame with data from a table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query used to retrieve the data.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_query(self, query: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n    \"\"\"Fill from query\n\n    Fills the data frame with data from a table in the database.\n\n    Args:\n        query (str):\n            The query used to retrieve the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n    \"\"\"\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_query\"\n    cmd[\"name_\"] = self.name\n    cmd[\"query_\"] = query\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_s3","title":"<code>read_s3(bucket, keys, region, append=False, sep=',', num_lines_read=0, skip=0, colnames=None, time_formats=None)</code>","text":"<p>Read CSV files from an S3 bucket.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_s3(\n    self,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Read CSV files from an S3 bucket.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(keys, list):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_s3(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"skip_\"] = skip\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.read_view","title":"<code>read_view(view, append=False)</code>","text":"<p>Read the data from a <code>View</code>.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>[`View`][getml.data.View]</code> <p>The view to read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_view(\n    self,\n    view: View,\n    append: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Read the data from a [`View`][getml.data.View].\n\n    Args:\n        view ([`View`][getml.data.View]):\n            The view to read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    view.check()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_view\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = view._getml_deserialize()\n\n    cmd[\"append_\"] = append\n\n    comm.send(cmd)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.refresh","title":"<code>refresh()</code>","text":"<p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> <p>This method can be used to avoid encoding conflicts. Note that <code>load</code> as well as several other methods automatically call <code>refresh</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Updated handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def refresh(self) -&gt; \"DataFrame\":\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    This method can be used to avoid encoding conflicts. Note that\n    [`load`][getml.DataFrame.load] as well as several other\n    methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.refresh\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    roles = json.loads(msg)\n\n    self.__init__(name=self.name, roles=roles)  # type: ignore\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.remove_subroles","title":"<code>remove_subroles(cols)</code>","text":"<p>Removes all <code>subroles</code> from one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_subroles(self, cols):\n    \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n    Args:\n        columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_subroles(name, subroles=[], append=False)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.remove_unit","title":"<code>remove_unit(cols)</code>","text":"<p>Removes the unit from one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_unit(self, cols):\n    \"\"\"Removes the unit from one or more columns.\n\n    Args:\n        columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_unit(name, \"\")\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.save","title":"<code>save()</code>","text":"<p>Writes the underlying data in the getML engine to disk.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: The current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def save(self) -&gt; \"DataFrame\":\n    \"\"\"Writes the underlying data in the getML engine to disk.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            The current instance.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.save\"\n    cmd[\"name_\"] = self.name\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.set_role","title":"<code>set_role(cols, role, time_formats=None)</code>","text":"<p>Assigns a new role to one or more columns.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret Time Stamps. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names of the columns.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Example <p><pre><code>data_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\ndf = getml.DataFrame.from_dict(data_df, \"animal_elections\")\ndf.set_role(['animal'], getml.data.roles.categorical)\ndf.set_role(['votes'], getml.data.roles.numerical)\ndf.set_role(\n    ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\ndf\n</code></pre> <pre><code>| date                        | animal      | votes     |\n| time stamp                  | categorical | numerical |\n---------------------------------------------------------\n| 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n| 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n| 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_role(self, cols, role, time_formats=None):\n    \"\"\"Assigns a new role to one or more columns.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names of the columns.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    Example:\n        ```python\n        data_df = dict(\n            animal=[\"hawk\", \"parrot\", \"goose\"],\n            votes=[12341, 5127, 65311],\n            date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n        df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n        df.set_role(['animal'], getml.data.roles.categorical)\n        df.set_role(['votes'], getml.data.roles.numerical)\n        df.set_role(\n            ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n        df\n        ```\n        ```\n        | date                        | animal      | votes     |\n        | time stamp                  | categorical | numerical |\n        ---------------------------------------------------------\n        | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n        | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n        | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n        ```\n    \"\"\"\n    # ------------------------------------------------------------\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # ------------------------------------------------------------\n\n    names = _handle_cols(cols)\n\n    if not isinstance(role, str):\n        raise TypeError(\"'role' must be str.\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    # ------------------------------------------------------------\n\n    for nname in names:\n        if nname not in self.colnames:\n            raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n    if role not in self._possible_keys:\n        raise ValueError(\n            \"'role' must be one of the following values: \"\n            + str(self._possible_keys)\n        )\n\n    # ------------------------------------------------------------\n\n    for name in names:\n        if self[name].role != role:\n            self._set_role(name, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.set_subroles","title":"<code>set_subroles(cols, subroles, append=True)</code>","text":"<p>Assigns one or several new <code>subroles</code> to one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned. Must be from <code>subroles</code>.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_subroles(self, cols, subroles, append=True):\n    \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n            Must be from [`subroles`][getml.data.subroles].\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if isinstance(subroles, str):\n        subroles = [subroles]\n\n    if not _is_non_empty_typed_list(subroles, str):\n        raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be a bool.\")\n\n    for name in names:\n        self._set_subroles(name, subroles, append)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.set_unit","title":"<code>set_unit(cols, unit, comparison_only=False)</code>","text":"<p>Assigns a new unit to one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_unit(self, cols, unit, comparison_only=False):\n    \"\"\"Assigns a new unit to one or more columns.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not isinstance(unit, str):\n        raise TypeError(\"Parameter 'unit' must be a str.\")\n\n    if comparison_only:\n        unit += COMPARISON_ONLY\n\n    for name in names:\n        self._set_unit(name, unit)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Creates a <code>pyarrow.Table</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <p>pyarrow.Table: Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_arrow(self):\n    \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n        pyarrow.Table:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_csv","title":"<code>to_csv(fname, quotechar='\"', sep=',', batch_size=0)</code>","text":"<p>Writes the underlying data into a newly created CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>0</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname (str):\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_db","title":"<code>to_db(table_name, conn=None)</code>","text":"<p>Writes the underlying data into a newly created table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_db(self, table_name, conn=None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name (str):\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or database.Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    cmd = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_db\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_html","title":"<code>to_html(max_rows=10)</code>","text":"<p>Represents the data frame in HTML format, optimized for an iPython notebook.</p> <p>Parameters:</p> Name Type Description Default <code>max_rows</code> <code>int</code> <p>The maximum number of rows to be displayed.</p> <code>10</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_html(self, max_rows=10):\n    \"\"\"\n    Represents the data frame in HTML format, optimized for an\n    iPython notebook.\n\n    Args:\n        max_rows (int):\n            The maximum number of rows to be displayed.\n    \"\"\"\n\n    if not _exists_in_memory(self.name):\n        return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n    formatted = self._format()\n    formatted.max_rows = max_rows\n\n    footer = self._collect_footer_data()\n\n    return formatted._render_html(footer=footer)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_json","title":"<code>to_json()</code>","text":"<p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Creates a <code>pandas.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs <code>pandas.DataFrame</code>.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: Pandas equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pandas(self):\n    \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    `pandas.DataFrame`.\n\n    Returns:\n        pandas.DataFrame:\n            Pandas equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_parquet","title":"<code>to_parquet(fname, compression='snappy')</code>","text":"<p>Writes the underlying data into a newly created parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> required <code>compression</code> <code>str</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <code>'snappy'</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_parquet(self, fname, compression=\"snappy\"):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname (str):\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression (str):\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_placeholder","title":"<code>to_placeholder(name=None)</code>","text":"<p>Generates a <code>Placeholder</code> from the current <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current data frame.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>Placeholder</code>: A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_placeholder(self, name=None):\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        name (str, optional):\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current data frame.\n\n    Returns:\n        [`Placeholder`][getml.data.Placeholder]:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_pyspark","title":"<code>to_pyspark(spark, name=None)</code>","text":"<p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The pyspark session in which you want to create the data frame.</p> required <code>name</code> <code>str or None</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pyspark(self, spark, name=None):\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark (pyspark.sql.SparkSession):\n            The pyspark session in which you want to\n            create the data frame.\n\n        name (str or None):\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.to_s3","title":"<code>to_s3(bucket, key, region, sep=',', batch_size=50000)</code>","text":"<p>Writes the underlying data into a newly created CSV file located in an S3 bucket. Note:     Note that S3 is not supported on Windows.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>key</code> <code>str</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>50000</code> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_df.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_s3(self, bucket: str, key: str, region: str, sep=\",\", batch_size=50000):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        key (str):\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region (str):\n            The region in which the bucket is located.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_df.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.unload","title":"<code>unload()</code>","text":"<p>Unloads the data frame from memory.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads the data frame from memory.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    self._delete(mem_only=True)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.where","title":"<code>where(index)</code>","text":"<p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Indicates the rows you want to select.</p> required Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def where(\n    self,\n    index: Union[\n        numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n    ],\n) -&gt; View:\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index (int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Indicates the rows you want to select.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(int(index), int(index) + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, _ = _make_default_slice(index, len(self))\n        selector = arange(start, stop, index.step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.with_column","title":"<code>with_column(col, name, role=None, subroles=None, unit='', time_formats=None)</code>","text":"<p>Returns a new <code>View</code> that contains an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`columns`][getml.columns]</code> <p>The column to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>getml.data.roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_column(\n    self, col, name, role=None, subroles=None, unit=\"\", time_formats=None\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col ([`columns`][getml.columns]):\n            The column to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from `getml.data.roles`.\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.with_name","title":"<code>with_name(name)</code>","text":"<p>Returns a new <code>View</code> with a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new view.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_name(self, name):\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.with_role","title":"<code>with_role(cols, role, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> with modified roles.</p> <p>The difference between <code>with_role</code> and <code>set_role</code> is that <code>with_role</code> returns a view that is lazily evaluated when needed whereas <code>set_role</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_role</code> are preferable.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_role(self, cols, role, time_formats=None):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    The difference between [`with_role`][getml.DataFrame.with_role] and\n    [`set_role`][getml.DataFrame.set_role] is that\n    [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n    evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to\n            parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, cols, role, time_formats)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.with_subroles","title":"<code>with_subroles(cols, subroles, append=True)</code>","text":"<p>Returns a new view with one or several new subroles on one or more columns.</p> <p>The difference between <code>with_subroles</code> and <code>set_subroles</code> is that <code>with_subroles</code> returns a view that is lazily evaluated when needed whereas <code>set_subroles</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_subroles</code> are preferable.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_subroles(self, cols, subroles, append=True):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n    [`set_subroles`][getml.DataFrame.set_subroles] is that\n    [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n    evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, cols, subroles, append)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataFrame.with_unit","title":"<code>with_unit(cols, unit, comparison_only=False)</code>","text":"<p>Returns a view that contains a new unit on one or more columns.</p> <p>The difference between <code>with_unit</code> and <code>set_unit</code> is that <code>with_unit</code> returns a view that is lazily evaluated when needed whereas <code>set_unit</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_unit</code> are preferable.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will also set the <code>compare</code> subrole.  The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_unit(self, cols, unit, comparison_only=False):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    The difference between [`with_unit`][getml.DataFrame.with_unit] and\n    [`set_unit`][getml.DataFrame.set_unit] is that\n    [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n    evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will also set the\n            [`compare`][getml.data.subroles.only.compare] subrole.  The feature\n            learning algorithms and the feature selectors will interpret this\n            accordingly.\n    \"\"\"\n    return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataModel","title":"<code>DataModel</code>","text":"<p>Abstract representation of the relationship between tables.</p> <p>You might also want to refer to <code>Placeholder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder representing the population table, which defines the statistical population and contains the targets.</p> required Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n    )\n</code></pre></p> <p>Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre>     If you want to check out a real-world example where this     is necessary, refer to the     CORA notebook.</p> Source code in <code>getml/data/data_model.py</code> <pre><code>class DataModel:\n    \"\"\"\nAbstract representation of the relationship between tables.\n\nYou might also want to refer to [`Placeholder`][getml.data.Placeholder].\n\nArgs:\n    population ([`Placeholder`][getml.data.Placeholder]):\n        The placeholder representing the population table,\n        which defines the\n        [statistical population](https://en.wikipedia.org/wiki/Statistical_population)\n        and contains the targets.\n\nExample:\n    This example will construct a data model in which the\n    'population_table' depends on the 'peripheral_table' via\n    the 'join_key' column. In addition, only those rows in\n    'peripheral_table' for which 'time_stamp' is smaller or\n    equal to the 'time_stamp' in 'population_table' are considered:\n    ```python\n    dm = getml.data.DataModel(\n        population_table.to_placeholder(\"POPULATION\")\n    )\n\n    dm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\"\n    )\n    ```\n    If you want to add more than one peripheral table, you can\n    use [`to_placeholder`][getml.data.to_placeholder]:\n    ```python\n    dm = getml.data.DataModel(\n        population_table.to_placeholder(\"POPULATION\")\n    )\n\n    dm.add(\n        getml.data.to_placeholder(\n            PERIPHERAL1=peripheral_table_1,\n            PERIPHERAL2=peripheral_table_2,\n        )\n    )\n    ```\n    If the relationship between two tables is many-to-one or one-to-one\n    you should clearly say so:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        relationship=getml.data.relationship.many_to_one,\n    )\n    ```\n    Please also refer to [`relationship`][getml.data.relationship].\n\n    If the join keys or time stamps are named differently in the two\n    different tables, use a tuple:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=(\"join_key\", \"other_join_key\"),\n        time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n    )\n    ```\n    You can join over more than one join key:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n        time_stamps=\"time_stamp\",\n    )\n    ```\n    You can also limit the scope of your joins using *memory*. This\n    can significantly speed up training time. For instance, if you\n    only want to consider data from the last seven days, you could\n    do something like this:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        memory=getml.data.time.days(7),\n    )\n    ```\n    In some use cases, particularly those involving time series, it\n    might be a good idea to use targets from the past. You can activate\n    this using *lagged_targets*. But if you do that, you must\n    also define a prediction *horizon*. For instance, if you want to\n    predict data for the next hour, using data from the last seven days,\n    you could do this:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        lagged_targets=True,\n        horizon=getml.data.time.hours(1),\n        memory=getml.data.time.days(7),\n    )\n    ```\n    Please also refer to [`time`][getml.data.time].\n\n    If the join involves many matches, it might be a good idea to set the\n    relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n    This forces the pipeline to always use a propositionalization\n    algorithm for this join, which can significantly speed things up.\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        relationship=getml.data.relationship.propositionalization,\n        )\n    ```\n\n    Please also refer to [`relationship`][getml.data.relationship].\n\n    In some cases, it is necessary to have more than one placeholder\n    on the same table. This is necessary to create more complicated\n    data models. In this case, you can do something like this:\n    ```python\n    dm.add(\n        getml.data.to_placeholder(\n            PERIPHERAL=[peripheral_table]*2,\n        )\n    )\n\n    # We can now access our two placeholders like this:\n    placeholder1 = dm.PERIPHERAL[0]\n    placeholder2 = dm.PERIPHERAL[1]\n    ```\n        If you want to check out a real-world example where this\n        is necessary, refer to the\n        [CORA notebook](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/cora.ipynb).\n    \"\"\"\n\n    def __init__(self, population):\n        if isinstance(population, str):\n            population = Placeholder(population)\n\n        if not isinstance(population, Placeholder):\n            raise TypeError(\n                \"'population' must be a getml.data.Placeholder or a str, got \"\n                + type(population).__name__\n                + \".\"\n            )\n\n        self.population = population\n\n        self.peripheral = {}\n\n    def _add(self, placeholder):\n        if placeholder.name in self.peripheral:\n            try:\n                self.peripheral[placeholder.name].append(placeholder)\n            except AttributeError:\n                self.peripheral[placeholder.name] = [\n                    self.peripheral[placeholder.name],\n                    placeholder,\n                ]\n        else:\n            self.peripheral.update({placeholder.name: placeholder})\n\n    def __dir__(self):\n        attrs = dir(type(self)) + list(vars(self))\n        attrs.extend(self.names)\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        population = vars(self)[\"population\"]\n        peripheral = vars(self)[\"peripheral\"]\n\n        phs = {\n            \"population\": population,\n            population.name: population,\n            **peripheral,\n        }\n\n        return phs[key]\n\n    def _getml_deserialize(self):\n        def deserialize(elem):\n            return (\n                [e._getml_deserialize() for e in elem]\n                if isinstance(elem, list)\n                else elem._getml_deserialize()\n            )\n\n        cmd = self.population._getml_deserialize()\n        cmd[\"peripheral_\"] = {k: deserialize(v) for (k, v) in self.peripheral.items()}\n        return cmd\n\n    def __iter__(self):\n        yield from [self.population.name] + [\"population\"] + list(self.peripheral)\n\n    def __repr__(self):\n        return \"\\n\\n\".join(repr(ph) for ph in self.population.to_list())\n\n    def _make_diagram(self):\n        return _Diagram(self.population).to_html()\n\n    def _make_staging(self):\n        headers = [[\"data frames\", \"staging table\"]]\n        rows = _make_staging_overview(self.population)\n        staging_table = _Formatter(headers=headers, rows=rows)._render_html()\n        return staging_table\n\n    def _repr_html_(self):\n        output = cleandoc(\n            f\"\"\"\n            &lt;div style='margin-top: 15px; margin-bottom: 5px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;diagram&lt;/div&gt;\n            {self._make_diagram()}\n            &lt;/div&gt;\n\n            &lt;div style='margin-top: 15px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;staging&lt;/div&gt;\n            {self._make_staging()}\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return output\n\n    def add(self, *placeholders):\n        \"\"\"\n        Adds peripheral placeholders to the data model.\n\n        Args:\n            placeholders ([`Placeholder`][getml.data.Placeholder]:\n                The placeholder or placeholders you would like to add.\n        \"\"\"\n\n        def to_list(elem):\n            return elem if isinstance(elem, list) else [elem]\n\n        # We want to be 100% sure that all handles are unique,\n        # so we need deepcopy.\n        placeholders_dc = [\n            deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n        ]\n\n        if not _is_typed_list(placeholders_dc, Placeholder):\n            raise TypeError(\n                \"'placeholders' must consist of getml.data.Placeholders \"\n                + \"or lists thereof.\"\n            )\n\n        for placeholder in placeholders_dc:\n            self._add(placeholder)\n\n    @property\n    def names(self):\n        \"\"\"\n        A list of the names of all tables contained in the DataModel.\n        \"\"\"\n        return [name for name in self]\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.DataModel.names","title":"<code>names</code>  <code>property</code>","text":"<p>A list of the names of all tables contained in the DataModel.</p>"},{"location":"reference/data/__init__/#getml.data.DataModel.add","title":"<code>add(*placeholders)</code>","text":"<p>Adds peripheral placeholders to the data model.</p> <p>Parameters:</p> Name Type Description Default <code>placeholders</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder or placeholders you would like to add.</p> <code>()</code> Source code in <code>getml/data/data_model.py</code> <pre><code>def add(self, *placeholders):\n    \"\"\"\n    Adds peripheral placeholders to the data model.\n\n    Args:\n        placeholders ([`Placeholder`][getml.data.Placeholder]:\n            The placeholder or placeholders you would like to add.\n    \"\"\"\n\n    def to_list(elem):\n        return elem if isinstance(elem, list) else [elem]\n\n    # We want to be 100% sure that all handles are unique,\n    # so we need deepcopy.\n    placeholders_dc = [\n        deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n    ]\n\n    if not _is_typed_list(placeholders_dc, Placeholder):\n        raise TypeError(\n            \"'placeholders' must consist of getml.data.Placeholders \"\n            + \"or lists thereof.\"\n        )\n\n    for placeholder in placeholders_dc:\n        self._add(placeholder)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Placeholder","title":"<code>Placeholder</code>","text":"<p>Abstract representation of tables and their relations.</p> <p>This class is an abstract representation of the <code>DataFrame</code> or <code>View</code>. However, it does not contain any actual data.</p> <p>You might also want to refer to <code>DataModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name used for this placeholder. This name will appear in the generated SQL code.</p> required Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre> If you want to check out a real-world example where this is necessary, refer to the CORA notebook .</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>class Placeholder:\n    \"\"\"Abstract representation of tables and their relations.\n\n    This class is an abstract representation of the\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n    However, it does not contain any actual data.\n\n    You might also want to refer to [`DataModel`][getml.data.DataModel].\n\n    Args:\n        name (str):\n            The name used for this placeholder. This name will appear\n            in the generated SQL code.\n\n    Example:\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n        ```python\n        dm = getml.data.DataModel(\n            population_table.to_placeholder(\"POPULATION\")\n        )\n\n        dm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n        If you want to add more than one peripheral table, you can\n        use [`to_placeholder`][getml.data.to_placeholder]:\n        ```python\n        dm = getml.data.DataModel(\n            population_table.to_placeholder(\"POPULATION\")\n        )\n\n        dm.add(\n            getml.data.to_placeholder(\n                PERIPHERAL1=peripheral_table_1,\n                PERIPHERAL2=peripheral_table_2,\n            )\n        )\n        ```\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n        You can join over more than one join key:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n        )\n        ```\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        In some cases, it is necessary to have more than one placeholder\n        on the same table. This is necessary to create more complicated\n        data models. In this case, you can do something like this:\n        ```python\n        dm.add(\n            getml.data.to_placeholder(\n                PERIPHERAL=[peripheral_table]*2,\n            )\n        )\n\n        # We can now access our two placeholders like this:\n        placeholder1 = dm.PERIPHERAL[0]\n        placeholder2 = dm.PERIPHERAL[1]\n        ```\n        If you want to check out a real-world example where this\n        is necessary, refer to the\n        [CORA notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/cora.ipynb).\n    \"\"\"\n\n    def __init__(\n        self, name: str, roles: Optional[Union[Roles, Dict[str, List[str]]]] = None\n    ):\n        self._name = name\n\n        if roles is None:\n            self._roles: Roles = Roles()\n        elif isinstance(roles, dict):\n            self._roles = Roles(**roles)\n        else:\n            self._roles = roles\n\n        self.joins: List[Join] = []\n        self.parent = None\n\n    def __dir__(self):\n        attrs = dir(type(self)) + list(self.__dict__.keys())\n        attrs.extend(col for col in self.columns if col.isidentifier())\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        if key in vars(self)[\"_roles\"].columns:\n            return key\n        else:\n            raise KeyError(\n                f\"No column with with name {key!r} on the Placeholder's signature.\"\n            )\n\n    def _getml_deserialize(self):\n        cmd = {\"name_\": self.name, \"roles_\": self.roles.to_dict()}\n\n        cmd[\"allow_lagged_targets_\"] = [join.lagged_targets for join in self.joins]\n\n        cmd[\"horizon_\"] = [join.horizon or 0.0 for join in self.joins]\n\n        cmd[\"join_keys_used_\"] = [_handle_on(join.on)[0] for join in self.joins]\n\n        cmd[\"joined_tables_\"] = [join.right._getml_deserialize() for join in self.joins]\n\n        cmd[\"memory_\"] = [join.memory or 0.0 for join in self.joins]\n\n        cmd[\"other_join_keys_used_\"] = [_handle_on(join.on)[1] for join in self.joins]\n\n        cmd[\"other_time_stamps_used_\"] = [\n            _handle_ts(join.time_stamps)[1] for join in self.joins\n        ]\n\n        cmd[\"relationship_\"] = [join.relationship for join in self.joins]\n\n        cmd[\"time_stamps_used_\"] = [\n            _handle_ts(join.time_stamps)[0] for join in self.joins\n        ]\n\n        cmd[\"upper_time_stamps_used_\"] = [\n            join.upper_time_stamp or \"\" for join in self.joins\n        ]\n\n        return cmd\n\n    def __repr__(self) -&gt; str:\n        template = cleandoc(\n            \"\"\"\n            {name}:\n              columns:\n            {cols}\n            \"\"\"\n        )\n\n        if self.joins:\n            template += \"\\n\\n\" + cleandoc(\n                \"\"\"\n                  joins:\n                {joins}\n                \"\"\"\n            )\n\n        def format_on(on, join: Join):\n            template = \"({left.name}.{on[0]}, {join.right.name}.{on[1]})\"\n\n            if isinstance(on, list) and all(isinstance(key, tuple) for key in on):\n                formatted = \"\\n- \" + \"\\n- \".join(\n                    template.format(on=keys, left=self, join=join) for keys in on\n                )\n                return indent(formatted, \" \" * 2)\n\n            return template.format(on=on, left=self, join=join)\n\n        cols = [\n            f\"- {col}: {role}\" for col, role in zip(self.columns, self.roles.to_list())\n        ]\n\n        if len(cols) &gt; 5:\n            cols = cols[:5] + [\"- ...\"]\n\n        joins = []\n\n        for join in self.joins:\n            for param, value in vars(join).items():\n                if param == \"right\":\n                    joins.append(f\"- right: {join.right.name!r}\")\n                    continue\n\n                if value is not None:\n                    if param == \"on\":\n                        joins.append(f\"  on: {format_on(value, join)}\")\n                    elif param == \"time_stamps\":\n                        joins.append(\n                            f\"  {param}: ({self.name}.{value[0]}, {join.right.name}.{value[1]})\"\n                        )\n                    else:\n                        joins.append(f\"  {param}: {value!r}\")\n\n        joins = indent(\"\\n\".join(joins), \" \" * 2)  # type: ignore\n\n        cols = indent(\"\\n\".join(cols), \" \" * 2)  # type: ignore\n\n        return template.format(name=self.name, cols=cols, joins=joins)\n\n    def _ipython_key_completions_(self):\n        return self.columns\n\n    @property\n    def children(self):\n        return set([self]) ^ set(self.to_list())\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    def join(\n        self,\n        right,\n        on: OnType = None,\n        time_stamps: TimeStampsType = None,\n        relationship: str = many_to_many,\n        memory: Optional[float] = None,\n        horizon: Optional[float] = None,\n        lagged_targets: bool = False,\n        upper_time_stamp: Optional[str] = None,\n    ):\n        \"\"\"\n        Joins another to placeholder to this placeholder.\n\n        Args:\n            right ([`Placeholder`][getml.data.Placeholder]):\n                The placeholder you would like to join.\n\n            on (None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]]):\n                The join keys to use. If none is passed, then everything\n                will be joined to everything else.\n\n            time_stamps (string or Tuple[str, str]):\n                The time stamps used to limit the join.\n\n            relationship (str):\n                The relationship between the two tables. Must be from\n                [`relationship`][getml.data.relationship].\n\n            memory (float):\n                The difference between the time stamps until data is 'forgotten'.\n                Limiting your joins using memory can significantly speed up\n                training time. Also refer to [`time`][getml.data.time].\n\n            horizon (float):\n                The prediction horizon to apply to this join.\n                Also refer to [`time`][getml.data.time].\n\n            lagged_targets (bool):\n                Whether you want to allow lagged targets. If this is set to True,\n                you must also pass a positive, non-zero *horizon*.\n\n            upper_time_stamp (str):\n                Name of a time stamp in *right* that serves as an upper limit\n                on the join.\n        \"\"\"\n\n        if not isinstance(right, type(self)):\n            msg = (\n                \"'right' must be a getml.data.Placeholder. \"\n                + \"You can create a placeholder by calling .to_placeholder() \"\n                + \"on DataFrames or Views.\"\n            )\n            raise TypeError(msg)\n\n        if self in right.to_list():\n            raise ValueError(\n                \"Cicular references to other placeholders are not allowed.\"\n            )\n\n        if isinstance(on, str):\n            on = (on, on)\n\n        if isinstance(time_stamps, str):\n            time_stamps = (time_stamps, time_stamps)\n\n        keys = (\n            list(zip(*on))\n            if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n            else on\n        )\n\n        for i, ph in enumerate([self, right]):\n            if ph.roles.join_key and keys:\n                not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n                if not_a_join_key:\n                    raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n            if ph.roles.time_stamp and time_stamps:\n                if time_stamps[i] not in ph.roles.time_stamp:\n                    raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n        if lagged_targets and horizon in (0.0, None):\n            raise ValueError(\n                \"If you allow lagged targets, then you must also set a \"\n                + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n            )\n\n        if horizon not in (0.0, None) and time_stamps is None:\n            raise ValueError(\n                \"Setting 'horizon' (i.e. a relative look-back window) \"\n                + \"requires a 'time_stamp'.\"\n            )\n\n        if memory not in (0.0, None) and time_stamps is None:\n            raise ValueError(\n                \"Setting 'memory' (i.e. a relative look-back window) \"\n                + \"requires a 'time_stamp'.\"\n            )\n\n        join = Join(\n            right=right,\n            on=on,\n            time_stamps=time_stamps,\n            relationship=relationship,\n            memory=memory,\n            horizon=horizon,\n            lagged_targets=lagged_targets,\n            upper_time_stamp=upper_time_stamp,\n        )\n\n        if any(join == existing for existing in self.joins):\n            raise ValueError(\n                \"A join with the following set of parameters already exists on \"\n                f\"the placeholder {self.name!r}:\"\n                f\"\\n\\n{join}\\n\\n\"\n                \"Redundant joins are not allowed.\"\n            )\n\n        self.joins.append(join)\n        right.parent = self  # type: ignore\n\n    @property\n    def population(self):\n        if self.parent is None:\n            return self\n        return self.parent.population\n\n    @property\n    def roles(self):\n        return self._roles\n\n    @roles.setter\n    def roles(self, roles):\n        if not isinstance(roles, (Roles, dict)):\n            raise TypeError(\"'roles' must be a dict or getml.data.Roles\")\n        if isinstance(roles, dict):\n            self._roles = Roles(**roles)\n        else:\n            self._roles = roles\n\n    def to_list(self):\n        \"\"\"\n        Returns a list of this placeholder and all of its descendants.\n        \"\"\"\n        return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n\n    def to_dict(self):\n        \"\"\"\n        Expresses this placeholder and all of its descendants as a dictionary.\n        \"\"\"\n        phs = {}\n        for ph in self.to_list():\n            key = ph.name\n            if ph.children:\n                i = 2\n                while key in phs:\n                    key = f\"{ph.name}{i}\"\n                    i += 1\n            phs[key] = ph\n        return phs\n\n    @property\n    def columns(self):\n        return self.roles.columns\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Placeholder.join","title":"<code>join(right, on=None, time_stamps=None, relationship=many_to_many, memory=None, horizon=None, lagged_targets=False, upper_time_stamp=None)</code>","text":"<p>Joins another to placeholder to this placeholder.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder you would like to join.</p> required <code>on</code> <code>(None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>time_stamps</code> <code>string or Tuple[str, str]</code> <p>The time stamps used to limit the join.</p> <code>None</code> <code>relationship</code> <code>str</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <code>many_to_many</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right that serves as an upper limit on the join.</p> <code>None</code> Source code in <code>getml/data/placeholder.py</code> <pre><code>def join(\n    self,\n    right,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins another to placeholder to this placeholder.\n\n    Args:\n        right ([`Placeholder`][getml.data.Placeholder]):\n            The placeholder you would like to join.\n\n        on (None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]]):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps (string or Tuple[str, str]):\n            The time stamps used to limit the join.\n\n        relationship (str):\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory (float):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str):\n            Name of a time stamp in *right* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right, type(self)):\n        msg = (\n            \"'right' must be a getml.data.Placeholder. \"\n            + \"You can create a placeholder by calling .to_placeholder() \"\n            + \"on DataFrames or Views.\"\n        )\n        raise TypeError(msg)\n\n    if self in right.to_list():\n        raise ValueError(\n            \"Cicular references to other placeholders are not allowed.\"\n        )\n\n    if isinstance(on, str):\n        on = (on, on)\n\n    if isinstance(time_stamps, str):\n        time_stamps = (time_stamps, time_stamps)\n\n    keys = (\n        list(zip(*on))\n        if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n        else on\n    )\n\n    for i, ph in enumerate([self, right]):\n        if ph.roles.join_key and keys:\n            not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n            if not_a_join_key:\n                raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n        if ph.roles.time_stamp and time_stamps:\n            if time_stamps[i] not in ph.roles.time_stamp:\n                raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n    if lagged_targets and horizon in (0.0, None):\n        raise ValueError(\n            \"If you allow lagged targets, then you must also set a \"\n            + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n        )\n\n    if horizon not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'horizon' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    if memory not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'memory' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    join = Join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    if any(join == existing for existing in self.joins):\n        raise ValueError(\n            \"A join with the following set of parameters already exists on \"\n            f\"the placeholder {self.name!r}:\"\n            f\"\\n\\n{join}\\n\\n\"\n            \"Redundant joins are not allowed.\"\n        )\n\n    self.joins.append(join)\n    right.parent = self  # type: ignore\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Placeholder.to_dict","title":"<code>to_dict()</code>","text":"<p>Expresses this placeholder and all of its descendants as a dictionary.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses this placeholder and all of its descendants as a dictionary.\n    \"\"\"\n    phs = {}\n    for ph in self.to_list():\n        key = ph.name\n        if ph.children:\n            i = 2\n            while key in phs:\n                key = f\"{ph.name}{i}\"\n                i += 1\n        phs[key] = ph\n    return phs\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Placeholder.to_list","title":"<code>to_list()</code>","text":"<p>Returns a list of this placeholder and all of its descendants.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list of this placeholder and all of its descendants.\n    \"\"\"\n    return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Roles","title":"<code>Roles</code>  <code>dataclass</code>","text":"<p>Roles can be passed to <code>DataFrame</code> to predefine the roles assigned to certain columns.</p> Example <pre><code>roles = getml.data.Roles(\n    categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n)\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n)\n</code></pre> Source code in <code>getml/data/roles_obj.py</code> <pre><code>@dataclass\nclass Roles:\n    \"\"\"\n    Roles can be passed to [`DataFrame`][getml.DataFrame] to\n    predefine the roles assigned to certain columns.\n\n    Example:\n        ```python\n        roles = getml.data.Roles(\n            categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n        )\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n        )\n        ```\n    \"\"\"\n\n    categorical: List[str] = field(default_factory=list)\n    join_key: List[str] = field(default_factory=list)\n    numerical: List[str] = field(default_factory=list)\n    target: List[str] = field(default_factory=list)\n    text: List[str] = field(default_factory=list)\n    time_stamp: List[str] = field(default_factory=list)\n    unused_float: List[str] = field(default_factory=list)\n    unused_string: List[str] = field(default_factory=list)\n\n    def __getitem__(self, key):\n        try:\n            return getattr(self, key)\n        except TypeError:\n            raise KeyError(key)\n\n    def __iter__(self):\n        yield from (field_.name for field_ in fields(self))\n\n    def __len__(self):\n        return len(fields(self))\n\n    def __repr__(self):\n        template = cleandoc(\n            \"\"\"\n            {role}:\n            - {cols}\n            \"\"\"\n        )\n\n        blocks = []\n\n        for role in self:\n            if self[role]:\n                cols = \"\\n- \".join(self[role])\n                blocks.append(template.format(role=role, cols=cols))\n\n        return \"\\n\\n\".join(blocks)\n\n    @property\n    def columns(self):\n        \"\"\"\n        The name of all columns contained in the roles object.\n        \"\"\"\n        return [r for role in self for r in self[role]]\n\n    def infer(self, colname):\n        \"\"\"\n        Infers the role of a column.\n\n        Args:\n            colname (str):\n                The name of the column to be inferred.\n        \"\"\"\n        for role in self:\n            if colname in self[role]:\n                return role\n        raise ValueError(\"Column named '\" + colname + \"' not found.\")\n\n    def to_dict(self):\n        \"\"\"\n        Expresses the roles object as a dictionary.\n        \"\"\"\n        return {role: self[role] for role in self}\n\n    def to_list(self):\n        \"\"\"\n        Returns a list containing the roles, without the corresponding\n        columns names.\n        \"\"\"\n        return [r for role in self for r in [role] * len(self[role])]\n\n    @property\n    def unused(self):\n        \"\"\"\n        Names of all unused columns (unused_float + unused_string).\n        \"\"\"\n        return self.unused_float + self.unused_string\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Roles.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>The name of all columns contained in the roles object.</p>"},{"location":"reference/data/__init__/#getml.data.Roles.unused","title":"<code>unused</code>  <code>property</code>","text":"<p>Names of all unused columns (unused_float + unused_string).</p>"},{"location":"reference/data/__init__/#getml.data.Roles.infer","title":"<code>infer(colname)</code>","text":"<p>Infers the role of a column.</p> <p>Parameters:</p> Name Type Description Default <code>colname</code> <code>str</code> <p>The name of the column to be inferred.</p> required Source code in <code>getml/data/roles_obj.py</code> <pre><code>def infer(self, colname):\n    \"\"\"\n    Infers the role of a column.\n\n    Args:\n        colname (str):\n            The name of the column to be inferred.\n    \"\"\"\n    for role in self:\n        if colname in self[role]:\n            return role\n    raise ValueError(\"Column named '\" + colname + \"' not found.\")\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Roles.to_dict","title":"<code>to_dict()</code>","text":"<p>Expresses the roles object as a dictionary.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses the roles object as a dictionary.\n    \"\"\"\n    return {role: self[role] for role in self}\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Roles.to_list","title":"<code>to_list()</code>","text":"<p>Returns a list containing the roles, without the corresponding columns names.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list containing the roles, without the corresponding\n    columns names.\n    \"\"\"\n    return [r for role in self for r in [role] * len(self[role])]\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.StarSchema","title":"<code>StarSchema</code>","text":"<p>A StarSchema is a simplifying abstraction that can be used for machine learning problems that can be organized in a simple star schema.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model.</p> <p>The class is designed using composition  - it is neither <code>Container</code> nor <code>DataModel</code>, but has both of them.</p> <p>This means that you can always fall back to the more flexible methods using <code>Container</code> and <code>DataModel</code> by directly accessing the attributes <code>container</code> and <code>data_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <code>None</code> <code>alias</code> <code>str</code> <p>The alias to be used for the population table. This is required, if population is a <code>View</code>.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>join</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>train</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>validation</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>test</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>kwargs</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in <code>Subset</code> s other than the predefined train, validation and test subsets. You can call these subsets anything you want to and can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\nstar_schema = getml.data.StarSchema(\n    my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(star_schema.my_subset)\n</code></pre></p> <code>{}</code> Example <p>Note that this example is taken from the loans notebook.</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code> and <code>Pipeline</code>.</p> <p><pre><code># First, we insert our data.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# StarSchema, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new)\n\ncontainer.add(\n    trans=trans_new,\n    order=order_new,\n    meta=meta_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# star_schema.train and star_schema.test\n# work just like above.\n</code></pre> Source code in <code>getml/data/star_schema.py</code> <pre><code>class StarSchema:\n    \"\"\"\n    A StarSchema is a simplifying abstraction that can be used\n    for machine learning problems that can be organized in a simple\n    [star schema](https://en.wikipedia.org/wiki/Star_schema).\n\n    It unifies [`Container`][getml.data.Container] and\n    [`DataModel`][getml.data.DataModel] thus abstracting away the need to\n    differentiate between the concrete data and the abstract data model.\n\n    The class is designed using\n    [composition ](https://en.wikipedia.org/wiki/Composition_over_inheritance)\n    - it *is* neither [`Container`][getml.data.Container] nor [`DataModel`][getml.data.DataModel],\n    but *has* both of them.\n\n    This means that you can always fall back to the more flexible methods using\n    [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel] by directly\n    accessing the attributes `container` and `data_model`.\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        alias (str, optional):\n            The alias to be used for the population table. This is required,\n            if *population* is a [`View`][getml.data.View].\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`join`][getml.data.StarSchema.join].\n\n        split ([`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset] s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        train ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *train*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        validation ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *validation*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        test ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *test*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        kwargs ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in [`Subset`][getml.data.Subset] s\n            other than the predefined *train*, *validation* and *test* subsets.\n            You can call these subsets anything you want to and can access them\n            just like *train*, *validation* and *test*.\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n            Example:\n                ```python\n                # Pass the subset.\n                star_schema = getml.data.StarSchema(\n                    my_subset=my_data_frame)\n\n                # You can access the subset just like train,\n                # validation or test\n                my_pipeline.fit(star_schema.my_subset)\n                ```\n\n    Example:\n        Note that this example is taken from the\n        [loans notebook](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n\n        You might also want to refer to\n        [`DataFrame`][getml.DataFrame], [`View`][getml.data.View]\n        and [`Pipeline`][getml.Pipeline].\n\n        ```python\n        # First, we insert our data.\n        # population_train and population_test are either\n        # DataFrames or Views. The population table\n        # defines the statistical population of your\n        # machine learning problem and contains the\n        # target variables.\n        star_schema = getml.data.StarSchema(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views.\n        # Because this is a star schema,\n        # all joins take place on the population\n        # table.\n        star_schema.join(\n            trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        star_schema.join(\n            order,\n            on=\"account_id\",\n        )\n\n        star_schema.join(\n            meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=star_schema.data_model,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # Now, we pass the actual data.\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(star_schema.train)\n\n        pipe.fit(star_schema.train)\n\n        pipe.score(star_schema.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # StarSchema, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new)\n\n        container.add(\n            trans=trans_new,\n            order=order_new,\n            meta=meta_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n        If you don't already have a train and test set,\n        you can use a function from the\n        [`split`][getml.data.split] module.\n\n        ```python\n        split = getml.data.split.random(\n            train=0.8, test=0.2)\n\n        star_schema = getml.data.StarSchema(\n            population=population_all,\n            split=split,\n        )\n\n        # The remaining code is the same as in\n        # the example above. In particular,\n        # star_schema.train and star_schema.test\n        # work just like above.\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        population=None,\n        alias=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        train=None,\n        validation=None,\n        test=None,\n        **kwargs,\n    ):\n        if (population is None or isinstance(population, View)) and alias is None:\n            raise ValueError(\n                \"If 'population' is None or a getml.data.View, you must set an alias.\"\n            )\n\n        self._alias = alias or population.name\n\n        self._container = Container(\n            population=population,\n            peripheral=peripheral,\n            split=split,\n            deep_copy=deep_copy,\n            train=train,\n            validation=validation,\n            test=test,\n            **kwargs,\n        )\n\n        def get_placeholder():\n            if population is not None:\n                return population.to_placeholder(alias)\n            if train is not None:\n                return train.to_placeholder(alias)\n            if validation is not None:\n                return validation.to_placeholder(alias)\n            if test is not None:\n                return test.to_placeholder(alias)\n            assert (\n                len(kwargs) &gt; 0\n            ), \"This should have been checked by Container.__init__.\"\n            return kwargs[list(kwargs.keys())[0]].to_placeholder(alias)\n\n        self._data_model = DataModel(get_placeholder())\n\n    def __dir__(self):\n        attrs = dir(type(self)) + [key[1:] for key in list(vars(self))]\n        attrs += dir(self.container)\n        attrs += dir(self.data_model)\n        return list(set(attrs))\n\n    def __iter__(self):\n        yield from [self.population] + list(self.peripheral.values())\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            return super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        attrs = vars(self)\n\n        if key in attrs:\n            return attrs[key]\n\n        if \"_\" + key in attrs:\n            return attrs[\"_\" + key]\n\n        try:\n            return attrs[\"_container\"][key]\n        except KeyError:\n            return attrs[\"_data_model\"][key]\n\n    def __repr__(self):\n        template = cleandoc(\n            \"\"\"\n            data model\n\n            {data_model}\n\n\n            container\n\n            {container}\n            \"\"\"\n        )\n        return template.format(\n            data_model=indent(repr(self.data_model), \"  \"),\n            container=indent(repr(self.container), \"  \"),\n        )\n\n    def _repr_html_(self):\n        template = cleandoc(\n            \"\"\"\n            &lt;span style='font-size: 1.2rem; font-weight: 500;'&gt;data model&lt;/span&gt;\n            {data_model}\n            &lt;span style='font-size: 1.2rem; font-weight: 500;'&gt;container&lt;/span&gt;\n            {container}\n            \"\"\"\n        )\n        return template.format(\n            data_model=self.data_model._repr_html_(),\n            container=self.container._repr_html_(),\n        )\n\n    @property\n    def container(self):\n        \"\"\"\n        The underlying [`Container`][getml.data.Container].\n        \"\"\"\n        return self._container\n\n    @property\n    def data_model(self):\n        \"\"\"\n        The underlying [`DataModel`][getml.data.DataModel].\n        \"\"\"\n        return self._data_model\n\n    def join(\n        self,\n        right_df,\n        alias=None,\n        on=None,\n        time_stamps=None,\n        relationship=many_to_many,\n        memory=None,\n        horizon=None,\n        lagged_targets=False,\n        upper_time_stamp=None,\n    ):\n        \"\"\"\n        Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n        to the population table.\n\n        In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n        all joins take place on the population table. If you want to create more\n        complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n        Example:\n            This example will construct a data model in which the\n            'population_table' depends on the 'peripheral_table' via\n            the 'join_key' column. In addition, only those rows in\n            'peripheral_table' for which 'time_stamp' is smaller or\n            equal to the 'time_stamp' in 'population_table' are considered:\n\n            ```python\n            star_schema = getml.data.StarSchema(\n                population=population_table, split=split)\n\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\"\n            )\n            ```\n\n            If the relationship between two tables is many-to-one or one-to-one\n            you should clearly say so:\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                relationship=getml.data.relationship.many_to_one,\n            )\n            ```\n            Please also refer to [`relationship`][getml.data.relationship].\n\n            If the join keys or time stamps are named differently in the two\n            different tables, use a tuple:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=(\"join_key\", \"other_join_key\"),\n                time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n            )\n            ```\n\n            You can join over more than one join key:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n                time_stamps=\"time_stamp\",\n                )\n            ```\n\n            You can also limit the scope of your joins using *memory*. This\n            can significantly speed up training time. For instance, if you\n            only want to consider data from the last seven days, you could\n            do something like this:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                memory=getml.data.time.days(7),\n            )\n            ```\n\n            In some use cases, particularly those involving time series, it\n            might be a good idea to use targets from the past. You can activate\n            this using *lagged_targets*. But if you do that, you must\n            also define a prediction *horizon*. For instance, if you want to\n            predict data for the next hour, using data from the last seven days,\n            you could do this:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                lagged_targets=True,\n                horizon=getml.data.time.hours(1),\n                memory=getml.data.time.days(7),\n            )\n            ```\n\n            Please also refer to [`time`][getml.data.time].\n\n            If the join involves many matches, it might be a good idea to set the\n            relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n            This forces the pipeline to always use a propositionalization\n            algorithm for this join, which can significantly speed things up.\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                relationship=getml.data.relationship.propositionalization,\n            )\n            ```\n\n            Please also refer to [`relationship`][getml.data.relationship].\n\n        Args:\n            right_df ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n                The data frame or view you would like to join.\n\n            alias (str or None):\n                The name as which you want *right_df* to be referred to in\n                the generated SQL code.\n\n            on (None, string, Tuple[str] or List[Union[str, Tuple[str]]]):\n                The join keys to use. If none is passed, then everything\n                will be joined to everything else.\n\n            time_stamps (string or Tuple[str]):\n                The time stamps used to limit the join.\n\n            relationship (str):\n                The relationship between the two tables. Must be from\n                [`relationship`][getml.data.relationship].\n\n            memory (float):\n                The difference between the time stamps until data is 'forgotten'.\n                Limiting your joins using memory can significantly speed up\n                training time. Also refer to [`time`][getml.data.time].\n\n            horizon (float):\n                The prediction horizon to apply to this join.\n                Also refer to [`time`][getml.data.time].\n\n            lagged_targets (bool):\n                Whether you want to allow lagged targets. If this is set to True,\n                you must also pass a positive, non-zero *horizon*.\n\n            upper_time_stamp (str):\n                Name of a time stamp in *right_df* that serves as an upper limit\n                on the join.\n        \"\"\"\n\n        if not isinstance(right_df, (DataFrame, View)):\n            raise TypeError(\n                f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n            )\n\n        if isinstance(right_df, View):\n            if alias is None:\n                raise ValueError(\n                    \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                    \"as a peripheral table.\"\n                )\n\n        def modify_join_keys(on):\n            if isinstance(on, list):\n                return [modify_join_keys(jk) for jk in on]\n\n            if isinstance(on, (str, StringColumn)):\n                on = (on, on)\n\n            if on is not None and on:\n                on = tuple(\n                    jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n                )\n\n            return on\n\n        def modify_time_stamps(time_stamps):\n            if isinstance(time_stamps, (str, FloatColumn)):\n                time_stamps = (time_stamps, time_stamps)\n\n            if time_stamps is not None:\n                time_stamps = tuple(\n                    time_stamp.name\n                    if isinstance(time_stamp, FloatColumn)\n                    else time_stamp\n                    for time_stamp in time_stamps\n                )\n\n            return time_stamps\n\n        on = modify_join_keys(on)\n\n        time_stamps = modify_time_stamps(time_stamps)\n\n        upper_time_stamp = (\n            upper_time_stamp.name\n            if isinstance(upper_time_stamp, FloatColumn)\n            else upper_time_stamp\n        )\n\n        right = right_df.to_placeholder(alias)\n\n        self.data_model.population.join(\n            right=right,\n            on=on,\n            time_stamps=time_stamps,\n            relationship=relationship,\n            memory=memory,\n            horizon=horizon,\n            lagged_targets=lagged_targets,\n            upper_time_stamp=upper_time_stamp,\n        )\n\n        alias = alias or right_df.name\n\n        self.container.add(**{alias: right_df})\n\n    def sync(self):\n        \"\"\"\n        Synchronizes the last change with the data to avoid warnings that the data\n        has been changed.\n\n        This is only a problem when `deep_copy=False`.\n        \"\"\"\n        self.container.sync()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.StarSchema.container","title":"<code>container</code>  <code>property</code>","text":"<p>The underlying <code>Container</code>.</p>"},{"location":"reference/data/__init__/#getml.data.StarSchema.data_model","title":"<code>data_model</code>  <code>property</code>","text":"<p>The underlying <code>DataModel</code>.</p>"},{"location":"reference/data/__init__/#getml.data.StarSchema.join","title":"<code>join(right_df, alias=None, on=None, time_stamps=None, relationship=many_to_many, memory=None, horizon=None, lagged_targets=False, upper_time_stamp=None)</code>","text":"<p>Joins a <code>DataFrame</code> or <code>View</code> to the population table.</p> <p>In a <code>StarSchema</code> or <code>TimeSeries</code>, all joins take place on the population table. If you want to create more complex data models, use <code>DataModel</code> instead.</p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered:</p> <pre><code>star_schema = getml.data.StarSchema(\n    population=population_table, split=split)\n\nstar_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> <p>If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> <p>You can join over more than one join key:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n    )\n</code></pre> <p>You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up.</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> <p>Please also refer to <code>relationship</code>.</p> <p>Parameters:</p> Name Type Description Default <code>right_df</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The data frame or view you would like to join.</p> required <code>alias</code> <code>str or None</code> <p>The name as which you want right_df to be referred to in the generated SQL code.</p> <code>None</code> <code>on</code> <code>(None, string, Tuple[str] or List[Union[str, Tuple[str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>time_stamps</code> <code>string or Tuple[str]</code> <p>The time stamps used to limit the join.</p> <code>None</code> <code>relationship</code> <code>str</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <code>many_to_many</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <code>None</code> Source code in <code>getml/data/star_schema.py</code> <pre><code>def join(\n    self,\n    right_df,\n    alias=None,\n    on=None,\n    time_stamps=None,\n    relationship=many_to_many,\n    memory=None,\n    horizon=None,\n    lagged_targets=False,\n    upper_time_stamp=None,\n):\n    \"\"\"\n    Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n    to the population table.\n\n    In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n    all joins take place on the population table. If you want to create more\n    complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n    Example:\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n\n        ```python\n        star_schema = getml.data.StarSchema(\n            population=population_table, split=split)\n\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n\n        You can join over more than one join key:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n            )\n        ```\n\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n\n        Please also refer to [`relationship`][getml.data.relationship].\n\n    Args:\n        right_df ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The data frame or view you would like to join.\n\n        alias (str or None):\n            The name as which you want *right_df* to be referred to in\n            the generated SQL code.\n\n        on (None, string, Tuple[str] or List[Union[str, Tuple[str]]]):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps (string or Tuple[str]):\n            The time stamps used to limit the join.\n\n        relationship (str):\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory (float):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str):\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right_df, (DataFrame, View)):\n        raise TypeError(\n            f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n        )\n\n    if isinstance(right_df, View):\n        if alias is None:\n            raise ValueError(\n                \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                \"as a peripheral table.\"\n            )\n\n    def modify_join_keys(on):\n        if isinstance(on, list):\n            return [modify_join_keys(jk) for jk in on]\n\n        if isinstance(on, (str, StringColumn)):\n            on = (on, on)\n\n        if on is not None and on:\n            on = tuple(\n                jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n            )\n\n        return on\n\n    def modify_time_stamps(time_stamps):\n        if isinstance(time_stamps, (str, FloatColumn)):\n            time_stamps = (time_stamps, time_stamps)\n\n        if time_stamps is not None:\n            time_stamps = tuple(\n                time_stamp.name\n                if isinstance(time_stamp, FloatColumn)\n                else time_stamp\n                for time_stamp in time_stamps\n            )\n\n        return time_stamps\n\n    on = modify_join_keys(on)\n\n    time_stamps = modify_time_stamps(time_stamps)\n\n    upper_time_stamp = (\n        upper_time_stamp.name\n        if isinstance(upper_time_stamp, FloatColumn)\n        else upper_time_stamp\n    )\n\n    right = right_df.to_placeholder(alias)\n\n    self.data_model.population.join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    alias = alias or right_df.name\n\n    self.container.add(**{alias: right_df})\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.StarSchema.sync","title":"<code>sync()</code>","text":"<p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    self.container.sync()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.Subset","title":"<code>Subset</code>  <code>dataclass</code>","text":"<p>A Subset consists of a population table and one or several peripheral tables.</p> <p>It is passed by a <code>Container</code>, <code>StarSchema</code> and <code>TimeSeries</code> to the <code>Pipeline</code>.</p> Example <pre><code>container = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# train and test are Subsets.\n# They contain population_train\n# and population_test respectively,\n# as well as their peripheral tables\n# meta, order and trans.\nmy_pipeline.fit(container.train)\n\nmy_pipeline.score(container.test)\n</code></pre> Source code in <code>getml/data/subset.py</code> <pre><code>@dataclass\nclass Subset:\n    \"\"\"\n    A Subset consists of a population table and one or several peripheral tables.\n\n    It is passed by a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n    and [`TimeSeries`][getml.data.TimeSeries] to the [`Pipeline`][getml.Pipeline].\n\n    Example:\n        ```python\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # train and test are Subsets.\n        # They contain population_train\n        # and population_test respectively,\n        # as well as their peripheral tables\n        # meta, order and trans.\n        my_pipeline.fit(container.train)\n\n        my_pipeline.score(container.test)\n        ```\n    \"\"\"\n\n    container_id: str\n    peripheral: Dict[str, Union[DataFrame, View]]\n    population: Union[DataFrame, View]\n\n    def _format(self):\n        headers_perph = [[\"name\", \"rows\", \"type\"]]\n\n        rows_perph = [\n            [perph.name, perph.nrows(), type(perph).__name__]\n            for perph in self.peripheral.values()\n        ]\n\n        names = [perph.name for perph in self.peripheral.values()]\n        aliases = list(self.peripheral.keys())\n\n        if any(alias not in names for alias in aliases):\n            headers_perph[0].insert(0, \"alias\")\n\n            for alias, row in zip(aliases, rows_perph):\n                row.insert(0, alias)\n\n        return self.population._format(), _Formatter(\n            headers=headers_perph, rows=rows_perph\n        )\n\n    def __repr__(self):\n        pop, perph = self._format()\n        pop_footer = self.population._collect_footer_data()\n\n        template = cleandoc(\n            \"\"\"\n            population\n            {pop}\n\n            peripheral\n            {perph}\n            \"\"\"\n        )\n\n        return template.format(\n            pop=pop._render_string(footer=pop_footer), perph=perph._render_string()\n        )\n\n    def _repr_html_(self):\n        pop, perph = self._format()\n        pop_footer = self.population._collect_footer_data()\n\n        template = cleandoc(\n            \"\"\"\n            &lt;div&gt;\n                &lt;div style='margin-bottom: 10px; font-size: 1rem'&gt;population&lt;/div&gt;\n                {pop}\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div style='margin-bottom: 10px; font-size: 1rem'&gt;peripheral&lt;/div&gt;\n                {perph}\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return template.format(\n            pop=pop._render_html(footer=pop_footer), perph=perph._render_html()\n        )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.TimeSeries","title":"<code>TimeSeries</code>","text":"<p>               Bases: <code>StarSchema</code></p> <p>A TimeSeries is a simplifying abstraction that can be used for machine learning problems on time series data.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model. It also abstracts away the need for self joins .</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> required <code>time_stamps</code> <code>str</code> <p>The time stamps used to limit the self-join.</p> required <code>alias</code> <code>str</code> <p>The alias to be used for the population table. If it isn't set, the 'population' will be used as the alias. To explicitly set an alias for the peripheral table, use <code>with_name</code>.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.columns.StringColumn] or [`StringColumnView`][getml.data.columns.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>on</code> <code>(None, string, Tuple[str] or List[Union[str, Tuple[str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <code>None</code> Example <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Source code in <code>getml/data/time_series.py</code> <pre><code>class TimeSeries(StarSchema):\n    \"\"\"\n    A TimeSeries is a simplifying abstraction that can be used\n    for machine learning problems on time series data.\n\n    It unifies [`Container`][getml.data.Container] and\n    [`DataModel`][getml.data.DataModel] thus abstracting away the need to\n    differentiate between the concrete data and the abstract data model.\n    It also abstracts away the need for\n    [self joins ](https://en.wikipedia.org/wiki/Join_(SQL)#Self-join).\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        time_stamps (str):\n            The time stamps used to limit the self-join.\n\n        alias (str, optional):\n            The alias to be used for the population table. If it isn't set, the 'population'\n            will be used as the alias. To explicitly set an alias for the\n            peripheral table, use [`with_name`][getml.DataFrame.with_name].\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`add`][getml.data.Container.add].\n\n        split ([`StringColumn`][getml.data.columns.columns.StringColumn] or [`StringColumnView`][getml.data.columns.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset] s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        on (None, string, Tuple[str] or List[Union[str, Tuple[str]]], optional):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        memory (float, optional):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float, optional):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool, optional):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str, optional):\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n\n    Example:\n        ```python\n        # All rows before row 10500 will be used for training.\n        split = getml.data.split.time(data_all, \"rowid\", test=10500)\n\n        time_series = getml.data.TimeSeries(\n            population=data_all,\n            time_stamps=\"rowid\",\n            split=split,\n            lagged_targets=False,\n            memory=30,\n        )\n\n        pipe = getml.Pipeline(\n            data_model=time_series.data_model,\n            feature_learners=[...],\n            predictors=...\n        )\n\n        pipe.check(time_series.train)\n\n        pipe.fit(time_series.train)\n\n        pipe.score(time_series.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # TimeSeries, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new,\n        )\n\n        # Add the data as a peripheral table, for the\n        # self-join.\n        container.add(population=population_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        population,\n        time_stamps,\n        alias=None,\n        alias2=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        on=None,\n        memory=None,\n        horizon=None,\n        lagged_targets=False,\n        upper_time_stamp=None,\n    ):\n\n        if not isinstance(population, (DataFrame, View)):\n            raise TypeError(\n                \"'population' must be a getml.DataFrame or a getml.data.View\"\n            )\n\n        if isinstance(time_stamps, FloatColumn):\n            time_stamps = time_stamps.name\n\n        if isinstance(time_stamps, FloatColumnView):\n            if \"rowid\" in _finditems(\"operator_\", time_stamps.cmd):\n                time_stamps = \"rowid\"\n\n        population = (\n            population.with_column(\n                population.rowid, name=\"rowid\", role=time_stamp\n            ).with_unit(names=\"rowid\", unit=\"rowid\", comparison_only=True)\n            if time_stamps == \"rowid\"\n            else population\n        )\n\n        alias = \"population\" if alias is None else alias\n\n        super().__init__(\n            population=population,\n            alias=alias,\n            peripheral=peripheral,\n            split=split,\n            deep_copy=deep_copy,\n        )\n\n        self.on = on\n        self.time_stamps = time_stamps\n        self.memory = memory\n        self.horizon = horizon\n        self.lagged_targets = lagged_targets\n        self.upper_time_stamp = upper_time_stamp\n\n        if not isinstance(on, list):\n            on = [on]\n\n        for o in on:\n            self._add_joins(o)\n\n    def _add_joins(self, on):\n        self.join(\n            right_df=self.population,\n            alias=self.population.name,\n            on=self.on,\n            time_stamps=self.time_stamps,\n            memory=self.memory,\n            horizon=self.horizon,\n            lagged_targets=self.lagged_targets,\n            upper_time_stamp=self.upper_time_stamp,\n        )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View","title":"<code>View</code>","text":"<p>A view is a lazily evaluated, immutable representation of a <code>DataFrame</code>.</p> <p>There are important differences between a <code>DataFrame</code> and a view:</p> <ul> <li> <p>Views are lazily evaluated. That means that views do not   contain any data themselves. Instead, they just refer to   an underlying data frame. If the underlying data frame changes,   so will the view (but such behavior will result in a warning).</p> </li> <li> <p>Views are immutable. In-place operations on a view are not   possible. Any operation on a view will result in a new view.</p> </li> <li> <p>Views have no direct representation on the getML engine, and   therefore they do not need to have an identifying name.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>A data frame or view used as the basis for this view.</p> required <code>name</code> <code>str</code> <p>The name assigned to this view.</p> <code>None</code> <code>subselection</code> <code>[`BooleanColumnView`][getml.data.columns.BooleanColumnView], [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Indicates which rows we would like to keep.</p> <code>None</code> <code>added</code> <code>dict</code> <p>A dictionary that describes a new column that has been added to the view.</p> <code>None</code> <code>dropped</code> <code>List[str]</code> <p>A list of columns that have been dropped.</p> <code>None</code> Example <p>You hardly ever directly create views. Instead, it is more likely that you will encounter them as a result of some operation on a <code>DataFrame</code>:</p> <p><pre><code># Creates a view on the first 100 lines\nview1 = data_frame[:100]\n\n# Creates a view without some columns.\nview2 = data_frame.drop([\"col1\", \"col2\"])\n\n# Creates a view in which some roles are reassigned.\nview3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n</code></pre> A recommended pattern is to assign 'baseline roles' to your data frames and then using views to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which <code>Container</code> you have used.</p> Source code in <code>getml/data/view.py</code> <pre><code>class View:\n    \"\"\"A view is a lazily evaluated, immutable representation of a [`DataFrame`][getml.DataFrame].\n\n    There are important differences between a\n    [`DataFrame`][getml.DataFrame] and a view:\n\n    - Views are lazily evaluated. That means that views do not\n      contain any data themselves. Instead, they just refer to\n      an underlying data frame. If the underlying data frame changes,\n      so will the view (but such behavior will result in a warning).\n\n    - Views are immutable. In-place operations on a view are not\n      possible. Any operation on a view will result in a new view.\n\n    - Views have no direct representation on the getML engine, and\n      therefore they do not need to have an identifying name.\n\n    Args:\n        base ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            A data frame or view used as the basis for this view.\n\n        name (str):\n            The name assigned to this view.\n\n        subselection ([`BooleanColumnView`][getml.data.columns.BooleanColumnView], [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Indicates which rows we would like to keep.\n\n        added (dict):\n            A dictionary that describes a new column\n            that has been added to the view.\n\n        dropped (List[str]):\n            A list of columns that have been dropped.\n\n    Example:\n        You hardly ever directly create views. Instead, it is more likely\n        that you will encounter them as a result of some operation on a\n        [`DataFrame`][getml.DataFrame]:\n\n        ```python\n\n        # Creates a view on the first 100 lines\n        view1 = data_frame[:100]\n\n        # Creates a view without some columns.\n        view2 = data_frame.drop([\"col1\", \"col2\"])\n\n        # Creates a view in which some roles are reassigned.\n        view3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        ```\n        A recommended pattern is to assign 'baseline roles' to your data frames\n        and then using views to tweak them:\n\n        ```python\n        # Assign baseline roles\n        data_frame.set_role([\"jk\"], getml.data.roles.join_key)\n        data_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        data_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\n        data_frame.set_role([\"col5\"], getml.data.roles.target)\n\n        # Make the data frame immutable, so in-place operations are\n        # no longer possible.\n        data_frame.freeze()\n\n        # Save the data frame.\n        data_frame.save()\n\n        # I suspect that col1 leads to overfitting, so I will drop it.\n        view = data_frame.drop([\"col1\"])\n\n        # Insert the view into a container.\n        container = getml.data.Container(...)\n        container.add(some_alias=view)\n        container.save()\n        ```\n\n        The advantage of using such a pattern is that it enables you to\n        always completely retrace your entire pipeline without creating\n        deep copies of the data frames whenever you have made a small\n        change like the one in our example. Note that the pipeline will\n        record which [`Container`][getml.data.Container] you have used.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    def __init__(\n        self,\n        base,\n        name: Optional[str] = None,\n        subselection: Union[\n            BooleanColumnView, FloatColumn, FloatColumnView, None\n        ] = None,\n        added=None,\n        dropped: Optional[List[str]] = None,\n    ):\n        self._added = added\n        self._base = deepcopy(base)\n        self._dropped = dropped or []\n        self._name = name\n        self._subselection = subselection\n\n        self._initial_timestamp: str = (\n            self._base._initial_timestamp\n            if isinstance(self._base, View)\n            else self._base.last_change\n        )\n\n        self._base.refresh()\n\n    # ------------------------------------------------------------\n\n    def _apply_subselection(self, col):\n        if self._subselection is not None:\n            return col[self._subselection]\n        return col\n\n    # ------------------------------------------------------------\n\n    def _collect_footer_data(self):\n        footer = namedtuple(\"footer\", [\"n_rows\", \"n_cols\", \"type\"])\n\n        n_rows = \"unknown number of\" if self.nrows() == \"unknown\" else self.nrows()\n\n        return footer(\n            n_rows=n_rows,\n            n_cols=len(self.colnames),\n            type=\"getml.data.View\",\n        )\n\n    # ------------------------------------------------------------\n\n    def _format(self):\n        return _ViewFormatter(self)\n\n    # ----------------------------------------------------------------\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            return super().__getattribute__(name)\n\n    # ------------------------------------------------------------\n\n    def __getitem__(self, name):\n        if isinstance(\n            name,\n            (numbers.Integral, slice, BooleanColumnView, FloatColumn, FloatColumnView),\n        ):\n            return self.where(index=name)\n\n        if isinstance(name, list):\n            not_in_colnames = set(name) - set(self.colnames)\n            if not_in_colnames:\n                raise KeyError(f\"{list(not_in_colnames)} not found.\")\n            dropped = [col for col in self.colnames if col not in name]\n            return View(base=self, dropped=dropped)\n\n        if name in self.__dict__[\"_dropped\"]:\n            raise KeyError(\n                \"Cannot retrieve column '\" + name + \"'. It has been dropped.\"\n            )\n\n        if (\n            self.__dict__[\"_added\"] is not None\n            and self.__dict__[\"_added\"][\"name_\"] == name\n        ):\n            return self._apply_subselection(\n                self.__dict__[\"_added\"][\"col_\"]\n                .with_subroles(self.__dict__[\"_added\"][\"subroles_\"], append=False)\n                .with_unit(self.__dict__[\"_added\"][\"unit_\"])\n            )\n\n        return self._apply_subselection(self.__dict__[\"_base\"][name])\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return self.nrows(force=True)\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View\"\n\n        if self._added is not None:\n            added = deepcopy(self._added)\n            col = deepcopy(added[\"col_\"].cmd)\n            col[\"last_change_\"] = added[\"col_\"].last_change\n            added[\"col_\"] = col\n            cmd[\"added_\"] = added\n\n        if self._subselection is not None:\n            subselection = deepcopy(self._subselection.cmd)\n            subselection[\"last_change_\"] = self.subselection.last_change\n            cmd[\"subselection_\"] = subselection\n\n        cmd[\"base_\"] = self._base._getml_deserialize()\n        cmd[\"dropped_\"] = self._dropped\n        cmd[\"name_\"] = self.name\n        cmd[\"last_change_\"] = self.last_change\n\n        return cmd\n\n    # ------------------------------------------------------------\n\n    def _modify_colnames(self, base_names, role):\n        remove_dropped = [name for name in base_names if name not in self._dropped]\n\n        if self._added is not None and self._added[\"role_\"] != role:\n            return [name for name in remove_dropped if name != self._added[\"name_\"]]\n\n        if (\n            self._added is not None\n            and self._added[\"role_\"] == role\n            and self._added[\"name_\"] not in remove_dropped\n        ):\n            return remove_dropped + [self._added[\"name_\"]]\n\n        return remove_dropped\n\n    # ------------------------------------------------------------\n\n    def __repr__(self):\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_string(footer=footer)\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_html(footer=footer)\n\n    # ------------------------------------------------------------\n\n    @property\n    def added(self):\n        \"\"\"\n        The column that has been added to the view.\n        \"\"\"\n        return deepcopy(self._added)\n\n    # ------------------------------------------------------------\n\n    @property\n    def base(self):\n        \"\"\"\n        The basis on which the view is created. Must be a\n        [`DataFrame`][getml.DataFrame] or a [`View`][getml.data.View].\n        \"\"\"\n        return deepcopy(self._base)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _categorical_names(self):\n        return self._modify_colnames(self._base._categorical_names, categorical)\n\n    # ------------------------------------------------------------\n\n    def check(self):\n        \"\"\"\n        Checks whether the underlying data frame has been changed\n        after the creation of the view.\n        \"\"\"\n        last_change = self.last_change\n        if last_change != self.__dict__[\"_initial_timestamp\"]:\n            logger.warning(\n                \"The data frame underlying view '\"\n                + self.name\n                + \"' was last changed at \"\n                + last_change\n                + \", which was after the creation of the view. \"\n                + \"This might lead to unexpected results. You might \"\n                + \"want to recreate the view. (Views are lazily \"\n                + \"evaluated, so recreating them is a very \"\n                + \"inexpensive operation).\"\n            )\n\n    # ------------------------------------------------------------\n\n    @property\n    def colnames(self):\n        \"\"\"\n        List of the names of all columns.\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return (\n            self._time_stamp_names\n            + self._join_key_names\n            + self._target_names\n            + self._categorical_names\n            + self._numerical_names\n            + self._text_names\n            + self._unused_names\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self):\n        \"\"\"\n        Alias for [`colnames`][getml.data.View.colnames].\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return self.colnames\n\n    # ------------------------------------------------------------\n\n    def drop(self, cols):\n        \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n        Args:\n            cols (str or List[str]):\n                The names of the columns to be dropped.\n        \"\"\"\n        if isinstance(cols, str):\n            cols = [cols]\n\n        if not _is_typed_list(cols, str):\n            raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n        return View(base=self, dropped=cols)\n\n    # ------------------------------------------------------------\n\n    @property\n    def dropped(self):\n        \"\"\"\n        The names of the columns that has been dropped.\n        \"\"\"\n        return deepcopy(self._dropped)\n\n    # ------------------------------------------------------------\n\n    @property\n    def last_change(self) -&gt; str:\n        \"\"\"\n        A string describing the last time this data frame has been changed.\n        \"\"\"\n        return self.__dict__[\"_base\"].last_change\n\n    # ------------------------------------------------------------\n\n    @property\n    def _join_key_names(self):\n        return self._modify_colnames(self._base._join_key_names, join_key)\n\n    # ------------------------------------------------------------\n\n    @property\n    def name(self):\n        \"\"\"\n        The name of the view. If no name is explicitly set,\n        the name will be identical to the name of the base.\n        \"\"\"\n        if self.__dict__[\"_name\"] is None:\n            return self.__dict__[\"_base\"].name\n        return deepcopy(self.__dict__[\"_name\"])\n\n    # ------------------------------------------------------------\n\n    def ncols(self):\n        \"\"\"\n        Number of columns in the current instance.\n\n        Returns:\n            int:\n                Overall number of columns\n        \"\"\"\n        return len(self.colnames)\n\n    # ------------------------------------------------------------\n\n    def nrows(self, force=False):\n        \"\"\"\n        Returns the number of rows in the current instance.\n\n        Args:\n            force (bool, optional):\n                If the number of rows is unknown,\n                do you want to force the engine to calculate it anyway?\n                This is a relatively expensive operation, therefore\n                you might not necessarily want this.\n        \"\"\"\n\n        self.refresh()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.get_nrows\"\n        cmd[\"name_\"] = \"\"\n\n        cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n        cmd[\"force_\"] = force\n\n        with comm.send_and_get_socket(cmd) as sock:\n            json_str = comm.recv_string(sock)\n\n        if json_str[0] != \"{\":\n            comm.engine_exception_handler(json_str)\n\n        result = json.loads(json_str)\n\n        if \"recordsTotal\" in result:\n            return int(result[\"recordsTotal\"])\n\n        return \"unknown\"\n\n    # ------------------------------------------------------------\n\n    @property\n    def _numerical_names(self):\n        return self._modify_colnames(self._base._numerical_names, numerical)\n\n    # --------------------------------------------------------------------------\n\n    def refresh(self):\n        \"\"\"Aligns meta-information of the current instance with the\n        corresponding data frame in the getML engine.\n\n        Returns:\n            [`View`][getml.data.View]:\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        \"\"\"\n        self._base = self.__dict__[\"_base\"].refresh()\n        return self\n\n    # ------------------------------------------------------------\n\n    @property\n    def roles(self):\n        \"\"\"\n        The roles of the columns included\n        in this View.\n        \"\"\"\n        return Roles(\n            categorical=self._categorical_names,\n            join_key=self._join_key_names,\n            numerical=self._numerical_names,\n            target=self._target_names,\n            text=self._text_names,\n            time_stamp=self._time_stamp_names,\n            unused_float=self._unused_float_names,\n            unused_string=self._unused_string_names,\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def rowid(self):\n        \"\"\"\n        The rowids for this view.\n        \"\"\"\n        return rowid()[: len(self)]\n\n    # ------------------------------------------------------------\n\n    @property\n    def subselection(self):\n        \"\"\"\n        The subselection that is applied to this view.\n        \"\"\"\n        return deepcopy(self._subselection)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _target_names(self):\n        return self._modify_colnames(self._base._target_names, target)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _text_names(self):\n        return self._modify_colnames(self._base._text_names, text)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _time_stamp_names(self):\n        return self._modify_colnames(self._base._time_stamp_names, time_stamp)\n\n    # ------------------------------------------------------------\n\n    @property\n    def shape(self):\n        \"\"\"\n        A tuple containing the number of rows and columns of\n        the View.\n        \"\"\"\n        self.refresh()\n        return (self.nrows(), self.ncols())\n\n    # ------------------------------------------------------------\n\n    def to_arrow(self):\n        \"\"\"Creates a `pyarrow.Table` from the view.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyarrow.Table`.\n\n        Returns:\n            `pyarrow.Table`:\n                Pyarrow equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self)\n\n    # ------------------------------------------------------------\n\n    def to_json(self):\n        \"\"\"Creates a JSON string from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a JSON string.\n        \"\"\"\n        return self.to_pandas().to_json()\n\n    # ------------------------------------------------------------\n\n    def to_csv(\n        self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file.\n\n        Args:\n            fname (str):\n                The name of the CSV file.\n                The ending \".csv\" and an optional batch number will\n                be added automatically.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        fname_ = os.path.abspath(fname)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"fname_\"] = fname_\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_db(self, table_name: str, conn: Optional[Connection] = None):\n        \"\"\"Writes the underlying data into a newly created table in the\n        database.\n\n        Args:\n            table_name (str):\n                Name of the table to be created.\n\n                If a table of that name already exists, it will be\n                replaced.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n        \"\"\"\n\n        conn = conn or Connection()\n\n        self.refresh()\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        if not isinstance(conn, Connection):\n            raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_db\"\n        cmd[\"name_\"] = \"\"\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"table_name_\"] = table_name\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_pandas(self):\n        \"\"\"Creates a `pandas.DataFrame` from the view.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pandas.DataFrame`.\n\n        Returns:\n            `pandas.DataFrame`:\n                Pandas equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self).to_pandas()\n\n    # ------------------------------------------------------------\n\n    def to_placeholder(self, name=None):\n        \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n        current [`View`][getml.data.View].\n\n        Args:\n            name (str, optional):\n                The name of the placeholder. If no\n                name is passed, then the name of the placeholder will\n                be identical to the name of the current view.\n\n        Returns:\n            [`Placeholder`][getml.data.Placeholder]:\n                A placeholder with the same name as this data frame.\n\n\n        \"\"\"\n        self.refresh()\n        return Placeholder(name=name or self.name, roles=self.roles)\n\n    # ------------------------------------------------------------\n\n    def to_parquet(self, fname, compression=\"snappy\"):\n        \"\"\"\n        Writes the underlying data into a newly created parquet file.\n\n        Args:\n            fname (str):\n                The name of the parquet file.\n                The ending \".parquet\" will be added automatically.\n\n            compression (str):\n                The compression format to use.\n                Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n        \"\"\"\n        _to_parquet(self, fname, compression)\n\n    # ----------------------------------------------------------------\n\n    def to_pyspark(self, spark, name=None):\n        \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyspark.sql.DataFrame`.\n\n        Args:\n            spark (pyspark.sql.SparkSession):\n                The pyspark session in which you want to\n                create the data frame.\n\n            name (str or None):\n                The name of the temporary view to be created on top\n                of the `pyspark.sql.DataFrame`,\n                with which it can be referred to\n                in Spark SQL (refer to\n                `pyspark.sql.DataFrame.createOrReplaceTempView`).\n                If none is passed, then the name of this\n                [`DataFrame`][getml.DataFrame] will be used.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Pyspark equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_pyspark(self, name, spark)\n\n    # ------------------------------------------------------------\n\n    def to_s3(\n        self,\n        bucket: str,\n        key: str,\n        region: str,\n        sep: str = \",\",\n        batch_size: int = 50000,\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file\n        located in an S3 bucket.\n        Note:\n            S3 is not supported on Windows.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            key (str):\n                The key in the S3 bucket in which you want to\n                write the output. The ending \".csv\" and an optional\n                batch number will be added automatically.\n\n            region (str):\n                The region in which the bucket is located.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n\n        Example:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            your_view.to_s3(\n                bucket=\"your-bucket-name\",\n                key=\"filename-on-s3\",\n                region=\"us-east-2\",\n                sep=';'\n            )\n            ```\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be of type str\")\n\n        if not isinstance(key, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"bucket_\"] = bucket\n        cmd[\"key_\"] = key\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_float_names(self):\n        return self._modify_colnames(self._base._unused_float_names, unused_float)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_names(self):\n        return self._unused_float_names + self._unused_string_names\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_string_names(self):\n        return self._modify_colnames(self._base._unused_string_names, unused_string)\n\n    # ------------------------------------------------------------\n\n    def where(self, index) -&gt; \"View\":\n        \"\"\"Extract a subset of rows.\n\n        Creates a new [`View`][getml.data.View] as a\n        subselection of the current instance.\n\n        Args:\n            index ([`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n                Boolean column indicating the rows you want to select.\n\n        Example:\n            Generate example data:\n            ```python\n            data = dict(\n                fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n                price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n                join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n            fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n            roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n            fruits\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 0        | banana      | 2.4       |\n            | 1        | apple       | 3         |\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            | 3        | melon       | 3.4       |\n            | 3        | pineapple   | 3.4       |\n            ```\n            Apply where condition. This creates a new DataFrame called \"cherries\":\n\n            ```python\n\n            cherries = fruits.where(\n                fruits[\"fruit\"] == \"cherry\")\n\n            cherries\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            ```\n        \"\"\"\n        if isinstance(index, numbers.Integral):\n            index = index if int(index) &gt; 0 else len(self) + index\n            selector = arange(index, index + 1)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, slice):\n            start, stop, step = _make_default_slice(index, len(self))\n            selector = arange(start, stop, step)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n            return View(base=self, subselection=index)\n\n        raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n\n    # ------------------------------------------------------------\n\n    def with_column(\n        self, col, name, role=None, unit=\"\", subroles=None, time_formats=None\n    ):\n        \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n        Args:\n            col ([`column`][getml.column]):\n                The column to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from [`roles`][getml.data.roles].\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n        \"\"\"\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n        return View(\n            base=self,\n            added={\n                \"col_\": col,\n                \"name_\": name,\n                \"role_\": role,\n                \"subroles_\": subroles,\n                \"unit_\": unit,\n            },\n        )\n\n    # ------------------------------------------------------------\n\n    def with_name(self, name):\n        \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n        Args:\n            name (str):\n                The name of the new view.\n        \"\"\"\n        return View(base=self, name=name)\n\n    # ------------------------------------------------------------\n\n    def with_role(self, names, role, time_formats=None):\n        \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret time\n        format string: `annotating_roles_time_stamp`. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n        \"\"\"\n        return _with_role(self, names, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    def with_subroles(self, names, subroles, append=True):\n        \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n        return _with_subroles(self, names, subroles, append)\n\n    # ------------------------------------------------------------\n\n    def with_unit(self, names, unit, comparison_only=False):\n        \"\"\"Returns a view that contains a new unit on one or more columns.\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n\n                If True, this will append \", comparison only\" to the unit.\n                The feature learning algorithms and the feature selectors will\n                interpret this accordingly.\n        \"\"\"\n        return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.added","title":"<code>added</code>  <code>property</code>","text":"<p>The column that has been added to the view.</p>"},{"location":"reference/data/__init__/#getml.data.View.base","title":"<code>base</code>  <code>property</code>","text":"<p>The basis on which the view is created. Must be a <code>DataFrame</code> or a <code>View</code>.</p>"},{"location":"reference/data/__init__/#getml.data.View.colnames","title":"<code>colnames</code>  <code>property</code>","text":"<p>List of the names of all columns.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/__init__/#getml.data.View.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Alias for <code>colnames</code>.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/__init__/#getml.data.View.dropped","title":"<code>dropped</code>  <code>property</code>","text":"<p>The names of the columns that has been dropped.</p>"},{"location":"reference/data/__init__/#getml.data.View.last_change","title":"<code>last_change: str</code>  <code>property</code>","text":"<p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/__init__/#getml.data.View.name","title":"<code>name</code>  <code>property</code>","text":"<p>The name of the view. If no name is explicitly set, the name will be identical to the name of the base.</p>"},{"location":"reference/data/__init__/#getml.data.View.roles","title":"<code>roles</code>  <code>property</code>","text":"<p>The roles of the columns included in this View.</p>"},{"location":"reference/data/__init__/#getml.data.View.rowid","title":"<code>rowid</code>  <code>property</code>","text":"<p>The rowids for this view.</p>"},{"location":"reference/data/__init__/#getml.data.View.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>A tuple containing the number of rows and columns of the View.</p>"},{"location":"reference/data/__init__/#getml.data.View.subselection","title":"<code>subselection</code>  <code>property</code>","text":"<p>The subselection that is applied to this view.</p>"},{"location":"reference/data/__init__/#getml.data.View.check","title":"<code>check()</code>","text":"<p>Checks whether the underlying data frame has been changed after the creation of the view.</p> Source code in <code>getml/data/view.py</code> <pre><code>def check(self):\n    \"\"\"\n    Checks whether the underlying data frame has been changed\n    after the creation of the view.\n    \"\"\"\n    last_change = self.last_change\n    if last_change != self.__dict__[\"_initial_timestamp\"]:\n        logger.warning(\n            \"The data frame underlying view '\"\n            + self.name\n            + \"' was last changed at \"\n            + last_change\n            + \", which was after the creation of the view. \"\n            + \"This might lead to unexpected results. You might \"\n            + \"want to recreate the view. (Views are lazily \"\n            + \"evaluated, so recreating them is a very \"\n            + \"inexpensive operation).\"\n        )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.drop","title":"<code>drop(cols)</code>","text":"<p>Returns a new <code>View</code> that has one or several columns removed.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str or List[str]</code> <p>The names of the columns to be dropped.</p> required Source code in <code>getml/data/view.py</code> <pre><code>def drop(self, cols):\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols (str or List[str]):\n            The names of the columns to be dropped.\n    \"\"\"\n    if isinstance(cols, str):\n        cols = [cols]\n\n    if not _is_typed_list(cols, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=cols)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.ncols","title":"<code>ncols()</code>","text":"<p>Number of columns in the current instance.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/view.py</code> <pre><code>def ncols(self):\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n        int:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.nrows","title":"<code>nrows(force=False)</code>","text":"<p>Returns the number of rows in the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If the number of rows is unknown, do you want to force the engine to calculate it anyway? This is a relatively expensive operation, therefore you might not necessarily want this.</p> <code>False</code> Source code in <code>getml/data/view.py</code> <pre><code>def nrows(self, force=False):\n    \"\"\"\n    Returns the number of rows in the current instance.\n\n    Args:\n        force (bool, optional):\n            If the number of rows is unknown,\n            do you want to force the engine to calculate it anyway?\n            This is a relatively expensive operation, therefore\n            you might not necessarily want this.\n    \"\"\"\n\n    self.refresh()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.get_nrows\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n    cmd[\"force_\"] = force\n\n    with comm.send_and_get_socket(cmd) as sock:\n        json_str = comm.recv_string(sock)\n\n    if json_str[0] != \"{\":\n        comm.engine_exception_handler(json_str)\n\n    result = json.loads(json_str)\n\n    if \"recordsTotal\" in result:\n        return int(result[\"recordsTotal\"])\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.refresh","title":"<code>refresh()</code>","text":"<p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> <p>Returns:</p> Type Description <p><code>View</code>: Updated handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/view.py</code> <pre><code>def refresh(self):\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    Returns:\n        [`View`][getml.data.View]:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n    self._base = self.__dict__[\"_base\"].refresh()\n    return self\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Creates a <code>pyarrow.Table</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <p><code>pyarrow.Table</code>: Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_arrow(self):\n    \"\"\"Creates a `pyarrow.Table` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n        `pyarrow.Table`:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_csv","title":"<code>to_csv(fname, quotechar='\"', sep=',', batch_size=0)</code>","text":"<p>Writes the underlying data into a newly created CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>0</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname (str):\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_db","title":"<code>to_db(table_name, conn=None)</code>","text":"<p>Writes the underlying data into a newly created table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name (str):\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    if not isinstance(conn, Connection):\n        raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_db\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"table_name_\"] = table_name\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_json","title":"<code>to_json()</code>","text":"<p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Creates a <code>pandas.DataFrame</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pandas.DataFrame</code>.</p> <p>Returns:</p> Type Description <p><code>pandas.DataFrame</code>: Pandas equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pandas(self):\n    \"\"\"Creates a `pandas.DataFrame` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pandas.DataFrame`.\n\n    Returns:\n        `pandas.DataFrame`:\n            Pandas equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_parquet","title":"<code>to_parquet(fname, compression='snappy')</code>","text":"<p>Writes the underlying data into a newly created parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> required <code>compression</code> <code>str</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <code>'snappy'</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_parquet(self, fname, compression=\"snappy\"):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname (str):\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression (str):\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_placeholder","title":"<code>to_placeholder(name=None)</code>","text":"<p>Generates a <code>Placeholder</code> from the current <code>View</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current view.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>Placeholder</code>: A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_placeholder(self, name=None):\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`View`][getml.data.View].\n\n    Args:\n        name (str, optional):\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current view.\n\n    Returns:\n        [`Placeholder`][getml.data.Placeholder]:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_pyspark","title":"<code>to_pyspark(spark, name=None)</code>","text":"<p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The pyspark session in which you want to create the data frame.</p> required <code>name</code> <code>str or None</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pyspark(self, spark, name=None):\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark (pyspark.sql.SparkSession):\n            The pyspark session in which you want to\n            create the data frame.\n\n        name (str or None):\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.to_s3","title":"<code>to_s3(bucket, key, region, sep=',', batch_size=50000)</code>","text":"<p>Writes the underlying data into a newly created CSV file located in an S3 bucket. Note:     S3 is not supported on Windows.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>key</code> <code>str</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>50000</code> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_view.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/view.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n    Note:\n        S3 is not supported on Windows.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        key (str):\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region (str):\n            The region in which the bucket is located.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_view.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.where","title":"<code>where(index)</code>","text":"<p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>[`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Boolean column indicating the rows you want to select.</p> required Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/view.py</code> <pre><code>def where(self, index) -&gt; \"View\":\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index ([`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Boolean column indicating the rows you want to select.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(index, index + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, step = _make_default_slice(index, len(self))\n        selector = arange(start, stop, step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.with_column","title":"<code>with_column(col, name, role=None, unit='', subroles=None, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> that contains an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`column`][getml.column]</code> <p>The column to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_column(\n    self, col, name, role=None, unit=\"\", subroles=None, time_formats=None\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col ([`column`][getml.column]):\n            The column to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.with_name","title":"<code>with_name(name)</code>","text":"<p>Returns a new <code>View</code> with a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new view.</p> required Source code in <code>getml/data/view.py</code> <pre><code>def with_name(self, name):\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.with_role","title":"<code>with_role(names, role, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> with modified roles.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_role(self, names, role, time_formats=None):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, names, role, time_formats)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.with_subroles","title":"<code>with_subroles(names, subroles, append=True)</code>","text":"<p>Returns a new view with one or several new subroles on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_subroles(self, names, subroles, append=True):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, names, subroles, append)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.View.with_unit","title":"<code>with_unit(names, unit, comparison_only=False)</code>","text":"<p>Returns a view that contains a new unit on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will append \", comparison only\" to the unit. The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <code>False</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_unit(self, names, unit, comparison_only=False):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will append \", comparison only\" to the unit.\n            The feature learning algorithms and the feature selectors will\n            interpret this accordingly.\n    \"\"\"\n    return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.arange","title":"<code>arange(start=0.0, stop=None, step=1.0)</code>","text":"<p>Returns evenly spaced variables, within a given interval.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>The beginning of the interval. Defaults to 0.</p> <code>0.0</code> <code>stop</code> <code>float</code> <p>The end of the interval.</p> <code>None</code> <code>step</code> <code>float</code> <p>The step taken. Defaults to 1.</p> <code>1.0</code> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start (float, optional):\n            The beginning of the interval. Defaults to 0.\n\n        stop (float):\n            The end of the interval.\n\n        step (float, optional):\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.delete","title":"<code>delete(name)</code>","text":"<p>If a data frame named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def delete(name):\n    \"\"\"\n    If a data frame named 'name' exists, it is deleted.\n\n    Args:\n        name (str):\n            Name of the data frame.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.exists","title":"<code>exists(name)</code>","text":"<p>Returns true if a data frame named 'name' exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def exists(name):\n    \"\"\"\n    Returns true if a data frame named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the data frame.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_df = list_data_frames()\n\n    return name in (all_df[\"in_memory\"] + all_df[\"on_disk\"])\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.list_data_frames","title":"<code>list_data_frames()</code>","text":"<p>Lists all available data frames of the project.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, List[str]]</code> <p>Dict containing lists of strings representing the names of the data frames objects</p> <ul> <li>'in_memory'     held in memory (RAM).</li> <li>'on_disk'     stored on disk.</li> </ul> Example <pre><code>d, _ = getml.datasets.make_numerical()\ngetml.data.list_data_frames()\nd.save()\ngetml.data.list_data_frames()\n</code></pre> Source code in <code>getml/data/helpers.py</code> <pre><code>def list_data_frames() -&gt; Dict[str, List[str]]:\n    \"\"\"Lists all available data frames of the project.\n\n    Returns:\n        dict:\n            Dict containing lists of strings representing the names of\n            the data frames objects\n\n            - 'in_memory'\n                held in memory (RAM).\n            - 'on_disk'\n                stored on disk.\n\n    Example:\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        getml.data.list_data_frames()\n        d.save()\n        getml.data.list_data_frames()\n        ```\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_data_frames\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.load_data_frame","title":"<code>load_data_frame(name)</code>","text":"<p>Retrieves a <code>DataFrame</code> handler of data in the getML engine.</p> <p>A data frame object can be loaded regardless if it is held in memory or not. It only has to be present in the current project and thus listed in the output of <code>list_data_frames</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required <p>Examples:</p> <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\nd2 = getml.data.load_data_frame('test')\n</code></pre> <p>Returns:     <code>DataFrame</code>:         Handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def load_data_frame(name):\n    \"\"\"Retrieves a [`DataFrame`][getml.DataFrame] handler of data in the\n    getML engine.\n\n    A data frame object can be loaded regardless if it is held in\n    memory or not. It only has to be present in the current project\n    and thus listed in the output of\n    [`list_data_frames`][getml.data.list_data_frames].\n\n    Args:\n        name (str):\n            Name of the data frame.\n\n    Examples:\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        d2 = getml.data.load_data_frame('test')\n        ```\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handle the underlying data frame in the getML engine.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    data_frames_available = list_data_frames()\n\n    if name in data_frames_available[\"in_memory\"]:\n        return DataFrame(name).refresh()\n\n    if name in data_frames_available[\"on_disk\"]:\n        return DataFrame(name).load()\n\n    raise ValueError(\n        \"No data frame holding the name '\" + name + \"' present on the getML engine.\"\n    )\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.make_target_columns","title":"<code>make_target_columns(base, colname)</code>","text":"<p>Returns a view containing binary target columns.</p> <p>getML expects binary target columns for classification problems. This helper function allows you to split up a column into such binary target columns.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The original view or data frame. <code>base</code> will remain unaffected by this function, instead you will get a view with the appropriate changes.</p> required <code>colname</code> <code>str</code> <p>The column you would like to split. A column named <code>colname</code> should appear on <code>base</code>.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def make_target_columns(base, colname):\n    \"\"\"\n    Returns a view containing binary target columns.\n\n    getML expects binary target columns for classification problems. This\n    helper function allows you to split up a column into such binary\n    target columns.\n\n    Args:\n        base ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The original view or data frame. `base` will remain unaffected\n            by this function, instead you will get a view with the appropriate\n            changes.\n\n        colname (str): The column you would like to split. A column named\n            `colname` should appear on `base`.\n    \"\"\"\n    if not isinstance(\n        base[colname], (FloatColumn, FloatColumnView, StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'\"\n            + colname\n            + \"' must be a FloatColumn, a FloatColumnView, \"\n            + \"a StringColumn or a StringColumnView.\"\n        )\n\n    unique_values = base[colname].unique()\n\n    if len(unique_values) &gt; 10:\n        logger.warning(\n            \"You are splitting the column into more than 10 target \"\n            + \"columns. This might take a long time to fit.\"\n        )\n\n    view = base\n\n    for label in unique_values:\n        col = (base[colname] == label).as_num()\n        name = colname + \"=\" + label\n        view = view.with_column(col=col, name=name, role=target)\n\n    return view.drop(colname)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.rowid","title":"<code>rowid()</code>","text":"<p>Get the row numbers of the table.</p> <p>Returns:</p> Type Description <p><code>FloatColumnView</code>: (numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid():\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/__init__/#getml.data.to_placeholder","title":"<code>to_placeholder(*args, **kwargs)</code>","text":"<p>Factory function for extracting placeholders from a <code>DataFrame</code> or <code>View</code>.</p> Example <p>Suppose we wanted to create a <code>DataModel</code>:</p> <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(meta.to_placeholder(\"meta\"))\ndm.add(order.to_placeholder(\"order\"))\ndm.add(trans.to_placeholder(\"trans\"))\n</code></pre> <p>But this is a bit repetitive. So instead, we can do the following: <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(getml.data.to_placeholder(\n    meta=meta, order=order, trans=trans))\n</code></pre></p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def to_placeholder(*args, **kwargs):\n    \"\"\"\n    Factory function for extracting placeholders from a\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n\n    Example:\n        Suppose we wanted to create a [`DataModel`][getml.data.DataModel]:\n\n\n\n            dm = getml.data.DataModel(\n                population_train.to_placeholder(\"population\")\n            )\n\n            # Add placeholders for the peripheral tables.\n            dm.add(meta.to_placeholder(\"meta\"))\n            dm.add(order.to_placeholder(\"order\"))\n            dm.add(trans.to_placeholder(\"trans\"))\n\n        But this is a bit repetitive. So instead, we can do\n        the following:\n        ```python\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        # Add placeholders for the peripheral tables.\n        dm.add(getml.data.to_placeholder(\n            meta=meta, order=order, trans=trans))\n        ```\n    \"\"\"\n\n    def to_ph_list(list_or_elem, key=None):\n        as_list = list_or_elem if isinstance(list_or_elem, list) else [list_or_elem]\n        return [elem.to_placeholder(key) for elem in as_list]\n\n    return [elem for item in args for elem in to_ph_list(item)] + [\n        elem for (k, v) in kwargs.items() for elem in to_ph_list(v, k)\n    ]\n</code></pre>"},{"location":"reference/data/access/","title":"Access","text":"<p>Manages the access to various data sources.</p>"},{"location":"reference/data/access/#getml.data.access.set_s3_access_key_id","title":"<code>set_s3_access_key_id(value)</code>","text":"<p>Sets the Access Key ID to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML engine, or you can set them from this module.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to which you want to set the Access Key ID.</p> required Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_access_key_id(value):\n    \"\"\"Sets the Access Key ID to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML engine, or you can set them from\n    this module.\n\n    Args:\n        value (str):\n            The value to which you want to set the Access Key ID.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML engine.\n        Make sure the engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_access_key_id\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/data/access/#getml.data.access.set_s3_secret_access_key","title":"<code>set_s3_secret_access_key(value)</code>","text":"<p>Sets the Secret Access Key to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML engine, or you can set them from this module.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The value to which you want to set the Secret Access Key.</p> required Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_secret_access_key(value):\n    \"\"\"Sets the Secret Access Key to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML engine, or you can set them from\n    this module.\n\n    Args:\n        value (str):\n            The value to which you want to set the Secret Access Key.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML engine.\n        Make sure the engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_secret_access_key\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/data/concat/","title":"Concat","text":"<p>Creates a new data frame by concatenating a list of existing ones.</p>"},{"location":"reference/data/concat/#getml.data.concat.concat","title":"<code>concat(name, data_frames)</code>","text":"<p>Creates a new data frame by concatenating a list of existing ones.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>data_frames</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]]</code> <p>The data frames to concatenate. Must be non-empty. However, it can contain only one data frame. Column names and roles must match. Columns will be appended by name, not order.</p> required <p>Examples:</p> <pre><code>new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n</code></pre> Source code in <code>getml/data/concat.py</code> <pre><code>def concat(name, data_frames: List[Union[DataFrame, View]]):\n    \"\"\"\n    Creates a new data frame by concatenating a list of existing ones.\n\n    Args:\n        name (str):\n            Name of the new column.\n\n        data_frames (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]]):\n            The data frames to concatenate.\n            Must be non-empty. However, it can contain only one data frame.\n            Column names and roles must match.\n            Columns will be appended by name, not order.\n\n    Examples:\n        ```python\n        new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    if not _is_non_empty_typed_list(data_frames, (View, DataFrame)):\n        raise TypeError(\n            \"'data_frames' must be a non-empty list of getml.data.Views \"\n            + \"or getml.DataFrames.\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [df._getml_deserialize() for df in data_frames]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/container/","title":"Container","text":"<p>For keeping and versioning data.</p>"},{"location":"reference/data/container/#getml.data.container.Container","title":"<code>Container</code>","text":"<p>A container holds the actual data in the form of a <code>DataFrame</code> or a <code>View</code>.</p> <p>The purpose of a container is twofold:</p> <ul> <li> <p>Assigning concrete data to an abstract <code>DataModel</code>.</p> </li> <li> <p>Storing data and allowing you to reproduce previous results.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code>s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>train</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>validation</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>test</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>kwargs</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in <code>Subset</code>s other than the predefined train, validation and test subsets. You can call these subsets anything you want to, and you can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\ncontainer = getml.data.Container(my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(container.my_subset)\n</code></pre></p> <code>{}</code> Example <p>A <code>DataModel</code> only contains abstract data. When we fit a pipeline, we need to assign concrete data.</p> <p>This example is taken from the loans notebook . Note that in the notebook the high level <code>StarSchema</code> implementation is used. For demonstration purposes we are proceeding now with the low level implementation.</p> <p><pre><code># The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# We now have abstract placeholders on something\n# called \"population\", \"meta\", \"order\" and \"trans\".\n# But how do we assign concrete data? By using\n# a container.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. Their aliases need\n# to match the names of the placeholders in the\n# data model.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# When we call 'train', the container\n# will return the train set and the\n# peripheral tables.\nmy_pipeline.fit(container.train)\n\n# Same for 'test'\nmy_pipeline.score(container.test)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\ncontainer = getml.data.Container(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# container.train and container.test\n# work just like above.\n</code></pre> <p>Containers can also be used for storage and reproducing your results. A recommended pattern is to assign 'baseline roles' to your data frames and then using a <code>View</code> to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which container you have used.</p> Source code in <code>getml/data/container.py</code> <pre><code>class Container:\n    \"\"\"\n    A container holds the actual data in the form of a [`DataFrame`][getml.DataFrame] or a [`View`][getml.data.View].\n\n    The purpose of a container is twofold:\n\n    - Assigning concrete data to an abstract [`DataModel`][getml.data.DataModel].\n\n    - Storing data and allowing you to reproduce previous results.\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`add`][getml.data.Container.add].\n\n        split ([`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset]s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        train ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *train*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        validation ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *validation*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        test ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *test*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        kwargs ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in [`Subset`][getml.data.Subset]s\n            other than the predefined *train*, *validation* and *test* subsets.\n            You can call these subsets anything you want to, and you can access them\n            just like *train*, *validation* and *test*.\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n            Example:\n                ```python\n                # Pass the subset.\n                container = getml.data.Container(my_subset=my_data_frame)\n\n                # You can access the subset just like train,\n                # validation or test\n                my_pipeline.fit(container.my_subset)\n                ```\n\n    Example:\n        A [`DataModel`][getml.data.DataModel] only contains abstract data. When we\n        fit a pipeline, we need to assign concrete data.\n\n        This example is taken from the\n        [loans notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n        Note that in the notebook the high level [`StarSchema`][getml.data.StarSchema] implementation is used. For\n        demonstration purposes we are proceeding now with the low level implementation.\n\n        ```python\n        # The abstract data model is constructed\n        # using the DataModel class. A data model\n        # does not contain any actual data. It just\n        # defines the abstract relational structure.\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        dm.add(getml.data.to_placeholder(\n            meta=meta,\n            order=order,\n            trans=trans)\n        )\n\n        dm.population.join(\n            dm.trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        dm.population.join(\n            dm.order,\n            on=\"account_id\",\n        )\n\n        dm.population.join(\n            dm.meta,\n            on=\"account_id\",\n        )\n\n        # We now have abstract placeholders on something\n        # called \"population\", \"meta\", \"order\" and \"trans\".\n        # But how do we assign concrete data? By using\n        # a container.\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views. Their aliases need\n        # to match the names of the placeholders in the\n        # data model.\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # Freezing makes the container immutable.\n        # This is not required, but often a good idea.\n        container.freeze()\n\n        # When we call 'train', the container\n        # will return the train set and the\n        # peripheral tables.\n        my_pipeline.fit(container.train)\n\n        # Same for 'test'\n        my_pipeline.score(container.test)\n        ```\n        If you don't already have a train and test set,\n        you can use a function from the\n        [`split`][getml.data.split] module.\n\n        ```python\n\n        split = getml.data.split.random(\n            train=0.8, test=0.2)\n\n        container = getml.data.Container(\n            population=population_all,\n            split=split,\n        )\n\n        # The remaining code is the same as in\n        # the example above. In particular,\n        # container.train and container.test\n        # work just like above.\n        ```\n\n        Containers can also be used for storage and reproducing your\n        results.\n        A recommended pattern is to assign 'baseline roles' to your data frames\n        and then using a [`View`][getml.data.View] to tweak them:\n\n        ```python\n\n        # Assign baseline roles\n        data_frame.set_role([\"jk\"], getml.data.roles.join_key)\n        data_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        data_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\n        data_frame.set_role([\"col5\"], getml.data.roles.target)\n\n        # Make the data frame immutable, so in-place operations are\n        # no longer possible.\n        data_frame.freeze()\n\n        # Save the data frame.\n        data_frame.save()\n\n        # I suspect that col1 leads to overfitting, so I will drop it.\n        view = data_frame.drop([\"col1\"])\n\n        # Insert the view into a container.\n        container = getml.data.Container(...)\n        container.add(some_alias=view)\n        container.save()\n        ```\n\n        The advantage of using such a pattern is that it enables you to\n        always completely retrace your entire pipeline without creating\n        deep copies of the data frames whenever you have made a small\n        change like the one in our example. Note that the pipeline will\n        record which container you have used.\n    \"\"\"\n\n    def __init__(\n        self,\n        population=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        train=None,\n        validation=None,\n        test=None,\n        **kwargs,\n    ):\n\n        if population is not None and not isinstance(population, (DataFrame, View)):\n            raise TypeError(\n                \"'population' must be a getml.DataFrame or a getml.data.View, got \"\n                + type(population).__name__\n                + \".\"\n            )\n\n        if peripheral is not None and not _is_typed_dict(\n            peripheral, str, [DataFrame, View]\n        ):\n            raise TypeError(\n                \"'peripheral' must be a dict \"\n                + \"of getml.DataFrames or getml.data.Views.\"\n            )\n\n        if split is not None and not isinstance(\n            split, (StringColumn, StringColumnView)\n        ):\n            raise TypeError(\n                \"'split' must be StringColumn or a StringColumnView, got \"\n                + type(split).__name__\n                + \".\"\n            )\n\n        if not isinstance(deep_copy, bool):\n            raise TypeError(\n                \"'deep_copy' must be a bool, got \" + type(split).__name__ + \".\"\n            )\n\n        exclusive = (population is not None) ^ (\n            len(_make_subsets_from_kwargs(train, validation, test, **kwargs)) != 0\n        )\n\n        if not exclusive:\n            raise ValueError(\n                \"'population' and 'train', 'validation', 'test' as well as \"\n                + \"other subsets signified by kwargs are mutually exclusive. \"\n                + \"You have to pass \"\n                + \"either 'population' or some subsets, but you cannot pass both.\"\n            )\n\n        if population is None and split is not None:\n            raise ValueError(\n                \"'split's are used for splitting population DataFrames.\"\n                \"Hence, if you supply 'split', you also have to supply \"\n                \"a population.\"\n            )\n\n        if population is not None and split is None:\n            logger.warning(\n                \"You have passed a population table without passing 'split'. \"\n                \"You can access the entire set to pass to your pipeline \"\n                \"using the .full attribute.\"\n            )\n            split = from_value(\"full\")\n\n        self._id = _make_id()\n\n        self._population = population\n        self._peripheral = peripheral or {}\n        self._split = split\n        self._deep_copy = deep_copy\n\n        self._subsets = (\n            _make_subsets_from_split(population, split)\n            if split is not None\n            else _make_subsets_from_kwargs(train, validation, test, **kwargs)\n        )\n\n        if split is None and not _is_typed_dict(self._subsets, str, [DataFrame, View]):\n            raise TypeError(\n                \"'train', 'validation', 'test' and all other subsets must be either a \"\n                \"getml.DataFrame or a getml.data.View.\"\n            )\n\n        if deep_copy:\n            self._population = _deep_copy(self._population, self._id)\n            self._peripheral = {\n                k: _deep_copy(v, self._id) for (k, v) in self._peripheral.items()\n            }\n            self._subsets = {\n                k: _deep_copy(v, self._id) for (k, v) in self._subsets.items()\n            }\n\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n        self._frozen_time = None\n\n    def __dir__(self):\n        attrs = dir(type(self)) + self._ipython_key_completion()\n        attrs = [x for x in attrs if x.isidentifier()]\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        if \"_\" + key in self.__dict__:\n            return self.__dict__[\"_\" + key]\n\n        if key in self.__dict__[\"_subsets\"]:\n            if self.__dict__[\"_deep_copy\"] and self._frozen_time is None:\n                raise ValueError(\n                    cleandoc(\n                        f\"\"\"\n                        If you set deep_copy=True, you must call\n                        {type(self).__name__}.freeze() before you can extract data. The\n                        idea of deep_copy is to ensure that you can always retrace and\n                        reproduce your results. That is why the container\n                        needs to be immutable before it can be used.\n                        \"\"\"\n                    )\n                )\n\n            last_change = _get_last_change(\n                self.__dict__[\"_population\"],\n                self.__dict__[\"_peripheral\"],\n                self.__dict__[\"_subsets\"],\n            )\n\n            if self.__dict__[\"_last_change\"] != last_change:\n                logger.warning(\n                    cleandoc(\n                        f\"\"\"\n                        Some of the data frames added to the {type(self).__name__} have\n                        been modified after they were added.  This might lead to\n                        unexpected results. To avoid these sort of problems, you can set\n                        deep_copy=True when creating the {type(self).__name__}.\n                        \"\"\"\n                    )\n                )\n\n            return Subset(\n                container_id=self.__dict__[\"_id\"],\n                population=self.__dict__[\"_subsets\"][key].with_name(key),\n                peripheral=self.__dict__[\"_peripheral\"],\n            )\n\n        if key in self.__dict__[\"_peripheral\"]:\n            return self.__dict__[\"_peripheral\"][key]\n\n        raise KeyError(f\"{type(self).__name__} holds no data with name {key!r}.\")\n\n    def __repr__(self):\n        pop, perph = self._format()\n\n        template = cleandoc(\n            \"\"\"\n            population\n            {pop}\n\n            peripheral\n            {perph}\n            \"\"\"\n        )\n\n        return template.format(pop=pop._render_string(), perph=perph._render_string())\n\n    def _repr_html_(self):\n        pop, perph = self._format()\n\n        template = cleandoc(\n            \"\"\"\n            &lt;div style='margin-top: 15px;'&gt;\n            &lt;div style='float: left; margin-right: 50px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;population&lt;/div&gt;\n                {pop}\n            &lt;/div&gt;\n            &lt;div style='float: left;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;peripheral&lt;/div&gt;\n                {perph}\n            &lt;/div&gt;\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return template.format(pop=pop._render_html(), perph=perph._render_html())\n\n    def _format(self):\n        headers_pop = [[\"subset\", \"name\", \"rows\", \"type\"]]\n        rows_pop = [\n            [key, subset.name, subset.nrows(), type(subset).__name__]\n            for key, subset in self.subsets.items()  # pytype: disable=attribute-error\n        ]\n\n        headers_perph = [[\"name\", \"rows\", \"type\"]]\n\n        rows_perph = [\n            [perph.name, perph.nrows(), type(perph).__name__]\n            for perph in self.peripheral.values()  # pytype: disable=attribute-error\n        ]\n\n        names = [\n            perph.name\n            for perph in self.peripheral.values()  # pytype: disable=attribute-error\n        ]\n        aliases = list(self.peripheral.keys())  # pytype: disable=attribute-error\n\n        if any(alias not in names for alias in aliases):\n            headers_perph[0].insert(0, \"alias\")\n\n            for alias, row in zip(aliases, rows_perph):\n                row.insert(0, alias)\n\n        return _Formatter(headers=headers_pop, rows=rows_pop), _Formatter(\n            headers=headers_perph, rows=rows_perph\n        )\n\n    def _getml_deserialize(self):\n        cmd = {k[1:] + \"_\": v for (k, v) in self.__dict__.items() if v is not None}\n\n        if self._population is not None:\n            cmd[\"population_\"] = self._population._getml_deserialize()\n\n        if self._split is not None:\n            cmd[\n                \"split_\"\n            ] = self._split._getml_deserialize()  # pytype: disable=attribute-error\n\n        cmd[\"peripheral_\"] = {\n            k: v._getml_deserialize() for (k, v) in self._peripheral.items()\n        }\n\n        cmd[\"subsets_\"] = {\n            k: v._getml_deserialize() for (k, v) in self._subsets.items()\n        }\n\n        return cmd\n\n    def _ipython_key_completion(self):\n        attrs = [v[1:] for v in list(vars(self))]\n        attrs.extend(self._peripheral)\n        if not self._deep_copy or self._frozen_time is not None:\n            attrs.extend(self._subsets)\n        return attrs\n\n    def __setattr__(self, name, value):\n        if not name or name[0] != \"_\":\n            raise ValueError(\"Attempting a write operation on read-only data.\")\n        vars(self)[name] = value\n\n    def add(self, *args, **kwargs):\n        \"\"\"\n        Adds new peripheral data frames or views.\n        \"\"\"\n        wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n        if wrong_type:\n            raise TypeError(\n                \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n            )\n\n        wrong_type = [\n            k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n        ]\n\n        if wrong_type:\n            raise TypeError(\n                \"You must pass getml.DataFrames or getml.data.Views, \"\n                f\"but the following arguments were neither: {wrong_type!r}.\"\n            )\n\n        kwargs = {**{item.name: item for item in args}, **kwargs}\n\n        if self._frozen_time is not None:\n            raise ValueError(\n                f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n            )\n\n        if self._deep_copy:\n            kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n        self._peripheral = {**self._peripheral, **kwargs}\n\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n    def freeze(self):\n        \"\"\"\n        Freezes the container, so that changes are no longer possible.\n\n        This is required before you can extract data when `deep_copy=True`. The idea of\n        `deep_copy` is to ensure that you can always retrace and reproduce your results.\n        That is why the container needs to be immutable before it can be\n        used.\n        \"\"\"\n        self.sync()\n        self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    def save(self):\n        \"\"\"\n        Saves the Container to disk.\n        \"\"\"\n\n        cmd = dict()\n        cmd[\"type_\"] = \"DataContainer.save\"\n        cmd[\"name_\"] = self._id\n\n        cmd[\"container_\"] = self._getml_deserialize()\n\n        comm.send(cmd)\n\n    def sync(self):\n        \"\"\"\n        Synchronizes the last change with the data to avoid warnings that the data\n        has been changed.\n\n        This is only a problem when `deep_copy=False`.\n        \"\"\"\n        if self._frozen_time is not None:\n            raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n        self._last_change = _get_last_change(\n            self._population, self._peripheral, self._subsets\n        )\n\n    def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n        \"\"\"\n        TODO\n        \"\"\"\n        subsets = (\n            {name: df.to_pandas() for name, df in self._subsets.items()}\n            if self._subsets\n            else {}\n        )\n        peripherals = (\n            {name: df.to_pandas() for name, df in self.peripheral.items()}\n            if self.peripheral\n            else {}\n        )\n        if subsets or peripherals:\n            return {**subsets, **peripherals}\n\n        raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/container/#getml.data.container.Container.add","title":"<code>add(*args, **kwargs)</code>","text":"<p>Adds new peripheral data frames or views.</p> Source code in <code>getml/data/container.py</code> <pre><code>def add(self, *args, **kwargs):\n    \"\"\"\n    Adds new peripheral data frames or views.\n    \"\"\"\n    wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n    if wrong_type:\n        raise TypeError(\n            \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n        )\n\n    wrong_type = [\n        k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n    ]\n\n    if wrong_type:\n        raise TypeError(\n            \"You must pass getml.DataFrames or getml.data.Views, \"\n            f\"but the following arguments were neither: {wrong_type!r}.\"\n        )\n\n    kwargs = {**{item.name: item for item in args}, **kwargs}\n\n    if self._frozen_time is not None:\n        raise ValueError(\n            f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n        )\n\n    if self._deep_copy:\n        kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n    self._peripheral = {**self._peripheral, **kwargs}\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.container.Container.freeze","title":"<code>freeze()</code>","text":"<p>Freezes the container, so that changes are no longer possible.</p> <p>This is required before you can extract data when <code>deep_copy=True</code>. The idea of <code>deep_copy</code> is to ensure that you can always retrace and reproduce your results. That is why the container needs to be immutable before it can be used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def freeze(self):\n    \"\"\"\n    Freezes the container, so that changes are no longer possible.\n\n    This is required before you can extract data when `deep_copy=True`. The idea of\n    `deep_copy` is to ensure that you can always retrace and reproduce your results.\n    That is why the container needs to be immutable before it can be\n    used.\n    \"\"\"\n    self.sync()\n    self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/data/container/#getml.data.container.Container.save","title":"<code>save()</code>","text":"<p>Saves the Container to disk.</p> Source code in <code>getml/data/container.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves the Container to disk.\n    \"\"\"\n\n    cmd = dict()\n    cmd[\"type_\"] = \"DataContainer.save\"\n    cmd[\"name_\"] = self._id\n\n    cmd[\"container_\"] = self._getml_deserialize()\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/container/#getml.data.container.Container.sync","title":"<code>sync()</code>","text":"<p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/container.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    if self._frozen_time is not None:\n        raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.container.Container.to_pandas","title":"<code>to_pandas()</code>","text":"<p>TODO</p> Source code in <code>getml/data/container.py</code> <pre><code>def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    TODO\n    \"\"\"\n    subsets = (\n        {name: df.to_pandas() for name, df in self._subsets.items()}\n        if self._subsets\n        else {}\n    )\n    peripherals = (\n        {name: df.to_pandas() for name, df in self.peripheral.items()}\n        if self.peripheral\n        else {}\n    )\n    if subsets or peripherals:\n        return {**subsets, **peripherals}\n\n    raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/data_frame/","title":"Data frame","text":"<p>Handler for the data stored in the getML engine.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame","title":"<code>DataFrame</code>","text":"<p>Handler for the data stored in the getML engine.</p> <p>The <code>DataFrame</code> class represents a data frame object in the getML engine but does not contain any actual data itself. To create such a data frame object, fill it with data via the Python API, and to retrieve a handler for it, you can use one of the <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code> class methods. The Importing Data section in the user guide explains the particularities of each of those flavors of the unified import interface.</p> <p>If the data frame object is already present in the engine - either in memory as a temporary object or on disk when <code>save</code> was called earlier -, the <code>load_data_frame</code> function will create a new handler without altering the underlying data. For more information about the lifecycle of the data in the getML engine and its synchronization with the Python API please see the corresponding User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier used to link the handler with the underlying data frame object in the engine.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> Example <p>Creating a new data frame object in the getML engine and importing data is done by one the class functions <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p><pre><code>random = numpy.random.RandomState(7263)\n\ntable = pandas.DataFrame()\ntable['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\ntable['join_key'] = numpy.arange(1000)\ntable['time_stamp'] = random.rand(1000)\ntable['target'] = random.rand(1000)\n\ndf_table = getml.DataFrame.from_pandas(table, name = 'table')\n</code></pre> In addition to creating a new data frame object in the getML engine and filling it with all the content of <code>table</code>, the <code>from_pandas</code> function also returns a <code>DataFrame</code> handler to the underlying data.</p> <p>You don't have to create the data frame objects anew for each session. You can use their <code>save</code> method to write them to disk, the <code>list_data_frames</code> function to list all available objects in the engine, and <code>load_data_frame</code> to create a <code>DataFrame</code> handler for a data set already present in the getML engine (see User Guide for details).</p> <pre><code>df_table.save()\n\ngetml.data.list_data_frames()\n\ndf_table_reloaded = getml.data.load_data_frame('table')\n</code></pre> Note <p>Although the Python API does not store the actual data itself, you can use the <code>to_csv</code>, <code>to_db</code>, <code>to_json</code>, and <code>to_pandas</code> methods to retrieve them.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>class DataFrame:\n    \"\"\"Handler for the data stored in the getML engine.\n\n    The [`DataFrame`][getml.DataFrame] class represents a data frame\n    object in the getML engine but does not contain any actual data\n    itself. To create such a data frame object, fill it with data via\n    the Python API, and to retrieve a handler for it, you can use one\n    of the [`from_csv`][getml.DataFrame.from_csv],\n    [`from_db`][getml.DataFrame.from_db],\n    [`from_json`][getml.DataFrame.from_json], or\n    [`from_pandas`][getml.DataFrame.from_pandas] class methods. The\n    [Importing Data][importing-data] section in the user guide explains the\n    particularities of each of those flavors of the unified\n    import interface.\n\n    If the data frame object is already present in the engine -\n    either in memory as a temporary object or on disk when\n    [`save`][getml.DataFrame.save] was called earlier -, the\n    [`load_data_frame`][getml.data.load_data_frame] function will create a new\n    handler without altering the underlying data. For more information\n    about the lifecycle of the data in the getML engine and its\n    synchronization with the Python API please see the\n    corresponding [User Guide][python-api-lifecycles].\n\n    Args:\n        name (str):\n            Unique identifier used to link the handler with\n            the underlying data frame object in the engine.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n    Example:\n        Creating a new data frame object in the getML engine and importing\n        data is done by one the class functions\n        [`from_csv`][getml.DataFrame.from_csv],\n        [`from_db`][getml.DataFrame.from_db],\n        [`from_json`][getml.DataFrame.from_json], or\n        [`from_pandas`][getml.DataFrame.from_pandas].\n\n        ```python\n        random = numpy.random.RandomState(7263)\n\n        table = pandas.DataFrame()\n        table['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\n        table['join_key'] = numpy.arange(1000)\n        table['time_stamp'] = random.rand(1000)\n        table['target'] = random.rand(1000)\n\n        df_table = getml.DataFrame.from_pandas(table, name = 'table')\n        ```\n        In addition to creating a new data frame object in the getML\n        engine and filling it with all the content of `table`, the\n        [`from_pandas`][getml.DataFrame.from_pandas] function also\n        returns a [`DataFrame`][getml.DataFrame] handler to the\n        underlying data.\n\n        You don't have to create the data frame objects anew for each\n        session. You can use their [`save`][getml.DataFrame.save]\n        method to write them to disk, the\n        [`list_data_frames`][getml.data.list_data_frames] function to list all\n        available objects in the engine, and\n        [`load_data_frame`][getml.data.load_data_frame] to create a\n        [`DataFrame`][getml.DataFrame] handler for a data set already\n        present in the getML engine (see\n        [User Guide][python-api] for details).\n\n        ```python\n        df_table.save()\n\n        getml.data.list_data_frames()\n\n        df_table_reloaded = getml.data.load_data_frame('table')\n        ```\n\n    Note:\n        Although the Python API does not store the actual data itself,\n        you can use the [`to_csv`][getml.DataFrame.to_csv],\n        [`to_db`][getml.DataFrame.to_db],\n        [`to_json`][getml.DataFrame.to_json], and\n        [`to_pandas`][getml.DataFrame.to_pandas] methods to retrieve\n        them.\n\n    \"\"\"\n\n    _categorical_roles = roles_._categorical_roles\n    _numerical_roles = roles_._numerical_roles\n    _possible_keys = _categorical_roles + _numerical_roles\n\n    def __init__(self, name, roles=None):\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        vars(self)[\"name\"] = name\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        roles = roles or dict()\n\n        if not isinstance(roles, dict):\n            raise TypeError(\"'roles' must be dict or a getml.data.Roles object\")\n\n        for key, val in roles.items():\n            if key not in self._possible_keys:\n                msg = \"'{}' is not a proper role and will be ignored\\n\"\n                msg += \"Possible roles are: {}\"\n                raise ValueError(msg.format(key, self._possible_keys))\n            if not _is_typed_list(val, str):\n                raise TypeError(\n                    \"'{}' must be None, an empty list, or a list of str.\".format(key)\n                )\n\n        # ------------------------------------------------------------\n\n        join_keys = roles.get(\"join_key\", [])\n        time_stamps = roles.get(\"time_stamp\", [])\n        categorical = roles.get(\"categorical\", [])\n        numerical = roles.get(\"numerical\", [])\n        targets = roles.get(\"target\", [])\n        text = roles.get(\"text\", [])\n        unused_floats = roles.get(\"unused_float\", [])\n        unused_strings = roles.get(\"unused_string\", [])\n\n        # ------------------------------------------------------------\n\n        vars(self)[\"_categorical_columns\"] = [\n            StringColumn(name=cname, role=roles_.categorical, df_name=self.name)\n            for cname in categorical\n        ]\n\n        vars(self)[\"_join_key_columns\"] = [\n            StringColumn(name=cname, role=roles_.join_key, df_name=self.name)\n            for cname in join_keys\n        ]\n\n        vars(self)[\"_numerical_columns\"] = [\n            FloatColumn(name=cname, role=roles_.numerical, df_name=self.name)\n            for cname in numerical\n        ]\n\n        vars(self)[\"_target_columns\"] = [\n            FloatColumn(name=cname, role=roles_.target, df_name=self.name)\n            for cname in targets\n        ]\n\n        vars(self)[\"_text_columns\"] = [\n            StringColumn(name=cname, role=roles_.text, df_name=self.name)\n            for cname in text\n        ]\n\n        vars(self)[\"_time_stamp_columns\"] = [\n            FloatColumn(name=cname, role=roles_.time_stamp, df_name=self.name)\n            for cname in time_stamps\n        ]\n\n        vars(self)[\"_unused_float_columns\"] = [\n            FloatColumn(name=cname, role=roles_.unused_float, df_name=self.name)\n            for cname in unused_floats\n        ]\n\n        vars(self)[\"_unused_string_columns\"] = [\n            StringColumn(name=cname, role=roles_.unused_string, df_name=self.name)\n            for cname in unused_strings\n        ]\n\n        # ------------------------------------------------------------\n\n        self._check_duplicates()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _columns(self):\n        return (\n            vars(self)[\"_categorical_columns\"]\n            + vars(self)[\"_join_key_columns\"]\n            + vars(self)[\"_numerical_columns\"]\n            + vars(self)[\"_target_columns\"]\n            + vars(self)[\"_text_columns\"]\n            + vars(self)[\"_time_stamp_columns\"]\n            + vars(self)[\"_unused_float_columns\"]\n            + vars(self)[\"_unused_string_columns\"]\n        )\n\n    # ----------------------------------------------------------------\n\n    def _delete(self, mem_only: bool = False):\n        \"\"\"Deletes the data frame from the getML engine.\n\n        If called with the `mem_only` option set to True, the data\n        frame corresponding to the handler represented by the current\n        instance can be reloaded using the\n        [`load`][getml.DataFrame.load] method.\n\n        Args:\n            mem_only (bool, optional):\n                If True, the data frame will not be deleted\n                permanently, but just from memory (RAM).\n        \"\"\"\n\n        if not isinstance(mem_only, bool):\n            raise TypeError(\"'mem_only' must be of type bool\")\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.delete\"\n        cmd[\"name_\"] = self.name\n        cmd[\"mem_only_\"] = mem_only\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def __delitem__(self, colname: str):\n        self._drop(colname)\n\n    # ----------------------------------------------------------------\n\n    def __eq__(self, other):\n        if not isinstance(other, DataFrame):\n            raise TypeError(\n                \"A DataFrame can only be compared to another getml.DataFrame\"\n            )\n\n        # ------------------------------------------------------------\n\n        for kkey in self.__dict__:\n            if kkey not in other.__dict__:\n                return False\n\n            # Take special care when comparing numbers.\n            if isinstance(self.__dict__[kkey], numbers.Real):\n                if not np.isclose(self.__dict__[kkey], other.__dict__[kkey]):\n                    return False\n\n            elif self.__dict__[kkey] != other.__dict__[kkey]:\n                return False\n\n        # ------------------------------------------------------------\n\n        return True\n\n    # ----------------------------------------------------------------\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            return super().__getattribute__(name)\n\n    # ------------------------------------------------------------\n\n    def __getitem__(self, name):\n        if isinstance(\n            name,\n            (numbers.Integral, slice, BooleanColumnView, FloatColumn, FloatColumnView),\n        ):\n            return self.where(index=name)\n\n        if isinstance(name, list):\n            not_in_colnames = set(name) - set(self.colnames)\n            if not_in_colnames:\n                raise KeyError(f\"{list(not_in_colnames)} not found.\")\n            dropped = [col for col in self.colnames if col not in name]\n            return View(base=self, dropped=dropped)\n\n        col = _get_column(name, self._columns)\n\n        if col is not None:\n            return col\n\n        raise KeyError(\"Column named '\" + name + \"' not found.\")\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame\"\n        cmd[\"name_\"] = self.name\n\n        return cmd\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return self.nrows()\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if not _exists_in_memory(self.name):\n            return _empty_data_frame()\n\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_string(footer=footer)\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        return self.to_html()\n\n    # ------------------------------------------------------------\n\n    def __setattr__(self, name, value):\n        if name in vars(self):\n            vars(self)[name] = value\n        else:\n            self.add(value, name)\n\n    # ------------------------------------------------------------\n\n    def __setitem__(self, name, col):\n        self.add(col, name)\n\n    # ------------------------------------------------------------\n\n    def _add_categorical_column(self, col, name, role, subroles, unit):\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.add_categorical_column\"\n        cmd[\"name_\"] = name\n\n        cmd[\"col_\"] = col.cmd\n        cmd[\"df_name_\"] = self.name\n        cmd[\"role_\"] = role\n        cmd[\"subroles_\"] = subroles\n        cmd[\"unit_\"] = unit\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.recv_issues(sock)\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _add_column(self, col, name, role, subroles, unit):\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.add_column\"\n        cmd[\"name_\"] = name\n\n        cmd[\"col_\"] = col.cmd\n        cmd[\"df_name_\"] = self.name\n        cmd[\"role_\"] = role\n        cmd[\"subroles_\"] = subroles\n        cmd[\"unit_\"] = unit\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.recv_issues(sock)\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _add_numpy_array(self, numpy_array, name, role, subroles, unit):\n        if len(numpy_array.shape) != 1:\n            raise TypeError(\n                \"\"\"numpy.ndarray needs to be one-dimensional!\n                Maybe you can call .ravel().\"\"\"\n            )\n\n        if role is None:\n            temp_df = pd.DataFrame()\n            temp_df[\"column\"] = numpy_array\n            if _is_numerical_type(temp_df.dtypes[0]):\n                role = roles_.unused_float\n            else:\n                role = roles_.unused_string\n\n        col = (\n            FloatColumn(name=name, role=role, df_name=self.name)\n            if role in self._numerical_roles\n            else StringColumn(name=name, role=role, df_name=self.name)\n        )\n\n        _send_numpy_array(col, numpy_array)\n\n        if subroles:\n            self._set_subroles(name, subroles, append=False)\n\n        if unit:\n            self._set_unit(name, unit)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _check_duplicates(self) -&gt; None:\n        all_colnames: List[str] = []\n\n        all_colnames = _check_if_exists(self._categorical_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._join_key_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._numerical_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._target_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._text_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._time_stamp_names, all_colnames)\n\n        all_colnames = _check_if_exists(self._unused_names, all_colnames)\n\n    # ------------------------------------------------------------\n\n    def _check_plausibility(self, data_frame):\n        self._check_duplicates()\n\n        for col in self._categorical_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._join_key_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._numerical_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._target_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._text_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._time_stamp_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n        for col in self._unused_names:\n            if col not in data_frame.columns:\n                raise ValueError(\"Column named '\" + col + \"' does not exist!\")\n\n    # ------------------------------------------------------------\n\n    def _collect_footer_data(self):\n        footer = namedtuple(\n            \"footer\", [\"n_rows\", \"n_cols\", \"memory_usage\", \"name\", \"type\", \"url\"]\n        )\n\n        return footer(\n            n_rows=self.nrows(),\n            n_cols=self.ncols(),\n            memory_usage=self.memory_usage,\n            name=self.name,\n            type=\"getml.DataFrame\",\n            url=self._monitor_url,\n        )\n\n    # ------------------------------------------------------------\n\n    def _close(self, sock):\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.close\"\n        cmd[\"name_\"] = self.name\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    # ------------------------------------------------------------\n\n    def _drop(self, colname: str):\n        if not isinstance(colname, str):\n            raise TypeError(\"'colname' must be either a string.\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.remove_column\"\n        cmd[\"name_\"] = colname\n\n        cmd[\"df_name_\"] = self.name\n\n        comm.send(cmd)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def _format(self):\n        formatted = _DataFrameFormatter(self)\n        return formatted\n\n    # ------------------------------------------------------------\n\n    def _set_role(self, name, role, time_formats):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        subroles = col.subroles\n\n        unit = TIME_STAMP + COMPARISON_ONLY if role == roles_.time_stamp else col.unit\n\n        self.add(\n            col,\n            name=name,\n            role=role,\n            subroles=subroles,\n            unit=unit,\n            time_formats=time_formats,\n        )\n\n    # ------------------------------------------------------------\n\n    def _set_subroles(self, name: str, subroles: List[str], append: bool):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd.update(col.cmd)\n\n        cmd[\"type_\"] += \".set_subroles\"\n\n        cmd[\"subroles_\"] = list(set(col.subroles + subroles)) if append else subroles\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def _set_unit(self, name: str, unit: str):\n        if not isinstance(name, str):\n            raise TypeError(\"Parameter 'name' must be a string!\")\n\n        col = self[name]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd.update(col.cmd)\n\n        cmd[\"type_\"] += \".set_unit\"\n\n        cmd[\"unit_\"] = unit\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def add(self, col, name, role=None, subroles=None, unit=\"\", time_formats=None):\n        \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n        Args:\n            col ([`column`][getml.column] or `numpy.ndarray`):\n                The column or numpy.ndarray to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from `getml.data.roles`.\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n        \"\"\"\n\n        if isinstance(col, np.ndarray):\n            self._add_numpy_array(col, name, role, subroles, unit)\n            return\n\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n\n        is_string = isinstance(col, (StringColumnView, StringColumn))\n\n        if is_string:\n            self._add_categorical_column(col, name, role, subroles, unit)\n        else:\n            self._add_column(col, name, role, subroles, unit)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _categorical_names(self):\n        return [col.name for col in self._categorical_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def colnames(self):\n        \"\"\"\n        List of the names of all columns.\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return (\n            self._time_stamp_names\n            + self._join_key_names\n            + self._target_names\n            + self._categorical_names\n            + self._numerical_names\n            + self._text_names\n            + self._unused_names\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self):\n        \"\"\"\n        Alias for [`colnames`][getml.DataFrame.colnames].\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return self.colnames\n\n    # ------------------------------------------------------------\n\n    def copy(self, name: str) -&gt; \"DataFrame\":\n        \"\"\"\n        Creates a deep copy of the data frame under a new name.\n\n        Args:\n            name (str):\n                The name of the new data frame.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                A handle to the deep copy.\n        \"\"\"\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be a string.\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.concat\"\n        cmd[\"name_\"] = name\n\n        cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n        comm.send(cmd)\n\n        return DataFrame(name=name).refresh()\n\n    # ------------------------------------------------------------\n\n    def delete(self):\n        \"\"\"\n        Permanently deletes the data frame. `delete` first unloads the data frame\n        from memory and then deletes it from disk.\n        \"\"\"\n        # ------------------------------------------------------------\n\n        self._delete()\n\n    # ------------------------------------------------------------\n\n    def drop(self, cols):\n        \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if not _is_typed_list(names, str):\n            raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n        return View(base=self, dropped=names)\n\n    # ------------------------------------------------------------\n\n    def freeze(self):\n        \"\"\"Freezes the data frame.\n\n        After you have frozen the data frame, the data frame is immutable\n        and in-place operations are no longer possible. However, you can\n        still create views. In other words, operations like\n        [`set_role`][getml.DataFrame.set_role] are no longer possible,\n        but operations like [`with_role`][getml.DataFrame.with_role] are.\n        \"\"\"\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.freeze\"\n        cmd[\"name_\"] = self.name\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_arrow(cls, table, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from an Arrow Table.\n\n        This is one of the fastest way to get data into the\n        getML engine.\n\n        Args:\n            table (pyarrow.Table):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(table, pa.Table):\n            raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_arrow(table)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_arrow(table=table, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_csv(\n        cls,\n        fnames,\n        name,\n        num_lines_sniffed=1000,\n        num_lines_read=0,\n        quotechar='\"',\n        sep=\",\",\n        skip=0,\n        colnames=None,\n        roles=None,\n        ignore=False,\n        dry=False,\n        verbose=True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from CSV files.\n\n        The getML engine will construct a data\n        frame object in the engine, fill it with the data read from\n        the CSV file(s), and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            fnames (List[str]):\n                CSV file paths to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            num_lines_sniffed (int, optional):\n                Number of lines analyzed by the sniffer.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional): The first line of a CSV file\n                usually contains the column names. When this is not the case,\n                you need to explicitly pass them.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n            verbose (bool, optional):\n                If True, when fnames are urls, the filenames are\n                printed to stdout during the download.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Note:\n            It is assumed that the first line of each CSV file\n            contains a header with the column names.\n\n        Example:\n            Let's assume you have two CSV files - *file1.csv* and\n            *file2.csv* - in the current working directory. You can\n            import their data into the getML engine using.\n            ```python\n            df_expd = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"'\n                )\n\n            # However, the CSV format lacks type safety. If you want to\n            # build a reliable pipeline, it is a good idea\n            # to hard-code the roles:\n\n            roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n            df_expd = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"',\n                roles=roles\n                )\n\n            # If you think that typing out all the roles by hand is too\n            # cumbersome, you can use a dry run:\n\n            roles = data.DataFrame.from_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY DATA FRAME\",\n                sep=';',\n                quotechar='\"',\n                dry=True\n            )\n            ```\n\n            This will return the roles dictionary it would have used. You\n            can now hard-code this.\n\n        \"\"\"\n\n        if not isinstance(fnames, list):\n            fnames = [fnames]\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(num_lines_sniffed, numbers.Real):\n            raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be str.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        fnames = _retrieve_urls(fnames, verbose=verbose)\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_csv(\n                fnames=fnames,\n                num_lines_sniffed=int(num_lines_sniffed),\n                quotechar=quotechar,\n                sep=sep,\n                skip=int(skip),\n                colnames=colnames,\n            )\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_csv(\n            fnames=fnames,\n            append=False,\n            quotechar=quotechar,\n            sep=sep,\n            num_lines_read=num_lines_read,\n            skip=skip,\n            colnames=colnames,\n        )\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_db(\n        cls, table_name, name=None, roles=None, ignore=False, dry=False, conn=None\n    ):\n        \"\"\"Create a DataFrame from a table in a database.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from table `table_name` in the connected\n        database (see [`database`][getml.database]), and return a\n        corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            table_name (str):\n                Name of the table to be read.\n\n            name (str):\n                Name of the data frame to be created. If not passed,\n                then the *table_name* will be used.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection, the engine\n                will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Example:\n            ```python\n            getml.database.connect_mysql(\n                host=\"db.relational-data.org\",\n                port=3306,\n                dbname=\"financial\",\n                user=\"guest\",\n                password=\"relational\"\n            )\n\n            loan = getml.DataFrame.from_db(\n                table_name='loan', name='data_frame_loan')\n            ```\n        \"\"\"\n\n        # -------------------------------------------\n\n        name = name or table_name\n\n        # -------------------------------------------\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\n                \"'roles' must be a getml.data.Roles object, a dict or None.\"\n            )\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # -------------------------------------------\n\n        conn = conn or database.Connection()\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_db(table_name, conn)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_dict(\n        cls, data: Dict[str, List[Any]], name: str, roles=None, ignore=False, dry=False\n    ):\n        \"\"\"Create a new DataFrame from a dict\n\n        Args:\n            data (dict):\n                The dict containing the data.\n                The data should be in the following format:\n                ```python\n                data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n                ```\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n        \"\"\"\n\n        if not isinstance(data, dict):\n            raise TypeError(\"'data' must be dict.\")\n\n        return cls.from_arrow(\n            table=pa.Table.from_pydict(data),\n            name=name,\n            roles=roles,\n            ignore=ignore,\n            dry=dry,\n        )\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_json(cls, json_str, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a new DataFrame from a JSON string.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the JSON string, and return a\n        corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            json_str (str):\n                The JSON string containing the data.\n                The json_str should be in the following format:\n                ```python\n                json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n                ```\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n        Returns:\n            [`DataFrame`][getml.data.DataFrame]: Handler of the underlying data.\n\n        \"\"\"\n\n        if not isinstance(json_str, str):\n            raise TypeError(\"'json_str' must be str.\")\n\n        return cls.from_dict(\n            data=json.loads(json_str),\n            name=name,\n            roles=roles,\n            ignore=ignore,\n            dry=dry,\n        )\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_pandas(cls, pandas_df, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the `pandas.DataFrame`, and\n        return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            pandas_df (pandas.DataFrame):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                 roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                          getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(pandas_df, pd.DataFrame):\n            raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        pandas_df_modified = _modify_pandas_columns(pandas_df)\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_pandas(pandas_df_modified)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_pandas(pandas_df=pandas_df_modified, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_parquet(cls, fname, name, roles=None, ignore=False, dry=False):\n        \"\"\"Create a DataFrame from parquet files.\n\n        This is one of the fastest way to get data into the\n        getML engine.\n\n        Args:\n            fname (str):\n                The path of the parquet file to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_parquet(fname)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_parquet(fname=fname, append=False)\n\n    # --------------------------------------------------------------------\n\n    @classmethod\n    def from_pyspark(cls, spark_df, name, roles=None, ignore=False, dry=False) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n        It will construct a data frame object in the engine, fill it\n        with the data read from the `pyspark.sql.DataFrame`, and\n        return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            spark_df (pyspark.sql.DataFrame):\n                The table to be read.\n\n            name (str):\n                Name of the data frame to be created.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        # The content of roles is checked in the class constructor called below.\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            head = spark_df.limit(2).toPandas()\n\n            sniffed_roles = _sniff_pandas(head)\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_pyspark(spark_df=spark_df, append=False)\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_s3(\n        cls,\n        bucket: str,\n        keys: List[str],\n        region: str,\n        name: str,\n        num_lines_sniffed=1000,\n        num_lines_read=0,\n        sep=\",\",\n        skip=0,\n        colnames=None,\n        roles=None,\n        ignore=False,\n        dry=False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n        This classmethod will construct a data\n        frame object in the engine, fill it with the data read from\n        the CSV file(s), and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            keys (List[str]):\n                The list of keys (files in the bucket) to be read.\n\n            region (str):\n                The region in which the bucket is located.\n\n            name (str):\n                Name of the data frame to be created.\n\n            num_lines_sniffed (int, optional):\n                Number of lines analyzed by the sniffer.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names. When this is not the case,\n                you need to explicitly pass them.\n\n            roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n                Maps the [`roles`][getml.data.roles] to the\n                column names (see [`colnames`][getml.DataFrame.colnames]).\n\n                The `roles` dictionary is expected to have the following format:\n                ```python\n                roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                         getml.data.role.target: [\"colname3\"]}\n                ```\n                Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n            ignore (bool, optional):\n                Only relevant when roles is not None.\n                Determines what you want to do with any colnames not\n                mentioned in roles. Do you want to ignore them (True)\n                or read them in as unused columns (False)?\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Handler of the underlying data.\n\n        Example:\n            Let's assume you have two CSV files - *file1.csv* and\n            *file2.csv* - in the bucket. You can\n            import their data into the getML engine using the following\n            commands:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            data_frame_expd = data.DataFrame.from_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY DATA FRAME\",\n                sep=';'\n            )\n            ```\n\n            You can also set the access credential as environment variables\n            before you launch the getML engine.\n\n            Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n            for further information on overriding the CSV sniffer for greater\n            type safety.\n\n        Note:\n            Not supported in the getML community edition.\n        \"\"\"\n\n        if isinstance(keys, str):\n            keys = [keys]\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be str.\")\n\n        if not _is_non_empty_typed_list(keys, str):\n            raise TypeError(\"'keys' must be either a string or a list of str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be str.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(num_lines_sniffed, numbers.Real):\n            raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if roles is not None and not isinstance(roles, (dict, Roles)):\n            raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n        if not isinstance(ignore, bool):\n            raise TypeError(\"'ignore' must be bool.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n        if roles is None or not ignore:\n            sniffed_roles = _sniff_s3(\n                bucket=bucket,\n                keys=keys,\n                region=region,\n                num_lines_sniffed=int(num_lines_sniffed),\n                sep=sep,\n                skip=int(skip),\n                colnames=colnames,\n            )\n\n            if roles is None:\n                roles = sniffed_roles\n            else:\n                roles = _update_sniffed_roles(sniffed_roles, roles)\n\n        if dry:\n            return roles\n\n        data_frame = cls(name, roles)\n\n        return data_frame.read_s3(\n            bucket=bucket,\n            keys=keys,\n            region=region,\n            append=False,\n            sep=sep,\n            num_lines_read=int(num_lines_read),\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n    # ------------------------------------------------------------\n\n    @classmethod\n    def from_view(\n        cls,\n        view,\n        name,\n        dry=False,\n    ):\n        \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n        This classmethod will construct a data\n        frame object in the engine, fill it with the data read from\n        the [`View`][getml.data.View], and return a corresponding\n        [`DataFrame`][getml.DataFrame] handle.\n\n        Args:\n            view ([`View`][getml.data.View]):\n                The view from which we want to read the data.\n\n            name (str):\n                Name of the data frame to be created.\n\n            dry (bool, optional):\n                If set to True, then the data\n                will not actually be read. Instead, the method will only\n                return the roles it would have used. This can be used\n                to hard-code roles when setting up a pipeline.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n\n        \"\"\"\n        # ------------------------------------------------------------\n\n        if not isinstance(view, View):\n            raise TypeError(\"'view' must be getml.data.View.\")\n\n        if not isinstance(name, str):\n            raise TypeError(\"'name' must be str.\")\n\n        if not isinstance(dry, bool):\n            raise TypeError(\"'dry' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        if dry:\n            return view.roles\n\n        data_frame = cls(name)\n\n        # ------------------------------------------------------------\n\n        return data_frame.read_view(view=view, append=False)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _join_key_names(self):\n        return [col.name for col in self._join_key_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def last_change(self) -&gt; str:\n        \"\"\"\n        A string describing the last time this data frame has been changed.\n        \"\"\"\n        return _last_change(self.name)\n\n    # ------------------------------------------------------------\n\n    def load(self) -&gt; \"DataFrame\":\n        \"\"\"Loads saved data from disk.\n\n        The data frame object holding the same name as the current\n        [`DataFrame`][getml.DataFrame] instance will be loaded from\n        disk into the getML engine and updates the current handler\n        using [`refresh`][getml.DataFrame.refresh].\n\n        Example:\n            First, we have to create and import data sets.\n            ```python\n            d, _ = getml.datasets.make_numerical(population_name = 'test')\n            getml.data.list_data_frames()\n            ```\n\n            In the output of [`list_data_frames`][getml.data.list_data_frames] we\n            can find our underlying data frame object 'test' listed\n            under the 'in_memory' key (it was created and imported by\n            [`make_numerical`][getml.datasets.make_numerical]). This means the\n            getML engine does only hold it in memory (RAM) yet, and we\n            still have to [`save`][getml.DataFrame.save] it to\n            disk in order to [`load`][getml.DataFrame.load] it\n            again or to prevent any loss of information between\n            different sessions.\n            ```python\n            d.save()\n            getml.data.list_data_frames()\n            d2 = getml.DataFrame(name = 'test').load()\n            ```\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        Note:\n            When invoking [`load`][getml.DataFrame.load] all\n            changes of the underlying data frame object that took\n            place after the last call to the\n            [`save`][getml.DataFrame.save] method will be\n            lost. Thus, this method  enables you to undo changes\n            applied to the [`DataFrame`][getml.DataFrame].\n            ```python\n            d, _ = getml.datasets.make_numerical()\n            d.save()\n\n            # Accidental change we want to undo\n            d.rm('column_01')\n\n            d.load()\n            ```\n            If [`save`][getml.DataFrame.save] hasn't been called\n            on the current instance yet, or it wasn't stored to disk in\n            a previous session, [`load`][getml.DataFrame.load]\n            will throw an exception\n\n                File or directory '../projects/X/data/Y/' not found!\n\n            Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n            offers an easier way of creating\n            [`DataFrame`][getml.DataFrame] handlers to data in the\n            getML engine.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.load\"\n        cmd[\"name_\"] = self.name\n        comm.send(cmd)\n        return self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def memory_usage(self):\n        \"\"\"\n        Convenience wrapper that returns the memory usage in MB.\n        \"\"\"\n        return self.nbytes() / 1e06\n\n    # ------------------------------------------------------------\n\n    @property\n    def _monitor_url(self) -&gt; Optional[str]:\n        \"\"\"\n        A link to the data frame in the getML monitor.\n        \"\"\"\n        url = comm._monitor_url()\n        return (\n            url + \"getdataframe/\" + comm._get_project_name() + \"/\" + self.name + \"/\"\n            if url\n            else None\n        )\n\n    # ------------------------------------------------------------\n\n    def nbytes(self):\n        \"\"\"Size of the data stored in the underlying data frame in the getML\n        engine.\n\n        Returns:\n            numpy.uint64:\n                Size of the underlying object in bytes.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.nbytes\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                sock.close()\n                comm.engine_exception_handler(msg)\n            nbytes = comm.recv_string(sock)\n\n        return np.uint64(nbytes)\n\n    # ------------------------------------------------------------\n\n    def ncols(self):\n        \"\"\"\n        Number of columns in the current instance.\n\n        Returns:\n            int:\n                Overall number of columns\n        \"\"\"\n        return len(self.colnames)\n\n    # ------------------------------------------------------------\n\n    def nrows(self):\n        \"\"\"\n        Number of rows in the current instance.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.nrows\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                sock.close()\n                comm.engine_exception_handler(msg)\n            nrows = comm.recv_string(sock)\n\n        return int(nrows)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _numerical_names(self):\n        return [col.name for col in self._numerical_columns]\n\n    # --------------------------------------------------------------------------\n\n    def read_arrow(self, table, append=False):\n        \"\"\"Uploads a `pyarrow.Table`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `table`.\n\n        Args:\n            table (pyarrow.Table):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Current instance.\n\n        Note:\n            For columns containing `pandas.Timestamp` there can\n            be small inconsistencies in the order of microseconds\n            when sending the data to the getML engine. This is due to\n            the way the underlying information is stored.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if not isinstance(table, pa.Table):\n            raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        # ------------------------------------------------------------\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_pandas(...).\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_arrow\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"append_\"] = append\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        with comm.send_and_get_socket(cmd) as sock:\n            with sock.makefile(mode=\"wb\") as sink:\n                batches = table.to_batches()\n                with pa.ipc.new_stream(sink, table.schema) as writer:\n                    for batch in batches:\n                        writer.write_batch(batch)\n            msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        return self.refresh()\n\n    # --------------------------------------------------------------------------\n\n    def read_csv(\n        self,\n        fnames,\n        append=False,\n        quotechar='\"',\n        sep=\",\",\n        num_lines_read=0,\n        skip=0,\n        colnames=None,\n        time_formats=None,\n        verbose=True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read CSV files.\n\n        It is assumed that the first line of each CSV file contains a\n        header with the column names.\n\n        Args:\n            fnames (List[str]):\n                CSV file paths to be read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names.\n                When this is not the case, you need to explicitly pass them.\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n            verbose (bool, optional):\n                If True, when `fnames` are urls, the filenames are printed to\n                stdout during the download.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if not isinstance(fnames, list):\n            fnames = [fnames]\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be str.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_csv(...).\"\"\"\n            )\n\n        if not _is_non_empty_typed_list(fnames, str):\n            raise TypeError(\n                \"\"\"'fnames' must be a list containing at\n                least one path to a CSV file\"\"\"\n            )\n\n        fnames_ = _retrieve_urls(fnames, verbose)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fnames_\"] = fnames_\n\n        cmd[\"append_\"] = append\n        cmd[\"num_lines_read_\"] = num_lines_read\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"skip_\"] = skip\n        cmd[\"time_formats_\"] = time_formats\n\n        if colnames is not None:\n            cmd[\"colnames_\"] = colnames\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_json(self, json_str, append=False, time_formats=None):\n        \"\"\"Fill from JSON\n\n        Fills the data frame with data from a JSON string.\n\n        Args:\n\n            json_str (str):\n                The JSON string containing the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                `json_str` be appended or replace the existing data?\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        Note:\n            This does not support NaN values. If you want support for NaN,\n            use [`from_json`][getml.DataFrame.from_json] instead.\n\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_json(...).\"\"\"\n            )\n\n        if not isinstance(json_str, str):\n            raise TypeError(\"'json_str' must be of type str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be of type bool\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\n                \"\"\"'time_formats' must be a list of strings\n                containing at least one time format\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.from_json\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n        cmd[\"time_formats_\"] = time_formats\n\n        with comm.send_and_get_socket(cmd) as sock:\n            comm.send_string(sock, json_str)\n            msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_parquet(\n        self,\n        fname: str,\n        append: bool = False,\n        verbose: bool = True,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read a parquet file.\n\n        Args:\n            fname (str):\n                The filepath of the parquet file to be read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n        \"\"\"\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than\n                zero columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_parquet(...).\"\"\"\n            )\n\n        fname_ = _retrieve_urls([fname], verbose)[0]\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_parquet\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fname_\"] = fname_\n        cmd[\"append_\"] = append\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_s3(\n        self,\n        bucket: str,\n        keys: List[str],\n        region: str,\n        append: bool = False,\n        sep: str = \",\",\n        num_lines_read: int = 0,\n        skip: int = 0,\n        colnames: Optional[List[str]] = None,\n        time_formats: Optional[List[str]] = None,\n    ):\n        \"\"\"Read CSV files from an S3 bucket.\n\n        It is assumed that the first line of each CSV file contains a\n        header with the column names.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            keys (List[str]):\n                The list of keys (files in the bucket) to be read.\n\n            region (str):\n                The region in which the bucket is located.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n            sep (str, optional):\n                The separator used for separating fields.\n\n            num_lines_read (int, optional):\n                Number of lines read from each file.\n                Set to 0 to read in the entire file.\n\n            skip (int, optional):\n                Number of lines to skip at the beginning of each file.\n\n            colnames (List[str] or None, optional):\n                The first line of a CSV file\n                usually contains the column names.\n                When this is not the case, you need to explicitly pass them.\n\n            time_formats (List[str], optional):\n                The list of formats tried when parsing time stamps.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        Note:\n            Not supported in the getML community edition.\n        \"\"\"\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        if not isinstance(keys, list):\n            keys = [keys]\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be str.\")\n\n        if not _is_non_empty_typed_list(keys, str):\n            raise TypeError(\"'keys' must be either a string or a list of str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be str.\")\n\n        if not isinstance(num_lines_read, numbers.Real):\n            raise TypeError(\"'num_lines_read' must be a real number\")\n\n        if not isinstance(skip, numbers.Real):\n            raise TypeError(\"'skip' must be a real number\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n            raise TypeError(\n                \"'colnames' must be either be None or a non-empty list of str.\"\n            )\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_s3(...).\"\"\"\n            )\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.read_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"append_\"] = append\n        cmd[\"bucket_\"] = bucket\n        cmd[\"keys_\"] = keys\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"time_formats_\"] = time_formats\n        cmd[\"num_lines_read_\"] = num_lines_read\n        cmd[\"skip_\"] = skip\n\n        if colnames is not None:\n            cmd[\"colnames_\"] = colnames\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def read_view(\n        self,\n        view: View,\n        append: bool = False,\n    ) -&gt; \"DataFrame\":\n        \"\"\"Read the data from a [`View`][getml.data.View].\n\n        Args:\n            view ([`View`][getml.data.View]):\n                The view to read.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                the CSV files in `fnames` be appended or replace the\n                existing data?\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n\n        \"\"\"\n\n        if not isinstance(view, View):\n            raise TypeError(\"'view' must be getml.data.View.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        view.check()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_view\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = view._getml_deserialize()\n\n        cmd[\"append_\"] = append\n\n        comm.send(cmd)\n\n        return self.refresh()\n\n    # --------------------------------------------------------------------------\n\n    def read_db(self, table_name: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n        \"\"\"\n        Fill from Database.\n\n        The DataFrame will be filled from a table in the database.\n\n        Args:\n            table_name (str):\n                Table from which we want to retrieve the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML, should the content of\n                `table_name` be appended or replace the existing data?\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n        \"\"\"\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be str.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_db(...).\"\"\"\n            )\n\n        conn = conn or database.Connection()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_db\"\n        cmd[\"name_\"] = self.name\n        cmd[\"table_name_\"] = table_name\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Uploads a `pandas.DataFrame`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `pandas_df`.\n\n        Args:\n            pandas_df (pandas.DataFrame):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n        Note:\n            For columns containing `pandas.Timestamp` there can\n            occur small inconsistencies in the order of microseconds\n            when sending the data to the getML engine. This is due to\n            the way the underlying information is stored.\n        \"\"\"\n\n        if not isinstance(pandas_df, pd.DataFrame):\n            raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_pandas(...).\"\"\"\n            )\n\n        table = pa.Table.from_pandas(_modify_pandas_columns(pandas_df))\n\n        return self.read_arrow(table, append=append)\n\n    # --------------------------------------------------------------------------\n\n    def read_pyspark(self, spark_df, append: bool = False) -&gt; \"DataFrame\":\n        \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n        Replaces the actual content of the underlying data frame in\n        the getML engine with `pandas_df`.\n\n        Args:\n            spark_df (pyspark.sql.DataFrame):\n                Data the underlying data frame object in the getML\n                engine should obtain.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n        \"\"\"\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be bool.\")\n\n        temp_dir = _retrieve_temp_dir()\n        os.makedirs(temp_dir, exist_ok=True)\n        path = os.path.join(temp_dir, self.name)\n        spark_df.write.mode(\"overwrite\").parquet(path)\n\n        filepaths = [\n            os.path.join(path, filepath)\n            for filepath in os.listdir(path)\n            if filepath[-8:] == \".parquet\"\n        ]\n\n        for i, filepath in enumerate(filepaths):\n            self.read_parquet(filepath, append or i &gt; 0)\n\n        shutil.rmtree(path)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def read_query(self, query: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n        \"\"\"Fill from query\n\n        Fills the data frame with data from a table in the database.\n\n        Args:\n            query (str):\n                The query used to retrieve the data.\n\n            append (bool, optional):\n                If a data frame object holding the same ``name`` is\n                already present in the getML engine, should the content in\n                `query` be appended or replace the existing data?\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                Handler of the underlying data.\n        \"\"\"\n\n        if self.ncols() == 0:\n            raise Exception(\n                \"\"\"Reading data is only possible in a DataFrame with more than zero\n                columns. You can pre-define columns during\n                initialization of the DataFrame or use the classmethod\n                from_db(...).\"\"\"\n            )\n\n        if not isinstance(query, str):\n            raise TypeError(\"'query' must be of type str\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be of type bool\")\n\n        conn = conn or database.Connection()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.from_query\"\n        cmd[\"name_\"] = self.name\n        cmd[\"query_\"] = query\n\n        cmd[\"categorical_\"] = self._categorical_names\n        cmd[\"join_keys_\"] = self._join_key_names\n        cmd[\"numerical_\"] = self._numerical_names\n        cmd[\"targets_\"] = self._target_names\n        cmd[\"text_\"] = self._text_names\n        cmd[\"time_stamps_\"] = self._time_stamp_names\n        cmd[\"unused_floats_\"] = self._unused_float_names\n        cmd[\"unused_strings_\"] = self._unused_string_names\n\n        cmd[\"append_\"] = append\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n        return self\n\n    # --------------------------------------------------------------------------\n\n    def refresh(self) -&gt; \"DataFrame\":\n        \"\"\"Aligns meta-information of the current instance with the\n        corresponding data frame in the getML engine.\n\n        This method can be used to avoid encoding conflicts. Note that\n        [`load`][getml.DataFrame.load] as well as several other\n        methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = \"DataFrame.refresh\"\n        cmd[\"name_\"] = self.name\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n        if msg[0] != \"{\":\n            comm.engine_exception_handler(msg)\n\n        roles = json.loads(msg)\n\n        self.__init__(name=self.name, roles=roles)  # type: ignore\n\n        return self\n\n    # ------------------------------------------------------------\n\n    @property\n    def roles(self):\n        \"\"\"\n        The roles of the columns included\n        in this DataFrame.\n        \"\"\"\n        return Roles(\n            categorical=self._categorical_names,\n            join_key=self._join_key_names,\n            numerical=self._numerical_names,\n            target=self._target_names,\n            text=self._text_names,\n            time_stamp=self._time_stamp_names,\n            unused_float=self._unused_float_names,\n            unused_string=self._unused_string_names,\n        )\n\n    # ------------------------------------------------------------\n\n    def remove_subroles(self, cols):\n        \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n        Args:\n            columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        for name in names:\n            self._set_subroles(name, subroles=[], append=False)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def remove_unit(self, cols):\n        \"\"\"Removes the unit from one or more columns.\n\n        Args:\n            columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        for name in names:\n            self._set_unit(name, \"\")\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def rowid(self):\n        \"\"\"\n        The rowids for this data frame.\n        \"\"\"\n        return rowid()[: self.nrows()]\n\n    # ------------------------------------------------------------\n\n    def save(self) -&gt; \"DataFrame\":\n        \"\"\"Writes the underlying data in the getML engine to disk.\n\n        Returns:\n            [`DataFrame`][getml.DataFrame]:\n                The current instance.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.save\"\n        cmd[\"name_\"] = self.name\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def set_role(self, cols, role, time_formats=None):\n        \"\"\"Assigns a new role to one or more columns.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names of the columns.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n\n        Example:\n            ```python\n            data_df = dict(\n                animal=[\"hawk\", \"parrot\", \"goose\"],\n                votes=[12341, 5127, 65311],\n                date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n            df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n            df.set_role(['animal'], getml.data.roles.categorical)\n            df.set_role(['votes'], getml.data.roles.numerical)\n            df.set_role(\n                ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n            df\n            ```\n            ```\n            | date                        | animal      | votes     |\n            | time stamp                  | categorical | numerical |\n            ---------------------------------------------------------\n            | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n            | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n            | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n            ```\n        \"\"\"\n        # ------------------------------------------------------------\n\n        time_formats = time_formats or constants.TIME_FORMATS\n\n        # ------------------------------------------------------------\n\n        names = _handle_cols(cols)\n\n        if not isinstance(role, str):\n            raise TypeError(\"'role' must be str.\")\n\n        if not _is_non_empty_typed_list(time_formats, str):\n            raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n        # ------------------------------------------------------------\n\n        for nname in names:\n            if nname not in self.colnames:\n                raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n        if role not in self._possible_keys:\n            raise ValueError(\n                \"'role' must be one of the following values: \"\n                + str(self._possible_keys)\n            )\n\n        # ------------------------------------------------------------\n\n        for name in names:\n            if self[name].role != role:\n                self._set_role(name, role, time_formats)\n\n        # ------------------------------------------------------------\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def set_subroles(self, cols, subroles, append=True):\n        \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n                Must be from [`subroles`][getml.data.subroles].\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if isinstance(subroles, str):\n            subroles = [subroles]\n\n        if not _is_non_empty_typed_list(subroles, str):\n            raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n        if not isinstance(append, bool):\n            raise TypeError(\"'append' must be a bool.\")\n\n        for name in names:\n            self._set_subroles(name, subroles, append)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    def set_unit(self, cols, unit, comparison_only=False):\n        \"\"\"Assigns a new unit to one or more columns.\n\n        Args:\n            cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n        \"\"\"\n\n        names = _handle_cols(cols)\n\n        if not isinstance(unit, str):\n            raise TypeError(\"Parameter 'unit' must be a str.\")\n\n        if comparison_only:\n            unit += COMPARISON_ONLY\n\n        for name in names:\n            self._set_unit(name, unit)\n\n        self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def shape(self):\n        \"\"\"\n        A tuple containing the number of rows and columns of\n        the DataFrame.\n        \"\"\"\n        self.refresh()\n        return (self.nrows(), self.ncols())\n\n    # ------------------------------------------------------------\n\n    @property\n    def _target_names(self):\n        return [col.name for col in self._target_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _text_names(self):\n        return [col.name for col in self._text_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _time_stamp_names(self):\n        return [col.name for col in self._time_stamp_columns]\n\n    # ----------------------------------------------------------------\n\n    def to_arrow(self):\n        \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyarrow.Table`.\n\n        Returns:\n            pyarrow.Table:\n                Pyarrow equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self)\n\n    # ------------------------------------------------------------\n\n    def to_csv(\n        self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file.\n\n        Args:\n            fname (str):\n                The name of the CSV file.\n                The ending \".csv\" and an optional batch number will\n                be added automatically.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        fname_ = os.path.abspath(fname)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"fname_\"] = fname_\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_db(self, table_name, conn=None):\n        \"\"\"Writes the underlying data into a newly created table in the\n        database.\n\n        Args:\n            table_name (str):\n                Name of the table to be created.\n\n                If a table of that name already exists, it will be\n                replaced.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n        \"\"\"\n\n        conn = conn or database.Connection()\n\n        self.refresh()\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        cmd = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_db\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"table_name_\"] = table_name\n\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n    # ----------------------------------------------------------------\n\n    def to_html(self, max_rows=10):\n        \"\"\"\n        Represents the data frame in HTML format, optimized for an\n        iPython notebook.\n\n        Args:\n            max_rows (int):\n                The maximum number of rows to be displayed.\n        \"\"\"\n\n        if not _exists_in_memory(self.name):\n            return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n        formatted = self._format()\n        formatted.max_rows = max_rows\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_html(footer=footer)\n\n    # ------------------------------------------------------------\n\n    def to_json(self):\n        \"\"\"Creates a JSON string from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a JSON string.\n        \"\"\"\n        return self.to_pandas().to_json()\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self):\n        \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        `pandas.DataFrame`.\n\n        Returns:\n            pandas.DataFrame:\n                Pandas equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_arrow(self).to_pandas()\n\n    # ------------------------------------------------------------\n\n    def to_parquet(self, fname, compression=\"snappy\"):\n        \"\"\"\n        Writes the underlying data into a newly created parquet file.\n\n        Args:\n            fname (str):\n                The name of the parquet file.\n                The ending \".parquet\" will be added automatically.\n\n            compression (str):\n                The compression format to use.\n                Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n        \"\"\"\n        _to_parquet(self, fname, compression)\n\n    # ----------------------------------------------------------------\n\n    def to_placeholder(self, name=None):\n        \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n        current [`DataFrame`][getml.DataFrame].\n\n        Args:\n            name (str, optional):\n                The name of the placeholder. If no\n                name is passed, then the name of the placeholder will\n                be identical to the name of the current data frame.\n\n        Returns:\n            [`Placeholder`][getml.data.Placeholder]:\n                A placeholder with the same name as this data frame.\n\n\n        \"\"\"\n        self.refresh()\n        return Placeholder(name=name or self.name, roles=self.roles)\n\n    # ----------------------------------------------------------------\n\n    def to_pyspark(self, spark, name=None):\n        \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyspark.sql.DataFrame`.\n\n        Args:\n            spark (pyspark.sql.SparkSession):\n                The pyspark session in which you want to\n                create the data frame.\n\n            name (str or None):\n                The name of the temporary view to be created on top\n                of the `pyspark.sql.DataFrame`,\n                with which it can be referred to\n                in Spark SQL (refer to\n                `pyspark.sql.DataFrame.createOrReplaceTempView`).\n                If none is passed, then the name of this\n                [`DataFrame`][getml.DataFrame] will be used.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Pyspark equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_pyspark(self, name, spark)\n\n    # ------------------------------------------------------------\n\n    def to_s3(self, bucket: str, key: str, region: str, sep=\",\", batch_size=50000):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file\n        located in an S3 bucket.\n        Note:\n            Note that S3 is not supported on Windows.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            key (str):\n                The key in the S3 bucket in which you want to\n                write the output. The ending \".csv\" and an optional\n                batch number will be added automatically.\n\n            region (str):\n                The region in which the bucket is located.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n\n        Example:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            your_df.to_s3(\n                bucket=\"your-bucket-name\",\n                key=\"filename-on-s3\",\n                region=\"us-east-2\",\n                sep=';'\n            )\n            ```\n\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be of type str\")\n\n        if not isinstance(key, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"DataFrame.to_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"bucket_\"] = bucket\n        cmd[\"key_\"] = key\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_float_names(self):\n        return [col.name for col in self._unused_float_columns]\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_names(self):\n        return self._unused_float_names + self._unused_string_names\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_string_names(self):\n        return [col.name for col in self._unused_string_columns]\n\n    # ------------------------------------------------------------\n\n    def unload(self):\n        \"\"\"\n        Unloads the data frame from memory.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        self._delete(mem_only=True)\n\n    # ------------------------------------------------------------\n\n    def where(\n        self,\n        index: Union[\n            numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n        ],\n    ) -&gt; View:\n        \"\"\"Extract a subset of rows.\n\n        Creates a new [`View`][getml.data.View] as a\n        subselection of the current instance.\n\n        Args:\n            index (int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n                Indicates the rows you want to select.\n\n        Example:\n            Generate example data:\n            ```python\n            data = dict(\n                fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n                price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n                join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n            fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n            roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n            fruits\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 0        | banana      | 2.4       |\n            | 1        | apple       | 3         |\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            | 3        | melon       | 3.4       |\n            | 3        | pineapple   | 3.4       |\n            ```\n            Apply where condition. This creates a new DataFrame called \"cherries\":\n\n            ```python\n            cherries = fruits.where(\n                fruits[\"fruit\"] == \"cherry\")\n\n            cherries\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            ```\n\n        \"\"\"\n        if isinstance(index, numbers.Integral):\n            index = index if int(index) &gt; 0 else len(self) + index\n            selector = arange(int(index), int(index) + 1)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, slice):\n            start, stop, _ = _make_default_slice(index, len(self))\n            selector = arange(start, stop, index.step)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n            return View(base=self, subselection=index)\n\n        raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n\n    # ------------------------------------------------------------\n\n    def with_column(\n        self, col, name, role=None, subroles=None, unit=\"\", time_formats=None\n    ):\n        \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n        Args:\n            col ([`columns`][getml.columns]):\n                The column to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from `getml.data.roles`.\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n\n        \"\"\"\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n        return View(\n            base=self,\n            added={\n                \"col_\": col,\n                \"name_\": name,\n                \"role_\": role,\n                \"subroles_\": subroles,\n                \"unit_\": unit,\n            },\n        )\n\n    # ------------------------------------------------------------\n\n    def with_name(self, name):\n        \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n        Args:\n            name (str):\n                The name of the new view.\n        \"\"\"\n        return View(base=self, name=name)\n\n    # ------------------------------------------------------------\n\n    def with_role(self, cols, role, time_formats=None):\n        \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n        The difference between [`with_role`][getml.DataFrame.with_role] and\n        [`set_role`][getml.DataFrame.set_role] is that\n        [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n        evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret time\n        format string: `annotating_roles_time_stamp`. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to\n                parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n        \"\"\"\n        return _with_role(self, cols, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    def with_subroles(self, cols, subroles, append=True):\n        \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n        The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n        [`set_subroles`][getml.DataFrame.set_subroles] is that\n        [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n        evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n        return _with_subroles(self, cols, subroles, append)\n\n    # ------------------------------------------------------------\n\n    def with_unit(self, cols, unit, comparison_only=False):\n        \"\"\"Returns a view that contains a new unit on one or more columns.\n\n        The difference between [`with_unit`][getml.DataFrame.with_unit] and\n        [`set_unit`][getml.DataFrame.set_unit] is that\n        [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n        evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n        is an in-place operation. From a memory perspective, in-place operations\n        like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n        Args:\n            cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n                The columns or the names thereof.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n\n                If True, this will also set the\n                [`compare`][getml.data.subroles.only.compare] subrole.  The feature\n                learning algorithms and the feature selectors will interpret this\n                accordingly.\n        \"\"\"\n        return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.colnames","title":"<code>colnames</code>  <code>property</code>","text":"<p>List of the names of all columns.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Alias for <code>colnames</code>.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.last_change","title":"<code>last_change: str</code>  <code>property</code>","text":"<p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.memory_usage","title":"<code>memory_usage</code>  <code>property</code>","text":"<p>Convenience wrapper that returns the memory usage in MB.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.roles","title":"<code>roles</code>  <code>property</code>","text":"<p>The roles of the columns included in this DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.rowid","title":"<code>rowid</code>  <code>property</code>","text":"<p>The rowids for this data frame.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>A tuple containing the number of rows and columns of the DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.add","title":"<code>add(col, name, role=None, subroles=None, unit='', time_formats=None)</code>","text":"<p>Adds a column to the current <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`column`][getml.column] or `numpy.ndarray`</code> <p>The column or numpy.ndarray to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>getml.data.roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def add(self, col, name, role=None, subroles=None, unit=\"\", time_formats=None):\n    \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        col ([`column`][getml.column] or `numpy.ndarray`):\n            The column or numpy.ndarray to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from `getml.data.roles`.\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n\n    if isinstance(col, np.ndarray):\n        self._add_numpy_array(col, name, role, subroles, unit)\n        return\n\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n\n    is_string = isinstance(col, (StringColumnView, StringColumn))\n\n    if is_string:\n        self._add_categorical_column(col, name, role, subroles, unit)\n    else:\n        self._add_column(col, name, role, subroles, unit)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.copy","title":"<code>copy(name)</code>","text":"<p>Creates a deep copy of the data frame under a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new data frame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: A handle to the deep copy.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def copy(self, name: str) -&gt; \"DataFrame\":\n    \"\"\"\n    Creates a deep copy of the data frame under a new name.\n\n    Args:\n        name (str):\n            The name of the new data frame.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            A handle to the deep copy.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.delete","title":"<code>delete()</code>","text":"<p>Permanently deletes the data frame. <code>delete</code> first unloads the data frame from memory and then deletes it from disk.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Permanently deletes the data frame. `delete` first unloads the data frame\n    from memory and then deletes it from disk.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    self._delete()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.drop","title":"<code>drop(cols)</code>","text":"<p>Returns a new <code>View</code> that has one or several columns removed.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def drop(self, cols):\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not _is_typed_list(names, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=names)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.freeze","title":"<code>freeze()</code>","text":"<p>Freezes the data frame.</p> <p>After you have frozen the data frame, the data frame is immutable and in-place operations are no longer possible. However, you can still create views. In other words, operations like <code>set_role</code> are no longer possible, but operations like <code>with_role</code> are.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def freeze(self):\n    \"\"\"Freezes the data frame.\n\n    After you have frozen the data frame, the data frame is immutable\n    and in-place operations are no longer possible. However, you can\n    still create views. In other words, operations like\n    [`set_role`][getml.DataFrame.set_role] are no longer possible,\n    but operations like [`with_role`][getml.DataFrame.with_role] are.\n    \"\"\"\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.freeze\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_arrow","title":"<code>from_arrow(table, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from an Arrow Table.</p> <p>This is one of the fastest way to get data into the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_arrow(cls, table, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from an Arrow Table.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        table (pyarrow.Table):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_arrow(table)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_arrow(table=table, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_csv","title":"<code>from_csv(fnames, name, num_lines_sniffed=1000, num_lines_read=0, quotechar='\"', sep=',', skip=0, colnames=None, roles=None, ignore=False, dry=False, verbose=True)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from CSV files.</p> <p>The getML engine will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>CSV file paths to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, when fnames are urls, the filenames are printed to stdout during the download.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Note <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the current working directory. You can import their data into the getML engine using. <pre><code>df_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"'\n    )\n\n# However, the CSV format lacks type safety. If you want to\n# build a reliable pipeline, it is a good idea\n# to hard-code the roles:\n\nroles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n    )\n\n# If you think that typing out all the roles by hand is too\n# cumbersome, you can use a dry run:\n\nroles = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True\n)\n</code></pre></p> <p>This will return the roles dictionary it would have used. You can now hard-code this.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    fnames,\n    name,\n    num_lines_sniffed=1000,\n    num_lines_read=0,\n    quotechar='\"',\n    sep=\",\",\n    skip=0,\n    colnames=None,\n    roles=None,\n    ignore=False,\n    dry=False,\n    verbose=True,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files.\n\n    The getML engine will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        fnames (List[str]):\n            CSV file paths to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional): The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        verbose (bool, optional):\n            If True, when fnames are urls, the filenames are\n            printed to stdout during the download.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Note:\n        It is assumed that the first line of each CSV file\n        contains a header with the column names.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the current working directory. You can\n        import their data into the getML engine using.\n        ```python\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"'\n            )\n\n        # However, the CSV format lacks type safety. If you want to\n        # build a reliable pipeline, it is a good idea\n        # to hard-code the roles:\n\n        roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n            )\n\n        # If you think that typing out all the roles by hand is too\n        # cumbersome, you can use a dry run:\n\n        roles = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            dry=True\n        )\n        ```\n\n        This will return the roles dictionary it would have used. You\n        can now hard-code this.\n\n    \"\"\"\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    fnames = _retrieve_urls(fnames, verbose=verbose)\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_csv(\n            fnames=fnames,\n            num_lines_sniffed=int(num_lines_sniffed),\n            quotechar=quotechar,\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_csv(\n        fnames=fnames,\n        append=False,\n        quotechar=quotechar,\n        sep=sep,\n        num_lines_read=num_lines_read,\n        skip=skip,\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_db","title":"<code>from_db(table_name, name=None, roles=None, ignore=False, dry=False, conn=None)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a table in a database.</p> <p>It will construct a data frame object in the engine, fill it with the data read from table <code>table_name</code> in the connected database (see <code>database</code>), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created. If not passed, then the table_name will be used.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Example <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    port=3306,\n    dbname=\"financial\",\n    user=\"guest\",\n    password=\"relational\"\n)\n\nloan = getml.DataFrame.from_db(\n    table_name='loan', name='data_frame_loan')\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_db(\n    cls, table_name, name=None, roles=None, ignore=False, dry=False, conn=None\n):\n    \"\"\"Create a DataFrame from a table in a database.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from table `table_name` in the connected\n    database (see [`database`][getml.database]), and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        table_name (str):\n            Name of the table to be read.\n\n        name (str):\n            Name of the data frame to be created. If not passed,\n            then the *table_name* will be used.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection, the engine\n            will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Example:\n        ```python\n        getml.database.connect_mysql(\n            host=\"db.relational-data.org\",\n            port=3306,\n            dbname=\"financial\",\n            user=\"guest\",\n            password=\"relational\"\n        )\n\n        loan = getml.DataFrame.from_db(\n            table_name='loan', name='data_frame_loan')\n        ```\n    \"\"\"\n\n    # -------------------------------------------\n\n    name = name or table_name\n\n    # -------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\n            \"'roles' must be a getml.data.Roles object, a dict or None.\"\n        )\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # -------------------------------------------\n\n    conn = conn or database.Connection()\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_db(table_name, conn)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_dict","title":"<code>from_dict(data, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a new DataFrame from a dict</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dict containing the data. The data should be in the following format: <pre><code>data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n</code></pre></p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, data: Dict[str, List[Any]], name: str, roles=None, ignore=False, dry=False\n):\n    \"\"\"Create a new DataFrame from a dict\n\n    Args:\n        data (dict):\n            The dict containing the data.\n            The data should be in the following format:\n            ```python\n            data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n            ```\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(data, dict):\n        raise TypeError(\"'data' must be dict.\")\n\n    return cls.from_arrow(\n        table=pa.Table.from_pydict(data),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_json","title":"<code>from_json(json_str, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a new DataFrame from a JSON string.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the JSON string, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The JSON string containing the data. The json_str should be in the following format: <pre><code>json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n</code></pre></p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>:</p> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_json(cls, json_str, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a new DataFrame from a JSON string.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the JSON string, and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        json_str (str):\n            The JSON string containing the data.\n            The json_str should be in the following format:\n            ```python\n            json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n            ```\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n    Returns:\n        [`DataFrame`][getml.data.DataFrame]: Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be str.\")\n\n    return cls.from_dict(\n        data=json.loads(json_str),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_pandas","title":"<code>from_pandas(pandas_df, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>pandas.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pandas.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>pandas_df</code> <code>DataFrame</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code> roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n          getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pandas(cls, pandas_df, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pandas.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        pandas_df (pandas.DataFrame):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n             roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                      getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    pandas_df_modified = _modify_pandas_columns(pandas_df)\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_pandas(pandas_df_modified)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pandas(pandas_df=pandas_df_modified, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_parquet","title":"<code>from_parquet(fname, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from parquet files.</p> <p>This is one of the fastest way to get data into the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The path of the parquet file to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_parquet(cls, fname, name, roles=None, ignore=False, dry=False):\n    \"\"\"Create a DataFrame from parquet files.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        fname (str):\n            The path of the parquet file to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_parquet(fname)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_parquet(fname=fname, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_pyspark","title":"<code>from_pyspark(spark_df, name, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>pyspark.sql.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pyspark.sql.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>DataFrame</code> <p>The table to be read.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre></p> <p>Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pyspark(cls, spark_df, name, roles=None, ignore=False, dry=False) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pyspark.sql.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        spark_df (pyspark.sql.DataFrame):\n            The table to be read.\n\n        name (str):\n            Name of the data frame to be created.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        head = spark_df.limit(2).toPandas()\n\n        sniffed_roles = _sniff_pandas(head)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pyspark(spark_df=spark_df, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_s3","title":"<code>from_s3(bucket, keys, region, name, num_lines_sniffed=1000, num_lines_read=0, sep=',', skip=0, colnames=None, roles=None, ignore=False, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from CSV files located in an S3 bucket.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>roles</code> <code>dict[str, List[str]] or [`Roles`][getml.data.Roles]</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <code>None</code> <code>ignore</code> <code>bool</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <code>False</code> <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Handler of the underlying data.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\ndata_frame_expd = data.DataFrame.from_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY DATA FRAME\",\n    sep=';'\n)\n</code></pre></p> <p>You can also set the access credential as environment variables before you launch the getML engine.</p> <p>Also refer to the documentation on <code>from_csv</code> for further information on overriding the CSV sniffer for greater type safety.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_s3(\n    cls,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    name: str,\n    num_lines_sniffed=1000,\n    num_lines_read=0,\n    sep=\",\",\n    skip=0,\n    colnames=None,\n    roles=None,\n    ignore=False,\n    dry=False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        name (str):\n            Name of the data frame to be created.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles (dict[str, List[str]] or [`Roles`][getml.data.Roles], optional):\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore (bool, optional):\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Handler of the underlying data.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        data_frame_expd = data.DataFrame.from_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY DATA FRAME\",\n            sep=';'\n        )\n        ```\n\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n        Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n        for further information on overriding the CSV sniffer for greater\n        type safety.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_s3(\n            bucket=bucket,\n            keys=keys,\n            region=region,\n            num_lines_sniffed=int(num_lines_sniffed),\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_s3(\n        bucket=bucket,\n        keys=keys,\n        region=region,\n        append=False,\n        sep=sep,\n        num_lines_read=int(num_lines_read),\n        skip=int(skip),\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.from_view","title":"<code>from_view(view, name, dry=False)</code>  <code>classmethod</code>","text":"<p>Create a DataFrame from a <code>View</code>.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the <code>View</code>, and return a corresponding <code>DataFrame</code> handle.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>[`View`][getml.data.View]</code> <p>The view from which we want to read the data.</p> required <code>name</code> <code>str</code> <p>Name of the data frame to be created.</p> required <code>dry</code> <code>bool</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_view(\n    cls,\n    view,\n    name,\n    dry=False,\n):\n    \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the [`View`][getml.data.View], and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        view ([`View`][getml.data.View]):\n            The view from which we want to read the data.\n\n        name (str):\n            Name of the data frame to be created.\n\n        dry (bool, optional):\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if dry:\n        return view.roles\n\n    data_frame = cls(name)\n\n    # ------------------------------------------------------------\n\n    return data_frame.read_view(view=view, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.load","title":"<code>load()</code>","text":"<p>Loads saved data from disk.</p> <p>The data frame object holding the same name as the current <code>DataFrame</code> instance will be loaded from disk into the getML engine and updates the current handler using <code>refresh</code>.</p> Example <p>First, we have to create and import data sets. <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\ngetml.data.list_data_frames()\n</code></pre></p> <p>In the output of <code>list_data_frames</code> we can find our underlying data frame object 'test' listed under the 'in_memory' key (it was created and imported by <code>make_numerical</code>). This means the getML engine does only hold it in memory (RAM) yet, and we still have to <code>save</code> it to disk in order to <code>load</code> it again or to prevent any loss of information between different sessions. <pre><code>d.save()\ngetml.data.list_data_frames()\nd2 = getml.DataFrame(name = 'test').load()\n</code></pre></p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Updated handle the underlying data frame in the getML engine.</p> Note <p>When invoking <code>load</code> all changes of the underlying data frame object that took place after the last call to the <code>save</code> method will be lost. Thus, this method  enables you to undo changes applied to the <code>DataFrame</code>. <pre><code>d, _ = getml.datasets.make_numerical()\nd.save()\n\n# Accidental change we want to undo\nd.rm('column_01')\n\nd.load()\n</code></pre> If <code>save</code> hasn't been called on the current instance yet, or it wasn't stored to disk in a previous session, <code>load</code> will throw an exception</p> <pre><code>File or directory '../projects/X/data/Y/' not found!\n</code></pre> <p>Alternatively, <code>load_data_frame</code> offers an easier way of creating <code>DataFrame</code> handlers to data in the getML engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def load(self) -&gt; \"DataFrame\":\n    \"\"\"Loads saved data from disk.\n\n    The data frame object holding the same name as the current\n    [`DataFrame`][getml.DataFrame] instance will be loaded from\n    disk into the getML engine and updates the current handler\n    using [`refresh`][getml.DataFrame.refresh].\n\n    Example:\n        First, we have to create and import data sets.\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        getml.data.list_data_frames()\n        ```\n\n        In the output of [`list_data_frames`][getml.data.list_data_frames] we\n        can find our underlying data frame object 'test' listed\n        under the 'in_memory' key (it was created and imported by\n        [`make_numerical`][getml.datasets.make_numerical]). This means the\n        getML engine does only hold it in memory (RAM) yet, and we\n        still have to [`save`][getml.DataFrame.save] it to\n        disk in order to [`load`][getml.DataFrame.load] it\n        again or to prevent any loss of information between\n        different sessions.\n        ```python\n        d.save()\n        getml.data.list_data_frames()\n        d2 = getml.DataFrame(name = 'test').load()\n        ```\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    Note:\n        When invoking [`load`][getml.DataFrame.load] all\n        changes of the underlying data frame object that took\n        place after the last call to the\n        [`save`][getml.DataFrame.save] method will be\n        lost. Thus, this method  enables you to undo changes\n        applied to the [`DataFrame`][getml.DataFrame].\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        d.save()\n\n        # Accidental change we want to undo\n        d.rm('column_01')\n\n        d.load()\n        ```\n        If [`save`][getml.DataFrame.save] hasn't been called\n        on the current instance yet, or it wasn't stored to disk in\n        a previous session, [`load`][getml.DataFrame.load]\n        will throw an exception\n\n            File or directory '../projects/X/data/Y/' not found!\n\n        Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n        offers an easier way of creating\n        [`DataFrame`][getml.DataFrame] handlers to data in the\n        getML engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.load\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.nbytes","title":"<code>nbytes()</code>","text":"<p>Size of the data stored in the underlying data frame in the getML engine.</p> <p>Returns:</p> Type Description <p>numpy.uint64: Size of the underlying object in bytes.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nbytes(self):\n    \"\"\"Size of the data stored in the underlying data frame in the getML\n    engine.\n\n    Returns:\n        numpy.uint64:\n            Size of the underlying object in bytes.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nbytes\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nbytes = comm.recv_string(sock)\n\n    return np.uint64(nbytes)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.ncols","title":"<code>ncols()</code>","text":"<p>Number of columns in the current instance.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def ncols(self):\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n        int:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.nrows","title":"<code>nrows()</code>","text":"<p>Number of rows in the current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nrows(self):\n    \"\"\"\n    Number of rows in the current instance.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nrows\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nrows = comm.recv_string(sock)\n\n    return int(nrows)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_arrow","title":"<code>read_arrow(table, append=False)</code>","text":"<p>Uploads a <code>pyarrow.Table</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Current instance.</p> Note <p>For columns containing <code>pandas.Timestamp</code> there can be small inconsistencies in the order of microseconds when sending the data to the getML engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_arrow(self, table, append=False):\n    \"\"\"Uploads a `pyarrow.Table`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `table`.\n\n    Args:\n        table (pyarrow.Table):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Current instance.\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        be small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_arrow\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    with comm.send_and_get_socket(cmd) as sock:\n        with sock.makefile(mode=\"wb\") as sink:\n            batches = table.to_batches()\n            with pa.ipc.new_stream(sink, table.schema) as writer:\n                for batch in batches:\n                    writer.write_batch(batch)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_csv","title":"<code>read_csv(fnames, append=False, quotechar='\"', sep=',', num_lines_read=0, skip=0, colnames=None, time_formats=None, verbose=True)</code>","text":"<p>Read CSV files.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>CSV file paths to be read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_csv(\n    self,\n    fnames,\n    append=False,\n    quotechar='\"',\n    sep=\",\",\n    num_lines_read=0,\n    skip=0,\n    colnames=None,\n    time_formats=None,\n    verbose=True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read CSV files.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        fnames (List[str]):\n            CSV file paths to be read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        verbose (bool, optional):\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_csv(...).\"\"\"\n        )\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\n            \"\"\"'fnames' must be a list containing at\n            least one path to a CSV file\"\"\"\n        )\n\n    fnames_ = _retrieve_urls(fnames, verbose)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fnames_\"] = fnames_\n\n    cmd[\"append_\"] = append\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"time_formats_\"] = time_formats\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_db","title":"<code>read_db(table_name, append=False, conn=None)</code>","text":"<p>Fill from Database.</p> <p>The DataFrame will be filled from a table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Table from which we want to retrieve the data.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of <code>table_name</code> be appended or replace the existing data?</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_db(self, table_name: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n    \"\"\"\n    Fill from Database.\n\n    The DataFrame will be filled from a table in the database.\n\n    Args:\n        table_name (str):\n            Table from which we want to retrieve the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `table_name` be appended or replace the existing data?\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_db\"\n    cmd[\"name_\"] = self.name\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_json","title":"<code>read_json(json_str, append=False, time_formats=None)</code>","text":"<p>Fill from JSON</p> <p>Fills the data frame with data from a JSON string.</p> <p>Args:</p> <pre><code>json_str (str):\n    The JSON string containing the data.\n\nappend (bool, optional):\n    If a data frame object holding the same ``name`` is\n    already present in the getML, should the content of\n    `json_str` be appended or replace the existing data?\n\ntime_formats (List[str], optional):\n    The list of formats tried when parsing time stamps.\n    The formats are allowed to contain the following\n    special characters:\n\n    * %w - abbreviated weekday (Mon, Tue, ...)\n    * %W - full weekday (Monday, Tuesday, ...)\n    * %b - abbreviated month (Jan, Feb, ...)\n    * %B - full month (January, February, ...)\n    * %d - zero-padded day of month (01 .. 31)\n    * %e - day of month (1 .. 31)\n    * %f - space-padded day of month ( 1 .. 31)\n    * %m - zero-padded month (01 .. 12)\n    * %n - month (1 .. 12)\n    * %o - space-padded month ( 1 .. 12)\n    * %y - year without century (70)\n    * %Y - year with century (1970)\n    * %H - hour (00 .. 23)\n    * %h - hour (00 .. 12)\n    * %a - am/pm\n    * %A - AM/PM\n    * %M - minute (00 .. 59)\n    * %S - second (00 .. 59)\n    * %s - seconds and microseconds (equivalent to %S.%F)\n    * %i - millisecond (000 .. 999)\n    * %c - centisecond (0 .. 9)\n    * %F - fractional seconds/microseconds (000000 - 999999)\n    * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n    * %Z - time zone differential in RFC format (GMT or +NNNN)\n    * %% - percent sign\n</code></pre> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Note <p>This does not support NaN values. If you want support for NaN, use <code>from_json</code> instead.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_json(self, json_str, append=False, time_formats=None):\n    \"\"\"Fill from JSON\n\n    Fills the data frame with data from a JSON string.\n\n    Args:\n\n        json_str (str):\n            The JSON string containing the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `json_str` be appended or replace the existing data?\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    Note:\n        This does not support NaN values. If you want support for NaN,\n        use [`from_json`][getml.DataFrame.from_json] instead.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_json(...).\"\"\"\n        )\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\n            \"\"\"'time_formats' must be a list of strings\n            containing at least one time format\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.from_json\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n    cmd[\"time_formats_\"] = time_formats\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, json_str)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_pandas","title":"<code>read_pandas(pandas_df, append=False)</code>","text":"<p>Uploads a <code>pandas.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pandas_df</code> <code>DataFrame</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> Note <p>For columns containing <code>pandas.Timestamp</code> there can occur small inconsistencies in the order of microseconds when sending the data to the getML engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pandas.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        pandas_df (pandas.DataFrame):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        occur small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    table = pa.Table.from_pandas(_modify_pandas_columns(pandas_df))\n\n    return self.read_arrow(table, append=append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_parquet","title":"<code>read_parquet(fname, append=False, verbose=True)</code>","text":"<p>Read a parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The filepath of the parquet file to be read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_parquet(\n    self,\n    fname: str,\n    append: bool = False,\n    verbose: bool = True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read a parquet file.\n\n    Args:\n        fname (str):\n            The filepath of the parquet file to be read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n    \"\"\"\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than\n            zero columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_parquet(...).\"\"\"\n        )\n\n    fname_ = _retrieve_urls([fname], verbose)[0]\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_parquet\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_pyspark","title":"<code>read_pyspark(spark_df, append=False)</code>","text":"<p>Uploads a <code>pyspark.sql.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark_df</code> <code>DataFrame</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pyspark(self, spark_df, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        spark_df (pyspark.sql.DataFrame):\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n    \"\"\"\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    temp_dir = _retrieve_temp_dir()\n    os.makedirs(temp_dir, exist_ok=True)\n    path = os.path.join(temp_dir, self.name)\n    spark_df.write.mode(\"overwrite\").parquet(path)\n\n    filepaths = [\n        os.path.join(path, filepath)\n        for filepath in os.listdir(path)\n        if filepath[-8:] == \".parquet\"\n    ]\n\n    for i, filepath in enumerate(filepaths):\n        self.read_parquet(filepath, append or i &gt; 0)\n\n    shutil.rmtree(path)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_query","title":"<code>read_query(query, append=False, conn=None)</code>","text":"<p>Fill from query</p> <p>Fills the data frame with data from a table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query used to retrieve the data.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <code>False</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_query(self, query: str, append: bool = False, conn=None) -&gt; \"DataFrame\":\n    \"\"\"Fill from query\n\n    Fills the data frame with data from a table in the database.\n\n    Args:\n        query (str):\n            The query used to retrieve the data.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n    \"\"\"\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_query\"\n    cmd[\"name_\"] = self.name\n    cmd[\"query_\"] = query\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_s3","title":"<code>read_s3(bucket, keys, region, append=False, sep=',', num_lines_read=0, skip=0, colnames=None, time_formats=None)</code>","text":"<p>Read CSV files from an S3 bucket.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <p>Returns:</p> Type Description <p><code>DataFrame</code>: Handler of the underlying data.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_s3(\n    self,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Read CSV files from an S3 bucket.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(keys, list):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_s3(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"skip_\"] = skip\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.read_view","title":"<code>read_view(view, append=False)</code>","text":"<p>Read the data from a <code>View</code>.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>[`View`][getml.data.View]</code> <p>The view to read.</p> required <code>append</code> <code>bool</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_view(\n    self,\n    view: View,\n    append: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Read the data from a [`View`][getml.data.View].\n\n    Args:\n        view ([`View`][getml.data.View]):\n            The view to read.\n\n        append (bool, optional):\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    view.check()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_view\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = view._getml_deserialize()\n\n    cmd[\"append_\"] = append\n\n    comm.send(cmd)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.refresh","title":"<code>refresh()</code>","text":"<p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> <p>This method can be used to avoid encoding conflicts. Note that <code>load</code> as well as several other methods automatically call <code>refresh</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>:</p> <p>Updated handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def refresh(self) -&gt; \"DataFrame\":\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    This method can be used to avoid encoding conflicts. Note that\n    [`load`][getml.DataFrame.load] as well as several other\n    methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.refresh\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    roles = json.loads(msg)\n\n    self.__init__(name=self.name, roles=roles)  # type: ignore\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.remove_subroles","title":"<code>remove_subroles(cols)</code>","text":"<p>Removes all <code>subroles</code> from one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_subroles(self, cols):\n    \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n    Args:\n        columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_subroles(name, subroles=[], append=False)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.remove_unit","title":"<code>remove_unit(cols)</code>","text":"<p>Removes the unit from one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_unit(self, cols):\n    \"\"\"Removes the unit from one or more columns.\n\n    Args:\n        columns (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_unit(name, \"\")\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.save","title":"<code>save()</code>","text":"<p>Writes the underlying data in the getML engine to disk.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p><code>DataFrame</code>: The current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def save(self) -&gt; \"DataFrame\":\n    \"\"\"Writes the underlying data in the getML engine to disk.\n\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            The current instance.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.save\"\n    cmd[\"name_\"] = self.name\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.set_role","title":"<code>set_role(cols, role, time_formats=None)</code>","text":"<p>Assigns a new role to one or more columns.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret Time Stamps. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names of the columns.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Example <p><pre><code>data_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\ndf = getml.DataFrame.from_dict(data_df, \"animal_elections\")\ndf.set_role(['animal'], getml.data.roles.categorical)\ndf.set_role(['votes'], getml.data.roles.numerical)\ndf.set_role(\n    ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\ndf\n</code></pre> <pre><code>| date                        | animal      | votes     |\n| time stamp                  | categorical | numerical |\n---------------------------------------------------------\n| 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n| 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n| 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_role(self, cols, role, time_formats=None):\n    \"\"\"Assigns a new role to one or more columns.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names of the columns.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    Example:\n        ```python\n        data_df = dict(\n            animal=[\"hawk\", \"parrot\", \"goose\"],\n            votes=[12341, 5127, 65311],\n            date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n        df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n        df.set_role(['animal'], getml.data.roles.categorical)\n        df.set_role(['votes'], getml.data.roles.numerical)\n        df.set_role(\n            ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n        df\n        ```\n        ```\n        | date                        | animal      | votes     |\n        | time stamp                  | categorical | numerical |\n        ---------------------------------------------------------\n        | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n        | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n        | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n        ```\n    \"\"\"\n    # ------------------------------------------------------------\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # ------------------------------------------------------------\n\n    names = _handle_cols(cols)\n\n    if not isinstance(role, str):\n        raise TypeError(\"'role' must be str.\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    # ------------------------------------------------------------\n\n    for nname in names:\n        if nname not in self.colnames:\n            raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n    if role not in self._possible_keys:\n        raise ValueError(\n            \"'role' must be one of the following values: \"\n            + str(self._possible_keys)\n        )\n\n    # ------------------------------------------------------------\n\n    for name in names:\n        if self[name].role != role:\n            self._set_role(name, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.set_subroles","title":"<code>set_subroles(cols, subroles, append=True)</code>","text":"<p>Assigns one or several new <code>subroles</code> to one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned. Must be from <code>subroles</code>.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_subroles(self, cols, subroles, append=True):\n    \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n            Must be from [`subroles`][getml.data.subroles].\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if isinstance(subroles, str):\n        subroles = [subroles]\n\n    if not _is_non_empty_typed_list(subroles, str):\n        raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be a bool.\")\n\n    for name in names:\n        self._set_subroles(name, subroles, append)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.set_unit","title":"<code>set_unit(cols, unit, comparison_only=False)</code>","text":"<p>Assigns a new unit to one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_unit(self, cols, unit, comparison_only=False):\n    \"\"\"Assigns a new unit to one or more columns.\n\n    Args:\n        cols (str, FloatColumn, StringColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not isinstance(unit, str):\n        raise TypeError(\"Parameter 'unit' must be a str.\")\n\n    if comparison_only:\n        unit += COMPARISON_ONLY\n\n    for name in names:\n        self._set_unit(name, unit)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Creates a <code>pyarrow.Table</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <p>pyarrow.Table: Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_arrow(self):\n    \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n        pyarrow.Table:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_csv","title":"<code>to_csv(fname, quotechar='\"', sep=',', batch_size=0)</code>","text":"<p>Writes the underlying data into a newly created CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>0</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname (str):\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_db","title":"<code>to_db(table_name, conn=None)</code>","text":"<p>Writes the underlying data into a newly created table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_db(self, table_name, conn=None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name (str):\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or database.Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    cmd = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_db\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_html","title":"<code>to_html(max_rows=10)</code>","text":"<p>Represents the data frame in HTML format, optimized for an iPython notebook.</p> <p>Parameters:</p> Name Type Description Default <code>max_rows</code> <code>int</code> <p>The maximum number of rows to be displayed.</p> <code>10</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_html(self, max_rows=10):\n    \"\"\"\n    Represents the data frame in HTML format, optimized for an\n    iPython notebook.\n\n    Args:\n        max_rows (int):\n            The maximum number of rows to be displayed.\n    \"\"\"\n\n    if not _exists_in_memory(self.name):\n        return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n    formatted = self._format()\n    formatted.max_rows = max_rows\n\n    footer = self._collect_footer_data()\n\n    return formatted._render_html(footer=footer)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_json","title":"<code>to_json()</code>","text":"<p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Creates a <code>pandas.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs <code>pandas.DataFrame</code>.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: Pandas equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pandas(self):\n    \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    `pandas.DataFrame`.\n\n    Returns:\n        pandas.DataFrame:\n            Pandas equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_parquet","title":"<code>to_parquet(fname, compression='snappy')</code>","text":"<p>Writes the underlying data into a newly created parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> required <code>compression</code> <code>str</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <code>'snappy'</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_parquet(self, fname, compression=\"snappy\"):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname (str):\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression (str):\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_placeholder","title":"<code>to_placeholder(name=None)</code>","text":"<p>Generates a <code>Placeholder</code> from the current <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current data frame.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>Placeholder</code>: A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_placeholder(self, name=None):\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        name (str, optional):\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current data frame.\n\n    Returns:\n        [`Placeholder`][getml.data.Placeholder]:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_pyspark","title":"<code>to_pyspark(spark, name=None)</code>","text":"<p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The pyspark session in which you want to create the data frame.</p> required <code>name</code> <code>str or None</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pyspark(self, spark, name=None):\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark (pyspark.sql.SparkSession):\n            The pyspark session in which you want to\n            create the data frame.\n\n        name (str or None):\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.to_s3","title":"<code>to_s3(bucket, key, region, sep=',', batch_size=50000)</code>","text":"<p>Writes the underlying data into a newly created CSV file located in an S3 bucket. Note:     Note that S3 is not supported on Windows.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>key</code> <code>str</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>50000</code> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_df.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_s3(self, bucket: str, key: str, region: str, sep=\",\", batch_size=50000):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        key (str):\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region (str):\n            The region in which the bucket is located.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_df.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.unload","title":"<code>unload()</code>","text":"<p>Unloads the data frame from memory.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads the data frame from memory.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    self._delete(mem_only=True)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.where","title":"<code>where(index)</code>","text":"<p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Indicates the rows you want to select.</p> required Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def where(\n    self,\n    index: Union[\n        numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n    ],\n) -&gt; View:\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index (int, slice, [`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Indicates the rows you want to select.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(int(index), int(index) + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, _ = _make_default_slice(index, len(self))\n        selector = arange(start, stop, index.step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.with_column","title":"<code>with_column(col, name, role=None, subroles=None, unit='', time_formats=None)</code>","text":"<p>Returns a new <code>View</code> that contains an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`columns`][getml.columns]</code> <p>The column to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>getml.data.roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_column(\n    self, col, name, role=None, subroles=None, unit=\"\", time_formats=None\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col ([`columns`][getml.columns]):\n            The column to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from `getml.data.roles`.\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.with_name","title":"<code>with_name(name)</code>","text":"<p>Returns a new <code>View</code> with a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new view.</p> required Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_name(self, name):\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.with_role","title":"<code>with_role(cols, role, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> with modified roles.</p> <p>The difference between <code>with_role</code> and <code>set_role</code> is that <code>with_role</code> returns a view that is lazily evaluated when needed whereas <code>set_role</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_role</code> are preferable.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_role(self, cols, role, time_formats=None):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    The difference between [`with_role`][getml.DataFrame.with_role] and\n    [`set_role`][getml.DataFrame.set_role] is that\n    [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n    evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to\n            parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, cols, role, time_formats)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.with_subroles","title":"<code>with_subroles(cols, subroles, append=True)</code>","text":"<p>Returns a new view with one or several new subroles on one or more columns.</p> <p>The difference between <code>with_subroles</code> and <code>set_subroles</code> is that <code>with_subroles</code> returns a view that is lazily evaluated when needed whereas <code>set_subroles</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_subroles</code> are preferable.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_subroles(self, cols, subroles, append=True):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n    [`set_subroles`][getml.DataFrame.set_subroles] is that\n    [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n    evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, cols, subroles, append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.data_frame.DataFrame.with_unit","title":"<code>with_unit(cols, unit, comparison_only=False)</code>","text":"<p>Returns a view that contains a new unit on one or more columns.</p> <p>The difference between <code>with_unit</code> and <code>set_unit</code> is that <code>with_unit</code> returns a view that is lazily evaluated when needed whereas <code>set_unit</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_unit</code> are preferable.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]</code> <p>The columns or the names thereof.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will also set the <code>compare</code> subrole.  The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <code>False</code> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_unit(self, cols, unit, comparison_only=False):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    The difference between [`with_unit`][getml.DataFrame.with_unit] and\n    [`set_unit`][getml.DataFrame.set_unit] is that\n    [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n    evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n    Args:\n        cols (str, FloatColumn, StingColumn, or List[str, FloatColumn, StringColumn]):\n            The columns or the names thereof.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will also set the\n            [`compare`][getml.data.subroles.only.compare] subrole.  The feature\n            learning algorithms and the feature selectors will interpret this\n            accordingly.\n    \"\"\"\n    return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/data_model/","title":"Data model","text":"<p>A container for the placeholders.</p>"},{"location":"reference/data/data_model/#getml.data.data_model.DataModel","title":"<code>DataModel</code>","text":"<p>Abstract representation of the relationship between tables.</p> <p>You might also want to refer to <code>Placeholder</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder representing the population table, which defines the statistical population and contains the targets.</p> required Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n    )\n</code></pre></p> <p>Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre>     If you want to check out a real-world example where this     is necessary, refer to the     CORA notebook.</p> Source code in <code>getml/data/data_model.py</code> <pre><code>class DataModel:\n    \"\"\"\nAbstract representation of the relationship between tables.\n\nYou might also want to refer to [`Placeholder`][getml.data.Placeholder].\n\nArgs:\n    population ([`Placeholder`][getml.data.Placeholder]):\n        The placeholder representing the population table,\n        which defines the\n        [statistical population](https://en.wikipedia.org/wiki/Statistical_population)\n        and contains the targets.\n\nExample:\n    This example will construct a data model in which the\n    'population_table' depends on the 'peripheral_table' via\n    the 'join_key' column. In addition, only those rows in\n    'peripheral_table' for which 'time_stamp' is smaller or\n    equal to the 'time_stamp' in 'population_table' are considered:\n    ```python\n    dm = getml.data.DataModel(\n        population_table.to_placeholder(\"POPULATION\")\n    )\n\n    dm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\"\n    )\n    ```\n    If you want to add more than one peripheral table, you can\n    use [`to_placeholder`][getml.data.to_placeholder]:\n    ```python\n    dm = getml.data.DataModel(\n        population_table.to_placeholder(\"POPULATION\")\n    )\n\n    dm.add(\n        getml.data.to_placeholder(\n            PERIPHERAL1=peripheral_table_1,\n            PERIPHERAL2=peripheral_table_2,\n        )\n    )\n    ```\n    If the relationship between two tables is many-to-one or one-to-one\n    you should clearly say so:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        relationship=getml.data.relationship.many_to_one,\n    )\n    ```\n    Please also refer to [`relationship`][getml.data.relationship].\n\n    If the join keys or time stamps are named differently in the two\n    different tables, use a tuple:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=(\"join_key\", \"other_join_key\"),\n        time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n    )\n    ```\n    You can join over more than one join key:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n        time_stamps=\"time_stamp\",\n    )\n    ```\n    You can also limit the scope of your joins using *memory*. This\n    can significantly speed up training time. For instance, if you\n    only want to consider data from the last seven days, you could\n    do something like this:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        memory=getml.data.time.days(7),\n    )\n    ```\n    In some use cases, particularly those involving time series, it\n    might be a good idea to use targets from the past. You can activate\n    this using *lagged_targets*. But if you do that, you must\n    also define a prediction *horizon*. For instance, if you want to\n    predict data for the next hour, using data from the last seven days,\n    you could do this:\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        lagged_targets=True,\n        horizon=getml.data.time.hours(1),\n        memory=getml.data.time.days(7),\n    )\n    ```\n    Please also refer to [`time`][getml.data.time].\n\n    If the join involves many matches, it might be a good idea to set the\n    relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n    This forces the pipeline to always use a propositionalization\n    algorithm for this join, which can significantly speed things up.\n    ```python\n    dm.POPULATION.join(\n        dm.PERIPHERAL,\n        on=\"join_key\",\n        time_stamps=\"time_stamp\",\n        relationship=getml.data.relationship.propositionalization,\n        )\n    ```\n\n    Please also refer to [`relationship`][getml.data.relationship].\n\n    In some cases, it is necessary to have more than one placeholder\n    on the same table. This is necessary to create more complicated\n    data models. In this case, you can do something like this:\n    ```python\n    dm.add(\n        getml.data.to_placeholder(\n            PERIPHERAL=[peripheral_table]*2,\n        )\n    )\n\n    # We can now access our two placeholders like this:\n    placeholder1 = dm.PERIPHERAL[0]\n    placeholder2 = dm.PERIPHERAL[1]\n    ```\n        If you want to check out a real-world example where this\n        is necessary, refer to the\n        [CORA notebook](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/cora.ipynb).\n    \"\"\"\n\n    def __init__(self, population):\n        if isinstance(population, str):\n            population = Placeholder(population)\n\n        if not isinstance(population, Placeholder):\n            raise TypeError(\n                \"'population' must be a getml.data.Placeholder or a str, got \"\n                + type(population).__name__\n                + \".\"\n            )\n\n        self.population = population\n\n        self.peripheral = {}\n\n    def _add(self, placeholder):\n        if placeholder.name in self.peripheral:\n            try:\n                self.peripheral[placeholder.name].append(placeholder)\n            except AttributeError:\n                self.peripheral[placeholder.name] = [\n                    self.peripheral[placeholder.name],\n                    placeholder,\n                ]\n        else:\n            self.peripheral.update({placeholder.name: placeholder})\n\n    def __dir__(self):\n        attrs = dir(type(self)) + list(vars(self))\n        attrs.extend(self.names)\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        population = vars(self)[\"population\"]\n        peripheral = vars(self)[\"peripheral\"]\n\n        phs = {\n            \"population\": population,\n            population.name: population,\n            **peripheral,\n        }\n\n        return phs[key]\n\n    def _getml_deserialize(self):\n        def deserialize(elem):\n            return (\n                [e._getml_deserialize() for e in elem]\n                if isinstance(elem, list)\n                else elem._getml_deserialize()\n            )\n\n        cmd = self.population._getml_deserialize()\n        cmd[\"peripheral_\"] = {k: deserialize(v) for (k, v) in self.peripheral.items()}\n        return cmd\n\n    def __iter__(self):\n        yield from [self.population.name] + [\"population\"] + list(self.peripheral)\n\n    def __repr__(self):\n        return \"\\n\\n\".join(repr(ph) for ph in self.population.to_list())\n\n    def _make_diagram(self):\n        return _Diagram(self.population).to_html()\n\n    def _make_staging(self):\n        headers = [[\"data frames\", \"staging table\"]]\n        rows = _make_staging_overview(self.population)\n        staging_table = _Formatter(headers=headers, rows=rows)._render_html()\n        return staging_table\n\n    def _repr_html_(self):\n        output = cleandoc(\n            f\"\"\"\n            &lt;div style='margin-top: 15px; margin-bottom: 5px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;diagram&lt;/div&gt;\n            {self._make_diagram()}\n            &lt;/div&gt;\n\n            &lt;div style='margin-top: 15px;'&gt;\n            &lt;div style='margin-bottom: 10px; font-size: 1rem;'&gt;staging&lt;/div&gt;\n            {self._make_staging()}\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return output\n\n    def add(self, *placeholders):\n        \"\"\"\n        Adds peripheral placeholders to the data model.\n\n        Args:\n            placeholders ([`Placeholder`][getml.data.Placeholder]:\n                The placeholder or placeholders you would like to add.\n        \"\"\"\n\n        def to_list(elem):\n            return elem if isinstance(elem, list) else [elem]\n\n        # We want to be 100% sure that all handles are unique,\n        # so we need deepcopy.\n        placeholders_dc = [\n            deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n        ]\n\n        if not _is_typed_list(placeholders_dc, Placeholder):\n            raise TypeError(\n                \"'placeholders' must consist of getml.data.Placeholders \"\n                + \"or lists thereof.\"\n            )\n\n        for placeholder in placeholders_dc:\n            self._add(placeholder)\n\n    @property\n    def names(self):\n        \"\"\"\n        A list of the names of all tables contained in the DataModel.\n        \"\"\"\n        return [name for name in self]\n</code></pre>"},{"location":"reference/data/data_model/#getml.data.data_model.DataModel.names","title":"<code>names</code>  <code>property</code>","text":"<p>A list of the names of all tables contained in the DataModel.</p>"},{"location":"reference/data/data_model/#getml.data.data_model.DataModel.add","title":"<code>add(*placeholders)</code>","text":"<p>Adds peripheral placeholders to the data model.</p> <p>Parameters:</p> Name Type Description Default <code>placeholders</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder or placeholders you would like to add.</p> <code>()</code> Source code in <code>getml/data/data_model.py</code> <pre><code>def add(self, *placeholders):\n    \"\"\"\n    Adds peripheral placeholders to the data model.\n\n    Args:\n        placeholders ([`Placeholder`][getml.data.Placeholder]:\n            The placeholder or placeholders you would like to add.\n    \"\"\"\n\n    def to_list(elem):\n        return elem if isinstance(elem, list) else [elem]\n\n    # We want to be 100% sure that all handles are unique,\n    # so we need deepcopy.\n    placeholders_dc = [\n        deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n    ]\n\n    if not _is_typed_list(placeholders_dc, Placeholder):\n        raise TypeError(\n            \"'placeholders' must consist of getml.data.Placeholders \"\n            + \"or lists thereof.\"\n        )\n\n    for placeholder in placeholders_dc:\n        self._add(placeholder)\n</code></pre>"},{"location":"reference/data/diagram/","title":"Diagram","text":"<p>Collection of functions for visualizing the data model that are not intended to be used by the end-user.</p>"},{"location":"reference/data/helpers/","title":"Helpers","text":"<p>Collection of helper functions that are not intended to be used by the end-user.</p>"},{"location":"reference/data/helpers/#getml.data.helpers.list_data_frames","title":"<code>list_data_frames()</code>","text":"<p>Lists all available data frames of the project.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, List[str]]</code> <p>Dict containing lists of strings representing the names of the data frames objects</p> <ul> <li>'in_memory'     held in memory (RAM).</li> <li>'on_disk'     stored on disk.</li> </ul> Example <pre><code>d, _ = getml.datasets.make_numerical()\ngetml.data.list_data_frames()\nd.save()\ngetml.data.list_data_frames()\n</code></pre> Source code in <code>getml/data/helpers.py</code> <pre><code>def list_data_frames() -&gt; Dict[str, List[str]]:\n    \"\"\"Lists all available data frames of the project.\n\n    Returns:\n        dict:\n            Dict containing lists of strings representing the names of\n            the data frames objects\n\n            - 'in_memory'\n                held in memory (RAM).\n            - 'on_disk'\n                stored on disk.\n\n    Example:\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        getml.data.list_data_frames()\n        d.save()\n        getml.data.list_data_frames()\n        ```\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_data_frames\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)\n</code></pre>"},{"location":"reference/data/helpers2/","title":"Helpers2","text":"<p>Helper functions that depend on the DataFrame class.</p>"},{"location":"reference/data/helpers2/#getml.data.helpers2.delete","title":"<code>delete(name)</code>","text":"<p>If a data frame named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def delete(name):\n    \"\"\"\n    If a data frame named 'name' exists, it is deleted.\n\n    Args:\n        name (str):\n            Name of the data frame.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/data/helpers2/#getml.data.helpers2.exists","title":"<code>exists(name)</code>","text":"<p>Returns true if a data frame named 'name' exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def exists(name):\n    \"\"\"\n    Returns true if a data frame named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the data frame.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_df = list_data_frames()\n\n    return name in (all_df[\"in_memory\"] + all_df[\"on_disk\"])\n</code></pre>"},{"location":"reference/data/helpers2/#getml.data.helpers2.load_data_frame","title":"<code>load_data_frame(name)</code>","text":"<p>Retrieves a <code>DataFrame</code> handler of data in the getML engine.</p> <p>A data frame object can be loaded regardless if it is held in memory or not. It only has to be present in the current project and thus listed in the output of <code>list_data_frames</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the data frame.</p> required <p>Examples:</p> <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\nd2 = getml.data.load_data_frame('test')\n</code></pre> <p>Returns:     <code>DataFrame</code>:         Handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def load_data_frame(name):\n    \"\"\"Retrieves a [`DataFrame`][getml.DataFrame] handler of data in the\n    getML engine.\n\n    A data frame object can be loaded regardless if it is held in\n    memory or not. It only has to be present in the current project\n    and thus listed in the output of\n    [`list_data_frames`][getml.data.list_data_frames].\n\n    Args:\n        name (str):\n            Name of the data frame.\n\n    Examples:\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        d2 = getml.data.load_data_frame('test')\n        ```\n    Returns:\n        [`DataFrame`][getml.DataFrame]:\n            Handle the underlying data frame in the getML engine.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    data_frames_available = list_data_frames()\n\n    if name in data_frames_available[\"in_memory\"]:\n        return DataFrame(name).refresh()\n\n    if name in data_frames_available[\"on_disk\"]:\n        return DataFrame(name).load()\n\n    raise ValueError(\n        \"No data frame holding the name '\" + name + \"' present on the getML engine.\"\n    )\n</code></pre>"},{"location":"reference/data/helpers2/#getml.data.helpers2.make_target_columns","title":"<code>make_target_columns(base, colname)</code>","text":"<p>Returns a view containing binary target columns.</p> <p>getML expects binary target columns for classification problems. This helper function allows you to split up a column into such binary target columns.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The original view or data frame. <code>base</code> will remain unaffected by this function, instead you will get a view with the appropriate changes.</p> required <code>colname</code> <code>str</code> <p>The column you would like to split. A column named <code>colname</code> should appear on <code>base</code>.</p> required Source code in <code>getml/data/helpers2.py</code> <pre><code>def make_target_columns(base, colname):\n    \"\"\"\n    Returns a view containing binary target columns.\n\n    getML expects binary target columns for classification problems. This\n    helper function allows you to split up a column into such binary\n    target columns.\n\n    Args:\n        base ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The original view or data frame. `base` will remain unaffected\n            by this function, instead you will get a view with the appropriate\n            changes.\n\n        colname (str): The column you would like to split. A column named\n            `colname` should appear on `base`.\n    \"\"\"\n    if not isinstance(\n        base[colname], (FloatColumn, FloatColumnView, StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'\"\n            + colname\n            + \"' must be a FloatColumn, a FloatColumnView, \"\n            + \"a StringColumn or a StringColumnView.\"\n        )\n\n    unique_values = base[colname].unique()\n\n    if len(unique_values) &gt; 10:\n        logger.warning(\n            \"You are splitting the column into more than 10 target \"\n            + \"columns. This might take a long time to fit.\"\n        )\n\n    view = base\n\n    for label in unique_values:\n        col = (base[colname] == label).as_num()\n        name = colname + \"=\" + label\n        view = view.with_column(col=col, name=name, role=target)\n\n    return view.drop(colname)\n</code></pre>"},{"location":"reference/data/helpers2/#getml.data.helpers2.to_placeholder","title":"<code>to_placeholder(*args, **kwargs)</code>","text":"<p>Factory function for extracting placeholders from a <code>DataFrame</code> or <code>View</code>.</p> Example <p>Suppose we wanted to create a <code>DataModel</code>:</p> <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(meta.to_placeholder(\"meta\"))\ndm.add(order.to_placeholder(\"order\"))\ndm.add(trans.to_placeholder(\"trans\"))\n</code></pre> <p>But this is a bit repetitive. So instead, we can do the following: <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(getml.data.to_placeholder(\n    meta=meta, order=order, trans=trans))\n</code></pre></p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def to_placeholder(*args, **kwargs):\n    \"\"\"\n    Factory function for extracting placeholders from a\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n\n    Example:\n        Suppose we wanted to create a [`DataModel`][getml.data.DataModel]:\n\n\n\n            dm = getml.data.DataModel(\n                population_train.to_placeholder(\"population\")\n            )\n\n            # Add placeholders for the peripheral tables.\n            dm.add(meta.to_placeholder(\"meta\"))\n            dm.add(order.to_placeholder(\"order\"))\n            dm.add(trans.to_placeholder(\"trans\"))\n\n        But this is a bit repetitive. So instead, we can do\n        the following:\n        ```python\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        # Add placeholders for the peripheral tables.\n        dm.add(getml.data.to_placeholder(\n            meta=meta, order=order, trans=trans))\n        ```\n    \"\"\"\n\n    def to_ph_list(list_or_elem, key=None):\n        as_list = list_or_elem if isinstance(list_or_elem, list) else [list_or_elem]\n        return [elem.to_placeholder(key) for elem in as_list]\n\n    return [elem for item in args for elem in to_ph_list(item)] + [\n        elem for (k, v) in kwargs.items() for elem in to_ph_list(v, k)\n    ]\n</code></pre>"},{"location":"reference/data/load_container/","title":"Load container","text":"<p>Loads a container.</p>"},{"location":"reference/data/load_container/#getml.data.load_container.load_container","title":"<code>load_container(container_id)</code>","text":"<p>Loads a container and all associated data frames from disk.</p> <p>Parameters:</p> Name Type Description Default <code>container_id</code> <code>str</code> <p>The id of the container you would like to load.</p> required Source code in <code>getml/data/load_container.py</code> <pre><code>def load_container(container_id):\n    \"\"\"\n    Loads a container and all associated data frames from disk.\n\n    Args:\n        container_id (str):\n            The id of the container you would like to load.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataContainer.load\"\n    cmd[\"name_\"] = container_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    cmd = json.loads(json_str)\n\n    population = _load_view(cmd[\"population_\"]) if \"population_\" in cmd else None\n\n    peripheral = {k: _load_view(v) for (k, v) in cmd[\"peripheral_\"].items()}\n\n    subsets = {k: _load_view(v) for (k, v) in cmd[\"subsets_\"].items()}\n\n    split = _parse(cmd[\"split_\"]) if \"split_\" in cmd else None\n\n    deep_copy = cmd[\"deep_copy_\"]\n    frozen_time = cmd[\"frozen_time_\"] if \"frozen_time_\" in cmd else None\n    last_change = cmd[\"last_change_\"]\n\n    container = Container(\n        population=population, peripheral=peripheral, deep_copy=deep_copy, **subsets\n    )\n\n    container._id = container_id\n    container._frozen_time = frozen_time\n    container._split = split\n    container._last_change = last_change\n\n    return container\n</code></pre>"},{"location":"reference/data/placeholder/","title":"Placeholder","text":"<p>Abstract representation of tables and their relations.</p>"},{"location":"reference/data/placeholder/#getml.data.placeholder.Placeholder","title":"<code>Placeholder</code>","text":"<p>Abstract representation of tables and their relations.</p> <p>This class is an abstract representation of the <code>DataFrame</code> or <code>View</code>. However, it does not contain any actual data.</p> <p>You might also want to refer to <code>DataModel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name used for this placeholder. This name will appear in the generated SQL code.</p> required Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre> If you want to check out a real-world example where this is necessary, refer to the CORA notebook .</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>class Placeholder:\n    \"\"\"Abstract representation of tables and their relations.\n\n    This class is an abstract representation of the\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n    However, it does not contain any actual data.\n\n    You might also want to refer to [`DataModel`][getml.data.DataModel].\n\n    Args:\n        name (str):\n            The name used for this placeholder. This name will appear\n            in the generated SQL code.\n\n    Example:\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n        ```python\n        dm = getml.data.DataModel(\n            population_table.to_placeholder(\"POPULATION\")\n        )\n\n        dm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n        If you want to add more than one peripheral table, you can\n        use [`to_placeholder`][getml.data.to_placeholder]:\n        ```python\n        dm = getml.data.DataModel(\n            population_table.to_placeholder(\"POPULATION\")\n        )\n\n        dm.add(\n            getml.data.to_placeholder(\n                PERIPHERAL1=peripheral_table_1,\n                PERIPHERAL2=peripheral_table_2,\n            )\n        )\n        ```\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n        You can join over more than one join key:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n        )\n        ```\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n        ```python\n        dm.POPULATION.join(\n            dm.PERIPHERAL,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        In some cases, it is necessary to have more than one placeholder\n        on the same table. This is necessary to create more complicated\n        data models. In this case, you can do something like this:\n        ```python\n        dm.add(\n            getml.data.to_placeholder(\n                PERIPHERAL=[peripheral_table]*2,\n            )\n        )\n\n        # We can now access our two placeholders like this:\n        placeholder1 = dm.PERIPHERAL[0]\n        placeholder2 = dm.PERIPHERAL[1]\n        ```\n        If you want to check out a real-world example where this\n        is necessary, refer to the\n        [CORA notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/cora.ipynb).\n    \"\"\"\n\n    def __init__(\n        self, name: str, roles: Optional[Union[Roles, Dict[str, List[str]]]] = None\n    ):\n        self._name = name\n\n        if roles is None:\n            self._roles: Roles = Roles()\n        elif isinstance(roles, dict):\n            self._roles = Roles(**roles)\n        else:\n            self._roles = roles\n\n        self.joins: List[Join] = []\n        self.parent = None\n\n    def __dir__(self):\n        attrs = dir(type(self)) + list(self.__dict__.keys())\n        attrs.extend(col for col in self.columns if col.isidentifier())\n        return attrs\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        if key in vars(self)[\"_roles\"].columns:\n            return key\n        else:\n            raise KeyError(\n                f\"No column with with name {key!r} on the Placeholder's signature.\"\n            )\n\n    def _getml_deserialize(self):\n        cmd = {\"name_\": self.name, \"roles_\": self.roles.to_dict()}\n\n        cmd[\"allow_lagged_targets_\"] = [join.lagged_targets for join in self.joins]\n\n        cmd[\"horizon_\"] = [join.horizon or 0.0 for join in self.joins]\n\n        cmd[\"join_keys_used_\"] = [_handle_on(join.on)[0] for join in self.joins]\n\n        cmd[\"joined_tables_\"] = [join.right._getml_deserialize() for join in self.joins]\n\n        cmd[\"memory_\"] = [join.memory or 0.0 for join in self.joins]\n\n        cmd[\"other_join_keys_used_\"] = [_handle_on(join.on)[1] for join in self.joins]\n\n        cmd[\"other_time_stamps_used_\"] = [\n            _handle_ts(join.time_stamps)[1] for join in self.joins\n        ]\n\n        cmd[\"relationship_\"] = [join.relationship for join in self.joins]\n\n        cmd[\"time_stamps_used_\"] = [\n            _handle_ts(join.time_stamps)[0] for join in self.joins\n        ]\n\n        cmd[\"upper_time_stamps_used_\"] = [\n            join.upper_time_stamp or \"\" for join in self.joins\n        ]\n\n        return cmd\n\n    def __repr__(self) -&gt; str:\n        template = cleandoc(\n            \"\"\"\n            {name}:\n              columns:\n            {cols}\n            \"\"\"\n        )\n\n        if self.joins:\n            template += \"\\n\\n\" + cleandoc(\n                \"\"\"\n                  joins:\n                {joins}\n                \"\"\"\n            )\n\n        def format_on(on, join: Join):\n            template = \"({left.name}.{on[0]}, {join.right.name}.{on[1]})\"\n\n            if isinstance(on, list) and all(isinstance(key, tuple) for key in on):\n                formatted = \"\\n- \" + \"\\n- \".join(\n                    template.format(on=keys, left=self, join=join) for keys in on\n                )\n                return indent(formatted, \" \" * 2)\n\n            return template.format(on=on, left=self, join=join)\n\n        cols = [\n            f\"- {col}: {role}\" for col, role in zip(self.columns, self.roles.to_list())\n        ]\n\n        if len(cols) &gt; 5:\n            cols = cols[:5] + [\"- ...\"]\n\n        joins = []\n\n        for join in self.joins:\n            for param, value in vars(join).items():\n                if param == \"right\":\n                    joins.append(f\"- right: {join.right.name!r}\")\n                    continue\n\n                if value is not None:\n                    if param == \"on\":\n                        joins.append(f\"  on: {format_on(value, join)}\")\n                    elif param == \"time_stamps\":\n                        joins.append(\n                            f\"  {param}: ({self.name}.{value[0]}, {join.right.name}.{value[1]})\"\n                        )\n                    else:\n                        joins.append(f\"  {param}: {value!r}\")\n\n        joins = indent(\"\\n\".join(joins), \" \" * 2)  # type: ignore\n\n        cols = indent(\"\\n\".join(cols), \" \" * 2)  # type: ignore\n\n        return template.format(name=self.name, cols=cols, joins=joins)\n\n    def _ipython_key_completions_(self):\n        return self.columns\n\n    @property\n    def children(self):\n        return set([self]) ^ set(self.to_list())\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    def join(\n        self,\n        right,\n        on: OnType = None,\n        time_stamps: TimeStampsType = None,\n        relationship: str = many_to_many,\n        memory: Optional[float] = None,\n        horizon: Optional[float] = None,\n        lagged_targets: bool = False,\n        upper_time_stamp: Optional[str] = None,\n    ):\n        \"\"\"\n        Joins another to placeholder to this placeholder.\n\n        Args:\n            right ([`Placeholder`][getml.data.Placeholder]):\n                The placeholder you would like to join.\n\n            on (None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]]):\n                The join keys to use. If none is passed, then everything\n                will be joined to everything else.\n\n            time_stamps (string or Tuple[str, str]):\n                The time stamps used to limit the join.\n\n            relationship (str):\n                The relationship between the two tables. Must be from\n                [`relationship`][getml.data.relationship].\n\n            memory (float):\n                The difference between the time stamps until data is 'forgotten'.\n                Limiting your joins using memory can significantly speed up\n                training time. Also refer to [`time`][getml.data.time].\n\n            horizon (float):\n                The prediction horizon to apply to this join.\n                Also refer to [`time`][getml.data.time].\n\n            lagged_targets (bool):\n                Whether you want to allow lagged targets. If this is set to True,\n                you must also pass a positive, non-zero *horizon*.\n\n            upper_time_stamp (str):\n                Name of a time stamp in *right* that serves as an upper limit\n                on the join.\n        \"\"\"\n\n        if not isinstance(right, type(self)):\n            msg = (\n                \"'right' must be a getml.data.Placeholder. \"\n                + \"You can create a placeholder by calling .to_placeholder() \"\n                + \"on DataFrames or Views.\"\n            )\n            raise TypeError(msg)\n\n        if self in right.to_list():\n            raise ValueError(\n                \"Cicular references to other placeholders are not allowed.\"\n            )\n\n        if isinstance(on, str):\n            on = (on, on)\n\n        if isinstance(time_stamps, str):\n            time_stamps = (time_stamps, time_stamps)\n\n        keys = (\n            list(zip(*on))\n            if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n            else on\n        )\n\n        for i, ph in enumerate([self, right]):\n            if ph.roles.join_key and keys:\n                not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n                if not_a_join_key:\n                    raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n            if ph.roles.time_stamp and time_stamps:\n                if time_stamps[i] not in ph.roles.time_stamp:\n                    raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n        if lagged_targets and horizon in (0.0, None):\n            raise ValueError(\n                \"If you allow lagged targets, then you must also set a \"\n                + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n            )\n\n        if horizon not in (0.0, None) and time_stamps is None:\n            raise ValueError(\n                \"Setting 'horizon' (i.e. a relative look-back window) \"\n                + \"requires a 'time_stamp'.\"\n            )\n\n        if memory not in (0.0, None) and time_stamps is None:\n            raise ValueError(\n                \"Setting 'memory' (i.e. a relative look-back window) \"\n                + \"requires a 'time_stamp'.\"\n            )\n\n        join = Join(\n            right=right,\n            on=on,\n            time_stamps=time_stamps,\n            relationship=relationship,\n            memory=memory,\n            horizon=horizon,\n            lagged_targets=lagged_targets,\n            upper_time_stamp=upper_time_stamp,\n        )\n\n        if any(join == existing for existing in self.joins):\n            raise ValueError(\n                \"A join with the following set of parameters already exists on \"\n                f\"the placeholder {self.name!r}:\"\n                f\"\\n\\n{join}\\n\\n\"\n                \"Redundant joins are not allowed.\"\n            )\n\n        self.joins.append(join)\n        right.parent = self  # type: ignore\n\n    @property\n    def population(self):\n        if self.parent is None:\n            return self\n        return self.parent.population\n\n    @property\n    def roles(self):\n        return self._roles\n\n    @roles.setter\n    def roles(self, roles):\n        if not isinstance(roles, (Roles, dict)):\n            raise TypeError(\"'roles' must be a dict or getml.data.Roles\")\n        if isinstance(roles, dict):\n            self._roles = Roles(**roles)\n        else:\n            self._roles = roles\n\n    def to_list(self):\n        \"\"\"\n        Returns a list of this placeholder and all of its descendants.\n        \"\"\"\n        return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n\n    def to_dict(self):\n        \"\"\"\n        Expresses this placeholder and all of its descendants as a dictionary.\n        \"\"\"\n        phs = {}\n        for ph in self.to_list():\n            key = ph.name\n            if ph.children:\n                i = 2\n                while key in phs:\n                    key = f\"{ph.name}{i}\"\n                    i += 1\n            phs[key] = ph\n        return phs\n\n    @property\n    def columns(self):\n        return self.roles.columns\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.placeholder.Placeholder.join","title":"<code>join(right, on=None, time_stamps=None, relationship=many_to_many, memory=None, horizon=None, lagged_targets=False, upper_time_stamp=None)</code>","text":"<p>Joins another to placeholder to this placeholder.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>[`Placeholder`][getml.data.Placeholder]</code> <p>The placeholder you would like to join.</p> required <code>on</code> <code>(None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>time_stamps</code> <code>string or Tuple[str, str]</code> <p>The time stamps used to limit the join.</p> <code>None</code> <code>relationship</code> <code>str</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <code>many_to_many</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right that serves as an upper limit on the join.</p> <code>None</code> Source code in <code>getml/data/placeholder.py</code> <pre><code>def join(\n    self,\n    right,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins another to placeholder to this placeholder.\n\n    Args:\n        right ([`Placeholder`][getml.data.Placeholder]):\n            The placeholder you would like to join.\n\n        on (None, string, Tuple[str, str] or List[Union[str, Tuple[str, str]]]):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps (string or Tuple[str, str]):\n            The time stamps used to limit the join.\n\n        relationship (str):\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory (float):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str):\n            Name of a time stamp in *right* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right, type(self)):\n        msg = (\n            \"'right' must be a getml.data.Placeholder. \"\n            + \"You can create a placeholder by calling .to_placeholder() \"\n            + \"on DataFrames or Views.\"\n        )\n        raise TypeError(msg)\n\n    if self in right.to_list():\n        raise ValueError(\n            \"Cicular references to other placeholders are not allowed.\"\n        )\n\n    if isinstance(on, str):\n        on = (on, on)\n\n    if isinstance(time_stamps, str):\n        time_stamps = (time_stamps, time_stamps)\n\n    keys = (\n        list(zip(*on))\n        if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n        else on\n    )\n\n    for i, ph in enumerate([self, right]):\n        if ph.roles.join_key and keys:\n            not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n            if not_a_join_key:\n                raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n        if ph.roles.time_stamp and time_stamps:\n            if time_stamps[i] not in ph.roles.time_stamp:\n                raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n    if lagged_targets and horizon in (0.0, None):\n        raise ValueError(\n            \"If you allow lagged targets, then you must also set a \"\n            + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n        )\n\n    if horizon not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'horizon' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    if memory not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'memory' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    join = Join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    if any(join == existing for existing in self.joins):\n        raise ValueError(\n            \"A join with the following set of parameters already exists on \"\n            f\"the placeholder {self.name!r}:\"\n            f\"\\n\\n{join}\\n\\n\"\n            \"Redundant joins are not allowed.\"\n        )\n\n    self.joins.append(join)\n    right.parent = self  # type: ignore\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.placeholder.Placeholder.to_dict","title":"<code>to_dict()</code>","text":"<p>Expresses this placeholder and all of its descendants as a dictionary.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses this placeholder and all of its descendants as a dictionary.\n    \"\"\"\n    phs = {}\n    for ph in self.to_list():\n        key = ph.name\n        if ph.children:\n            i = 2\n            while key in phs:\n                key = f\"{ph.name}{i}\"\n                i += 1\n        phs[key] = ph\n    return phs\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.placeholder.Placeholder.to_list","title":"<code>to_list()</code>","text":"<p>Returns a list of this placeholder and all of its descendants.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list of this placeholder and all of its descendants.\n    \"\"\"\n    return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n</code></pre>"},{"location":"reference/data/relationship/","title":"Relationship","text":"<p>Marks the relationship between joins in <code>Placeholder</code></p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_many","title":"<code>many_to_many = 'many-to-many'</code>  <code>module-attribute</code>","text":"<p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_one","title":"<code>many_to_one = 'many-to-one'</code>  <code>module-attribute</code>","text":"<p>Used for many-to-one relationships.</p> <p>If two tables are guaranteed to be in a many-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked many-to-one, but the assumption is violated, the pipeline will raise an exception.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_many","title":"<code>one_to_many = 'one-to-many'</code>  <code>module-attribute</code>","text":"<p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_one","title":"<code>one_to_one = 'one-to-one'</code>  <code>module-attribute</code>","text":"<p>Used for one-to-one relationships.</p> <p>If two tables are guaranteed to be in a one-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked one-to-one, but the assumption is violated, the pipeline will raise an exception. If you are unsure whether you want to use many_to_one or one_to_one, user many_to_one.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.propositionalization","title":"<code>propositionalization = 'propositionalization'</code>  <code>module-attribute</code>","text":"<p>Used for one-to-many or many-to-many relationships.</p> <p>The flag means that you want a propositionalization algorithm to be used for this particular join. This is recommended when there are very many matches within the join and normal algorithms would take too long.</p>"},{"location":"reference/data/roles/","title":"Roles","text":"<p>A role determines if and how <code>columns</code> are handled during the construction of the <code>DataModel</code> and used by the feature learning algorithm (see <code>feature_learning</code>).</p> Example <pre><code>data_frame = getml.DataFrame.from_db(\n    name=name,\n    table_name=table_name,\n    conn=conn\n)\n\ndata_frame.set_role(\n    [\"store_nbr\", \"item_nbr\"], getml.data.roles.join_key)\ndata_frame.set_role(\"date\", getml.data.roles.time_stamp)\ndata_frame.set_role(\"units\", getml.data.roles.target)\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.categorical","title":"<code>categorical = 'categorical'</code>  <code>module-attribute</code>","text":"<p>Marks categorical columns.</p> <p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integer instead of strings in your provided data set.</p>"},{"location":"reference/data/roles/#getml.data.roles.join_key","title":"<code>join_key = 'join_key'</code>  <code>module-attribute</code>","text":"<p>Marks join keys.</p> <p>Role required to establish a relation between two <code>Placeholder</code>, the abstract representation of the <code>DataFrame</code>, by using the <code>join</code> method. Please refer to the chapter Data Model for details.</p> <p>The content of this column is allowed to contain NULL values. But beware, columns with NULL in their join keys won't be matched to anything, not even to NULL in other join keys.</p> <p><code>columns</code> of this role will not be handled by the feature learning algorithm.</p>"},{"location":"reference/data/roles/#getml.data.roles.numerical","title":"<code>numerical = 'numerical'</code>  <code>module-attribute</code>","text":"<p>Marks numerical columns.</p> <p>This role tells the getML engine to include the associated <code>FloatColumn</code> during feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch.</p>"},{"location":"reference/data/roles/#getml.data.roles.target","title":"<code>target = 'target'</code>  <code>module-attribute</code>","text":"<p>Marks the column(s) we would like to predict.</p> <p>The associated <code>columns</code> contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>lagged_target</code> in <code>join</code>). But they are such an important part of the analysis that the population table is required to contain at least one of them (refer to Data Model Tables).</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>.</p>"},{"location":"reference/data/roles/#getml.data.roles.text","title":"<code>text = 'text'</code>  <code>module-attribute</code>","text":"<p>Marks text columns.</p> <p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering. Unlike categorical columns, text columns can not be used as a whole. Instead, the feature learners have to apply basic text mining techniques before they are able to use them.</p>"},{"location":"reference/data/roles/#getml.data.roles.time_stamp","title":"<code>time_stamp = 'time_stamp'</code>  <code>module-attribute</code>","text":"<p>Marks time stamps.</p> <p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps: <pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre></p> <p>This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a <code>StringColumn</code>, you need to specify the format of this string. You can do so by using the <code>time_formats</code> argument of <code>set_role</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p>If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL.</p> Example <p><pre><code>data_df = dict(\n        date1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\n        date2=['1971-01-01', '1971-01-02', '1971-01-03'],\n        date3=['1|1|71', '1|2|71', '1|3|71'],\n    )\ndf = getml.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n</code></pre> <pre><code>| date1                       | date2                       | date3                       |\n| time stamp                  | time stamp                  | time stamp                  |\n-------------------------------------------------------------------------------------------\n| 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n| 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n| 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre></p> Note <p>getML time stamps are actually floats expressing the number of seconds since UNIX time (1970-01-01T00:00:00).</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_float","title":"<code>unused_float = 'unused_float'</code>  <code>module-attribute</code>","text":"<p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_string","title":"<code>unused_string = 'unused_string'</code>  <code>module-attribute</code>","text":"<p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles_obj/","title":"Roles obj","text":"<p>Dataclass for handling the roles.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles","title":"<code>Roles</code>  <code>dataclass</code>","text":"<p>Roles can be passed to <code>DataFrame</code> to predefine the roles assigned to certain columns.</p> Example <pre><code>roles = getml.data.Roles(\n    categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n)\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n)\n</code></pre> Source code in <code>getml/data/roles_obj.py</code> <pre><code>@dataclass\nclass Roles:\n    \"\"\"\n    Roles can be passed to [`DataFrame`][getml.DataFrame] to\n    predefine the roles assigned to certain columns.\n\n    Example:\n        ```python\n        roles = getml.data.Roles(\n            categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n        )\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n        )\n        ```\n    \"\"\"\n\n    categorical: List[str] = field(default_factory=list)\n    join_key: List[str] = field(default_factory=list)\n    numerical: List[str] = field(default_factory=list)\n    target: List[str] = field(default_factory=list)\n    text: List[str] = field(default_factory=list)\n    time_stamp: List[str] = field(default_factory=list)\n    unused_float: List[str] = field(default_factory=list)\n    unused_string: List[str] = field(default_factory=list)\n\n    def __getitem__(self, key):\n        try:\n            return getattr(self, key)\n        except TypeError:\n            raise KeyError(key)\n\n    def __iter__(self):\n        yield from (field_.name for field_ in fields(self))\n\n    def __len__(self):\n        return len(fields(self))\n\n    def __repr__(self):\n        template = cleandoc(\n            \"\"\"\n            {role}:\n            - {cols}\n            \"\"\"\n        )\n\n        blocks = []\n\n        for role in self:\n            if self[role]:\n                cols = \"\\n- \".join(self[role])\n                blocks.append(template.format(role=role, cols=cols))\n\n        return \"\\n\\n\".join(blocks)\n\n    @property\n    def columns(self):\n        \"\"\"\n        The name of all columns contained in the roles object.\n        \"\"\"\n        return [r for role in self for r in self[role]]\n\n    def infer(self, colname):\n        \"\"\"\n        Infers the role of a column.\n\n        Args:\n            colname (str):\n                The name of the column to be inferred.\n        \"\"\"\n        for role in self:\n            if colname in self[role]:\n                return role\n        raise ValueError(\"Column named '\" + colname + \"' not found.\")\n\n    def to_dict(self):\n        \"\"\"\n        Expresses the roles object as a dictionary.\n        \"\"\"\n        return {role: self[role] for role in self}\n\n    def to_list(self):\n        \"\"\"\n        Returns a list containing the roles, without the corresponding\n        columns names.\n        \"\"\"\n        return [r for role in self for r in [role] * len(self[role])]\n\n    @property\n    def unused(self):\n        \"\"\"\n        Names of all unused columns (unused_float + unused_string).\n        \"\"\"\n        return self.unused_float + self.unused_string\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>The name of all columns contained in the roles object.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.unused","title":"<code>unused</code>  <code>property</code>","text":"<p>Names of all unused columns (unused_float + unused_string).</p>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.infer","title":"<code>infer(colname)</code>","text":"<p>Infers the role of a column.</p> <p>Parameters:</p> Name Type Description Default <code>colname</code> <code>str</code> <p>The name of the column to be inferred.</p> required Source code in <code>getml/data/roles_obj.py</code> <pre><code>def infer(self, colname):\n    \"\"\"\n    Infers the role of a column.\n\n    Args:\n        colname (str):\n            The name of the column to be inferred.\n    \"\"\"\n    for role in self:\n        if colname in self[role]:\n            return role\n    raise ValueError(\"Column named '\" + colname + \"' not found.\")\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.to_dict","title":"<code>to_dict()</code>","text":"<p>Expresses the roles object as a dictionary.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses the roles object as a dictionary.\n    \"\"\"\n    return {role: self[role] for role in self}\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.to_list","title":"<code>to_list()</code>","text":"<p>Returns a list containing the roles, without the corresponding columns names.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list containing the roles, without the corresponding\n    columns names.\n    \"\"\"\n    return [r for role in self for r in [role] * len(self[role])]\n</code></pre>"},{"location":"reference/data/staging/","title":"Staging","text":"<p>Collection of functions for simulating the effects of staging, not meant to be used by the end user.</p>"},{"location":"reference/data/star_schema/","title":"Star schema","text":"<p>Special container for star schemata.</p>"},{"location":"reference/data/star_schema/#getml.data.star_schema.StarSchema","title":"<code>StarSchema</code>","text":"<p>A StarSchema is a simplifying abstraction that can be used for machine learning problems that can be organized in a simple star schema.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model.</p> <p>The class is designed using composition  - it is neither <code>Container</code> nor <code>DataModel</code>, but has both of them.</p> <p>This means that you can always fall back to the more flexible methods using <code>Container</code> and <code>DataModel</code> by directly accessing the attributes <code>container</code> and <code>data_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <code>None</code> <code>alias</code> <code>str</code> <p>The alias to be used for the population table. This is required, if population is a <code>View</code>.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>join</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>train</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>validation</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>test</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <code>None</code> <code>kwargs</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table used in <code>Subset</code> s other than the predefined train, validation and test subsets. You can call these subsets anything you want to and can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\nstar_schema = getml.data.StarSchema(\n    my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(star_schema.my_subset)\n</code></pre></p> <code>{}</code> Example <p>Note that this example is taken from the loans notebook.</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code> and <code>Pipeline</code>.</p> <p><pre><code># First, we insert our data.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# StarSchema, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new)\n\ncontainer.add(\n    trans=trans_new,\n    order=order_new,\n    meta=meta_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# star_schema.train and star_schema.test\n# work just like above.\n</code></pre> Source code in <code>getml/data/star_schema.py</code> <pre><code>class StarSchema:\n    \"\"\"\n    A StarSchema is a simplifying abstraction that can be used\n    for machine learning problems that can be organized in a simple\n    [star schema](https://en.wikipedia.org/wiki/Star_schema).\n\n    It unifies [`Container`][getml.data.Container] and\n    [`DataModel`][getml.data.DataModel] thus abstracting away the need to\n    differentiate between the concrete data and the abstract data model.\n\n    The class is designed using\n    [composition ](https://en.wikipedia.org/wiki/Composition_over_inheritance)\n    - it *is* neither [`Container`][getml.data.Container] nor [`DataModel`][getml.data.DataModel],\n    but *has* both of them.\n\n    This means that you can always fall back to the more flexible methods using\n    [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel] by directly\n    accessing the attributes `container` and `data_model`.\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        alias (str, optional):\n            The alias to be used for the population table. This is required,\n            if *population* is a [`View`][getml.data.View].\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`join`][getml.data.StarSchema.join].\n\n        split ([`StringColumn`][getml.data.columns.StringColumn] or [`StringColumnView`][getml.data.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset] s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        train ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *train*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        validation ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *validation*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        test ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in the *test*\n            [`Subset`][getml.data.Subset].\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n        kwargs ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View], optional):\n            The population table used in [`Subset`][getml.data.Subset] s\n            other than the predefined *train*, *validation* and *test* subsets.\n            You can call these subsets anything you want to and can access them\n            just like *train*, *validation* and *test*.\n            You can either pass *population* and *split* or you can pass\n            the subsets separately using *train*, *validation*, *test*\n            and *kwargs*.\n\n            Example:\n                ```python\n                # Pass the subset.\n                star_schema = getml.data.StarSchema(\n                    my_subset=my_data_frame)\n\n                # You can access the subset just like train,\n                # validation or test\n                my_pipeline.fit(star_schema.my_subset)\n                ```\n\n    Example:\n        Note that this example is taken from the\n        [loans notebook](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n\n        You might also want to refer to\n        [`DataFrame`][getml.DataFrame], [`View`][getml.data.View]\n        and [`Pipeline`][getml.Pipeline].\n\n        ```python\n        # First, we insert our data.\n        # population_train and population_test are either\n        # DataFrames or Views. The population table\n        # defines the statistical population of your\n        # machine learning problem and contains the\n        # target variables.\n        star_schema = getml.data.StarSchema(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views.\n        # Because this is a star schema,\n        # all joins take place on the population\n        # table.\n        star_schema.join(\n            trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        star_schema.join(\n            order,\n            on=\"account_id\",\n        )\n\n        star_schema.join(\n            meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=star_schema.data_model,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # Now, we pass the actual data.\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(star_schema.train)\n\n        pipe.fit(star_schema.train)\n\n        pipe.score(star_schema.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # StarSchema, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new)\n\n        container.add(\n            trans=trans_new,\n            order=order_new,\n            meta=meta_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n        If you don't already have a train and test set,\n        you can use a function from the\n        [`split`][getml.data.split] module.\n\n        ```python\n        split = getml.data.split.random(\n            train=0.8, test=0.2)\n\n        star_schema = getml.data.StarSchema(\n            population=population_all,\n            split=split,\n        )\n\n        # The remaining code is the same as in\n        # the example above. In particular,\n        # star_schema.train and star_schema.test\n        # work just like above.\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        population=None,\n        alias=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        train=None,\n        validation=None,\n        test=None,\n        **kwargs,\n    ):\n        if (population is None or isinstance(population, View)) and alias is None:\n            raise ValueError(\n                \"If 'population' is None or a getml.data.View, you must set an alias.\"\n            )\n\n        self._alias = alias or population.name\n\n        self._container = Container(\n            population=population,\n            peripheral=peripheral,\n            split=split,\n            deep_copy=deep_copy,\n            train=train,\n            validation=validation,\n            test=test,\n            **kwargs,\n        )\n\n        def get_placeholder():\n            if population is not None:\n                return population.to_placeholder(alias)\n            if train is not None:\n                return train.to_placeholder(alias)\n            if validation is not None:\n                return validation.to_placeholder(alias)\n            if test is not None:\n                return test.to_placeholder(alias)\n            assert (\n                len(kwargs) &gt; 0\n            ), \"This should have been checked by Container.__init__.\"\n            return kwargs[list(kwargs.keys())[0]].to_placeholder(alias)\n\n        self._data_model = DataModel(get_placeholder())\n\n    def __dir__(self):\n        attrs = dir(type(self)) + [key[1:] for key in list(vars(self))]\n        attrs += dir(self.container)\n        attrs += dir(self.data_model)\n        return list(set(attrs))\n\n    def __iter__(self):\n        yield from [self.population] + list(self.peripheral.values())\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            return super().__getattribute__(key)\n\n    def __getitem__(self, key):\n        attrs = vars(self)\n\n        if key in attrs:\n            return attrs[key]\n\n        if \"_\" + key in attrs:\n            return attrs[\"_\" + key]\n\n        try:\n            return attrs[\"_container\"][key]\n        except KeyError:\n            return attrs[\"_data_model\"][key]\n\n    def __repr__(self):\n        template = cleandoc(\n            \"\"\"\n            data model\n\n            {data_model}\n\n\n            container\n\n            {container}\n            \"\"\"\n        )\n        return template.format(\n            data_model=indent(repr(self.data_model), \"  \"),\n            container=indent(repr(self.container), \"  \"),\n        )\n\n    def _repr_html_(self):\n        template = cleandoc(\n            \"\"\"\n            &lt;span style='font-size: 1.2rem; font-weight: 500;'&gt;data model&lt;/span&gt;\n            {data_model}\n            &lt;span style='font-size: 1.2rem; font-weight: 500;'&gt;container&lt;/span&gt;\n            {container}\n            \"\"\"\n        )\n        return template.format(\n            data_model=self.data_model._repr_html_(),\n            container=self.container._repr_html_(),\n        )\n\n    @property\n    def container(self):\n        \"\"\"\n        The underlying [`Container`][getml.data.Container].\n        \"\"\"\n        return self._container\n\n    @property\n    def data_model(self):\n        \"\"\"\n        The underlying [`DataModel`][getml.data.DataModel].\n        \"\"\"\n        return self._data_model\n\n    def join(\n        self,\n        right_df,\n        alias=None,\n        on=None,\n        time_stamps=None,\n        relationship=many_to_many,\n        memory=None,\n        horizon=None,\n        lagged_targets=False,\n        upper_time_stamp=None,\n    ):\n        \"\"\"\n        Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n        to the population table.\n\n        In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n        all joins take place on the population table. If you want to create more\n        complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n        Example:\n            This example will construct a data model in which the\n            'population_table' depends on the 'peripheral_table' via\n            the 'join_key' column. In addition, only those rows in\n            'peripheral_table' for which 'time_stamp' is smaller or\n            equal to the 'time_stamp' in 'population_table' are considered:\n\n            ```python\n            star_schema = getml.data.StarSchema(\n                population=population_table, split=split)\n\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\"\n            )\n            ```\n\n            If the relationship between two tables is many-to-one or one-to-one\n            you should clearly say so:\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                relationship=getml.data.relationship.many_to_one,\n            )\n            ```\n            Please also refer to [`relationship`][getml.data.relationship].\n\n            If the join keys or time stamps are named differently in the two\n            different tables, use a tuple:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=(\"join_key\", \"other_join_key\"),\n                time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n            )\n            ```\n\n            You can join over more than one join key:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n                time_stamps=\"time_stamp\",\n                )\n            ```\n\n            You can also limit the scope of your joins using *memory*. This\n            can significantly speed up training time. For instance, if you\n            only want to consider data from the last seven days, you could\n            do something like this:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                memory=getml.data.time.days(7),\n            )\n            ```\n\n            In some use cases, particularly those involving time series, it\n            might be a good idea to use targets from the past. You can activate\n            this using *lagged_targets*. But if you do that, you must\n            also define a prediction *horizon*. For instance, if you want to\n            predict data for the next hour, using data from the last seven days,\n            you could do this:\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                lagged_targets=True,\n                horizon=getml.data.time.hours(1),\n                memory=getml.data.time.days(7),\n            )\n            ```\n\n            Please also refer to [`time`][getml.data.time].\n\n            If the join involves many matches, it might be a good idea to set the\n            relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n            This forces the pipeline to always use a propositionalization\n            algorithm for this join, which can significantly speed things up.\n\n            ```python\n            star_schema.join(\n                peripheral_table,\n                on=\"join_key\",\n                time_stamps=\"time_stamp\",\n                relationship=getml.data.relationship.propositionalization,\n            )\n            ```\n\n            Please also refer to [`relationship`][getml.data.relationship].\n\n        Args:\n            right_df ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n                The data frame or view you would like to join.\n\n            alias (str or None):\n                The name as which you want *right_df* to be referred to in\n                the generated SQL code.\n\n            on (None, string, Tuple[str] or List[Union[str, Tuple[str]]]):\n                The join keys to use. If none is passed, then everything\n                will be joined to everything else.\n\n            time_stamps (string or Tuple[str]):\n                The time stamps used to limit the join.\n\n            relationship (str):\n                The relationship between the two tables. Must be from\n                [`relationship`][getml.data.relationship].\n\n            memory (float):\n                The difference between the time stamps until data is 'forgotten'.\n                Limiting your joins using memory can significantly speed up\n                training time. Also refer to [`time`][getml.data.time].\n\n            horizon (float):\n                The prediction horizon to apply to this join.\n                Also refer to [`time`][getml.data.time].\n\n            lagged_targets (bool):\n                Whether you want to allow lagged targets. If this is set to True,\n                you must also pass a positive, non-zero *horizon*.\n\n            upper_time_stamp (str):\n                Name of a time stamp in *right_df* that serves as an upper limit\n                on the join.\n        \"\"\"\n\n        if not isinstance(right_df, (DataFrame, View)):\n            raise TypeError(\n                f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n            )\n\n        if isinstance(right_df, View):\n            if alias is None:\n                raise ValueError(\n                    \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                    \"as a peripheral table.\"\n                )\n\n        def modify_join_keys(on):\n            if isinstance(on, list):\n                return [modify_join_keys(jk) for jk in on]\n\n            if isinstance(on, (str, StringColumn)):\n                on = (on, on)\n\n            if on is not None and on:\n                on = tuple(\n                    jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n                )\n\n            return on\n\n        def modify_time_stamps(time_stamps):\n            if isinstance(time_stamps, (str, FloatColumn)):\n                time_stamps = (time_stamps, time_stamps)\n\n            if time_stamps is not None:\n                time_stamps = tuple(\n                    time_stamp.name\n                    if isinstance(time_stamp, FloatColumn)\n                    else time_stamp\n                    for time_stamp in time_stamps\n                )\n\n            return time_stamps\n\n        on = modify_join_keys(on)\n\n        time_stamps = modify_time_stamps(time_stamps)\n\n        upper_time_stamp = (\n            upper_time_stamp.name\n            if isinstance(upper_time_stamp, FloatColumn)\n            else upper_time_stamp\n        )\n\n        right = right_df.to_placeholder(alias)\n\n        self.data_model.population.join(\n            right=right,\n            on=on,\n            time_stamps=time_stamps,\n            relationship=relationship,\n            memory=memory,\n            horizon=horizon,\n            lagged_targets=lagged_targets,\n            upper_time_stamp=upper_time_stamp,\n        )\n\n        alias = alias or right_df.name\n\n        self.container.add(**{alias: right_df})\n\n    def sync(self):\n        \"\"\"\n        Synchronizes the last change with the data to avoid warnings that the data\n        has been changed.\n\n        This is only a problem when `deep_copy=False`.\n        \"\"\"\n        self.container.sync()\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.star_schema.StarSchema.container","title":"<code>container</code>  <code>property</code>","text":"<p>The underlying <code>Container</code>.</p>"},{"location":"reference/data/star_schema/#getml.data.star_schema.StarSchema.data_model","title":"<code>data_model</code>  <code>property</code>","text":"<p>The underlying <code>DataModel</code>.</p>"},{"location":"reference/data/star_schema/#getml.data.star_schema.StarSchema.join","title":"<code>join(right_df, alias=None, on=None, time_stamps=None, relationship=many_to_many, memory=None, horizon=None, lagged_targets=False, upper_time_stamp=None)</code>","text":"<p>Joins a <code>DataFrame</code> or <code>View</code> to the population table.</p> <p>In a <code>StarSchema</code> or <code>TimeSeries</code>, all joins take place on the population table. If you want to create more complex data models, use <code>DataModel</code> instead.</p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered:</p> <pre><code>star_schema = getml.data.StarSchema(\n    population=population_table, split=split)\n\nstar_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> <p>If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> <p>You can join over more than one join key:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n    )\n</code></pre> <p>You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up.</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> <p>Please also refer to <code>relationship</code>.</p> <p>Parameters:</p> Name Type Description Default <code>right_df</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The data frame or view you would like to join.</p> required <code>alias</code> <code>str or None</code> <p>The name as which you want right_df to be referred to in the generated SQL code.</p> <code>None</code> <code>on</code> <code>(None, string, Tuple[str] or List[Union[str, Tuple[str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>time_stamps</code> <code>string or Tuple[str]</code> <p>The time stamps used to limit the join.</p> <code>None</code> <code>relationship</code> <code>str</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <code>many_to_many</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <code>None</code> Source code in <code>getml/data/star_schema.py</code> <pre><code>def join(\n    self,\n    right_df,\n    alias=None,\n    on=None,\n    time_stamps=None,\n    relationship=many_to_many,\n    memory=None,\n    horizon=None,\n    lagged_targets=False,\n    upper_time_stamp=None,\n):\n    \"\"\"\n    Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n    to the population table.\n\n    In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n    all joins take place on the population table. If you want to create more\n    complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n    Example:\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n\n        ```python\n        star_schema = getml.data.StarSchema(\n            population=population_table, split=split)\n\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n\n        You can join over more than one join key:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n            )\n        ```\n\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n\n        Please also refer to [`relationship`][getml.data.relationship].\n\n    Args:\n        right_df ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The data frame or view you would like to join.\n\n        alias (str or None):\n            The name as which you want *right_df* to be referred to in\n            the generated SQL code.\n\n        on (None, string, Tuple[str] or List[Union[str, Tuple[str]]]):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps (string or Tuple[str]):\n            The time stamps used to limit the join.\n\n        relationship (str):\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory (float):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str):\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right_df, (DataFrame, View)):\n        raise TypeError(\n            f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n        )\n\n    if isinstance(right_df, View):\n        if alias is None:\n            raise ValueError(\n                \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                \"as a peripheral table.\"\n            )\n\n    def modify_join_keys(on):\n        if isinstance(on, list):\n            return [modify_join_keys(jk) for jk in on]\n\n        if isinstance(on, (str, StringColumn)):\n            on = (on, on)\n\n        if on is not None and on:\n            on = tuple(\n                jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n            )\n\n        return on\n\n    def modify_time_stamps(time_stamps):\n        if isinstance(time_stamps, (str, FloatColumn)):\n            time_stamps = (time_stamps, time_stamps)\n\n        if time_stamps is not None:\n            time_stamps = tuple(\n                time_stamp.name\n                if isinstance(time_stamp, FloatColumn)\n                else time_stamp\n                for time_stamp in time_stamps\n            )\n\n        return time_stamps\n\n    on = modify_join_keys(on)\n\n    time_stamps = modify_time_stamps(time_stamps)\n\n    upper_time_stamp = (\n        upper_time_stamp.name\n        if isinstance(upper_time_stamp, FloatColumn)\n        else upper_time_stamp\n    )\n\n    right = right_df.to_placeholder(alias)\n\n    self.data_model.population.join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    alias = alias or right_df.name\n\n    self.container.add(**{alias: right_df})\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.star_schema.StarSchema.sync","title":"<code>sync()</code>","text":"<p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    self.container.sync()\n</code></pre>"},{"location":"reference/data/subset/","title":"Subset","text":"<p>Subset class intended to be passed to the pipeline.</p>"},{"location":"reference/data/subset/#getml.data.subset.Subset","title":"<code>Subset</code>  <code>dataclass</code>","text":"<p>A Subset consists of a population table and one or several peripheral tables.</p> <p>It is passed by a <code>Container</code>, <code>StarSchema</code> and <code>TimeSeries</code> to the <code>Pipeline</code>.</p> Example <pre><code>container = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# train and test are Subsets.\n# They contain population_train\n# and population_test respectively,\n# as well as their peripheral tables\n# meta, order and trans.\nmy_pipeline.fit(container.train)\n\nmy_pipeline.score(container.test)\n</code></pre> Source code in <code>getml/data/subset.py</code> <pre><code>@dataclass\nclass Subset:\n    \"\"\"\n    A Subset consists of a population table and one or several peripheral tables.\n\n    It is passed by a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n    and [`TimeSeries`][getml.data.TimeSeries] to the [`Pipeline`][getml.Pipeline].\n\n    Example:\n        ```python\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # train and test are Subsets.\n        # They contain population_train\n        # and population_test respectively,\n        # as well as their peripheral tables\n        # meta, order and trans.\n        my_pipeline.fit(container.train)\n\n        my_pipeline.score(container.test)\n        ```\n    \"\"\"\n\n    container_id: str\n    peripheral: Dict[str, Union[DataFrame, View]]\n    population: Union[DataFrame, View]\n\n    def _format(self):\n        headers_perph = [[\"name\", \"rows\", \"type\"]]\n\n        rows_perph = [\n            [perph.name, perph.nrows(), type(perph).__name__]\n            for perph in self.peripheral.values()\n        ]\n\n        names = [perph.name for perph in self.peripheral.values()]\n        aliases = list(self.peripheral.keys())\n\n        if any(alias not in names for alias in aliases):\n            headers_perph[0].insert(0, \"alias\")\n\n            for alias, row in zip(aliases, rows_perph):\n                row.insert(0, alias)\n\n        return self.population._format(), _Formatter(\n            headers=headers_perph, rows=rows_perph\n        )\n\n    def __repr__(self):\n        pop, perph = self._format()\n        pop_footer = self.population._collect_footer_data()\n\n        template = cleandoc(\n            \"\"\"\n            population\n            {pop}\n\n            peripheral\n            {perph}\n            \"\"\"\n        )\n\n        return template.format(\n            pop=pop._render_string(footer=pop_footer), perph=perph._render_string()\n        )\n\n    def _repr_html_(self):\n        pop, perph = self._format()\n        pop_footer = self.population._collect_footer_data()\n\n        template = cleandoc(\n            \"\"\"\n            &lt;div&gt;\n                &lt;div style='margin-bottom: 10px; font-size: 1rem'&gt;population&lt;/div&gt;\n                {pop}\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div style='margin-bottom: 10px; font-size: 1rem'&gt;peripheral&lt;/div&gt;\n                {perph}\n            &lt;/div&gt;\n            \"\"\"\n        )\n\n        return template.format(\n            pop=pop._render_html(footer=pop_footer), perph=perph._render_html()\n        )\n</code></pre>"},{"location":"reference/data/time/","title":"Time","text":"<p>Convenience functions for the handling of time stamps.</p> <p>In getML, time stamps are always expressed as a floating point value. This float measures the number of seconds since UNIX time (January 1, 1970, 00:00:00). Smaller units of time are expressed as fractions of a second.</p> <p>To make this a bit easier to handle, this module contains simple convenience functions that express other time units in terms of seconds.</p>"},{"location":"reference/data/time/#getml.data.time.datetime","title":"<code>datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0)</code>","text":"<p>Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).</p> <p>Parameters:</p> Name Type Description Default <code>year</code> <code>int</code> <p>Year component of the date.</p> required <code>month</code> <code>int</code> <p>Month component of the date.</p> required <code>day</code> <code>int</code> <p>Day component of the date.</p> required <code>hour</code> <code>int</code> <p>Hour component of the date.</p> <code>0</code> <code>minute</code> <code>int</code> <p>Minute component of the date.</p> <code>0</code> <code>second</code> <code>int</code> <p>Second component of the date.</p> <code>0</code> <code>microsecond</code> <code>int</code> <p>Microsecond component of the date.</p> <code>0</code> Source code in <code>getml/data/time.py</code> <pre><code>def datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0):\n    \"\"\"\n    Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).\n\n    Args:\n        year (int):\n            Year component of the date.\n\n        month (int):\n            Month component of the date.\n\n        day (int):\n            Day component of the date.\n\n        hour (int, optional):\n            Hour component of the date.\n\n        minute (int, optional):\n            Minute component of the date.\n\n        second (int, optional):\n            Second component of the date.\n\n        microsecond (int, optional):\n            Microsecond component of the date.\n    \"\"\"\n    return dt.datetime(\n        year,\n        month,\n        day,\n        hour,\n        minute,\n        second,\n        microsecond,\n        tzinfo=dt.timezone.utc,\n    ).timestamp()\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.days","title":"<code>days(num)</code>","text":"<p>Expresses num days in terms of seconds.</p> <p>Parameters:</p> Name Type Description Default <code>num(float)</code> <p>The number of days.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num days expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def days(num):\n    \"\"\"\n    Expresses *num* days in terms of seconds.\n\n    Args:\n        num(float):\n            The number of days.\n\n    Returns:\n        float: *num* days expressed in terms of seconds.\n    \"\"\"\n    return hours(num) * 24.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.hours","title":"<code>hours(num)</code>","text":"<p>Expresses num hours in terms of seconds.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of hours.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num hours expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def hours(num):\n    \"\"\"\n    Expresses *num* hours in terms of seconds.\n\n    Args:\n        num (float):\n            The number of hours.\n\n    Returns:\n        float: *num* hours expressed in terms of seconds.\n    \"\"\"\n    return minutes(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.microseconds","title":"<code>microseconds(num)</code>","text":"<p>Expresses num microseconds in terms of fractions of a second.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of microseconds.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num microseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def microseconds(num):\n    \"\"\"\n    Expresses *num* microseconds in terms of fractions of a second.\n\n    Args:\n        num (float):\n            The number of microseconds.\n\n    Returns:\n        float: *num* microseconds expressed in terms of seconds.\n    \"\"\"\n    return milliseconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.milliseconds","title":"<code>milliseconds(num)</code>","text":"<p>Expresses num milliseconds in terms of fractions of a second.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of milliseconds.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num milliseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def milliseconds(num):\n    \"\"\"\n    Expresses *num* milliseconds in terms of fractions of a second.\n\n    Args:\n        num (float):\n            The number of milliseconds.\n\n    Returns:\n        float: *num* milliseconds expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.minutes","title":"<code>minutes(num)</code>","text":"<p>Expresses num minutes in terms of seconds.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of minutes.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num minutes expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def minutes(num):\n    \"\"\"\n    Expresses *num* minutes in terms of seconds.\n\n    Args:\n        num (float):\n            The number of minutes.\n\n    Returns:\n        float: *num* minutes expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.seconds","title":"<code>seconds(num)</code>","text":"<p>Transforms num into a float64.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of seconds.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num as a float64.</p> Source code in <code>getml/data/time.py</code> <pre><code>def seconds(num):\n    \"\"\"\n    Transforms *num* into a float64.\n\n    Args:\n        num (float):\n            The number of seconds.\n\n    Returns:\n        float: *num* as a float64.\n    \"\"\"\n    return np.float64(num)\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.weeks","title":"<code>weeks(num)</code>","text":"<p>Expresses num weeks in terms of seconds.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>float</code> <p>The number of weeks.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>num weeks expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def weeks(num):\n    \"\"\"\n    Expresses *num* weeks in terms of seconds.\n\n    Args:\n        num (float):\n            The number of weeks.\n\n    Returns:\n        float: *num* weeks expressed in terms of seconds.\n    \"\"\"\n    return days(num) * 7.0\n</code></pre>"},{"location":"reference/data/time_series/","title":"Time series","text":"<p>Special container for time series - abstracts away self-joins.</p>"},{"location":"reference/data/time_series/#getml.data.time_series.TimeSeries","title":"<code>TimeSeries</code>","text":"<p>               Bases: <code>StarSchema</code></p> <p>A TimeSeries is a simplifying abstraction that can be used for machine learning problems on time series data.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model. It also abstracts away the need for self joins .</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> required <code>time_stamps</code> <code>str</code> <p>The time stamps used to limit the self-join.</p> required <code>alias</code> <code>str</code> <p>The alias to be used for the population table. If it isn't set, the 'population' will be used as the alias. To explicitly set an alias for the peripheral table, use <code>with_name</code>.</p> <code>None</code> <code>peripheral</code> <code>dict</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <code>None</code> <code>split</code> <code>[`StringColumn`][getml.data.columns.columns.StringColumn] or [`StringColumnView`][getml.data.columns.columns.StringColumnView]</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <code>None</code> <code>deep_copy</code> <code>bool</code> <p>Whether you want to create deep copies or your tables.</p> <code>False</code> <code>on</code> <code>(None, string, Tuple[str] or List[Union[str, Tuple[str]]])</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <code>None</code> <code>memory</code> <code>float</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <code>None</code> <code>horizon</code> <code>float</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <code>None</code> <code>lagged_targets</code> <code>bool</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <code>False</code> <code>upper_time_stamp</code> <code>str</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <code>None</code> Example <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Source code in <code>getml/data/time_series.py</code> <pre><code>class TimeSeries(StarSchema):\n    \"\"\"\n    A TimeSeries is a simplifying abstraction that can be used\n    for machine learning problems on time series data.\n\n    It unifies [`Container`][getml.data.Container] and\n    [`DataModel`][getml.data.DataModel] thus abstracting away the need to\n    differentiate between the concrete data and the abstract data model.\n    It also abstracts away the need for\n    [self joins ](https://en.wikipedia.org/wiki/Join_(SQL)#Self-join).\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The population table defines the\n            [statistical population ](https://en.wikipedia.org/wiki/Statistical_population)\n            of the machine learning problem and contains the target variables.\n\n        time_stamps (str):\n            The time stamps used to limit the self-join.\n\n        alias (str, optional):\n            The alias to be used for the population table. If it isn't set, the 'population'\n            will be used as the alias. To explicitly set an alias for the\n            peripheral table, use [`with_name`][getml.DataFrame.with_name].\n\n        peripheral (dict, optional):\n            The peripheral tables are joined onto *population* or other\n            peripheral tables. Note that you can also pass them using\n            [`add`][getml.data.Container.add].\n\n        split ([`StringColumn`][getml.data.columns.columns.StringColumn] or [`StringColumnView`][getml.data.columns.columns.StringColumnView], optional):\n            Contains information on how you want to split *population* into\n            different [`Subset`][getml.data.Subset] s.\n            Also refer to [`split`][getml.data.split].\n\n        deep_copy (bool, optional):\n            Whether you want to create deep copies or your tables.\n\n        on (None, string, Tuple[str] or List[Union[str, Tuple[str]]], optional):\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        memory (float, optional):\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon (float, optional):\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets (bool, optional):\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp (str, optional):\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n\n    Example:\n        ```python\n        # All rows before row 10500 will be used for training.\n        split = getml.data.split.time(data_all, \"rowid\", test=10500)\n\n        time_series = getml.data.TimeSeries(\n            population=data_all,\n            time_stamps=\"rowid\",\n            split=split,\n            lagged_targets=False,\n            memory=30,\n        )\n\n        pipe = getml.Pipeline(\n            data_model=time_series.data_model,\n            feature_learners=[...],\n            predictors=...\n        )\n\n        pipe.check(time_series.train)\n\n        pipe.fit(time_series.train)\n\n        pipe.score(time_series.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # TimeSeries, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new,\n        )\n\n        # Add the data as a peripheral table, for the\n        # self-join.\n        container.add(population=population_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        population,\n        time_stamps,\n        alias=None,\n        alias2=None,\n        peripheral=None,\n        split=None,\n        deep_copy=False,\n        on=None,\n        memory=None,\n        horizon=None,\n        lagged_targets=False,\n        upper_time_stamp=None,\n    ):\n\n        if not isinstance(population, (DataFrame, View)):\n            raise TypeError(\n                \"'population' must be a getml.DataFrame or a getml.data.View\"\n            )\n\n        if isinstance(time_stamps, FloatColumn):\n            time_stamps = time_stamps.name\n\n        if isinstance(time_stamps, FloatColumnView):\n            if \"rowid\" in _finditems(\"operator_\", time_stamps.cmd):\n                time_stamps = \"rowid\"\n\n        population = (\n            population.with_column(\n                population.rowid, name=\"rowid\", role=time_stamp\n            ).with_unit(names=\"rowid\", unit=\"rowid\", comparison_only=True)\n            if time_stamps == \"rowid\"\n            else population\n        )\n\n        alias = \"population\" if alias is None else alias\n\n        super().__init__(\n            population=population,\n            alias=alias,\n            peripheral=peripheral,\n            split=split,\n            deep_copy=deep_copy,\n        )\n\n        self.on = on\n        self.time_stamps = time_stamps\n        self.memory = memory\n        self.horizon = horizon\n        self.lagged_targets = lagged_targets\n        self.upper_time_stamp = upper_time_stamp\n\n        if not isinstance(on, list):\n            on = [on]\n\n        for o in on:\n            self._add_joins(o)\n\n    def _add_joins(self, on):\n        self.join(\n            right_df=self.population,\n            alias=self.population.name,\n            on=self.on,\n            time_stamps=self.time_stamps,\n            memory=self.memory,\n            horizon=self.horizon,\n            lagged_targets=self.lagged_targets,\n            upper_time_stamp=self.upper_time_stamp,\n        )\n</code></pre>"},{"location":"reference/data/view/","title":"View","text":"<p>Contains the view.</p>"},{"location":"reference/data/view/#getml.data.view.View","title":"<code>View</code>","text":"<p>A view is a lazily evaluated, immutable representation of a <code>DataFrame</code>.</p> <p>There are important differences between a <code>DataFrame</code> and a view:</p> <ul> <li> <p>Views are lazily evaluated. That means that views do not   contain any data themselves. Instead, they just refer to   an underlying data frame. If the underlying data frame changes,   so will the view (but such behavior will result in a warning).</p> </li> <li> <p>Views are immutable. In-place operations on a view are not   possible. Any operation on a view will result in a new view.</p> </li> <li> <p>Views have no direct representation on the getML engine, and   therefore they do not need to have an identifying name.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>A data frame or view used as the basis for this view.</p> required <code>name</code> <code>str</code> <p>The name assigned to this view.</p> <code>None</code> <code>subselection</code> <code>[`BooleanColumnView`][getml.data.columns.BooleanColumnView], [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Indicates which rows we would like to keep.</p> <code>None</code> <code>added</code> <code>dict</code> <p>A dictionary that describes a new column that has been added to the view.</p> <code>None</code> <code>dropped</code> <code>List[str]</code> <p>A list of columns that have been dropped.</p> <code>None</code> Example <p>You hardly ever directly create views. Instead, it is more likely that you will encounter them as a result of some operation on a <code>DataFrame</code>:</p> <p><pre><code># Creates a view on the first 100 lines\nview1 = data_frame[:100]\n\n# Creates a view without some columns.\nview2 = data_frame.drop([\"col1\", \"col2\"])\n\n# Creates a view in which some roles are reassigned.\nview3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n</code></pre> A recommended pattern is to assign 'baseline roles' to your data frames and then using views to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which <code>Container</code> you have used.</p> Source code in <code>getml/data/view.py</code> <pre><code>class View:\n    \"\"\"A view is a lazily evaluated, immutable representation of a [`DataFrame`][getml.DataFrame].\n\n    There are important differences between a\n    [`DataFrame`][getml.DataFrame] and a view:\n\n    - Views are lazily evaluated. That means that views do not\n      contain any data themselves. Instead, they just refer to\n      an underlying data frame. If the underlying data frame changes,\n      so will the view (but such behavior will result in a warning).\n\n    - Views are immutable. In-place operations on a view are not\n      possible. Any operation on a view will result in a new view.\n\n    - Views have no direct representation on the getML engine, and\n      therefore they do not need to have an identifying name.\n\n    Args:\n        base ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            A data frame or view used as the basis for this view.\n\n        name (str):\n            The name assigned to this view.\n\n        subselection ([`BooleanColumnView`][getml.data.columns.BooleanColumnView], [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Indicates which rows we would like to keep.\n\n        added (dict):\n            A dictionary that describes a new column\n            that has been added to the view.\n\n        dropped (List[str]):\n            A list of columns that have been dropped.\n\n    Example:\n        You hardly ever directly create views. Instead, it is more likely\n        that you will encounter them as a result of some operation on a\n        [`DataFrame`][getml.DataFrame]:\n\n        ```python\n\n        # Creates a view on the first 100 lines\n        view1 = data_frame[:100]\n\n        # Creates a view without some columns.\n        view2 = data_frame.drop([\"col1\", \"col2\"])\n\n        # Creates a view in which some roles are reassigned.\n        view3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        ```\n        A recommended pattern is to assign 'baseline roles' to your data frames\n        and then using views to tweak them:\n\n        ```python\n        # Assign baseline roles\n        data_frame.set_role([\"jk\"], getml.data.roles.join_key)\n        data_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n        data_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\n        data_frame.set_role([\"col5\"], getml.data.roles.target)\n\n        # Make the data frame immutable, so in-place operations are\n        # no longer possible.\n        data_frame.freeze()\n\n        # Save the data frame.\n        data_frame.save()\n\n        # I suspect that col1 leads to overfitting, so I will drop it.\n        view = data_frame.drop([\"col1\"])\n\n        # Insert the view into a container.\n        container = getml.data.Container(...)\n        container.add(some_alias=view)\n        container.save()\n        ```\n\n        The advantage of using such a pattern is that it enables you to\n        always completely retrace your entire pipeline without creating\n        deep copies of the data frames whenever you have made a small\n        change like the one in our example. Note that the pipeline will\n        record which [`Container`][getml.data.Container] you have used.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    def __init__(\n        self,\n        base,\n        name: Optional[str] = None,\n        subselection: Union[\n            BooleanColumnView, FloatColumn, FloatColumnView, None\n        ] = None,\n        added=None,\n        dropped: Optional[List[str]] = None,\n    ):\n        self._added = added\n        self._base = deepcopy(base)\n        self._dropped = dropped or []\n        self._name = name\n        self._subselection = subselection\n\n        self._initial_timestamp: str = (\n            self._base._initial_timestamp\n            if isinstance(self._base, View)\n            else self._base.last_change\n        )\n\n        self._base.refresh()\n\n    # ------------------------------------------------------------\n\n    def _apply_subselection(self, col):\n        if self._subselection is not None:\n            return col[self._subselection]\n        return col\n\n    # ------------------------------------------------------------\n\n    def _collect_footer_data(self):\n        footer = namedtuple(\"footer\", [\"n_rows\", \"n_cols\", \"type\"])\n\n        n_rows = \"unknown number of\" if self.nrows() == \"unknown\" else self.nrows()\n\n        return footer(\n            n_rows=n_rows,\n            n_cols=len(self.colnames),\n            type=\"getml.data.View\",\n        )\n\n    # ------------------------------------------------------------\n\n    def _format(self):\n        return _ViewFormatter(self)\n\n    # ----------------------------------------------------------------\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            return super().__getattribute__(name)\n\n    # ------------------------------------------------------------\n\n    def __getitem__(self, name):\n        if isinstance(\n            name,\n            (numbers.Integral, slice, BooleanColumnView, FloatColumn, FloatColumnView),\n        ):\n            return self.where(index=name)\n\n        if isinstance(name, list):\n            not_in_colnames = set(name) - set(self.colnames)\n            if not_in_colnames:\n                raise KeyError(f\"{list(not_in_colnames)} not found.\")\n            dropped = [col for col in self.colnames if col not in name]\n            return View(base=self, dropped=dropped)\n\n        if name in self.__dict__[\"_dropped\"]:\n            raise KeyError(\n                \"Cannot retrieve column '\" + name + \"'. It has been dropped.\"\n            )\n\n        if (\n            self.__dict__[\"_added\"] is not None\n            and self.__dict__[\"_added\"][\"name_\"] == name\n        ):\n            return self._apply_subselection(\n                self.__dict__[\"_added\"][\"col_\"]\n                .with_subroles(self.__dict__[\"_added\"][\"subroles_\"], append=False)\n                .with_unit(self.__dict__[\"_added\"][\"unit_\"])\n            )\n\n        return self._apply_subselection(self.__dict__[\"_base\"][name])\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return self.nrows(force=True)\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View\"\n\n        if self._added is not None:\n            added = deepcopy(self._added)\n            col = deepcopy(added[\"col_\"].cmd)\n            col[\"last_change_\"] = added[\"col_\"].last_change\n            added[\"col_\"] = col\n            cmd[\"added_\"] = added\n\n        if self._subselection is not None:\n            subselection = deepcopy(self._subselection.cmd)\n            subselection[\"last_change_\"] = self.subselection.last_change\n            cmd[\"subselection_\"] = subselection\n\n        cmd[\"base_\"] = self._base._getml_deserialize()\n        cmd[\"dropped_\"] = self._dropped\n        cmd[\"name_\"] = self.name\n        cmd[\"last_change_\"] = self.last_change\n\n        return cmd\n\n    # ------------------------------------------------------------\n\n    def _modify_colnames(self, base_names, role):\n        remove_dropped = [name for name in base_names if name not in self._dropped]\n\n        if self._added is not None and self._added[\"role_\"] != role:\n            return [name for name in remove_dropped if name != self._added[\"name_\"]]\n\n        if (\n            self._added is not None\n            and self._added[\"role_\"] == role\n            and self._added[\"name_\"] not in remove_dropped\n        ):\n            return remove_dropped + [self._added[\"name_\"]]\n\n        return remove_dropped\n\n    # ------------------------------------------------------------\n\n    def __repr__(self):\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_string(footer=footer)\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        formatted = self._format()\n\n        footer = self._collect_footer_data()\n\n        return formatted._render_html(footer=footer)\n\n    # ------------------------------------------------------------\n\n    @property\n    def added(self):\n        \"\"\"\n        The column that has been added to the view.\n        \"\"\"\n        return deepcopy(self._added)\n\n    # ------------------------------------------------------------\n\n    @property\n    def base(self):\n        \"\"\"\n        The basis on which the view is created. Must be a\n        [`DataFrame`][getml.DataFrame] or a [`View`][getml.data.View].\n        \"\"\"\n        return deepcopy(self._base)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _categorical_names(self):\n        return self._modify_colnames(self._base._categorical_names, categorical)\n\n    # ------------------------------------------------------------\n\n    def check(self):\n        \"\"\"\n        Checks whether the underlying data frame has been changed\n        after the creation of the view.\n        \"\"\"\n        last_change = self.last_change\n        if last_change != self.__dict__[\"_initial_timestamp\"]:\n            logger.warning(\n                \"The data frame underlying view '\"\n                + self.name\n                + \"' was last changed at \"\n                + last_change\n                + \", which was after the creation of the view. \"\n                + \"This might lead to unexpected results. You might \"\n                + \"want to recreate the view. (Views are lazily \"\n                + \"evaluated, so recreating them is a very \"\n                + \"inexpensive operation).\"\n            )\n\n    # ------------------------------------------------------------\n\n    @property\n    def colnames(self):\n        \"\"\"\n        List of the names of all columns.\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return (\n            self._time_stamp_names\n            + self._join_key_names\n            + self._target_names\n            + self._categorical_names\n            + self._numerical_names\n            + self._text_names\n            + self._unused_names\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self):\n        \"\"\"\n        Alias for [`colnames`][getml.data.View.colnames].\n\n        Returns:\n            List[str]:\n                List of the names of all columns.\n        \"\"\"\n        return self.colnames\n\n    # ------------------------------------------------------------\n\n    def drop(self, cols):\n        \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n        Args:\n            cols (str or List[str]):\n                The names of the columns to be dropped.\n        \"\"\"\n        if isinstance(cols, str):\n            cols = [cols]\n\n        if not _is_typed_list(cols, str):\n            raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n        return View(base=self, dropped=cols)\n\n    # ------------------------------------------------------------\n\n    @property\n    def dropped(self):\n        \"\"\"\n        The names of the columns that has been dropped.\n        \"\"\"\n        return deepcopy(self._dropped)\n\n    # ------------------------------------------------------------\n\n    @property\n    def last_change(self) -&gt; str:\n        \"\"\"\n        A string describing the last time this data frame has been changed.\n        \"\"\"\n        return self.__dict__[\"_base\"].last_change\n\n    # ------------------------------------------------------------\n\n    @property\n    def _join_key_names(self):\n        return self._modify_colnames(self._base._join_key_names, join_key)\n\n    # ------------------------------------------------------------\n\n    @property\n    def name(self):\n        \"\"\"\n        The name of the view. If no name is explicitly set,\n        the name will be identical to the name of the base.\n        \"\"\"\n        if self.__dict__[\"_name\"] is None:\n            return self.__dict__[\"_base\"].name\n        return deepcopy(self.__dict__[\"_name\"])\n\n    # ------------------------------------------------------------\n\n    def ncols(self):\n        \"\"\"\n        Number of columns in the current instance.\n\n        Returns:\n            int:\n                Overall number of columns\n        \"\"\"\n        return len(self.colnames)\n\n    # ------------------------------------------------------------\n\n    def nrows(self, force=False):\n        \"\"\"\n        Returns the number of rows in the current instance.\n\n        Args:\n            force (bool, optional):\n                If the number of rows is unknown,\n                do you want to force the engine to calculate it anyway?\n                This is a relatively expensive operation, therefore\n                you might not necessarily want this.\n        \"\"\"\n\n        self.refresh()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.get_nrows\"\n        cmd[\"name_\"] = \"\"\n\n        cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n        cmd[\"force_\"] = force\n\n        with comm.send_and_get_socket(cmd) as sock:\n            json_str = comm.recv_string(sock)\n\n        if json_str[0] != \"{\":\n            comm.engine_exception_handler(json_str)\n\n        result = json.loads(json_str)\n\n        if \"recordsTotal\" in result:\n            return int(result[\"recordsTotal\"])\n\n        return \"unknown\"\n\n    # ------------------------------------------------------------\n\n    @property\n    def _numerical_names(self):\n        return self._modify_colnames(self._base._numerical_names, numerical)\n\n    # --------------------------------------------------------------------------\n\n    def refresh(self):\n        \"\"\"Aligns meta-information of the current instance with the\n        corresponding data frame in the getML engine.\n\n        Returns:\n            [`View`][getml.data.View]:\n                Updated handle the underlying data frame in the getML\n                engine.\n\n        \"\"\"\n        self._base = self.__dict__[\"_base\"].refresh()\n        return self\n\n    # ------------------------------------------------------------\n\n    @property\n    def roles(self):\n        \"\"\"\n        The roles of the columns included\n        in this View.\n        \"\"\"\n        return Roles(\n            categorical=self._categorical_names,\n            join_key=self._join_key_names,\n            numerical=self._numerical_names,\n            target=self._target_names,\n            text=self._text_names,\n            time_stamp=self._time_stamp_names,\n            unused_float=self._unused_float_names,\n            unused_string=self._unused_string_names,\n        )\n\n    # ------------------------------------------------------------\n\n    @property\n    def rowid(self):\n        \"\"\"\n        The rowids for this view.\n        \"\"\"\n        return rowid()[: len(self)]\n\n    # ------------------------------------------------------------\n\n    @property\n    def subselection(self):\n        \"\"\"\n        The subselection that is applied to this view.\n        \"\"\"\n        return deepcopy(self._subselection)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _target_names(self):\n        return self._modify_colnames(self._base._target_names, target)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _text_names(self):\n        return self._modify_colnames(self._base._text_names, text)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _time_stamp_names(self):\n        return self._modify_colnames(self._base._time_stamp_names, time_stamp)\n\n    # ------------------------------------------------------------\n\n    @property\n    def shape(self):\n        \"\"\"\n        A tuple containing the number of rows and columns of\n        the View.\n        \"\"\"\n        self.refresh()\n        return (self.nrows(), self.ncols())\n\n    # ------------------------------------------------------------\n\n    def to_arrow(self):\n        \"\"\"Creates a `pyarrow.Table` from the view.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyarrow.Table`.\n\n        Returns:\n            `pyarrow.Table`:\n                Pyarrow equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self)\n\n    # ------------------------------------------------------------\n\n    def to_json(self):\n        \"\"\"Creates a JSON string from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a JSON string.\n        \"\"\"\n        return self.to_pandas().to_json()\n\n    # ------------------------------------------------------------\n\n    def to_csv(\n        self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file.\n\n        Args:\n            fname (str):\n                The name of the CSV file.\n                The ending \".csv\" and an optional batch number will\n                be added automatically.\n\n            quotechar (str, optional):\n                The character used to wrap strings.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(fname, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(quotechar, str):\n            raise TypeError(\"'quotechar' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        fname_ = os.path.abspath(fname)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_csv\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"fname_\"] = fname_\n        cmd[\"quotechar_\"] = quotechar\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_db(self, table_name: str, conn: Optional[Connection] = None):\n        \"\"\"Writes the underlying data into a newly created table in the\n        database.\n\n        Args:\n            table_name (str):\n                Name of the table to be created.\n\n                If a table of that name already exists, it will be\n                replaced.\n\n            conn ([`Connection`][getml.database.Connection], optional):\n                The database connection to be used.\n                If you don't explicitly pass a connection,\n                the engine will use the default connection.\n        \"\"\"\n\n        conn = conn or Connection()\n\n        self.refresh()\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        if not isinstance(conn, Connection):\n            raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_db\"\n        cmd[\"name_\"] = \"\"\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"table_name_\"] = table_name\n        cmd[\"conn_id_\"] = conn.conn_id\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def to_pandas(self):\n        \"\"\"Creates a `pandas.DataFrame` from the view.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pandas.DataFrame`.\n\n        Returns:\n            `pandas.DataFrame`:\n                Pandas equivalent of the current instance including\n                its underlying data.\n        \"\"\"\n        return _to_arrow(self).to_pandas()\n\n    # ------------------------------------------------------------\n\n    def to_placeholder(self, name=None):\n        \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n        current [`View`][getml.data.View].\n\n        Args:\n            name (str, optional):\n                The name of the placeholder. If no\n                name is passed, then the name of the placeholder will\n                be identical to the name of the current view.\n\n        Returns:\n            [`Placeholder`][getml.data.Placeholder]:\n                A placeholder with the same name as this data frame.\n\n\n        \"\"\"\n        self.refresh()\n        return Placeholder(name=name or self.name, roles=self.roles)\n\n    # ------------------------------------------------------------\n\n    def to_parquet(self, fname, compression=\"snappy\"):\n        \"\"\"\n        Writes the underlying data into a newly created parquet file.\n\n        Args:\n            fname (str):\n                The name of the parquet file.\n                The ending \".parquet\" will be added automatically.\n\n            compression (str):\n                The compression format to use.\n                Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n        \"\"\"\n        _to_parquet(self, fname, compression)\n\n    # ----------------------------------------------------------------\n\n    def to_pyspark(self, spark, name=None):\n        \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n        Loads the underlying data from the getML engine and constructs\n        a `pyspark.sql.DataFrame`.\n\n        Args:\n            spark (pyspark.sql.SparkSession):\n                The pyspark session in which you want to\n                create the data frame.\n\n            name (str or None):\n                The name of the temporary view to be created on top\n                of the `pyspark.sql.DataFrame`,\n                with which it can be referred to\n                in Spark SQL (refer to\n                `pyspark.sql.DataFrame.createOrReplaceTempView`).\n                If none is passed, then the name of this\n                [`DataFrame`][getml.DataFrame] will be used.\n\n        Returns:\n            pyspark.sql.DataFrame:\n                Pyspark equivalent of the current instance including\n                its underlying data.\n\n        \"\"\"\n        return _to_pyspark(self, name, spark)\n\n    # ------------------------------------------------------------\n\n    def to_s3(\n        self,\n        bucket: str,\n        key: str,\n        region: str,\n        sep: str = \",\",\n        batch_size: int = 50000,\n    ):\n        \"\"\"\n        Writes the underlying data into a newly created CSV file\n        located in an S3 bucket.\n        Note:\n            S3 is not supported on Windows.\n\n        Args:\n            bucket (str):\n                The bucket from which to read the files.\n\n            key (str):\n                The key in the S3 bucket in which you want to\n                write the output. The ending \".csv\" and an optional\n                batch number will be added automatically.\n\n            region (str):\n                The region in which the bucket is located.\n\n            sep (str, optional):\n                The character used for separating fields.\n\n            batch_size (int, optional):\n                Maximum number of lines per file. Set to 0 to read\n                the entire data frame into a single file.\n\n        Example:\n            ```python\n            getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n            getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n            your_view.to_s3(\n                bucket=\"your-bucket-name\",\n                key=\"filename-on-s3\",\n                region=\"us-east-2\",\n                sep=';'\n            )\n            ```\n        \"\"\"\n\n        self.refresh()\n\n        if not isinstance(bucket, str):\n            raise TypeError(\"'bucket' must be of type str\")\n\n        if not isinstance(key, str):\n            raise TypeError(\"'fname' must be of type str\")\n\n        if not isinstance(region, str):\n            raise TypeError(\"'region' must be of type str\")\n\n        if not isinstance(sep, str):\n            raise TypeError(\"'sep' must be of type str\")\n\n        if not isinstance(batch_size, numbers.Real):\n            raise TypeError(\"'batch_size' must be a real number\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"View.to_s3\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"view_\"] = self._getml_deserialize()\n        cmd[\"bucket_\"] = bucket\n        cmd[\"key_\"] = key\n        cmd[\"region_\"] = region\n        cmd[\"sep_\"] = sep\n        cmd[\"batch_size_\"] = batch_size\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_float_names(self):\n        return self._modify_colnames(self._base._unused_float_names, unused_float)\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_names(self):\n        return self._unused_float_names + self._unused_string_names\n\n    # ------------------------------------------------------------\n\n    @property\n    def _unused_string_names(self):\n        return self._modify_colnames(self._base._unused_string_names, unused_string)\n\n    # ------------------------------------------------------------\n\n    def where(self, index) -&gt; \"View\":\n        \"\"\"Extract a subset of rows.\n\n        Creates a new [`View`][getml.data.View] as a\n        subselection of the current instance.\n\n        Args:\n            index ([`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n                Boolean column indicating the rows you want to select.\n\n        Example:\n            Generate example data:\n            ```python\n            data = dict(\n                fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n                price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n                join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n            fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n            roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n            fruits\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 0        | banana      | 2.4       |\n            | 1        | apple       | 3         |\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            | 3        | melon       | 3.4       |\n            | 3        | pineapple   | 3.4       |\n            ```\n            Apply where condition. This creates a new DataFrame called \"cherries\":\n\n            ```python\n\n            cherries = fruits.where(\n                fruits[\"fruit\"] == \"cherry\")\n\n            cherries\n            ```\n            ```\n            | join_key | fruit       | price     |\n            | join key | categorical | numerical |\n            --------------------------------------\n            | 2        | cherry      | 1.2       |\n            | 2        | cherry      | 1.4       |\n            ```\n        \"\"\"\n        if isinstance(index, numbers.Integral):\n            index = index if int(index) &gt; 0 else len(self) + index\n            selector = arange(index, index + 1)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, slice):\n            start, stop, step = _make_default_slice(index, len(self))\n            selector = arange(start, stop, step)\n            return View(base=self, subselection=selector)\n\n        if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n            return View(base=self, subselection=index)\n\n        raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n\n    # ------------------------------------------------------------\n\n    def with_column(\n        self, col, name, role=None, unit=\"\", subroles=None, time_formats=None\n    ):\n        \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n        Args:\n            col ([`column`][getml.column]):\n                The column to be added.\n\n            name (str):\n                Name of the new column.\n\n            role (str, optional):\n                Role of the new column. Must be from [`roles`][getml.data.roles].\n\n            subroles (str, List[str] or None, optional):\n                Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n            unit (str, optional):\n                Unit of the column.\n\n            time_formats (str, optional):\n                Formats to be used to parse the time stamps.\n\n                This is only necessary, if an implicit conversion from\n                a [`StringColumn`][getml.data.columns.StringColumn] to a time\n                stamp is taking place.\n\n                The formats are allowed to contain the following\n                special characters:\n\n                * %w - abbreviated weekday (Mon, Tue, ...)\n                * %W - full weekday (Monday, Tuesday, ...)\n                * %b - abbreviated month (Jan, Feb, ...)\n                * %B - full month (January, February, ...)\n                * %d - zero-padded day of month (01 .. 31)\n                * %e - day of month (1 .. 31)\n                * %f - space-padded day of month ( 1 .. 31)\n                * %m - zero-padded month (01 .. 12)\n                * %n - month (1 .. 12)\n                * %o - space-padded month ( 1 .. 12)\n                * %y - year without century (70)\n                * %Y - year with century (1970)\n                * %H - hour (00 .. 23)\n                * %h - hour (00 .. 12)\n                * %a - am/pm\n                * %A - AM/PM\n                * %M - minute (00 .. 59)\n                * %S - second (00 .. 59)\n                * %s - seconds and microseconds (equivalent to %S.%F)\n                * %i - millisecond (000 .. 999)\n                * %c - centisecond (0 .. 9)\n                * %F - fractional seconds/microseconds (000000 - 999999)\n                * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n                * %Z - time zone differential in RFC format (GMT or +NNNN)\n                * %% - percent sign\n        \"\"\"\n        col, role, subroles = _with_column(\n            col, name, role, subroles, unit, time_formats\n        )\n        return View(\n            base=self,\n            added={\n                \"col_\": col,\n                \"name_\": name,\n                \"role_\": role,\n                \"subroles_\": subroles,\n                \"unit_\": unit,\n            },\n        )\n\n    # ------------------------------------------------------------\n\n    def with_name(self, name):\n        \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n        Args:\n            name (str):\n                The name of the new view.\n        \"\"\"\n        return View(base=self, name=name)\n\n    # ------------------------------------------------------------\n\n    def with_role(self, names, role, time_formats=None):\n        \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n        When switching from a role based on type float to a role based on type\n        string or vice verse, an implicit type conversion will be conducted.\n        The `time_formats` argument is used to interpret time\n        format string: `annotating_roles_time_stamp`. For more information on\n        roles, please refer to the [User Guide][annotating-data].\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            role (str):\n                The role to be assigned.\n\n            time_formats (str or List[str], optional):\n                Formats to be used to parse the time stamps.\n                This is only necessary, if an implicit conversion from a StringColumn to\n                a time stamp is taking place.\n        \"\"\"\n        return _with_role(self, names, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    def with_subroles(self, names, subroles, append=True):\n        \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            subroles (str or List[str]):\n                The subroles to be assigned.\n\n            append (bool, optional):\n                Whether you want to append the\n                new subroles to the existing subroles.\n        \"\"\"\n        return _with_subroles(self, names, subroles, append)\n\n    # ------------------------------------------------------------\n\n    def with_unit(self, names, unit, comparison_only=False):\n        \"\"\"Returns a view that contains a new unit on one or more columns.\n\n        Args:\n            names (str or List[str]):\n                The name or names of the column.\n\n            unit (str):\n                The unit to be assigned.\n\n            comparison_only (bool):\n                Whether you want the column to\n                be used for comparison only. This means that the column can\n                only be used in comparison to other columns of the same unit.\n\n                An example might be a bank account number: The number in itself\n                is hardly interesting, but it might be useful to know how often\n                we have seen that same bank account number in another table.\n\n                If True, this will append \", comparison only\" to the unit.\n                The feature learning algorithms and the feature selectors will\n                interpret this accordingly.\n        \"\"\"\n        return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.added","title":"<code>added</code>  <code>property</code>","text":"<p>The column that has been added to the view.</p>"},{"location":"reference/data/view/#getml.data.view.View.base","title":"<code>base</code>  <code>property</code>","text":"<p>The basis on which the view is created. Must be a <code>DataFrame</code> or a <code>View</code>.</p>"},{"location":"reference/data/view/#getml.data.view.View.colnames","title":"<code>colnames</code>  <code>property</code>","text":"<p>List of the names of all columns.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.view.View.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>Alias for <code>colnames</code>.</p> <p>Returns:</p> Type Description <p>List[str]: List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.view.View.dropped","title":"<code>dropped</code>  <code>property</code>","text":"<p>The names of the columns that has been dropped.</p>"},{"location":"reference/data/view/#getml.data.view.View.last_change","title":"<code>last_change: str</code>  <code>property</code>","text":"<p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/view/#getml.data.view.View.name","title":"<code>name</code>  <code>property</code>","text":"<p>The name of the view. If no name is explicitly set, the name will be identical to the name of the base.</p>"},{"location":"reference/data/view/#getml.data.view.View.roles","title":"<code>roles</code>  <code>property</code>","text":"<p>The roles of the columns included in this View.</p>"},{"location":"reference/data/view/#getml.data.view.View.rowid","title":"<code>rowid</code>  <code>property</code>","text":"<p>The rowids for this view.</p>"},{"location":"reference/data/view/#getml.data.view.View.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>A tuple containing the number of rows and columns of the View.</p>"},{"location":"reference/data/view/#getml.data.view.View.subselection","title":"<code>subselection</code>  <code>property</code>","text":"<p>The subselection that is applied to this view.</p>"},{"location":"reference/data/view/#getml.data.view.View.check","title":"<code>check()</code>","text":"<p>Checks whether the underlying data frame has been changed after the creation of the view.</p> Source code in <code>getml/data/view.py</code> <pre><code>def check(self):\n    \"\"\"\n    Checks whether the underlying data frame has been changed\n    after the creation of the view.\n    \"\"\"\n    last_change = self.last_change\n    if last_change != self.__dict__[\"_initial_timestamp\"]:\n        logger.warning(\n            \"The data frame underlying view '\"\n            + self.name\n            + \"' was last changed at \"\n            + last_change\n            + \", which was after the creation of the view. \"\n            + \"This might lead to unexpected results. You might \"\n            + \"want to recreate the view. (Views are lazily \"\n            + \"evaluated, so recreating them is a very \"\n            + \"inexpensive operation).\"\n        )\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.drop","title":"<code>drop(cols)</code>","text":"<p>Returns a new <code>View</code> that has one or several columns removed.</p> <p>Parameters:</p> Name Type Description Default <code>cols</code> <code>str or List[str]</code> <p>The names of the columns to be dropped.</p> required Source code in <code>getml/data/view.py</code> <pre><code>def drop(self, cols):\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols (str or List[str]):\n            The names of the columns to be dropped.\n    \"\"\"\n    if isinstance(cols, str):\n        cols = [cols]\n\n    if not _is_typed_list(cols, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=cols)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.ncols","title":"<code>ncols()</code>","text":"<p>Number of columns in the current instance.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/view.py</code> <pre><code>def ncols(self):\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n        int:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.nrows","title":"<code>nrows(force=False)</code>","text":"<p>Returns the number of rows in the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If the number of rows is unknown, do you want to force the engine to calculate it anyway? This is a relatively expensive operation, therefore you might not necessarily want this.</p> <code>False</code> Source code in <code>getml/data/view.py</code> <pre><code>def nrows(self, force=False):\n    \"\"\"\n    Returns the number of rows in the current instance.\n\n    Args:\n        force (bool, optional):\n            If the number of rows is unknown,\n            do you want to force the engine to calculate it anyway?\n            This is a relatively expensive operation, therefore\n            you might not necessarily want this.\n    \"\"\"\n\n    self.refresh()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.get_nrows\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n    cmd[\"force_\"] = force\n\n    with comm.send_and_get_socket(cmd) as sock:\n        json_str = comm.recv_string(sock)\n\n    if json_str[0] != \"{\":\n        comm.engine_exception_handler(json_str)\n\n    result = json.loads(json_str)\n\n    if \"recordsTotal\" in result:\n        return int(result[\"recordsTotal\"])\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.refresh","title":"<code>refresh()</code>","text":"<p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> <p>Returns:</p> Type Description <p><code>View</code>: Updated handle the underlying data frame in the getML engine.</p> Source code in <code>getml/data/view.py</code> <pre><code>def refresh(self):\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    Returns:\n        [`View`][getml.data.View]:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n    self._base = self.__dict__[\"_base\"].refresh()\n    return self\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Creates a <code>pyarrow.Table</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> <p>Returns:</p> Type Description <p><code>pyarrow.Table</code>: Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_arrow(self):\n    \"\"\"Creates a `pyarrow.Table` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n        `pyarrow.Table`:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_csv","title":"<code>to_csv(fname, quotechar='\"', sep=',', batch_size=0)</code>","text":"<p>Writes the underlying data into a newly created CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>0</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname (str):\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_db","title":"<code>to_db(table_name, conn=None)</code>","text":"<p>Writes the underlying data into a newly created table in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name (str):\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    if not isinstance(conn, Connection):\n        raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_db\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"table_name_\"] = table_name\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_json","title":"<code>to_json()</code>","text":"<p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Creates a <code>pandas.DataFrame</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pandas.DataFrame</code>.</p> <p>Returns:</p> Type Description <p><code>pandas.DataFrame</code>: Pandas equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pandas(self):\n    \"\"\"Creates a `pandas.DataFrame` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pandas.DataFrame`.\n\n    Returns:\n        `pandas.DataFrame`:\n            Pandas equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_parquet","title":"<code>to_parquet(fname, compression='snappy')</code>","text":"<p>Writes the underlying data into a newly created parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> required <code>compression</code> <code>str</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <code>'snappy'</code> Source code in <code>getml/data/view.py</code> <pre><code>def to_parquet(self, fname, compression=\"snappy\"):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname (str):\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression (str):\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_placeholder","title":"<code>to_placeholder(name=None)</code>","text":"<p>Generates a <code>Placeholder</code> from the current <code>View</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current view.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>Placeholder</code>: A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_placeholder(self, name=None):\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`View`][getml.data.View].\n\n    Args:\n        name (str, optional):\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current view.\n\n    Returns:\n        [`Placeholder`][getml.data.Placeholder]:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_pyspark","title":"<code>to_pyspark(spark, name=None)</code>","text":"<p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The pyspark session in which you want to create the data frame.</p> required <code>name</code> <code>str or None</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <code>None</code> <p>Returns:</p> Type Description <p>pyspark.sql.DataFrame: Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pyspark(self, spark, name=None):\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark (pyspark.sql.SparkSession):\n            The pyspark session in which you want to\n            create the data frame.\n\n        name (str or None):\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n        pyspark.sql.DataFrame:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.to_s3","title":"<code>to_s3(bucket, key, region, sep=',', batch_size=50000)</code>","text":"<p>Writes the underlying data into a newly created CSV file located in an S3 bucket. Note:     S3 is not supported on Windows.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>key</code> <code>str</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>batch_size</code> <code>int</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <code>50000</code> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_view.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/view.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n    Note:\n        S3 is not supported on Windows.\n\n    Args:\n        bucket (str):\n            The bucket from which to read the files.\n\n        key (str):\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region (str):\n            The region in which the bucket is located.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        batch_size (int, optional):\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_view.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.where","title":"<code>where(index)</code>","text":"<p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>[`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]</code> <p>Boolean column indicating the rows you want to select.</p> required Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/view.py</code> <pre><code>def where(self, index) -&gt; \"View\":\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index ([`BooleanColumnView`][getml.data.columns.BooleanColumnView] or [`FloatColumnView`][getml.data.columns.FloatColumnView] or [`FloatColumn`][getml.data.columns.FloatColumn]):\n            Boolean column indicating the rows you want to select.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(index, index + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, step = _make_default_slice(index, len(self))\n        selector = arange(start, stop, step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.with_column","title":"<code>with_column(col, name, role=None, unit='', subroles=None, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> that contains an additional column.</p> <p>Parameters:</p> Name Type Description Default <code>col</code> <code>[`column`][getml.column]</code> <p>The column to be added.</p> required <code>name</code> <code>str</code> <p>Name of the new column.</p> required <code>role</code> <code>str</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <code>None</code> <code>subroles</code> <code>(str, List[str] or None)</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <code>None</code> <code>unit</code> <code>str</code> <p>Unit of the column.</p> <code>''</code> <code>time_formats</code> <code>str</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_column(\n    self, col, name, role=None, unit=\"\", subroles=None, time_formats=None\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col ([`column`][getml.column]):\n            The column to be added.\n\n        name (str):\n            Name of the new column.\n\n        role (str, optional):\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles (str, List[str] or None, optional):\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit (str, optional):\n            Unit of the column.\n\n        time_formats (str, optional):\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.with_name","title":"<code>with_name(name)</code>","text":"<p>Returns a new <code>View</code> with a new name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new view.</p> required Source code in <code>getml/data/view.py</code> <pre><code>def with_name(self, name):\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.with_role","title":"<code>with_role(names, role, time_formats=None)</code>","text":"<p>Returns a new <code>View</code> with modified roles.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>role</code> <code>str</code> <p>The role to be assigned.</p> required <code>time_formats</code> <code>str or List[str]</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <code>None</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_role(self, names, role, time_formats=None):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        role (str):\n            The role to be assigned.\n\n        time_formats (str or List[str], optional):\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, names, role, time_formats)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.with_subroles","title":"<code>with_subroles(names, subroles, append=True)</code>","text":"<p>Returns a new view with one or several new subroles on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>subroles</code> <code>str or List[str]</code> <p>The subroles to be assigned.</p> required <code>append</code> <code>bool</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <code>True</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_subroles(self, names, subroles, append=True):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        subroles (str or List[str]):\n            The subroles to be assigned.\n\n        append (bool, optional):\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, names, subroles, append)\n</code></pre>"},{"location":"reference/data/view/#getml.data.view.View.with_unit","title":"<code>with_unit(names, unit, comparison_only=False)</code>","text":"<p>Returns a view that contains a new unit on one or more columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>str or List[str]</code> <p>The name or names of the column.</p> required <code>unit</code> <code>str</code> <p>The unit to be assigned.</p> required <code>comparison_only</code> <code>bool</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will append \", comparison only\" to the unit. The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <code>False</code> Source code in <code>getml/data/view.py</code> <pre><code>def with_unit(self, names, unit, comparison_only=False):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    Args:\n        names (str or List[str]):\n            The name or names of the column.\n\n        unit (str):\n            The unit to be assigned.\n\n        comparison_only (bool):\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will append \", comparison only\" to the unit.\n            The feature learning algorithms and the feature selectors will\n            interpret this accordingly.\n    \"\"\"\n    return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/columns/__init__/","title":"init","text":"<p>Handlers for 1-d arrays storing the data of an individual variable.</p> <p>Like the <code>DataFrame</code>, the <code>columns</code> do not contain any actual data themselves but are only handlers to objects within the getML engine. These containers store data of a single variable in a one-dimensional array of a uniform type.</p> <p>Columns are immutable and lazily evaluated.</p> <ul> <li> <p>Immutable means that there are no in-place   operation on the columns. Any change to the column   will return a new, changed column.</p> </li> <li> <p>Lazy evaluation means that operations won't be   executed until results are required. This is reflected   in the column views: Column views do not exist   until they are required.</p> </li> </ul> Example <p>This is what some column operations might look like:</p> <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\n# col2 is a column view.\n# The operation is not executed yet.\ncol2 = 2.0 - col1\n\n# This is when '2.0 - col1' is actually\n# executed.\nmy_df[\"column_02\"] = col2\nmy_df.set_role(\"column_02\", roles.numerical)\n\n# If you want to update column_01,\n# you can't do that in-place.\n# You need to replace it with a new column\ncol1 = col1 + col2\nmy_df[\"column_01\"] = col1\nmy_df.set_role(\"column_01\", roles.numerical)\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.BooleanColumnView","title":"<code>BooleanColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Handle for a lazily evaluated boolean column view.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> <p>They can be used to take subselection of the data frame or to update other columns.</p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\nnames = my_df[\"names\"]\n\n# This is a virtual boolean column.\na_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n# Creates a view containing\n# only those entries, where \"names\" contains a or p.\nmy_view = my_df[a_or_p_in_names]\n\n# ----------------\n\n# Returns a new column, where all names\n# containing \"rick\" are replaced by \"Patrick\".\n# Again, columns are immutable - this returns an updated\n# version, but leaves the original column unchanged.\nnew_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\nmy_df[\"new_names\"] = new_names\n\n# ----------------\n\n# Boolean columns can also be used to\n# create binary target variables.\ntarget = (names == \"phil\")\n\nmy_df[\"target\"] = target\nmy_df.set_role(target, roles.target)\n\n# By the way, instead of using the\n# __setitem__ operator and .set_role(...)\n# you can just use .add(...).\nmy_df.add(target, \"target\", roles.target)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class BooleanColumnView(_View):\n    \"\"\"\n    Handle for a lazily evaluated boolean column view.\n\n    Column views do not actually exist - they will be lazily\n    evaluated when necessary.\n\n    They can be used to take subselection of the data frame\n    or to update other columns.\n\n    Example:\n        ```python\n        import numpy as np\n\n        import getml.data as data\n        import getml.engine as engine\n        import getml.data.roles as roles\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n        # Create a data frame from a JSON string\n\n        json_str = \\\"\\\"\\\"{\n            \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n            \"column_01\": [2.4, 3.0, 1.2, 1.4],\n            \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n            \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n        }\\\"\\\"\\\"\n\n        my_df = data.DataFrame(\n            \"MY DF\",\n            roles={\n                \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n                \"unused_float\": [\"column_01\"]}\n        ).read_json(\n            json_str\n        )\n\n        # ----------------\n\n        names = my_df[\"names\"]\n\n        # This is a virtual boolean column.\n        a_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n        # Creates a view containing\n        # only those entries, where \"names\" contains a or p.\n        my_view = my_df[a_or_p_in_names]\n\n        # ----------------\n\n        # Returns a new column, where all names\n        # containing \"rick\" are replaced by \"Patrick\".\n        # Again, columns are immutable - this returns an updated\n        # version, but leaves the original column unchanged.\n        new_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\n        my_df[\"new_names\"] = new_names\n\n        # ----------------\n\n        # Boolean columns can also be used to\n        # create binary target variables.\n        target = (names == \"phil\")\n\n        my_df[\"target\"] = target\n        my_df.set_role(target, roles.target)\n\n        # By the way, instead of using the\n        # __setitem__ operator and .set_role(...)\n        # you can just use .add(...).\n        my_df.add(target, \"target\", roles.target)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: OptionalOperandType,\n        operand2: OptionalOperandType,\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = BOOLEAN_COLUMN_VIEW\n\n        self.cmd[\"operator_\"] = operator\n\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def __and__(self, other):\n        return BooleanColumnView(\n            operator=\"and\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __eq__(self, other):\n        return BooleanColumnView(\n            operator=\"bool_equal_to\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __invert__(self):\n        return self.is_false()\n\n    # -----------------------------------------------------------------------------\n\n    def __or__(self, other):\n        return BooleanColumnView(\n            operator=\"or\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __ne__(self, other):\n        return BooleanColumnView(\n            operator=\"bool_not_equal_to\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __xor__(self, other):\n        return BooleanColumnView(\n            operator=\"xor\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(\n        self,\n        operand: OperandType,\n    ):\n        if isinstance(operand, (bool, str, numbers.Number, float, int, np.datetime64)):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a BooleanColumnView must be a\n                boolean, string, a number, a numpy.datetime64\n                or a getml.data.Column!\"\"\"\n            )\n\n        if self.cmd[\"operator_\"] in [\"and\", \"or\", \"not\", \"xor\"]:\n            if operand.cmd[\"type_\"] != BOOLEAN_COLUMN_VIEW:\n                raise TypeError(\"This operator can only be applied to a BooleanColumn!\")\n\n        return operand.cmd\n\n    # -----------------------------------------------------------------------------\n\n    def is_false(self):\n        \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n        return BooleanColumnView(\n            operator=\"not\",\n            operand1=self,\n            operand2=None,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def as_num(self):\n        \"\"\"Transforms the boolean column into a numerical column\"\"\"\n        return FloatColumnView(\n            operator=\"boolean_as_num\",\n            operand1=self,\n            operand2=None,\n        )\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.BooleanColumnView.as_num","title":"<code>as_num()</code>","text":"<p>Transforms the boolean column into a numerical column</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def as_num(self):\n    \"\"\"Transforms the boolean column into a numerical column\"\"\"\n    return FloatColumnView(\n        operator=\"boolean_as_num\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.BooleanColumnView.is_false","title":"<code>is_false()</code>","text":"<p>Whether an entry is False - effectively inverts the Boolean column.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def is_false(self):\n    \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n    return BooleanColumnView(\n        operator=\"not\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.FloatColumn","title":"<code>FloatColumn</code>","text":"<p>               Bases: <code>_Column</code></p> <p>Handle for numerical data in the engine.</p> <p>This is a handler for all numerical data in the getML engine, including time stamps.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the categorical column.</p> <code>''</code> <code>role</code> <code>str</code> <p>Role that the column plays.</p> <code>'numerical'</code> <code>df_name</code> <code>str</code> <p><code>name</code> instance variable of the <code>DataFrame</code>  containing this column.</p> <code>''</code> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\ncol2 = 2.0 - col1\n\nmy_df.add(col2, \"name\", roles.numerical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_float.\n\ncol3 = (col1 + 2.0*col2) / 3.0\n\nmy_df[\"column_03\"] = col3\nmy_df.set_role(\"column_03\", roles.numerical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class FloatColumn(_Column):\n    \"\"\"Handle for numerical data in the engine.\n\n    This is a handler for all numerical data in the getML engine,\n    including time stamps.\n\n    Args:\n        name (str, optional):\n            Name of the categorical column.\n\n        role (str, optional):\n            Role that the column plays.\n\n        df_name (str, optional):\n            ``name`` instance variable of the\n            [`DataFrame`][getml.DataFrame]  containing this column.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"column_01\"]\n\n    # ----------------\n\n    col2 = 2.0 - col1\n\n    my_df.add(col2, \"name\", roles.numerical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_float.\n\n    col3 = (col1 + 2.0*col2) / 3.0\n\n    my_df[\"column_03\"] = col3\n    my_df.set_role(\"column_03\", roles.numerical)\n    ```\n    \"\"\"\n\n    _num_columns = 0\n\n    def __init__(self, name: str = \"\", role: str = \"numerical\", df_name: str = \"\"):\n        super().__init__()\n\n        FloatColumn._num_columns += 1\n        if name == \"\":\n            name = FLOAT_COLUMN + \" \" + str(FloatColumn._num_columns)\n\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"operator_\"] = FLOAT_COLUMN\n\n        self.cmd[\"df_name_\"] = df_name\n\n        self.cmd[\"name_\"] = name\n\n        self.cmd[\"role_\"] = role\n\n        self.cmd[\"type_\"] = FLOAT_COLUMN\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.FloatColumnView","title":"<code>FloatColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>FloatColumn</code>.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class FloatColumnView(_View):\n    \"\"\"\n    Lazily evaluated view on a [`FloatColumn`][getml.data.columns.FloatColumn].\n\n    Column views do not actually exist - they will be lazily\n    evaluated when necessary.\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: Optional[Union[float, int, np.datetime64, _Column, _View]],\n        operand2: Optional[Union[float, int, np.datetime64, _Column, _View]],\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = FLOAT_COLUMN_VIEW\n\n        self.cmd[\"operator_\"] = operator\n\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(self, operand: Union[float, int, np.datetime64, _Column, _View]):\n        if isinstance(operand, (numbers.Number, int, float, np.datetime64)):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a FloatColumnView must\n                   be a number or a column!\"\"\"\n            )\n\n        special_ops = [\"as_num\", \"as_ts\", \"boolean_as_num\", \"num_subselection\"]\n        oper = self.cmd[\"operator_\"]\n        optype = operand.cmd[\"type_\"]\n\n        if oper not in special_ops:\n            wrong_coltype = optype not in [FLOAT_COLUMN, FLOAT_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a FloatColumn!\")\n\n        if (\n            oper in special_ops\n            and oper != \"boolean_as_num\"\n            and oper != \"num_subselection\"\n        ):\n            wrong_coltype = optype not in [STRING_COLUMN, STRING_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a StringColumn!\")\n\n        if oper == \"boolean_as_num\" and optype != BOOLEAN_COLUMN_VIEW:\n            raise TypeError(\"This operator can only be applied to a BooleanColumn!\")\n\n        if oper == \"num_subselection\":\n            wrong_coltype = optype not in [\n                STRING_COLUMN,\n                STRING_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"The subselection operator can only be applied to FloatColumn!\"\n                )\n\n        return operand.cmd\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.StringColumn","title":"<code>StringColumn</code>","text":"<p>               Bases: <code>_Column</code></p> <p>Handle for categorical data that is kept in the getML engine</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the categorical column.</p> <code>''</code> <code>role</code> <code>str</code> <p>Role that the column plays.</p> <code>'categorical'</code> <code>df_name</code> <code>str</code> <p><code>name</code> instance variable of the <code>DataFrame</code> containing this column.</p> <code>''</code> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\ncol2 = col1.substr(4, 3)\n\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\ncol3 = \"user-\" + col1 + \"-\" + col2\n\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class StringColumn(_Column):\n    \"\"\"Handle for categorical data that is kept in the getML engine\n\n    Args:\n        name (str, optional):\n            Name of the categorical column.\n\n        role (str, optional):\n            Role that the column plays.\n\n        df_name (str, optional):\n            ``name`` instance variable of the\n            [`DataFrame`][getml.DataFrame] containing this column.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"names\"]\n\n    # ----------------\n\n    col2 = col1.substr(4, 3)\n\n    my_df.add(col2, \"short_names\", roles.categorical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_string.\n\n    col3 = \"user-\" + col1 + \"-\" + col2\n\n    my_df[\"new_names\"] = col3\n    my_df.set_role(\"new_names\", roles.categorical)\n    ```\n    \"\"\"\n\n    _num_columns = 0\n\n    def __init__(self, name: str = \"\", role: str = \"categorical\", df_name: str = \"\"):\n        super().__init__()\n\n        StringColumn._num_columns += 1\n        if name == \"\":\n            name = STRING_COLUMN + \" \" + str(StringColumn._num_columns)\n\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"operator_\"] = STRING_COLUMN\n        self.cmd[\"df_name_\"] = df_name\n        self.cmd[\"name_\"] = name\n        self.cmd[\"role_\"] = role\n        self.cmd[\"type_\"] = STRING_COLUMN\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.StringColumnView","title":"<code>StringColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>StringColumn</code>.</p> <p>Columns views do not actually exist - they will be lazily evaluated when necessary.</p> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\n# col2 is a virtual column.\n# The substring operation is not\n# executed yet.\ncol2 = col1.substr(4, 3)\n\n# This is where the engine executes\n# the substring operation.\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\n# col3 is a virtual column.\n# The operation is not\n# executed yet.\ncol3 = \"user-\" + col1 + \"-\" + col2\n\n# This is where the operation is\n# is executed.\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class StringColumnView(_View):\n    \"\"\"\n    Lazily evaluated view on a [`StringColumn`][getml.data.columns.StringColumn].\n\n    Columns views do not actually exist - they will be lazily\n    evaluated when necessary.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"names\"]\n\n    # ----------------\n\n    # col2 is a virtual column.\n    # The substring operation is not\n    # executed yet.\n    col2 = col1.substr(4, 3)\n\n    # This is where the engine executes\n    # the substring operation.\n    my_df.add(col2, \"short_names\", roles.categorical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_string.\n\n    # col3 is a virtual column.\n    # The operation is not\n    # executed yet.\n    col3 = \"user-\" + col1 + \"-\" + col2\n\n    # This is where the operation is\n    # is executed.\n    my_df[\"new_names\"] = col3\n    my_df.set_role(\"new_names\", roles.categorical)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: Optional[Union[str, _Column, _View]],\n        operand2: Optional[Union[str, _Column, _View]],\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = STRING_COLUMN_VIEW\n        self.cmd[\"operator_\"] = operator\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(self, operand: Union[str, _Column, _View]):\n        if isinstance(operand, str):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a StringColumnView must\n                   be a string or a column!\"\"\"\n            )\n\n        oper = self.cmd[\"operator_\"]\n\n        optype = operand.cmd[\"type_\"]\n\n        if oper == \"as_str\":\n            wrong_coltype = optype not in [\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"This operator can only be applied to a \"\n                    + \"FloatColumn or a BooleanColumn!\"\n                )\n\n        elif oper == \"str_subselection\":\n            wrong_coltype = optype not in [\n                STRING_COLUMN,\n                STRING_COLUMN_VIEW,\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"Columns or Views can only be subset by \"\n                    + \"StringColumn) or a BooleanColumn!\"\n                )\n\n        else:\n            wrong_coltype = optype not in [STRING_COLUMN, STRING_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a StringColumn!\")\n\n        return operand.cmd\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.arange","title":"<code>arange(start=0.0, stop=None, step=1.0)</code>","text":"<p>Returns evenly spaced variables, within a given interval.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>The beginning of the interval. Defaults to 0.</p> <code>0.0</code> <code>stop</code> <code>float</code> <p>The end of the interval.</p> <code>None</code> <code>step</code> <code>float</code> <p>The step taken. Defaults to 1.</p> <code>1.0</code> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start (float, optional):\n            The beginning of the interval. Defaults to 0.\n\n        stop (float):\n            The end of the interval.\n\n        step (float, optional):\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/columns/__init__/#getml.data.columns.rowid","title":"<code>rowid()</code>","text":"<p>Get the row numbers of the table.</p> <p>Returns:</p> Type Description <p><code>FloatColumnView</code>: (numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid():\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/columns/aggregation/","title":"Aggregation","text":"<p>Lazily evaluated aggregation over a column.</p>"},{"location":"reference/data/columns/aggregation/#getml.data.columns.aggregation.Aggregation","title":"<code>Aggregation</code>","text":"<p>Lazily evaluated aggregation over a column.</p> Example <pre><code>my_data_frame[\"my_column\"].avg()\n3.0\n</code></pre> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>class Aggregation:\n    \"\"\"\n    Lazily evaluated aggregation over a column.\n\n    Example:\n        ```python\n        my_data_frame[\"my_column\"].avg()\n        3.0\n        ```\n    \"\"\"\n\n    def __init__(self, alias, col, agg_type):\n        self.cmd: Dict[str, Any] = {}\n        self.cmd[\"as_\"] = alias\n        self.cmd[\"col_\"] = col.cmd\n        self.cmd[\"type_\"] = agg_type\n\n    # -----------------------------------------------------------------------------\n\n    def __repr__(self):\n        return str(self)\n\n    # -----------------------------------------------------------------------------\n\n    def __str__(self):\n        val = self.get()\n        return self.cmd[\"type_\"].upper() + \" aggregation, value: \" + str(val) + \".\"\n\n    # --------------------------------------------------------------------------\n\n    def get(self):\n        \"\"\"\n        Receives the value of the aggregation over the column.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"name_\"] = \"\"\n        cmd[\"type_\"] = \"FloatColumn.aggregate\"\n\n        cmd[\"aggregation_\"] = self.cmd\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            mat = comm.recv_float_matrix(sock)\n\n        return mat.ravel()[0]\n</code></pre>"},{"location":"reference/data/columns/aggregation/#getml.data.columns.aggregation.Aggregation.get","title":"<code>get()</code>","text":"<p>Receives the value of the aggregation over the column.</p> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>def get(self):\n    \"\"\"\n    Receives the value of the aggregation over the column.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"FloatColumn.aggregate\"\n\n    cmd[\"aggregation_\"] = self.cmd\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        mat = comm.recv_float_matrix(sock)\n\n    return mat.ravel()[0]\n</code></pre>"},{"location":"reference/data/columns/collect_footer_data/","title":"Collect footer data","text":"<p>Collects the data necessary for displaying the column footer.</p>"},{"location":"reference/data/columns/collect_footer_data/#getml.data.columns.collect_footer_data.Footer","title":"<code>Footer</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the data to be shown in the footer of the data frame or column.</p> Source code in <code>getml/data/columns/collect_footer_data.py</code> <pre><code>class Footer(NamedTuple):\n    \"\"\"\n    Contains the data to be shown\n    in the footer of the data frame or\n    column.\n    \"\"\"\n\n    n_rows: Union[int, str]\n    type: str\n    data_frame: Optional[str] = None\n    url: Optional[str] = None\n</code></pre>"},{"location":"reference/data/columns/column/","title":"Column","text":"<p>Base object not meant to be called directly.</p>"},{"location":"reference/data/columns/columns/","title":"Columns","text":"<p>Contains the actual columns.</p>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.BooleanColumnView","title":"<code>BooleanColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Handle for a lazily evaluated boolean column view.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> <p>They can be used to take subselection of the data frame or to update other columns.</p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\nnames = my_df[\"names\"]\n\n# This is a virtual boolean column.\na_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n# Creates a view containing\n# only those entries, where \"names\" contains a or p.\nmy_view = my_df[a_or_p_in_names]\n\n# ----------------\n\n# Returns a new column, where all names\n# containing \"rick\" are replaced by \"Patrick\".\n# Again, columns are immutable - this returns an updated\n# version, but leaves the original column unchanged.\nnew_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\nmy_df[\"new_names\"] = new_names\n\n# ----------------\n\n# Boolean columns can also be used to\n# create binary target variables.\ntarget = (names == \"phil\")\n\nmy_df[\"target\"] = target\nmy_df.set_role(target, roles.target)\n\n# By the way, instead of using the\n# __setitem__ operator and .set_role(...)\n# you can just use .add(...).\nmy_df.add(target, \"target\", roles.target)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class BooleanColumnView(_View):\n    \"\"\"\n    Handle for a lazily evaluated boolean column view.\n\n    Column views do not actually exist - they will be lazily\n    evaluated when necessary.\n\n    They can be used to take subselection of the data frame\n    or to update other columns.\n\n    Example:\n        ```python\n        import numpy as np\n\n        import getml.data as data\n        import getml.engine as engine\n        import getml.data.roles as roles\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n        # Create a data frame from a JSON string\n\n        json_str = \\\"\\\"\\\"{\n            \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n            \"column_01\": [2.4, 3.0, 1.2, 1.4],\n            \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n            \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n        }\\\"\\\"\\\"\n\n        my_df = data.DataFrame(\n            \"MY DF\",\n            roles={\n                \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n                \"unused_float\": [\"column_01\"]}\n        ).read_json(\n            json_str\n        )\n\n        # ----------------\n\n        names = my_df[\"names\"]\n\n        # This is a virtual boolean column.\n        a_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n        # Creates a view containing\n        # only those entries, where \"names\" contains a or p.\n        my_view = my_df[a_or_p_in_names]\n\n        # ----------------\n\n        # Returns a new column, where all names\n        # containing \"rick\" are replaced by \"Patrick\".\n        # Again, columns are immutable - this returns an updated\n        # version, but leaves the original column unchanged.\n        new_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\n        my_df[\"new_names\"] = new_names\n\n        # ----------------\n\n        # Boolean columns can also be used to\n        # create binary target variables.\n        target = (names == \"phil\")\n\n        my_df[\"target\"] = target\n        my_df.set_role(target, roles.target)\n\n        # By the way, instead of using the\n        # __setitem__ operator and .set_role(...)\n        # you can just use .add(...).\n        my_df.add(target, \"target\", roles.target)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: OptionalOperandType,\n        operand2: OptionalOperandType,\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = BOOLEAN_COLUMN_VIEW\n\n        self.cmd[\"operator_\"] = operator\n\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def __and__(self, other):\n        return BooleanColumnView(\n            operator=\"and\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __eq__(self, other):\n        return BooleanColumnView(\n            operator=\"bool_equal_to\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __invert__(self):\n        return self.is_false()\n\n    # -----------------------------------------------------------------------------\n\n    def __or__(self, other):\n        return BooleanColumnView(\n            operator=\"or\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __ne__(self, other):\n        return BooleanColumnView(\n            operator=\"bool_not_equal_to\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def __xor__(self, other):\n        return BooleanColumnView(\n            operator=\"xor\",\n            operand1=self,\n            operand2=other,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(\n        self,\n        operand: OperandType,\n    ):\n        if isinstance(operand, (bool, str, numbers.Number, float, int, np.datetime64)):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a BooleanColumnView must be a\n                boolean, string, a number, a numpy.datetime64\n                or a getml.data.Column!\"\"\"\n            )\n\n        if self.cmd[\"operator_\"] in [\"and\", \"or\", \"not\", \"xor\"]:\n            if operand.cmd[\"type_\"] != BOOLEAN_COLUMN_VIEW:\n                raise TypeError(\"This operator can only be applied to a BooleanColumn!\")\n\n        return operand.cmd\n\n    # -----------------------------------------------------------------------------\n\n    def is_false(self):\n        \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n        return BooleanColumnView(\n            operator=\"not\",\n            operand1=self,\n            operand2=None,\n        )\n\n    # -----------------------------------------------------------------------------\n\n    def as_num(self):\n        \"\"\"Transforms the boolean column into a numerical column\"\"\"\n        return FloatColumnView(\n            operator=\"boolean_as_num\",\n            operand1=self,\n            operand2=None,\n        )\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.BooleanColumnView.as_num","title":"<code>as_num()</code>","text":"<p>Transforms the boolean column into a numerical column</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def as_num(self):\n    \"\"\"Transforms the boolean column into a numerical column\"\"\"\n    return FloatColumnView(\n        operator=\"boolean_as_num\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.BooleanColumnView.is_false","title":"<code>is_false()</code>","text":"<p>Whether an entry is False - effectively inverts the Boolean column.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def is_false(self):\n    \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n    return BooleanColumnView(\n        operator=\"not\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.FloatColumn","title":"<code>FloatColumn</code>","text":"<p>               Bases: <code>_Column</code></p> <p>Handle for numerical data in the engine.</p> <p>This is a handler for all numerical data in the getML engine, including time stamps.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the categorical column.</p> <code>''</code> <code>role</code> <code>str</code> <p>Role that the column plays.</p> <code>'numerical'</code> <code>df_name</code> <code>str</code> <p><code>name</code> instance variable of the <code>DataFrame</code>  containing this column.</p> <code>''</code> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\ncol2 = 2.0 - col1\n\nmy_df.add(col2, \"name\", roles.numerical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_float.\n\ncol3 = (col1 + 2.0*col2) / 3.0\n\nmy_df[\"column_03\"] = col3\nmy_df.set_role(\"column_03\", roles.numerical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class FloatColumn(_Column):\n    \"\"\"Handle for numerical data in the engine.\n\n    This is a handler for all numerical data in the getML engine,\n    including time stamps.\n\n    Args:\n        name (str, optional):\n            Name of the categorical column.\n\n        role (str, optional):\n            Role that the column plays.\n\n        df_name (str, optional):\n            ``name`` instance variable of the\n            [`DataFrame`][getml.DataFrame]  containing this column.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"column_01\"]\n\n    # ----------------\n\n    col2 = 2.0 - col1\n\n    my_df.add(col2, \"name\", roles.numerical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_float.\n\n    col3 = (col1 + 2.0*col2) / 3.0\n\n    my_df[\"column_03\"] = col3\n    my_df.set_role(\"column_03\", roles.numerical)\n    ```\n    \"\"\"\n\n    _num_columns = 0\n\n    def __init__(self, name: str = \"\", role: str = \"numerical\", df_name: str = \"\"):\n        super().__init__()\n\n        FloatColumn._num_columns += 1\n        if name == \"\":\n            name = FLOAT_COLUMN + \" \" + str(FloatColumn._num_columns)\n\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"operator_\"] = FLOAT_COLUMN\n\n        self.cmd[\"df_name_\"] = df_name\n\n        self.cmd[\"name_\"] = name\n\n        self.cmd[\"role_\"] = role\n\n        self.cmd[\"type_\"] = FLOAT_COLUMN\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.FloatColumnView","title":"<code>FloatColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>FloatColumn</code>.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class FloatColumnView(_View):\n    \"\"\"\n    Lazily evaluated view on a [`FloatColumn`][getml.data.columns.FloatColumn].\n\n    Column views do not actually exist - they will be lazily\n    evaluated when necessary.\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: Optional[Union[float, int, np.datetime64, _Column, _View]],\n        operand2: Optional[Union[float, int, np.datetime64, _Column, _View]],\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = FLOAT_COLUMN_VIEW\n\n        self.cmd[\"operator_\"] = operator\n\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(self, operand: Union[float, int, np.datetime64, _Column, _View]):\n        if isinstance(operand, (numbers.Number, int, float, np.datetime64)):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a FloatColumnView must\n                   be a number or a column!\"\"\"\n            )\n\n        special_ops = [\"as_num\", \"as_ts\", \"boolean_as_num\", \"num_subselection\"]\n        oper = self.cmd[\"operator_\"]\n        optype = operand.cmd[\"type_\"]\n\n        if oper not in special_ops:\n            wrong_coltype = optype not in [FLOAT_COLUMN, FLOAT_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a FloatColumn!\")\n\n        if (\n            oper in special_ops\n            and oper != \"boolean_as_num\"\n            and oper != \"num_subselection\"\n        ):\n            wrong_coltype = optype not in [STRING_COLUMN, STRING_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a StringColumn!\")\n\n        if oper == \"boolean_as_num\" and optype != BOOLEAN_COLUMN_VIEW:\n            raise TypeError(\"This operator can only be applied to a BooleanColumn!\")\n\n        if oper == \"num_subselection\":\n            wrong_coltype = optype not in [\n                STRING_COLUMN,\n                STRING_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"The subselection operator can only be applied to FloatColumn!\"\n                )\n\n        return operand.cmd\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.StringColumn","title":"<code>StringColumn</code>","text":"<p>               Bases: <code>_Column</code></p> <p>Handle for categorical data that is kept in the getML engine</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the categorical column.</p> <code>''</code> <code>role</code> <code>str</code> <p>Role that the column plays.</p> <code>'categorical'</code> <code>df_name</code> <code>str</code> <p><code>name</code> instance variable of the <code>DataFrame</code> containing this column.</p> <code>''</code> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\ncol2 = col1.substr(4, 3)\n\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\ncol3 = \"user-\" + col1 + \"-\" + col2\n\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class StringColumn(_Column):\n    \"\"\"Handle for categorical data that is kept in the getML engine\n\n    Args:\n        name (str, optional):\n            Name of the categorical column.\n\n        role (str, optional):\n            Role that the column plays.\n\n        df_name (str, optional):\n            ``name`` instance variable of the\n            [`DataFrame`][getml.DataFrame] containing this column.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"names\"]\n\n    # ----------------\n\n    col2 = col1.substr(4, 3)\n\n    my_df.add(col2, \"short_names\", roles.categorical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_string.\n\n    col3 = \"user-\" + col1 + \"-\" + col2\n\n    my_df[\"new_names\"] = col3\n    my_df.set_role(\"new_names\", roles.categorical)\n    ```\n    \"\"\"\n\n    _num_columns = 0\n\n    def __init__(self, name: str = \"\", role: str = \"categorical\", df_name: str = \"\"):\n        super().__init__()\n\n        StringColumn._num_columns += 1\n        if name == \"\":\n            name = STRING_COLUMN + \" \" + str(StringColumn._num_columns)\n\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"operator_\"] = STRING_COLUMN\n        self.cmd[\"df_name_\"] = df_name\n        self.cmd[\"name_\"] = name\n        self.cmd[\"role_\"] = role\n        self.cmd[\"type_\"] = STRING_COLUMN\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.StringColumnView","title":"<code>StringColumnView</code>","text":"<p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>StringColumn</code>.</p> <p>Columns views do not actually exist - they will be lazily evaluated when necessary.</p> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\n# col2 is a virtual column.\n# The substring operation is not\n# executed yet.\ncol2 = col1.substr(4, 3)\n\n# This is where the engine executes\n# the substring operation.\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\n# col3 is a virtual column.\n# The operation is not\n# executed yet.\ncol3 = \"user-\" + col1 + \"-\" + col2\n\n# This is where the operation is\n# is executed.\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>class StringColumnView(_View):\n    \"\"\"\n    Lazily evaluated view on a [`StringColumn`][getml.data.columns.StringColumn].\n\n    Columns views do not actually exist - they will be lazily\n    evaluated when necessary.\n\n    Examples:\n    ```python\n    import numpy as np\n\n    import getml.data as data\n    import getml.engine as engine\n    import getml.data.roles as roles\n\n    # ----------------\n\n    engine.set_project(\"examples\")\n\n    # ----------------\n    # Create a data frame from a JSON string\n\n    json_str = \\\"\\\"\\\"{\n        \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n        \"column_01\": [2.4, 3.0, 1.2, 1.4],\n        \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n        \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n    }\\\"\\\"\\\"\n\n    my_df = data.DataFrame(\n        \"MY DF\",\n        roles={\n            \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n            \"unused_float\": [\"column_01\"]}\n    ).read_json(\n        json_str\n    )\n\n    # ----------------\n\n    col1 = my_df[\"names\"]\n\n    # ----------------\n\n    # col2 is a virtual column.\n    # The substring operation is not\n    # executed yet.\n    col2 = col1.substr(4, 3)\n\n    # This is where the engine executes\n    # the substring operation.\n    my_df.add(col2, \"short_names\", roles.categorical)\n\n    # ----------------\n    # If you do not explicitly set a role,\n    # the assigned role will either be\n    # roles.unused_string.\n\n    # col3 is a virtual column.\n    # The operation is not\n    # executed yet.\n    col3 = \"user-\" + col1 + \"-\" + col2\n\n    # This is where the operation is\n    # is executed.\n    my_df[\"new_names\"] = col3\n    my_df.set_role(\"new_names\", roles.categorical)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        operator: str,\n        operand1: Optional[Union[str, _Column, _View]],\n        operand2: Optional[Union[str, _Column, _View]],\n    ):\n        self.cmd: Dict[str, Any] = {}\n\n        self.cmd[\"type_\"] = STRING_COLUMN_VIEW\n        self.cmd[\"operator_\"] = operator\n        if operand1 is not None:\n            self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n        if operand2 is not None:\n            self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n\n    # -----------------------------------------------------------------------------\n\n    def _parse_operand(self, operand: Union[str, _Column, _View]):\n        if isinstance(operand, str):\n            return _value_to_cmd(operand)\n\n        if not hasattr(operand, \"cmd\"):\n            raise TypeError(\n                \"\"\"Operand for a StringColumnView must\n                   be a string or a column!\"\"\"\n            )\n\n        oper = self.cmd[\"operator_\"]\n\n        optype = operand.cmd[\"type_\"]\n\n        if oper == \"as_str\":\n            wrong_coltype = optype not in [\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"This operator can only be applied to a \"\n                    + \"FloatColumn or a BooleanColumn!\"\n                )\n\n        elif oper == \"str_subselection\":\n            wrong_coltype = optype not in [\n                STRING_COLUMN,\n                STRING_COLUMN_VIEW,\n                FLOAT_COLUMN,\n                FLOAT_COLUMN_VIEW,\n                BOOLEAN_COLUMN_VIEW,\n            ]\n            if wrong_coltype:\n                raise TypeError(\n                    \"Columns or Views can only be subset by \"\n                    + \"StringColumn) or a BooleanColumn!\"\n                )\n\n        else:\n            wrong_coltype = optype not in [STRING_COLUMN, STRING_COLUMN_VIEW]\n            if wrong_coltype:\n                raise TypeError(\"This operator can only be applied to a StringColumn!\")\n\n        return operand.cmd\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.arange","title":"<code>arange(start=0.0, stop=None, step=1.0)</code>","text":"<p>Returns evenly spaced variables, within a given interval.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>The beginning of the interval. Defaults to 0.</p> <code>0.0</code> <code>stop</code> <code>float</code> <p>The end of the interval.</p> <code>None</code> <code>step</code> <code>float</code> <p>The step taken. Defaults to 1.</p> <code>1.0</code> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start (float, optional):\n            The beginning of the interval. Defaults to 0.\n\n        stop (float):\n            The end of the interval.\n\n        step (float, optional):\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/columns/columns/#getml.data.columns.columns.rowid","title":"<code>rowid()</code>","text":"<p>Get the row numbers of the table.</p> <p>Returns:</p> Type Description <p><code>FloatColumnView</code>: (numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid():\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/columns/constants/","title":"Constants","text":""},{"location":"reference/data/columns/format/","title":"Format","text":"<p>Format the column</p>"},{"location":"reference/data/columns/from_value/","title":"From value","text":"<p>Generates an appropriate column from a value.</p>"},{"location":"reference/data/columns/from_value/#getml.data.columns.from_value.from_value","title":"<code>from_value(val)</code>","text":"<p>Creates an infinite column that contains the same value in all of its elements.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>(bool, str or number)</code> <p>The value you want to insert into your column.</p> required Source code in <code>getml/data/columns/from_value.py</code> <pre><code>def from_value(val: Union[bool, str, int, float, np.datetime64]) -&gt; ReturnType:\n    \"\"\"\n    Creates an infinite column that contains the same\n    value in all of its elements.\n\n    Args:\n        val (bool, str or number):\n            The value you want to insert into your column.\n    \"\"\"\n    cmd = _value_to_cmd(val)\n\n    if isinstance(val, bool):\n        col: ReturnType = BooleanColumnView(\n            operator=\"const\",\n            operand1=None,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, str):\n        col = StringColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, (int, float, numbers.Number)):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, np.datetime64):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=np.datetime64(val, \"s\").astype(float),\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    raise TypeError(\"val must be bool, str or a number.\")\n</code></pre>"},{"location":"reference/data/columns/get_scalar/","title":"Get scalar","text":""},{"location":"reference/data/columns/last_change/","title":"Last change","text":"<p>Returns the last time a data frame has been changed.</p>"},{"location":"reference/data/columns/last_change_from_col/","title":"Last change from col","text":"<p>The last time any of the underlying data frames has been changed.</p>"},{"location":"reference/data/columns/length/","title":"Length","text":"<p>Returns the length of the column</p>"},{"location":"reference/data/columns/length_property/","title":"Length property","text":"<p>The length of the column (number of rows in the data frame).</p>"},{"location":"reference/data/columns/make_iter/","title":"Make iter","text":"<p>Factory function for a function that can be used to iterate through a column.</p>"},{"location":"reference/data/columns/parse/","title":"Parse","text":"<p>Parses the columns from a cmd</p>"},{"location":"reference/data/columns/random/","title":"Random","text":"<p>Generates random numbers</p>"},{"location":"reference/data/columns/random/#getml.data.columns.random.random","title":"<code>random(seed=5849)</code>","text":"<p>Create random column.</p> <p>The numbers will be uniformly distributed from 0.0 to 1.0. This can be used to randomly split a population table into a training and a test set</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Seed used for the random number generator.</p> <code>5849</code> <p>Returns:</p> Type Description <code>FloatColumnView</code> <p><code>FloatColumnView</code>: FloatColumn containing random numbers</p> Example <pre><code>population = getml.DataFrame('population')\npopulation.add(numpy.zeros(100), 'column_01')\n\nidx = random(seed=42)\npopulation_train = population[idx &gt; 0.7]\npopulation_test = population[idx &lt;= 0.7]\n</code></pre> Source code in <code>getml/data/columns/random.py</code> <pre><code>def random(seed: int = 5849) -&gt; FloatColumnView:\n    \"\"\"\n    Create random column.\n\n    The numbers will be uniformly distributed from 0.0 to 1.0. This can be\n    used to randomly split a population table into a training and a test\n    set\n\n    Args:\n        seed (int):\n            Seed used for the random number generator.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            FloatColumn containing random numbers\n\n    Example:\n        ```python\n        population = getml.DataFrame('population')\n        population.add(numpy.zeros(100), 'column_01')\n\n        idx = random(seed=42)\n        population_train = population[idx &gt; 0.7]\n        population_test = population[idx &lt;= 0.7]\n        ```\n    \"\"\"\n\n    if not isinstance(seed, numbers.Real):\n        raise TypeError(\"'seed' must be a real number\")\n\n    col = FloatColumnView(operator=\"random\", operand1=None, operand2=None)\n    col.cmd[\"seed_\"] = seed\n    return col\n</code></pre>"},{"location":"reference/data/columns/repr/","title":"Repr","text":"<p>ASCII representation of the column.</p>"},{"location":"reference/data/columns/repr_html/","title":"Repr html","text":"<p>HTML representation of the column.</p>"},{"location":"reference/data/columns/subroles/","title":"Subroles","text":"<p>The subroles of this column.</p>"},{"location":"reference/data/columns/to_arrow/","title":"To arrow","text":"<p>Transform column to a pyarrow.ChunkedArray</p>"},{"location":"reference/data/columns/to_numpy/","title":"To numpy","text":"<p>Transform column to a numpy array.</p>"},{"location":"reference/data/columns/unique/","title":"Unique","text":"<p>Transform column to numpy array containing unique values</p>"},{"location":"reference/data/columns/unit/","title":"Unit","text":"<p>The unit of this column.</p>"},{"location":"reference/data/split/__init__/","title":"init","text":"<p>Helps you split data into a training, testing, validation or other sets.</p> <p>Examples:</p> <p>Split at random:</p> <p><pre><code>split = getml.data.split.random(\n    train=0.8, test=0.1, validation=0.1\n)\n\ntrain_set = data_frame[split=='train']\nvalidation_set = data_frame[split=='validation']\ntest_set = data_frame[split=='test']\n</code></pre> Split over time:</p> <pre><code>validation_begin = getml.data.time.datetime(2010, 1, 1)\ntest_begin = getml.data.time.datetime(2011, 1, 1)\n\nsplit = getml.data.split.time(\n    population=data_frame,\n    time_stamp=\"ds\",\n    test=test_begin,\n    validation=validation_begin\n)\n\n# Contains all data before 2010-01-01 (not included)\ntrain_set = data_frame[split=='train']\n\n# Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\nvalidation_set = data_frame[split=='validation']\n\n# Contains all data after 2011-01-01 (included)\ntest_set = data_frame[split=='test']\n</code></pre>"},{"location":"reference/data/split/concat/","title":"Concat","text":"<p>Concatenates data.</p>"},{"location":"reference/data/split/concat/#getml.data.split.concat.concat","title":"<code>concat(name, **kwargs)</code>","text":"<p>Concatenates several data frames into and produces a split column that keeps track of their origin.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the data frame you would like to create.</p> required <code>kwargs</code> <code>DataFrame</code> <p>The data frames you would like to concat with the name in which they should appear in the split column.</p> <code>{}</code> Example <p>A common use case for this functionality are <code>TimeSeries</code>: <pre><code>data_train = getml.DataFrame.from_pandas(\n    datatraining_pandas, name='data_train')\n\ndata_validate = getml.DataFrame.from_pandas(\n    datatest_pandas, name='data_validate')\n\ndata_test = getml.DataFrame.from_pandas(\n    datatest2_pandas, name='data_test')\n\npopulation, split = getml.data.split.concat(\n    \"population\", train=data_train, validate=data_validate, test=data_test)\n\n...\n\ntime_series = getml.data.TimeSeries(\n    population=population, split=split)\n\nmy_pipeline.fit(time_series.train)\n</code></pre></p> Source code in <code>getml/data/split/concat.py</code> <pre><code>def concat(name: str, **kwargs: DataFrame) -&gt; Tuple[DataFrame, StringColumnView]:\n    \"\"\"\n    Concatenates several data frames into and produces a split\n    column that keeps track of their origin.\n\n    Args:\n        name (str):\n            The name of the data frame you would like to create.\n\n        kwargs:\n            The data frames you would like\n            to concat with the name in which they should appear\n            in the split column.\n\n    Example:\n        A common use case for this functionality are [`TimeSeries`][getml.data.TimeSeries]:\n        ```python\n        data_train = getml.DataFrame.from_pandas(\n            datatraining_pandas, name='data_train')\n\n        data_validate = getml.DataFrame.from_pandas(\n            datatest_pandas, name='data_validate')\n\n        data_test = getml.DataFrame.from_pandas(\n            datatest2_pandas, name='data_test')\n\n        population, split = getml.data.split.concat(\n            \"population\", train=data_train, validate=data_validate, test=data_test)\n\n        ...\n\n        time_series = getml.data.TimeSeries(\n            population=population, split=split)\n\n        my_pipeline.fit(time_series.train)\n        ```\n    \"\"\"\n\n    if not _is_non_empty_typed_list(list(kwargs.values()), [DataFrame, View]):\n        raise ValueError(\n            \"'kwargs' must be non-empty and contain getml.DataFrames \"\n            + \"or getml.data.Views.\"\n        )\n\n    names = list(kwargs.keys())\n\n    first = kwargs[names[0]]\n\n    population = first.copy(name) if isinstance(first, DataFrame) else first.to_df(name)\n\n    split = from_value(names[0])\n\n    assert isinstance(split, StringColumnView), \"Should be a StringColumnView\"\n\n    for new_df_name in names[1:]:\n        split = split.update(rowid() &gt; population.nrows(), new_df_name)  # type: ignore\n        population = _concat(name, [population, kwargs[new_df_name]])\n\n    return population, split[: population.nrows()]  # type: ignore\n</code></pre>"},{"location":"reference/data/split/random/","title":"Random","text":"<p>Splits data at random.</p>"},{"location":"reference/data/split/random/#getml.data.split.random.random","title":"<code>random(seed=5849, train=0.8, test=0.2, validation=0, **kwargs)</code>","text":"<p>Returns a <code>StringColumnView</code> that can be used to randomly divide data into training, testing, validation or other sets.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Seed used for the random number generator.</p> <code>5849</code> <code>train</code> <code>float</code> <p>The share of random samples assigned to the training set.</p> <code>0.8</code> <code>validation</code> <code>float</code> <p>The share of random samples assigned to the validation set.</p> <code>0</code> <code>test</code> <code>float</code> <p>The share of random samples assigned to the test set.</p> <code>0.2</code> <code>kwargs</code> <code>float</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <code>{}</code> Example <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.1, validation=0.05, other=0.05\n)\n\ntrain_set = data_frame[split=='train']\nvalidation_set = data_frame[split=='validation']\ntest_set = data_frame[split=='test']\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/random.py</code> <pre><code>def random(\n    seed=5849, train=0.8, test=0.2, validation=0, **kwargs: float\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that\n    can be used to randomly divide data into training, testing,\n    validation or other sets.\n\n    Args:\n        seed (int):\n            Seed used for the random number generator.\n\n        train (float, optional):\n            The share of random samples assigned to\n            the training set.\n\n        validation (float, optional):\n            The share of random samples assigned to\n            the validation set.\n\n        test (float, optional):\n            The share of random samples assigned to\n            the test set.\n\n        kwargs (float, optional):\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    Example:\n        ```python\n        split = getml.data.split.random(\n            train=0.8, test=0.1, validation=0.05, other=0.05\n        )\n\n        train_set = data_frame[split=='train']\n        validation_set = data_frame[split=='validation']\n        test_set = data_frame[split=='test']\n        other_set = data_frame[split=='other']\n        ```\n\n    \"\"\"\n\n    values = np.asarray([train, validation, test] + list(kwargs.values()))\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    if np.abs(np.sum(values) - 1.0) &gt; 0.0001:\n        raise ValueError(\n            \"'train', 'validation', 'test' and all other sets must add up to 1, \"\n            + \"but add up to \"\n            + str(np.sum(values))\n            + \".\"\n        )\n\n    upper_bounds = np.cumsum(values)\n    lower_bounds = upper_bounds - values\n\n    names = [\"train\", \"validation\", \"test\"] + list(kwargs.keys())\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            (random_col(seed=seed) &gt;= lower_bounds[i])  # type: ignore\n            &amp; (random_col(seed=seed) &lt; upper_bounds[i]),\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/split/time/","title":"Time","text":"<p>Splits data at random.</p>"},{"location":"reference/data/split/time/#getml.data.split.time.time","title":"<code>time(population, time_stamp, validation=None, test=None, **kwargs)</code>","text":"<p>Returns a <code>StringColumnView</code> that can be used to divide data into training, testing, validation or other sets.</p> <p>The arguments are <code>key=value</code> pairs of names (<code>key</code>) and starting points (<code>value</code>). The starting point defines the left endpoint of the subset. Intervals are left closed and right open, such that \\([value, next value)\\).  The (unnamed) subset left from the first named starting point, i.e.  \\([0, first value)\\), is always considered to be the training set.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>[`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]</code> <p>The population table you would like to split.</p> required <code>time_stamp</code> <code>str</code> <p>The name of the time stamp column in the population table you want to use. Ideally, the role of said column would be <code>time_stamp</code>. If you want to split on the rowid, then pass \"rowid\" to <code>time_stamp</code>.</p> required <code>validation</code> <code>float</code> <p>The start date of the validation set.</p> <code>None</code> <code>test</code> <code>float</code> <p>The start date of the test set.</p> <code>None</code> <code>kwargs</code> <code>float</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <code>{}</code> Example <pre><code>validation_begin = getml.data.time.datetime(2010, 1, 1)\ntest_begin = getml.data.time.datetime(2011, 1, 1)\nother_begin = getml.data.time.datetime(2012, 1, 1)\n\nsplit = getml.data.split.time(\n    population=data_frame,\n    time_stamp=\"ds\",\n    test=test_begin,\n    validation=validation_begin,\n    other=other_begin\n)\n\n# Contains all data before 2010-01-01 (not included)\ntrain_set = data_frame[split=='train']\n\n# Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\nvalidation_set = data_frame[split=='validation']\n\n# Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\ntest_set = data_frame[split=='test']\n\n# Contains all data after 2012-01-01 (included)\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/time.py</code> <pre><code>def time(\n    population: DataFrame,\n    time_stamp: Union[str, FloatColumn, FloatColumnView],\n    validation: Optional[TimeStampType] = None,\n    test: Optional[TimeStampType] = None,\n    **kwargs: TimeStampType\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that can be used to divide\n    data into training, testing, validation or other sets.\n\n    The arguments are\n    `key=value` pairs of names (`key`) and starting points (`value`).\n    The starting point defines the left endpoint of the subset. Intervals are left\n    closed and right open, such that $[value, next value)$.  The (unnamed) subset\n    left from the first named starting point, i.e.  $[0, first value)$, is always\n    considered to be the training set.\n\n    Args:\n        population ([`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]):\n            The population table you would like to split.\n\n        time_stamp (str):\n            The name of the time stamp column in the population table\n            you want to use. Ideally, the role of said column would be\n            [`time_stamp`][getml.data.roles.time_stamp]. If you want to split on the rowid,\n            then pass \"rowid\" to `time_stamp`.\n\n        validation (float, optional):\n            The start date of the validation set.\n\n        test (float, optional):\n            The start date of the test set.\n\n        kwargs (float, optional):\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    Example:\n        ```python\n        validation_begin = getml.data.time.datetime(2010, 1, 1)\n        test_begin = getml.data.time.datetime(2011, 1, 1)\n        other_begin = getml.data.time.datetime(2012, 1, 1)\n\n        split = getml.data.split.time(\n            population=data_frame,\n            time_stamp=\"ds\",\n            test=test_begin,\n            validation=validation_begin,\n            other=other_begin\n        )\n\n        # Contains all data before 2010-01-01 (not included)\n        train_set = data_frame[split=='train']\n\n        # Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\n        validation_set = data_frame[split=='validation']\n\n        # Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\n        test_set = data_frame[split=='test']\n\n        # Contains all data after 2012-01-01 (included)\n        other_set = data_frame[split=='other']\n        ```\n    \"\"\"\n    if not isinstance(population, (DataFrame, View)):\n        raise ValueError(\"'population' must be a DataFrame or a View.\")\n\n    if not isinstance(time_stamp, (str, FloatColumn, FloatColumnView)):\n        raise ValueError(\n            \"'time_stamp' must be a string, a FloatColumn, or a FloatColumnView.\"\n        )\n\n    if not test and not validation and not kwargs:\n        raise ValueError(\"You have to supply at least one starting point.\")\n\n    defaults: Dict[str, Optional[TimeStampType]] = {\n        \"test\": test,\n        \"validation\": validation,\n    }\n\n    sets = {name: value for name, value in defaults.items() if value is not None}\n\n    sets.update({**kwargs})\n\n    values = np.asarray(list(sets.values()))\n    index = np.argsort(values)\n    values = values[index]\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    names = np.asarray(list(sets.keys()))\n    names = names[index]\n\n    if isinstance(time_stamp, str):\n        time_stamp_col = (\n            population[time_stamp] if time_stamp != \"rowid\" else population.rowid\n        )\n    else:\n        time_stamp_col = time_stamp\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            time_stamp_col &gt;= values[i],\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/subroles/__init__/","title":"init","text":"<p>Subroles allow for more fine-granular control of how certain columns will be used by the pipeline.</p> <p>A column can have no subrole, one subrole or several subroles.</p> Example <pre><code># The Relboost feature learning algorithm will\n# ignore this column.\nmy_data_frame.set_subroles(\n    \"my_column\", getml.data.subroles.exclude.relboost)\n\n# The Substring preprocessor will be applied to this column.\n# But other preprocessors, feature learners or predictors\n# are not excluded from using it as well.\nmy_data_frame.set_subroles(\n    \"ucc\", getml.data.subroles.include.substring)\n\n# Only the EmailDomain preprocessor will be applied\n# to \"emails\". All other preprocessors, feature learners,\n# feature selectors and predictors will ignore this column.\nmy_data_frame.set_subroles(\"emails\", getml.data.subroles.only.email)\n</code></pre>"},{"location":"reference/data/subroles/exclude/","title":"Exclude","text":"<p>Columns marked with a subrole in this submodule will not be used for the specified purpose.</p> Example <pre><code># The Relboost feature learning algorithm will\n# ignore this column.\nmy_data_frame.set_subroles(\n    \"my_column\", getml.data.subroles.exclude.relboost)\n</code></pre>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.category_trimmer","title":"<code>category_trimmer = 'exclude category trimmer'</code>  <code>module-attribute</code>","text":"<p>The <code>CategoryTrimmer</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.fastprop","title":"<code>fastprop = 'exclude fastprop'</code>  <code>module-attribute</code>","text":"<p><code>FastProp</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.feature_learners","title":"<code>feature_learners = 'exclude feature learners'</code>  <code>module-attribute</code>","text":"<p>All feature learners (<code>feature_learning</code>) will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.imputation","title":"<code>imputation = 'exclude imputation'</code>  <code>module-attribute</code>","text":"<p>The <code>Imputation</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.mapping","title":"<code>mapping = 'exclude mapping'</code>  <code>module-attribute</code>","text":"<p>The <code>Mapping</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.multirel","title":"<code>multirel = 'exclude multirel'</code>  <code>module-attribute</code>","text":"<p><code>Multirel</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.predictors","title":"<code>predictors = 'exclude predictors'</code>  <code>module-attribute</code>","text":"<p>All <code>predictors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.preprocessors","title":"<code>preprocessors = 'exclude preprocessors'</code>  <code>module-attribute</code>","text":"<p>All <code>preprocessors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.relboost","title":"<code>relboost = 'exclude relboost'</code>  <code>module-attribute</code>","text":"<p><code>Relboost</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.relmt","title":"<code>relmt = 'exclude relmt'</code>  <code>module-attribute</code>","text":"<p><code>RelMT</code> will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.seasonal","title":"<code>seasonal = 'exclude seasonal'</code>  <code>module-attribute</code>","text":"<p>The <code>Seasonal</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/exclude/#getml.data.subroles.exclude.text_field_splitter","title":"<code>text_field_splitter = 'exclude text field splitter'</code>  <code>module-attribute</code>","text":"<p>The <code>TextFieldSplitter</code> will ignore this column.</p>"},{"location":"reference/data/subroles/include/","title":"Include","text":"<p>Columns marked with a subrole in this submodule will be used for the specified purpose without excluding other purposes.</p> Example <pre><code># The Substring preprocessor will be applied to this column.\n# But other preprocessors, feature learners or predictors\n# are not excluded from using it as well.\nmy_data_frame.set_subroles(\n    \"ucc\", getml.data.subroles.include.substring)\n</code></pre>"},{"location":"reference/data/subroles/include/#getml.data.subroles.include.email","title":"<code>email = 'include email'</code>  <code>module-attribute</code>","text":"<p>A column with this subrole will be used for the <code>EmailDomain</code> preprocessor.</p>"},{"location":"reference/data/subroles/include/#getml.data.subroles.include.substring","title":"<code>substring = 'include substring'</code>  <code>module-attribute</code>","text":"<p>A column with this subrole will be used for the <code>Substring</code> preprocessor.</p>"},{"location":"reference/data/subroles/only/","title":"Only","text":"<p>Columns marked with a subrole in this submodule will only be used for the specified purpose and nothing else.</p> Example <pre><code># Only the EmailDomain preprocessor will be applied\n# to \"emails\". All other preprocessors, feature learners,\n# feature selectors and predictors will ignore this column.\nmy_data_frame.set_subroles(\"emails\", getml.data.subroles.only.email)\n</code></pre>"},{"location":"reference/data/subroles/only/#getml.data.subroles.only.email","title":"<code>email = 'only email'</code>  <code>module-attribute</code>","text":"<p>A column with this subrole will only be used for the <code>EmailDomain</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/data/subroles/only/#getml.data.subroles.only.substring","title":"<code>substring = 'only substring'</code>  <code>module-attribute</code>","text":"<p>A column with this subrole will only be used for the <code>Substring</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/database/__init__/","title":"init","text":"<p>This module contains communication routines to access various databases.</p> <p>The <code>connect_bigquery</code>, <code>connect_hana</code>, <code>connect_greenplum</code>, <code>connect_mariadb</code>, <code>connect_mysql</code>, <code>connect_postgres</code>, and <code>connect_sqlite3</code> functions establish a connection between a database and the getML engine. During the data import using either the <code>read_db</code> or <code>read_query</code> methods of a <code>DataFrame</code> instance or the corresponding <code>from_db</code> class method all data will be directly loaded from the database into the engine without ever passing the Python interpreter.</p> <p>In addition, several auxiliary functions that might be handy during the analysis and interaction with the database are provided.</p>"},{"location":"reference/database/__init__/#getml.database.Connection","title":"<code>Connection</code>","text":"<p>A handle to a database connection on the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>The name you want to use to reference the connection. You can call it anything you want to. If a database connection with the same conn_id already exists, that connection will be removed automatically and the new connection will take its place. The default conn_id is \"default\", which refers to the default connection. If you do not explicitly pass a connection handle to any function that relates to a database, the default connection will be used automatically.</p> <code>'default'</code> Source code in <code>getml/database/connection.py</code> <pre><code>class Connection:\n    \"\"\"\n    A handle to a database connection on the getML engine.\n\n    Args:\n        conn_id (str, optional):\n            The name you want to use to reference the connection.\n            You can call it\n            anything you want to. If a database\n            connection with the same conn_id already exists, that connection\n            will be removed automatically and the new connection will take its place.\n            The default conn_id is \"default\", which refers to the default connection.\n            If you do not explicitly pass a connection handle to any function that\n            relates to a database, the default connection will be used automatically.\n    \"\"\"\n\n    def __init__(self, conn_id: str = \"default\"):\n        self.conn_id = conn_id\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"name_\"] = self.conn_id\n        cmd[\"type_\"] = \"Database.describe_connection\"\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            description = comm.recv_string(sock)\n\n        json_obj = json.loads(description)\n\n        json_obj[\"type\"] = \"Connection\"\n\n        sig = _SignatureFormatter(data=json_obj)\n\n        return sig._format()\n</code></pre>"},{"location":"reference/database/connect_bigquery/","title":"Connect bigquery","text":"<p>Creates a new BigQuery database connection.</p>"},{"location":"reference/database/connect_bigquery/#getml.database.connect_bigquery.connect_bigquery","title":"<code>connect_bigquery(database_id, project_id, google_application_credentials, time_formats=None, conn_id='default')</code>","text":"<p>Creates a new BigQuery database connection.</p> <p>Parameters:</p> Name Type Description Default <code>database_id</code> <code>str</code> <p>The ID of the database to connect to.</p> required <code>project_id</code> <code>str</code> <p>The ID of the project to connect to.</p> required <code>google_application_credentials</code> <code>str or Path</code> <p>The path of the Google application credentials. (Must be located on the machine hosting the getML engine).</p> required <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/database/connect_bigquery.py</code> <pre><code>def connect_bigquery(\n    database_id: str,\n    project_id: str,\n    google_application_credentials: Union[str, Path],\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new BigQuery database connection.\n\n    Args:\n        database_id (str):\n            The ID of the database to connect to.\n\n        project_id (str):\n            The ID of the project to connect to.\n\n        google_application_credentials (str or pathlib.Path):\n            The path of the Google application credentials.\n            (Must be located on the machine hosting the getML engine).\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"database_id_\"] = database_id\n    cmd[\"project_id_\"] = project_id\n    cmd[\"google_application_credentials_\"] = os.path.abspath(\n        str(google_application_credentials)\n    )\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"bigquery\"\n\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The API expects a password, but in this case there is none\n        comm.send_string(sock, \"\")\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_greenplum/","title":"Connect greenplum","text":"<p>Creates a new Greenplum database connection.</p>"},{"location":"reference/database/connect_greenplum/#getml.database.connect_greenplum.connect_greenplum","title":"<code>connect_greenplum(dbname, user, password, host, hostaddr, port=5432, time_formats=None, conn_id='default')</code>","text":"<p>Creates a new Greenplum database connection.</p> <p>But first, make sure your database is running, and you can reach it from your command line.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the database to which you want to connect.</p> required <code>user</code> <code>str</code> <p>Username with which to log into the Greenplum database.</p> required <code>password</code> <code>str</code> <p>Password with which to log into the Greenplum database.</p> required <code>host</code> <code>str</code> <p>Host of the Greenplum database.</p> required <code>hostaddr</code> <code>str</code> <p>IP address of the Greenplum database.</p> required <code>port(int,</code> <code>optional</code> <p>Port of the Greenplum database.</p> <p>The default port used by Greenplum is 5432.</p> <p>If you do not know, which port to use, type the following into your Greenplum client:</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> required <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the Greenplum database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_greenplum.py</code> <pre><code>def connect_greenplum(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    hostaddr: str,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"Creates a new Greenplum database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from your command line.\n\n    Args:\n        dbname (str):\n            The name of the database to which you want to connect.\n\n        user (str):\n            Username with which to log into the Greenplum database.\n\n        password (str):\n            Password with which to log into the Greenplum database.\n\n        host (str):\n            Host of the Greenplum database.\n\n        hostaddr (str):\n            IP address of the Greenplum database.\n\n        port(int, optional):\n            Port of the Greenplum database.\n\n            The default port used by Greenplum is 5432.\n\n            If you do not know, which port to use, type the following into your\n            Greenplum client:\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the Greenplum\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"greenplum\"\n\n    cmd[\"host_\"] = host\n    cmd[\"hostaddr_\"] = hostaddr\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_hana/","title":"Connect hana","text":"<p>Creates a new HANA database connection.</p>"},{"location":"reference/database/connect_hana/#getml.database.connect_hana.connect_hana","title":"<code>connect_hana(user, password, host, port=39017, default_schema='public', ping_interval=0, time_formats=None, conn_id='default')</code>","text":"<p>Creates a new HANA database connection.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str</code> <p>Username with which to log into the HANA database.</p> required <code>password</code> <code>str</code> <p>Password with which to log into the HANA database.</p> required <code>host</code> <code>str</code> <p>Host of the HANA database.</p> required <code>port</code> <code>int</code> <p>Port of the database.</p> <code>39017</code> <code>default_schema</code> <code>str</code> <p>The schema within the database you want to connect use unless another schema is explicitly set.</p> <code>'public'</code> <code>ping_interval</code> <code>int</code> <p>The interval at which you want to ping the database, in seconds. Set to 0 for no pings at all.</p> <code>0</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/database/connect_hana.py</code> <pre><code>def connect_hana(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 39017,\n    default_schema: str = \"public\",\n    ping_interval: int = 0,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new HANA database connection.\n\n    Args:\n        user (str):\n            Username with which to log into the HANA database.\n\n        password (str):\n            Password with which to log into the HANA database.\n\n        host (str):\n            Host of the HANA database.\n\n        port (int, optional):\n            Port of the database.\n\n        default_schema (str, optional):\n            The schema within the database you want to connect\n            use unless another schema is explicitly set.\n\n        ping_interval (int, optional):\n            The interval at which you want to ping the database,\n            in seconds. Set to 0 for no pings at all.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"sap_hana\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"default_schema_\"] = default_schema\n    cmd[\"user_\"] = user\n    cmd[\"ping_interval_\"] = ping_interval\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_mariadb/","title":"Connect mariadb","text":"<p>Creates a new MariaDB database connection.</p>"},{"location":"reference/database/connect_mariadb/#getml.database.connect_mariadb.connect_mariadb","title":"<code>connect_mariadb(dbname, user, password, host, port=3306, unix_socket='/var/run/mysqld/mysqld.sock', time_formats=None, conn_id='default')</code>","text":"<p>Creates a new MariaDB database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the database to which you want to connect.</p> required <code>user</code> <code>str</code> <p>Username with which to log into the MariaDB database.</p> required <code>password</code> <code>str</code> <p>Password with which to log into the MariaDB database.</p> required <code>host</code> <code>str</code> <p>Host of the MariaDB database.</p> required <code>port</code> <code>int</code> <p>Port of the MariaDB database.</p> <p>The default port for MariaDB is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your MariaDB client.</p> <code>3306</code> <code>unix_socket</code> <code>str</code> <p>The UNIX socket used to connect to the MariaDB database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your MariaDB client.</p> <code>'/var/run/mysqld/mysqld.sock'</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MariaDB database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mariadb.py</code> <pre><code>def connect_mariadb(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new MariaDB database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    Args:\n        dbname (str):\n            The name of the database to which you want to connect.\n\n        user (str):\n            Username with which to log into the MariaDB database.\n\n        password (str):\n            Password with which to log into the MariaDB database.\n\n        host (str):\n            Host of the MariaDB database.\n\n        port (int, optional):\n            Port of the MariaDB database.\n\n            The default port for MariaDB is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your MariaDB client.\n\n        unix_socket (str, optional):\n            The UNIX socket used to connect to the MariaDB database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your MariaDB client.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MariaDB\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions\n        of the target variables to new, unseen data and stores the result into\n        the corresponding table.\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_mysql/","title":"Connect mysql","text":"<p>Creates a new MySQL database connection.</p>"},{"location":"reference/database/connect_mysql/#getml.database.connect_mysql.connect_mysql","title":"<code>connect_mysql(dbname, user, password, host, port=3306, unix_socket='/var/run/mysqld/mysqld.sock', time_formats=None, conn_id='default')</code>","text":"<p>Creates a new MySQL database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the database to which you want to connect.</p> required <code>user</code> <code>str</code> <p>Username with which to log into the MySQL database.</p> required <code>password</code> <code>str</code> <p>Password with which to log into the MySQL database.</p> required <code>host</code> <code>str</code> <p>Host of the MySQL database.</p> required <code>port</code> <code>int</code> <p>Port of the MySQL database.</p> <p>The default port for MySQL is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your mysql client.</p> <code>3306</code> <code>unix_socket</code> <code>str</code> <p>The UNIX socket used to connect to the MySQL database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your mysql client.</p> <code>'/var/run/mysqld/mysqld.sock'</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MySQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mysql.py</code> <pre><code>def connect_mysql(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new MySQL database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    Args:\n        dbname (str):\n            The name of the database to which you want to connect.\n\n        user (str):\n            Username with which to log into the MySQL database.\n\n        password (str):\n            Password with which to log into the MySQL database.\n\n        host (str):\n            Host of the MySQL database.\n\n        port (int, optional):\n            Port of the MySQL database.\n\n            The default port for MySQL is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your mysql client.\n\n        unix_socket (str, optional):\n            The UNIX socket used to connect to the MySQL database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your mysql client.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MySQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_odbc/","title":"Connect odbc","text":"<p>Creates a new ODBC database connection.</p>"},{"location":"reference/database/connect_odbc/#getml.database.connect_odbc.connect_odbc","title":"<code>connect_odbc(server_name, user='', password='', escape_chars='\"', double_precision='DOUBLE PRECISION', integer='INTEGER', text='TEXT', time_formats=None, conn_id='default')</code>","text":"<p>Creates a new ODBC database connection.</p> <p>ODBC is standardized format that can be used to connect to almost any database.</p> <p>Before you use the ODBC connector, make sure the database is up and running and that the appropriate ODBC drivers are installed.</p> <p>Parameters:</p> Name Type Description Default <code>server_name</code> <code>str</code> <p>The server name, as referenced in your .obdc.ini file.</p> required <code>user</code> <code>str</code> <p>Username with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <code>''</code> <code>password</code> <code>str</code> <p>Password with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <code>''</code> <code>escape_chars</code> <code>str</code> <p>ODBC drivers are supposed to support escaping table and column names using '\"' characters irrespective of the syntax in the target database. Unfortunately, not all ODBC drivers follow this standard. This is why some tuning might be necessary.</p> <p>The escape_chars value behaves as follows:</p> <ul> <li> <p>If you pass an empty string, schema, table and column names will not   be escaped at all. This is not a problem unless some table   or column names are identical to SQL keywords.</p> </li> <li> <p>If you pass a single character, schema, table and column names will   be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)   or <code>TABLE_NAME</code>.<code>COLUMN_NAME</code> (MySQL/MariaDB style).</p> </li> <li> <p>If you pass two characters, table, column and schema names will be   enveloped between these to characters. For instance, if you pass \"[]\",   the produced queries look as follows:   [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).</p> </li> <li> <p>If you pass more than two characters, the engine will throw an exception.</p> </li> </ul> <code>'\"'</code> <code>double_precision</code> <code>str</code> <p>The keyword used for double precision columns.</p> <code>'DOUBLE PRECISION'</code> <code>integer</code> <code>str</code> <p>The keyword used for integer columns.</p> <code>'INTEGER'</code> <code>text</code> <code>str</code> <p>The keyword used for text columns.</p> <code>'TEXT'</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/database/connect_odbc.py</code> <pre><code>def connect_odbc(\n    server_name: str,\n    user: str = \"\",\n    password: str = \"\",\n    escape_chars: str = '\"',\n    double_precision: str = \"DOUBLE PRECISION\",\n    integer: str = \"INTEGER\",\n    text: str = \"TEXT\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new ODBC database connection.\n\n    ODBC is standardized format that can be used to connect to almost any\n    database.\n\n    Before you use the ODBC connector, make sure the database is up and\n    running and that the appropriate ODBC drivers are installed.\n\n    Args:\n        server_name (str):\n            The server name, as referenced in your .obdc.ini file.\n\n        user (str, optional):\n            Username with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        password (str, optional):\n            Password with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        escape_chars (str, optional):\n            ODBC drivers are supposed to support\n            escaping table and column names using '\"' characters irrespective of the\n            syntax in the target database. Unfortunately, not all ODBC drivers\n            follow this standard. This is why some\n            tuning might be necessary.\n\n            The escape_chars value behaves as follows:\n\n            * If you pass an empty string, schema, table and column names will not\n              be escaped at all. This is not a problem unless some table\n              or column names are identical to SQL keywords.\n\n            * If you pass a single character, schema, table and column names will\n              be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)\n              or `TABLE_NAME`.`COLUMN_NAME` (MySQL/MariaDB style).\n\n            * If you pass two characters, table, column and schema names will be\n              enveloped between these to characters. For instance, if you pass \"[]\",\n              the produced queries look as follows:\n              [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).\n\n            * If you pass more than two characters, the engine will throw an exception.\n\n        double_precision (str, optional):\n            The keyword used for double precision columns.\n\n        integer (str, optional):\n            The keyword used for integer columns.\n\n        text (str, optional):\n            The keyword used for text columns.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"odbc\"\n\n    cmd[\"server_name_\"] = server_name\n    cmd[\"user_\"] = user\n    cmd[\"escape_chars_\"] = escape_chars\n    cmd[\"double_precision_\"] = double_precision\n    cmd[\"integer_\"] = integer\n    cmd[\"text_\"] = text\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_postgres/","title":"Connect postgres","text":"<p>Creates a new PostgreSQL database connection.</p>"},{"location":"reference/database/connect_postgres/#getml.database.connect_postgres.connect_postgres","title":"<code>connect_postgres(dbname, user, password, host=None, hostaddr=None, port=5432, time_formats=None, conn_id='default')</code>","text":"<p>Creates a new PostgreSQL database connection.</p> <p>But first, make sure your database is running, and you can reach it from via your command line.</p> <p>Parameters:</p> Name Type Description Default <code>dbname</code> <code>str</code> <p>The name of the database to which you want to connect.</p> required <code>user</code> <code>str</code> <p>Username with which to log into the PostgreSQL database.</p> required <code>password</code> <code>str</code> <p>Password with which to log into the PostgreSQL database.</p> required <code>host</code> <code>str</code> <p>Host of the PostgreSQL database.</p> <code>None</code> <code>hostaddr</code> <code>str</code> <p>IP address of the PostgreSQL database. This should be in the standard IPv4 address format, e.g., 172.28.40.9. If your machine supports IPv6, you can also use those addresses. TCP/IP communication is always used when a nonempty string is specified for this parameter.</p> <code>None</code> <code>port(int,</code> <code>optional</code> <p>Port of the PostgreSQL database.</p> <p>The default port used by PostgreSQL is 5432.</p> <p>If you do not know, which port to use, type the following into your PostgreSQL client</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> required <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the PostgreSQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_postgres.py</code> <pre><code>def connect_postgres(\n    dbname: str,\n    user: str,\n    password: str,\n    host: Optional[str] = None,\n    hostaddr: Optional[str] = None,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"\n    Creates a new PostgreSQL database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from via your command line.\n\n    Args:\n        dbname (str):\n            The name of the database to which you want to connect.\n\n        user (str):\n            Username with which to log into the PostgreSQL database.\n\n        password (str):\n            Password with which to log into the PostgreSQL database.\n\n        host (str):\n            Host of the PostgreSQL database.\n\n        hostaddr (str):\n            IP address of the PostgreSQL database.\n            This should be in the standard IPv4 address format, e.g., 172.28.40.9.\n            If your machine supports IPv6, you can also use those addresses.\n            TCP/IP communication is always used when a nonempty string is specified\n            for this parameter.\n\n        port(int, optional):\n            Port of the PostgreSQL database.\n\n            The default port used by PostgreSQL is 5432.\n\n            If you do not know, which port to use, type the following into your\n            PostgreSQL client\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the PostgreSQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"postgres\"\n\n    if host is not None:\n        cmd[\"host_\"] = host\n\n    if hostaddr is not None:\n        cmd[\"hostaddr_\"] = hostaddr\n\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connect_sqlite3/","title":"Connect sqlite3","text":"<p>Creates a new SQLite3 database connection.</p>"},{"location":"reference/database/connect_sqlite3/#getml.database.connect_sqlite3.connect_sqlite3","title":"<code>connect_sqlite3(name=':memory:', time_formats=None, conn_id='default')</code>","text":"<p>Creates a new SQLite3 database connection.</p> <p>SQLite3 is a popular in-memory database. It is faster than distributed databases, like PostgreSQL, but less stable under massive parallel access, consumes more memory and requires all contained data sets to be loaded into memory, which might fill up too much of your RAM, especially for large data sets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the sqlite3 file.  If the file does not exist, it will be created. Set to \":memory:\" for a purely in-memory SQLite3 database.</p> <code>':memory:'</code> <code>time_formats</code> <code>List[str]</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <code>None</code> <code>conn_id</code> <code>str</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <code>'default'</code> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the SQLite3 database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_sqlite3.py</code> <pre><code>def connect_sqlite3(\n    name: str = \":memory:\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n):\n    \"\"\"Creates a new SQLite3 database connection.\n\n    SQLite3 is a popular in-memory database. It is faster than\n    distributed databases, like PostgreSQL, but less stable under massive\n    parallel access, consumes more memory and requires all contained\n    data sets to be loaded into memory, which might fill up too much\n    of your RAM, especially for large data sets.\n\n    Args:\n        name (str, optional):\n            Name of the sqlite3 file.  If the file does not exist, it\n            will be created. Set to \":memory:\" for a purely in-memory SQLite3\n            database.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the SQLite3\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # We want to use the absolute path, unless it is a pure\n    # in-memory version.\n    name_ = name\n\n    if name_ != \":memory:\":\n        name_ = os.path.abspath(name)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name_\n    cmd[\"type_\"] = \"Database.new\"\n\n    cmd[\"db_\"] = \"sqlite3\"\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is usually sent separately,\n        # so it doesn't\n        # end up in the logs. However, Sqlite3 does not\n        # need a password, so we just send a dummy.\n        comm.send_string(sock, \"none\")\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/connection/","title":"Connection","text":"<p>A handle to a database connection on the getML engine.</p>"},{"location":"reference/database/connection/#getml.database.connection.Connection","title":"<code>Connection</code>","text":"<p>A handle to a database connection on the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>The name you want to use to reference the connection. You can call it anything you want to. If a database connection with the same conn_id already exists, that connection will be removed automatically and the new connection will take its place. The default conn_id is \"default\", which refers to the default connection. If you do not explicitly pass a connection handle to any function that relates to a database, the default connection will be used automatically.</p> <code>'default'</code> Source code in <code>getml/database/connection.py</code> <pre><code>class Connection:\n    \"\"\"\n    A handle to a database connection on the getML engine.\n\n    Args:\n        conn_id (str, optional):\n            The name you want to use to reference the connection.\n            You can call it\n            anything you want to. If a database\n            connection with the same conn_id already exists, that connection\n            will be removed automatically and the new connection will take its place.\n            The default conn_id is \"default\", which refers to the default connection.\n            If you do not explicitly pass a connection handle to any function that\n            relates to a database, the default connection will be used automatically.\n    \"\"\"\n\n    def __init__(self, conn_id: str = \"default\"):\n        self.conn_id = conn_id\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"name_\"] = self.conn_id\n        cmd[\"type_\"] = \"Database.describe_connection\"\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            description = comm.recv_string(sock)\n\n        json_obj = json.loads(description)\n\n        json_obj[\"type\"] = \"Connection\"\n\n        sig = _SignatureFormatter(data=json_obj)\n\n        return sig._format()\n</code></pre>"},{"location":"reference/database/copy_table/","title":"Copy table","text":"<p>Copies a table from one database connection to another.</p>"},{"location":"reference/database/copy_table/#getml.database.copy_table.copy_table","title":"<code>copy_table(source_conn, target_conn, source_table, target_table=None)</code>","text":"<p>Copies a table from one database connection to another.</p> <p>Parameters:</p> Name Type Description Default <code>source_conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be copied from.</p> required <code>target_conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be copied to.</p> required <code>source_table</code> <code>str</code> <p>The name of the table in the source connection.</p> required <code>target_table</code> <code>str</code> <p>The name of the table in the target connection. If you do not explicitly pass a target_table, the name will be identical to the source_table.</p> <code>None</code> Example <p>A frequent use case for this function is to copy data from a data source into sqlite. This is a good idea, because sqlite is faster than most standard, ACID-compliant databases, and also you want to avoid messing up a productive environment.</p> <p>It is important to explicitly pass conn_id, otherwise the source connection will be closed when you create the target connection. What you pass as conn_id is entirely up to you, as long as the conn_id of the source and the target are different. It can be any name you see fit.</p> <pre><code>source_conn = getml.database.connect_odbc(\n    \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\ntarget_conn = getml.database.connect_sqlite3(\n    \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\ndata.database.copy_table(\n        source_conn=source_conn,\n        target_conn=target_conn,\n        source_table=\"MY-TABLE\"\n)\n</code></pre> Source code in <code>getml/database/copy_table.py</code> <pre><code>def copy_table(\n    source_conn: Connection,\n    target_conn: Connection,\n    source_table: str,\n    target_table: Optional[str] = None,\n):\n    \"\"\"\n    Copies a table from one database connection to another.\n\n    Args:\n        source_conn ([`Connection`][getml.database.Connection]):\n            The database connection to be copied from.\n\n        target_conn ([`Connection`][getml.database.Connection]):\n            The database connection to be copied to.\n\n        source_table (str):\n            The name of the table in the source connection.\n\n        target_table (str, optional):\n            The name of the table in the target\n            connection. If you do not explicitly pass a target_table, the\n            name will be identical to the source_table.\n\n    Example:\n        A frequent use case for this function is to copy data from a data source into\n        sqlite. This is a good idea, because sqlite is faster than most standard,\n        ACID-compliant databases, and also you want to avoid messing up a productive\n        environment.\n\n        It is important to explicitly pass conn_id, otherwise the source connection\n        will be closed\n        when you create the target connection. What you pass as conn_id is entirely\n        up to you,\n        as long as the conn_id of the source and the target are different. It can\n        be any name you see fit.\n\n        ```python\n        source_conn = getml.database.connect_odbc(\n            \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\n        target_conn = getml.database.connect_sqlite3(\n            \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\n        data.database.copy_table(\n                source_conn=source_conn,\n                target_conn=target_conn,\n                source_table=\"MY-TABLE\"\n        )\n        ```\n\n    \"\"\"\n\n    # -------------------------------------------\n\n    target_table = target_table or source_table\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.copy_table\"\n\n    cmd[\"source_conn_id_\"] = source_conn.conn_id\n    cmd[\"target_conn_id_\"] = target_conn.conn_id\n    cmd[\"source_table_\"] = source_table\n    cmd[\"target_table_\"] = target_table\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/drop_table/","title":"Drop table","text":"<p>Drops a table from the database.</p>"},{"location":"reference/database/drop_table/#getml.database.drop_table.drop_table","title":"<code>drop_table(name, conn=None)</code>","text":"<p>Drops a table from the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The table to be dropped.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/database/drop_table.py</code> <pre><code>def drop_table(name: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Drops a table from the database.\n\n    Args:\n        name (str):\n            The table to be dropped.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.drop_table\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/execute/","title":"Execute","text":"<p>Executes an SQL query on the database.</p>"},{"location":"reference/database/execute/#getml.database.execute.execute","title":"<code>execute(query, conn=None)</code>","text":"<p>Executes an SQL query on the database.</p> <p>Please note that this is not meant to return results. If you want to get results, use database.get(...) instead.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to be executed.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/database/execute.py</code> <pre><code>def execute(query: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Executes an SQL query on the database.\n\n    Please note that this is not meant to return results. If you want to\n    get results, use database.get(...) instead.\n\n    Args:\n        query (str):\n            The SQL query to be executed.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.execute\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/database/get/","title":"Get","text":"<p>Executes an SQL query on the database and returns the result as a pandas dataframe.</p>"},{"location":"reference/database/get/#getml.database.get.get","title":"<code>get(query, conn=None)</code>","text":"<p>Executes an SQL query on the database and returns the result as a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to be executed.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/database/get.py</code> <pre><code>def get(query: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Executes an SQL query on the database and returns the result as\n    a pandas dataframe.\n\n    Args:\n        query (str):\n            The SQL query to be executed.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.get\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return pd.read_json(json_str)\n</code></pre>"},{"location":"reference/database/get_colnames/","title":"Get colnames","text":"<p>Lists the colnames of a table held in the database.</p>"},{"location":"reference/database/get_colnames/#getml.database.get_colnames.get_colnames","title":"<code>get_colnames(name, conn=None)</code>","text":"<p>Lists the colnames of a table held in the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table in the database.</p> required <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/database/get_colnames.py</code> <pre><code>def get_colnames(name: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Lists the colnames of a table held in the database.\n\n    Args:\n        name (str):\n            The name of the table in the database.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.get_colnames\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return arr\n</code></pre>"},{"location":"reference/database/helpers/","title":"Helpers","text":"<p>Collection of helper functions not meant to be used by the enduser.</p>"},{"location":"reference/database/list_connections/","title":"List connections","text":"<p>Returns a list handles to all connections that are currently active on the engine.</p>"},{"location":"reference/database/list_connections/#getml.database.list_connections.list_connections","title":"<code>list_connections()</code>","text":"<p>Returns a list handles to all connections that are currently active on the engine.</p> Source code in <code>getml/database/list_connections.py</code> <pre><code>def list_connections() -&gt; List[Connection]:\n    \"\"\"\n    Returns a list handles to all connections\n    that are currently active on the engine.\n    \"\"\"\n\n    cmd: Dict[Any, str] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.list_connections\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return [Connection(elem) for elem in arr]\n</code></pre>"},{"location":"reference/database/list_tables/","title":"List tables","text":"<p>Lists all tables and views currently held in the database.</p>"},{"location":"reference/database/list_tables/#getml.database.list_tables.list_tables","title":"<code>list_tables(conn=None)</code>","text":"<p>Lists all tables and views currently held in the database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Source code in <code>getml/database/list_tables.py</code> <pre><code>def list_tables(conn: Optional[Connection] = None) -&gt; List[str]:\n    \"\"\"\n    Lists all tables and views currently held in the database.\n\n    Args:\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.list_tables\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return json.loads(comm.recv_string(sock))\n</code></pre>"},{"location":"reference/database/read_csv/","title":"Read csv","text":"<p>Reads a CSV file into the database.</p>"},{"location":"reference/database/read_csv/#getml.database.read_csv.read_csv","title":"<code>read_csv(name, fnames, quotechar='\"', sep=',', num_lines_read=0, skip=0, colnames=None, conn=None)</code>","text":"<p>Reads a CSV file into the database.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>fnames</code> <code>List[str]</code> <p>The list of CSV file names to be read.</p> required <code>quotechar</code> <code>str</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields. Default:<code>,</code></p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv . You can import their data into the database using the following commands:</p> <pre><code>stmt = data.database.sniff_csv(\n        fnames=[\"file1.csv\", \"file2.csv\"],\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\nstmt = data.database.read_csv(\n        fnames=[\"file1.csv\", \"file2.csv\"],\n        name=\"MY_TABLE\",\n        sep=';'\n)\n</code></pre> Source code in <code>getml/database/read_csv.py</code> <pre><code>def read_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n):\n    \"\"\"\n    Reads a CSV file into the database.\n\n    Args:\n        name (str):\n            Name of the table in which the data is to be inserted.\n\n        fnames (List[str]):\n            The list of CSV file names to be read.\n\n        quotechar (str, optional):\n            The character used to wrap strings. Default:`\"`\n\n        sep (str, optional):\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* . You can import their data into the database\n        using the following commands:\n\n        ```python\n        stmt = data.database.sniff_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        stmt = data.database.read_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n        ```\n\n    \"\"\"\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    fnames_ = _retrieve_urls(fnames)\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.read_csv\"\n\n    cmd[\"fnames_\"] = fnames_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/read_s3/","title":"Read s3","text":"<p>Reads a list of CSV files located in an S3 bucket.</p>"},{"location":"reference/database/read_s3/#getml.database.read_s3.read_s3","title":"<code>read_s3(name, bucket, keys, region, sep=',', num_lines_read=0, skip=0, colnames=None, conn=None)</code>","text":"<p>Reads a list of CSV files located in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>sep</code> <code>str</code> <p>The separator used for separating fields. Default:<code>,</code></p> <code>','</code> <code>num_lines_read</code> <code>int</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <code>0</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\nstmt = data.database.read_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML engine.</p> Source code in <code>getml/database/read_s3.py</code> <pre><code>def read_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n):\n    \"\"\"\n    Reads a list of CSV files located in an S3 bucket.\n\n    Args:\n        name (str):\n            Name of the table in which the data is to be inserted.\n\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        sep (str, optional):\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read (int, optional):\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        stmt = data.database.read_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n    \"\"\"\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.read_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/sniff_csv/","title":"Sniff csv","text":"<p>Sniffs a list of CSV files.</p>"},{"location":"reference/database/sniff_csv/#getml.database.sniff_csv.sniff_csv","title":"<code>sniff_csv(name, fnames, num_lines_sniffed=1000, quotechar='\"', sep=',', skip=0, colnames=None, conn=None)</code>","text":"<p>Sniffs a list of CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>fnames</code> <code>List[str]</code> <p>The list of CSV file names to be read.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields. Default:<code>,</code></p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/database/sniff_csv.py</code> <pre><code>def sniff_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of CSV files.\n\n    Args:\n        name (str):\n            Name of the table in which the data is to be inserted.\n\n        fnames (List[str]):\n            The list of CSV file names to be read.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        quotechar (str, optional):\n            The character used to wrap strings. Default:`\"`\n\n        sep (str, optional):\n            The separator used for separating fields. Default:`,`\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        str: Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    fnames_ = _retrieve_urls(fnames)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.sniff_csv\"\n\n    cmd[\"fnames_\"] = fnames_\n    cmd[\"num_lines_sniffed_\"] = num_lines_sniffed\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return comm.recv_string(sock)\n</code></pre>"},{"location":"reference/database/sniff_s3/","title":"Sniff s3","text":"<p>Sniffs a list of CSV files located in an S3 bucket.</p>"},{"location":"reference/database/sniff_s3/#getml.database.sniff_s3.sniff_s3","title":"<code>sniff_s3(name, bucket, keys, region, num_lines_sniffed=1000, sep=',', skip=0, colnames=None, conn=None)</code>","text":"<p>Sniffs a list of CSV files located in an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>bucket</code> <code>str</code> <p>The bucket from which to read the files.</p> required <code>keys</code> <code>List[str]</code> <p>The list of keys (files in the bucket) to be read.</p> required <code>region</code> <code>str</code> <p>The region in which the bucket is located.</p> required <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>sep</code> <code>str</code> <p>The character used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <code>None</code> <code>conn</code> <code>[`Connection`][getml.database.Connection]</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML engine.</p> Source code in <code>getml/database/sniff_s3.py</code> <pre><code>def sniff_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    num_lines_sniffed: int = 1000,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n):\n    \"\"\"\n    Sniffs a list of CSV files located in an S3 bucket.\n\n\n    Args:\n        name (str):\n            Name of the table in which the data is to be inserted.\n\n        bucket (str):\n            The bucket from which to read the files.\n\n        keys (List[str]):\n            The list of keys (files in the bucket) to be read.\n\n        region (str):\n            The region in which the bucket is located.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        sep (str, optional):\n            The character used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each file.\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn ([`Connection`][getml.database.Connection], optional):\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        str: Appropriate `CREATE TABLE` statement.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.sniff_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_sniffed_\"] = num_lines_sniffed\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return comm.recv_string(sock)\n</code></pre>"},{"location":"reference/datasets/__init__/","title":"init","text":"<p>The <code>datasets</code> module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</p>"},{"location":"reference/datasets/base/","title":"Base","text":"<p>Load preprocessed datasets</p>"},{"location":"reference/datasets/base/#getml.datasets.base.BUCKET","title":"<code>BUCKET: str = 'https://static.getml.com/datasets'</code>  <code>module-attribute</code>","text":"<p>S3 bucket containing the CSV files</p>"},{"location":"reference/datasets/base/#getml.datasets.base.load_air_pollution","title":"<code>load_air_pollution(roles=True, as_pandas=False)</code>","text":"<p>Regression dataset on air pollution in Beijing, China</p> <p>The dataset consists of a single table split into train and test sets around 2014-01-01.</p> <p>The original publication is: Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather impact, APEC and winter heating. Proceedings of the Royal Society A, 471, 20150257.</p> <p>Parameters:</p> Name Type Description Default <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code>s</p> <code>False</code> <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrameT]</code> <p>getml.data.DataFrame: A DataFrame holding the data described above.</p> <code>Union[DataFrameT]</code> <p>The following DataFrames are returned:</p> <code>Union[DataFrameT]</code> <ul> <li>air_pollution</li> </ul> Example <p><pre><code>air_pollution = getml.datasets.load_air_pollution()\ntype(air_pollution)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-demo .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_air_pollution(\n    roles: bool = True,\n    as_pandas: bool = False,\n) -&gt; Union[DataFrameT]:\n    \"\"\"\n    Regression dataset on air pollution in Beijing, China\n\n    The dataset consists of a single table split into train and test sets\n    around 2014-01-01.\n\n    The original publication is:\n    Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and\n    Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather\n    impact, APEC and winter heating. Proceedings of the Royal Society A, 471,\n    20150257.\n\n    Args:\n        as_pandas (bool):\n            Return data as `pandas.DataFrame`s\n\n        roles (bool):\n            Return data with roles set\n\n    Returns:\n        getml.data.DataFrame:\n            A DataFrame holding the data described above.\n\n        The following DataFrames are returned:\n\n        * air_pollution\n\n    Example:\n        ```python\n        air_pollution = getml.datasets.load_air_pollution()\n        type(air_pollution)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-demo\n        ](https://github.com/getml/getml-demo/blob/master/air_pollution.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder]s.\n\n    \"\"\"\n\n    ds_name = \"air_pollution\"\n\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_atherosclerosis","title":"<code>load_atherosclerosis(roles=True, as_pandas=False, as_dict=False)</code>","text":"<p>Binary classification dataset on the lethality of atherosclerosis</p> <p>The atherosclerosis dataset is a medical dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository) . It contains information from a longitudinal study on 1417 middle-aged men observed over the course of 20 years. After preprocessing, it consists of 2 tables with 76 and 66 columns:</p> <ul> <li> <p><code>population</code>: Data on the study's participants</p> </li> <li> <p><code>contr</code>: Data on control dates</p> </li> </ul> <p>The population table is split into a training (70%), a testing (15%) set and a validation (15%) set.</p> <p>Parameters:</p> Name Type Description Default <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <code>False</code> <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>as_dict</code> <code>bool</code> <p>Return data as dict with <code>df.name</code> s as keys and <code>df</code> s as values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>s) the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True) or</p> <code>dict</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>The following DataFrames are returned:</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>population</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>contr</code></li> </ul> Example <p><pre><code>population, contr = getml.datasets.load_atherosclerosis()\ntype(population)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_atherosclerosis(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on the lethality of atherosclerosis\n\n    The atherosclerosis dataset is a medical dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)\n    ](https://relational-data.org/dataset/Atherosclerosis). It contains\n    information from a longitudinal study on 1417 middle-aged men observed over\n    the course of 20 years. After preprocessing, it consists of 2 tables with 76\n    and 66 columns:\n\n    * `population`: Data on the study's participants\n\n    * `contr`: Data on control dates\n\n    The population table is split into a training (70%), a testing (15%) set and a\n    validation (15%) set.\n\n    Args:\n        as_pandas (bool):\n            Return data as `pandas.DataFrame` s\n\n        roles (bool):\n            Return data with roles set\n\n        as_dict (bool):\n            Return data as dict with `df.name` s as keys and\n            `df` s as values.\n\n    Returns:\n        tuple:\n            Tuple containing (sorted alphabetically by `df.name`s) the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True) or\n        dict:\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n        The following DataFrames are returned:\n\n        * `population`\n        * `contr`\n\n    Example:\n        ```python\n        population, contr = getml.datasets.load_atherosclerosis()\n        type(population)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/atherosclerosis.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"atherosclerosis\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_biodegradability","title":"<code>load_biodegradability(roles=True, as_pandas=False, as_dict=False)</code>","text":"<p>Regression dataset on molecule weight prediction</p> <p>The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy). The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE).</p> <p>The original publication is: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878</p> <p>The dataset was collected through the Relational Dataset Repository (former CTU Prague Relational Learning Repository)</p> <p>It contains information on 1309 molecules with 6166 bonds. It consists of 5 tables.</p> <p>The population table is split into a training (50 %) and a testing (25%) and validation (25%) sets.</p> <p>Parameters:</p> Name Type Description Default <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <code>False</code> <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>as_dict</code> <code>bool</code> <p>Return data as dict with <code>df.name</code> s as keys and <code>df</code> s as values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>s) the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True) or</p> <code>dict</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>The following DataFrames are returned:</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>molecule</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>atom</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>bond</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>gmember</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>group</code></li> </ul> Example <pre><code>biodegradability = getml.datasets.load_biodegradability(as_dict=True)\ntype(biodegradability[\"molecule_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_biodegradability(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Regression dataset on molecule weight prediction\n\n    The QSAR biodegradation dataset was built in the Milano Chemometrics and\n    QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy).\n    The data have been used to develop QSAR (Quantitative Structure Activity\n    Relationships) models for the study of the relationships between chemical\n    structure and biodegradation of molecules. Biodegradation experimental\n    values of 1055 chemicals were collected from the webpage of the National\n    Institute of Technology and Evaluation of Japan (NITE).\n\n    The original publication is:\n    Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V.\n    (2013). Quantitative Structure - Activity Relationship models for ready\n    biodegradability of chemicals. Journal of Chemical Information and Modeling,\n    53, 867-878\n\n    The dataset was collected through the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Biodegradability)\n\n    It contains information on 1309 molecules with 6166 bonds. It consists of 5\n    tables.\n\n    The population table is split into a training (50 %) and a testing (25%) and\n    validation (25%) sets.\n\n    Args:\n        as_pandas (bool):\n            Return data as `pandas.DataFrame` s\n\n        roles (bool):\n            Return data with roles set\n\n        as_dict (bool):\n            Return data as dict with `df.name` s as keys and\n            `df` s as values.\n\n    Returns:\n        tuple:\n            Tuple containing (sorted alphabetically by `df.name`s) the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True) or\n        dict:\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n        The following DataFrames are returned:\n\n        * `molecule`\n        * `atom`\n        * `bond`\n        * `gmember`\n        * `group`\n\n    Example:\n        ```python\n        biodegradability = getml.datasets.load_biodegradability(as_dict=True)\n        type(biodegradability[\"molecule_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"biodegradability\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_consumer_expenditures","title":"<code>load_consumer_expenditures(roles=True, units=True, as_pandas=False, as_dict=False)</code>","text":"<p>Binary classification dataset on consumer expenditures</p> <p>The Consumer Expenditure Data Set is a public domain data set provided by the American Bureau of Labor Statistics. It includes the diary entries, where American consumers are asked to keep record of the products they have purchased each month.</p> <p>We use this dataset to classify whether an item was purchased as a gift or not.</p> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>units</code> <code>bool</code> <p>Return data with units set</p> <code>True</code> <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <code>False</code> <code>as_dict</code> <code>bool</code> <p>Return data as dict with <code>df.name</code> s as keys and <code>df</code> s as values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>s) the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True) or</p> <code>dict</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>The following DataFrames are returned:</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>population</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>expd</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>fmld</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>memd</code></li> </ul> Example <p><pre><code>ce = getml.datasets.load_consumer_expenditures(as_dict=True)\ntype(ce[\"expd\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_consumer_expenditures(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on consumer expenditures\n\n    The Consumer Expenditure Data Set is a public domain data set provided by\n    the [American Bureau of Labor Statistics](https://www.bls.gov/cex/pumd.htm).\n    It includes the diary entries, where American consumers are asked to keep\n    record of the products they have purchased each month.\n\n    We use this dataset to classify whether an item was purchased as a gift or not.\n\n    Args:\n        roles (bool):\n            Return data with roles set\n\n        units (bool):\n            Return data with units set\n\n        as_pandas (bool):\n            Return data as `pandas.DataFrame` s\n\n        as_dict (bool):\n            Return data as dict with `df.name` s as keys and\n            `df` s as values.\n\n    Returns:\n        tuple:\n            Tuple containing (sorted alphabetically by `df.name`s) the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True) or\n        dict:\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n        The following DataFrames are returned:\n\n        * `population`\n        * `expd`\n        * `fmld`\n        * `memd`\n\n    Example:\n        ```python\n        ce = getml.datasets.load_consumer_expenditures(as_dict=True)\n        type(ce[\"expd\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/consumer_expenditures.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float].\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"consumer_expenditures\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_interstate94","title":"<code>load_interstate94(roles=True, units=True, as_pandas=False)</code>","text":"<p>Regression dataset on traffic volume prediction</p> <p>The interstate94 dataset is a multivariate time series containing the hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is based on data provided by the MN Department of Transportation . Some additional data preparation done by John Hogue. The dataset features some particular interesting characteristics common for time series, which classical models may struggle to appropriately deal with. Such characteristics are:</p> <ul> <li>High frequency (hourly)</li> <li>Dependence on irregular events (holidays)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>units</code> <code>bool</code> <p>Return data with units set</p> <code>True</code> <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrameT]</code> <p>getml.data.DataFrame: A DataFrame holding the data described above.</p> <code>Union[DataFrameT]</code> <p>The following DataFrames are returned:</p> <code>Union[DataFrameT]</code> <ul> <li><code>traffic</code></li> </ul> Example <p><pre><code>traffic = getml.datasets.load_interstate94()\ntype(traffic)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the interstate94 dataset including all necessary preprocessing steps please refer to getml-examples.</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_interstate94(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n) -&gt; Union[DataFrameT]:\n    \"\"\"\n    Regression dataset on traffic volume prediction\n\n    The interstate94 dataset is a multivariate time series containing the\n    hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is\n    based on data provided by the [MN Department of Transportation\n    ](https://www.dot.state.mn.us/). Some additional data preparation done by\n    [John Hogue](https://github.com/dreyco676/Anomaly_Detection_A_to_Z/). The\n    dataset features some particular interesting characteristics common for\n    time series, which classical models may struggle to appropriately deal\n    with. Such characteristics are:\n\n    * High frequency (hourly)\n    * Dependence on irregular events (holidays)\n    * Strong and overlapping cycles (daily, weekly)\n    * Anomalies\n    * Multiple seasonalities\n\n    Args:\n        roles (bool):\n            Return data with roles set\n\n        units (bool):\n            Return data with units set\n\n        as_pandas (bool):\n            Return data as `pandas.DataFrame` s\n\n    Returns:\n        getml.data.DataFrame:\n            A DataFrame holding the data described above.\n\n        The following DataFrames are returned:\n\n        * `traffic`\n\n    Example:\n        ```python\n        traffic = getml.datasets.load_interstate94()\n        type(traffic)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the interstate94 dataset including all necessary\n        preprocessing steps please refer to [getml-examples](https://github.com/getml/getml-demo/blob/master/interstate94.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"interstate94\"\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_loans","title":"<code>load_loans(roles=True, units=True, as_pandas=False, as_dict=False)</code>","text":"<p>Binary classification dataset on loan default</p> <p>The loans dataset is based on a financial dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository).</p> <p>The original publication is: Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.</p> <p>The dataset contains information on 606 successful and 76 unsuccessful loans. After some preprocessing it contains 5 tables</p> <ul> <li> <p><code>account</code>: Information about the borrower(s) of a given loan.</p> </li> <li> <p><code>loan</code>: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)</p> </li> <li> <p><code>meta</code>: Meta information about the obligor, such as gender and geo-information</p> </li> <li> <p><code>order</code>: Information about permanent orders, debited payments and account balances.</p> </li> <li> <p><code>trans</code>: Information about transactions and accounts balances.</p> </li> </ul> <p>The population table is split into a training and a testing set at 80% of the main population.</p> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>units</code> <code>bool</code> <p>Return data with units set</p> <code>True</code> <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code>s</p> <code>False</code> <code>as_dict</code> <code>bool</code> <p>Return data as dict with <code>df.name</code> s as keys and <code>df</code> s as values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>s) the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True) or</p> <code>dict</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>The following DataFrames are returned:</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>account</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>loan</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>meta</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>order</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>trans</code></li> </ul> Example <p><pre><code>loans = getml.datasets.load_loans(as_dict=True)\ntype(loans[\"population_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the loans dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_loans(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on loan default\n\n    The loans dataset is based on a financial dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Financial).\n\n    The original publication is:\n    Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.\n\n    The dataset contains information on 606 successful and 76 unsuccessful\n    loans. After some preprocessing it contains 5 tables\n\n    * `account`: Information about the borrower(s) of a given loan.\n\n    * `loan`: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)\n\n    * `meta`: Meta information about the obligor, such as gender and geo-information\n\n    * `order`: Information about permanent orders, debited payments and account balances.\n\n    * `trans`: Information about transactions and accounts balances.\n\n    The population table is split into a training and a testing set at 80% of the main population.\n\n    Args:\n        roles (bool):\n            Return data with roles set\n\n        units (bool):\n            Return data with units set\n\n        as_pandas (bool):\n            Return data as `pandas.DataFrame`s\n\n        as_dict (bool):\n            Return data as dict with `df.name` s as keys and\n            `df` s as values.\n\n    Returns:\n        tuple:\n            Tuple containing (sorted alphabetically by `df.name`s) the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True) or\n        dict:\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n        The following DataFrames are returned:\n\n        * `account`\n        * `loan`\n        * `meta`\n        * `order`\n        * `trans`\n\n    Example:\n        ```python\n        loans = getml.datasets.load_loans(as_dict=True)\n        type(loans[\"population_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the loans dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/loans.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"loans\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/base/#getml.datasets.base.load_occupancy","title":"<code>load_occupancy(roles=True, as_pandas=False, as_dict=False)</code>","text":"<p>Binary classification dataset on occupancy detection</p> <p>The occupancy detection dataset is a very simple multivariate time series from the UCI Machine Learning Repository . It is a binary classification problem. The task is to predict room occupancy from Temperature, Humidity, Light and CO2.</p> <p>The original publication is: Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Energy and Buildings, 112, 28-39.</p> <p>Parameters:</p> Name Type Description Default <code>roles</code> <code>bool</code> <p>Return data with roles set</p> <code>True</code> <code>as_pandas</code> <code>bool</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <code>False</code> <code>as_dict</code> <code>bool</code> <p>Return data as dict with <code>df.name</code> s as keys and <code>df</code> s as values.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>s) the data as <code>DataFrame</code>s or <code>pandas.DataFrame</code>s (if <code>as_pandas</code> is True) or</p> <code>dict</code> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> s or <code>pandas.DataFrame</code> s (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>The following DataFrames are returned:</p> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>population_train</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>population_test</code></li> </ul> <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <ul> <li><code>population_validation</code></li> </ul> Example <p><pre><code>population_train, population_test, _ = getml.datasets.load_occupancy()\ntype(occupancy_train)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code>s have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>s.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_occupancy(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on occupancy detection\n\n    The occupancy detection dataset is a very simple multivariate time series\n    from the [UCI Machine Learning Repository\n    ](https://archive.ics.uci.edu/dataset/357/occupancy+detection). It is a\n    binary classification problem. The task is to predict room occupancy\n    from Temperature, Humidity, Light and CO2.\n\n    The original publication is:\n    Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an\n    office room from light, temperature, humidity and CO2 measurements using\n    statistical learning models. Energy and Buildings, 112, 28-39.\n\n    Args:\n        roles (bool):\n            Return data with roles set\n\n        as_pandas (bool):\n            Return data as `pandas.DataFrame` s\n\n        as_dict (bool):\n            Return data as dict with `df.name` s as keys and\n            `df` s as values.\n\n    Returns:\n        tuple:\n            Tuple containing (sorted alphabetically by `df.name`s) the data as\n            [`DataFrame`][getml.DataFrame]s or `pandas.DataFrame`s (if `as_pandas`\n            is True) or\n        dict:\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] s or `pandas.DataFrame` s (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n        The following DataFrames are returned:\n\n        * `population_train`\n        * `population_test`\n        * `population_validation`\n\n    Example:\n        ```python\n        population_train, population_test, _ = getml.datasets.load_occupancy()\n        type(occupancy_train)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/occupancy.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame]s have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder]s.\n    \"\"\"\n\n    ds_name = \"occupancy\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/samples_generator/","title":"Samples generator","text":"<p>Generate samples of artificial data sets.</p>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_categorical","title":"<code>make_categorical(n_rows_population=500, n_rows_peripheral=125000, random_state=None, population_name='', peripheral_name='', aggregation=aggregations.Count)</code>","text":"<p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9. The SQL definition of the target variable read like this</li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral</code> <code>int</code> <p>Number of rows in the peripheral table.</p> required <code>random_state</code> <code>Optional[int]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_peripheral_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation</code> <code>string</code> <p><code>aggregations</code> used to generate the 'target' column.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9. The SQL definition of the target variable read like this\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral (int, optional):\n            Number of rows in the peripheral table.\n\n        random_state (Optional[int], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation (string, optional):\n            [`aggregations`][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(0, 10, n_rows_peripheral).astype(str)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] != \"1\")\n        &amp; (temp[\"column_01\"] != \"2\")\n        &amp; (temp[\"column_01\"] != \"9\")\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"categorical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"categorical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_discrete","title":"<code>make_discrete(n_rows_population=500, n_rows_peripheral=125000, random_state=None, population_name='', peripheral_name='', aggregation=aggregations.Count)</code>","text":"<p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random integer between -10 and 10</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the minimum value greater than 0   in the peripheral table for which   <code>time_stamp_peripheral &lt; time_stamp_population</code>   and the join key matches <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 &gt; 0 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n         t1.time_stamp;\n</code></pre></li> </ul> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral</code> <code>int</code> <p>Number of rows in the peripheral table.</p> required <code>random_state</code> <code>Optional[int]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_peripheral_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation</code> <code>string</code> <p>aggregations used to generate the 'target' column.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_discrete(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random integer between -10 and 10\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the minimum value greater than 0\n      in the peripheral table for which\n      ``time_stamp_peripheral &lt; time_stamp_population``\n      and the join key matches\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 &gt; 0 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n             t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral (int, optional):\n            Number of rows in the peripheral table.\n\n        random_state (Optional[int], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_population_` and the seed of the random\n            number generator.\n\n        peripheral_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(-11, 11, n_rows_peripheral)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] &gt; 0.0)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"discrete_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"discrete_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_numerical","title":"<code>make_numerical(n_rows_population=500, n_rows_peripheral=125000, random_state=None, population_name='', peripheral_name='', aggregation=aggregations.Count)</code>","text":"<p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral</code> <code>int</code> <p>Number of rows in the peripheral table.</p> required <code>random_state</code> <code>Optional[int]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_peripheral_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation</code> <code>string</code> <p>aggregations used to generate the 'target' column.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral (int, optional):\n            Number of rows in the peripheral table.\n\n        random_state (Optional[int], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"time_stamp_peripheral\"] &gt;= temp[\"time_stamp_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_same_units_categorical","title":"<code>make_same_units_categorical(n_rows_population=500, n_rows_peripheral=125000, random_state=None, population_name='', peripheral_name='', aggregation=aggregations.Count)</code>","text":"<p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9</li> </ul> <pre><code>SELECT aggregation( column_02 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 == t2.column_01 )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral</code> <code>int</code> <p>Number of rows in the peripheral table.</p> required <code>random_state</code> <code>Optional[int]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_peripheral_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation</code> <code>string</code> <p>aggregations used to generate the 'target' column.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9\n\n    ```sql\n    SELECT aggregation( column_02 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 == t2.column_01 )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral (int, optional):\n            Number of rows in the peripheral table.\n\n        random_state (Optional[int], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        (random.rand(n_rows_population) * 10.0).astype(np.int32).astype(str)\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        (random.rand(n_rows_peripheral) * 10.0).astype(np.int32).astype(str)\n    )\n    peripheral_table[\"column_02\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] == temp[\"column_01_population\"])\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_02\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_02\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    population_name = (\n        population_name\n        or \"make_same_units_categorical_population__\" + str(random_state)\n    )\n\n    peripheral_name = (\n        peripheral_name\n        or \"make_same_units_categorical_peripheral__\" + str(random_state)\n    )\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"numerical\": [\"column_02\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_same_units_numerical","title":"<code>make_same_units_numerical(n_rows_population=500, n_rows_peripheral=125000, random_state=None, population_name='', peripheral_name='', aggregation=aggregations.Count)</code>","text":"<p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral</code> <code>int</code> <p>Number of rows in the peripheral table.</p> required <code>random_state</code> <code>Union[int, None]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name</code> <code>string</code> <p>Name assigned to <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_peripheral_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation</code> <code>string</code> <p>aggregations used to generate the 'target' column.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral (int, optional):\n            Number of rows in the peripheral table.\n\n        random_state (Union[int, None], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name (string, optional):\n            Name assigned to\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        random.rand(n_rows_population) * 2.0 - 1.0\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        random.rand(n_rows_peripheral) * 2.0 - 1.0\n    )\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] &gt; temp[\"column_01_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = (\n        temp[[\"column_01_peripheral\", \"join_key\"]]\n        .groupby([\"join_key\"], as_index=False)\n        .count()\n    )\n\n    temp = temp.rename(index=str, columns={\"column_01_peripheral\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"same_unit_numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"same_unit_numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/samples_generator/#getml.datasets.samples_generator.make_snowflake","title":"<code>make_snowflake(n_rows_population=500, n_rows_peripheral1=5000, n_rows_peripheral2=125000, random_state=None, population_name='', peripheral_name1='', peripheral_name2='', aggregation1=aggregations.Sum, aggregation2=aggregations.Count)</code>","text":"<p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and two peripheral tables.</p> <p>The first peripheral table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>join_key2</code>: unique integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The second peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key2</code>: random integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable as defined by the SQL block below:</li> </ul> <pre><code>SELECT aggregation1( feature_1_1 )\nFROM POPULATION t1\nLEFT JOIN (\n    SELECT aggregation2( t4.column_01 ) AS feature_1_1\n    FROM PERIPHERAL t3\n    LEFT JOIN PERIPHERAL2 t4\n    ON t3.join_key2 = t4.join_key2\n    WHERE (\n       ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n    ) AND t4.time_stamp &lt;= t3.time_stamp\n    GROUP BY t3.join_key,\n         t3.time_stamp\n) t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_rows_population</code> <code>int</code> <p>Number of rows in the population table.</p> <code>500</code> <code>n_row_peripheral1</code> <code>int</code> <p>Number of rows in the first peripheral table.</p> required <code>n_row_peripheral2</code> <code>int</code> <p>Number of rows in the second peripheral table.</p> required <code>random_state</code> <code>Union[int, None]</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <code>None</code> <code>population_name</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_population_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name1</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the first peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_1_</code> and the seed of the random number generator.</p> <code>''</code> <code>peripheral_name2</code> <code>string</code> <p>Name assigned to the <code>DataFrame</code> holding the second peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_2_</code> and the seed of the random number generator.</p> <code>''</code> <code>aggregation1</code> <code>string</code> <p>aggregations used to generate the 'target' column in the first peripheral table.</p> <code>Sum</code> <code>aggregation2</code> <code>string</code> <p>aggregations used to generate the 'target' column in the second peripheral table.</p> <code>Count</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> <li>peripheral_2 (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_snowflake(\n    n_rows_population: int = 500,\n    n_rows_peripheral1: int = 5000,\n    n_rows_peripheral2: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name1: str = \"\",\n    peripheral_name2: str = \"\",\n    aggregation1: str = aggregations.Sum,\n    aggregation2: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and two peripheral tables.\n\n    The first peripheral table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `join_key2`: unique integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The second peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key2`: random integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable as defined by the SQL block below:\n\n    ```sql\n    SELECT aggregation1( feature_1_1 )\n    FROM POPULATION t1\n    LEFT JOIN (\n        SELECT aggregation2( t4.column_01 ) AS feature_1_1\n        FROM PERIPHERAL t3\n        LEFT JOIN PERIPHERAL2 t4\n        ON t3.join_key2 = t4.join_key2\n        WHERE (\n           ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n        ) AND t4.time_stamp &lt;= t3.time_stamp\n        GROUP BY t3.join_key,\n             t3.time_stamp\n    ) t2\n    ON t1.join_key = t2.join_key\n    WHERE t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population (int, optional):\n            Number of rows in the population table.\n\n        n_row_peripheral1 (int, optional):\n            Number of rows in the first peripheral table.\n\n        n_row_peripheral2 (int, optional):\n            Number of rows in the second peripheral table.\n\n        random_state (Union[int, None], optional):\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `snowflake_population_` and the seed of the random\n            number generator.\n\n        peripheral_name1 (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the first\n            peripheral table. If set to a name already existing on the\n            getML engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_1_` and the seed of the\n            random number generator.\n\n        peripheral_name2 (string, optional):\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the second\n            peripheral table. If set to a name already existing on the\n            getML engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_2_` and the seed of the\n            random number generator.\n\n        aggregation1 (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the first peripheral table.\n\n        aggregation2 (string, optional):\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the second peripheral table.\n\n    Returns:\n        tuple:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n            * peripheral_2 ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral1) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral1)\n    ]\n    peripheral_table[\"join_key2\"] = range(n_rows_peripheral1)\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral1)\n\n    peripheral_table2 = pd.DataFrame()\n    peripheral_table2[\"column_01\"] = random.rand(n_rows_peripheral2) * 2.0 - 1.0\n    peripheral_table2[\"join_key2\"] = [\n        int(float(n_rows_peripheral1) * random.rand(1)[0])\n        for i in range(n_rows_peripheral2)\n    ]\n    peripheral_table2[\"time_stamp_peripheral2\"] = random.rand(n_rows_peripheral2)\n\n    # ----------------\n    # Merge peripheral_table with peripheral_table2\n\n    temp = peripheral_table2.merge(\n        peripheral_table[[\"join_key2\", \"time_stamp_peripheral\"]],\n        how=\"left\",\n        on=\"join_key2\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral2\"] &lt;= temp[\"time_stamp_peripheral\"])\n        &amp; (temp[\"time_stamp_peripheral2\"] &gt;= temp[\"time_stamp_peripheral\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation2, \"column_01\", \"join_key2\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"temporary\"})\n\n    peripheral_table = peripheral_table.merge(temp, how=\"left\", on=\"join_key2\")\n\n    del temp\n\n    # Replace NaN with 0.0\n    peripheral_table[\"temporary\"] = [\n        0.0 if val != val else val for val in peripheral_table[\"temporary\"]\n    ]\n\n    # ----------------\n    # Merge population_table with peripheral_table\n\n    temp2 = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp2 = temp2[(temp2[\"time_stamp_peripheral\"] &lt;= temp2[\"time_stamp_population\"])]\n\n    # Define the aggregation\n    temp2 = _aggregate(temp2, aggregation1, \"temporary\", \"join_key\")\n\n    temp2 = temp2.rename(index=str, columns={\"temporary\": \"targets\"})\n\n    population_table = population_table.merge(temp2, how=\"left\", on=\"join_key\")\n\n    del temp2\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # Remove temporary column.\n    del peripheral_table[\"temporary\"]\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    peripheral_table2 = peripheral_table2.rename(\n        index=str, columns={\"time_stamp_peripheral2\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"snowflake_population_\" + str(random_state)\n    if not peripheral_name1:\n        peripheral_name1 = \"snowflake_peripheral_1_\" + str(random_state)\n    if not peripheral_name2:\n        peripheral_name2 = \"snowflake_peripheral_2_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name1,\n        roles={\n            \"join_key\": [\"join_key\", \"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    peripheral_on_engine2 = data.DataFrame(\n        name=peripheral_name2,\n        roles={\n            \"join_key\": [\"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table2)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine, peripheral_on_engine2\n</code></pre>"},{"location":"reference/engine/__init__/","title":"init","text":"<p>This module is a collection of utility functions for the overall communication and the session management of the getML engine.</p> <p>In order to log into the engine, you have to open your favorite internet browser and enter http://localhost:1709 in the navigation bar. This tells it to connect to a local TCP socket at port 1709 opened by the getML monitor. This will only be possible from within the same device!</p> Example <p>First of all, you need to start the getML engine. Next, you need to create a new project or load an existing one.</p> <p><pre><code>getml.engine.list_projects()\ngetml.engine.set_project('test')\n</code></pre> After doing all calculations for today you can shut down the getML engine.</p> <pre><code>print(getml.engine.is_alive())\ngetml.engine.shutdown()\n</code></pre> Note <p>The Python process and the getML engine must be located on the same machine. If you intend to run the engine on a remote host, make sure to start your Python session on that device as well. Also, when using SSH sessions, make sure to start Python using <code>python &amp;</code> followed by <code>disown</code> or using <code>nohup python</code>. This ensures the Python process and all the script it has to run won't be killed the moment your remote connection becomes unstable, and you are able to recover them later on (see <code>remote_access</code>).</p> <p>All data frame objects and models in the getML engine are bundled in projects. When loading an existing project, the current memory of the engine will be flushed and all changes applied to <code>DataFrame</code> instances after calling their <code>save</code> method will be lost. Afterwards, all <code>Pipeline</code> will be loaded into memory automatically. The data frame objects will not be loaded automatically since they consume significantly more memory than the pipelines. They can be loaded manually using <code>load_data_frame</code> or <code>load</code>.</p> <p>The getML engine reflects the separation of data into individual projects on the level of the filesystem too. All data belonging to a single project is stored in a dedicated folder in the 'projects' directory located in '.getML' in your home folder. These projects can be copied and shared between different platforms and architectures without any loss of information. However, you must copy the entire project and not just individual data frames or pipelines.</p>"},{"location":"reference/engine/__init__/#getml.engine.delete_project","title":"<code>delete_project(name)</code>","text":"<p>Deletes a project.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of your project.</p> required Note <p>All data and models contained in the project directory will be permanently lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def delete_project(name):\n    \"\"\"Deletes a project.\n\n    Args:\n        name (str):\n            Name of your project.\n\n    Note:\n        All data and models contained in the project directory will be\n        permanently lost.\n\n    \"\"\"\n    _delete_project(name)\n</code></pre>"},{"location":"reference/engine/__init__/#getml.engine.list_projects","title":"<code>list_projects()</code>","text":"<p>List all projects on the getML engine.</p> <p>Returns:</p> Type Description <p>List[str]: Lists the name of all the projects.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_projects():\n    \"\"\"\n    List all projects on the getML engine.\n\n    Returns:\n        List[str]:\n            Lists the name of all the projects.\n    \"\"\"\n    return _list_projects_impl(running_only=False)\n</code></pre>"},{"location":"reference/engine/__init__/#getml.engine.list_running_projects","title":"<code>list_running_projects()</code>","text":"<p>List all projects on the getML engine that are currently running.</p> <p>Returns:</p> Type Description <p>List[str]: Lists the name of all the projects currently running.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_running_projects():\n    \"\"\"\n    List all projects on the getML engine that are currently running.\n\n    Returns:\n        List[str]: Lists the name of all the projects currently running.\n    \"\"\"\n    return _list_projects_impl(running_only=True)\n</code></pre>"},{"location":"reference/engine/__init__/#getml.engine.set_project","title":"<code>set_project(name)</code>","text":"<p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the engine, a new one will be created.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new project.</p> required Source code in <code>getml/engine/helpers.py</code> <pre><code>def set_project(name):\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the engine, a new one will\n    be created.\n\n    Args:\n        name (str):\n            Name of the new project.\n    \"\"\"\n    _set_project(name)\n</code></pre>"},{"location":"reference/engine/__init__/#getml.engine.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down the getML engine.</p> Note <p>All changes applied to the <code>DataFrame</code> after calling their <code>save</code> method will be lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def shutdown():\n    \"\"\"Shuts down the getML engine.\n\n    Note:\n        All changes applied to the [`DataFrame`][getml.DataFrame]\n        after calling their [`save`][getml.DataFrame.save]\n        method will be lost.\n\n    \"\"\"\n    _shutdown()\n</code></pre>"},{"location":"reference/engine/__init__/#getml.engine.suspend_project","title":"<code>suspend_project(name)</code>","text":"<p>Suspends a project that is currently running.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of your project.</p> required Source code in <code>getml/engine/helpers.py</code> <pre><code>def suspend_project(name):\n    \"\"\"Suspends a project that is currently running.\n\n    Args:\n        name (str):\n            Name of your project.\n    \"\"\"\n    _suspend_project(name)\n</code></pre>"},{"location":"reference/engine/helpers/","title":"Helpers","text":"<p>Contains various helper functions related to the getML engine.</p>"},{"location":"reference/engine/helpers/#getml.engine.helpers.delete_project","title":"<code>delete_project(name)</code>","text":"<p>Deletes a project.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of your project.</p> required Note <p>All data and models contained in the project directory will be permanently lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def delete_project(name):\n    \"\"\"Deletes a project.\n\n    Args:\n        name (str):\n            Name of your project.\n\n    Note:\n        All data and models contained in the project directory will be\n        permanently lost.\n\n    \"\"\"\n    _delete_project(name)\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.is_engine_alive","title":"<code>is_engine_alive()</code>","text":"<p>Checks if the getML engine is running.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the getML engine is running and ready to accept commands and False otherwise.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def is_engine_alive():\n    \"\"\"Checks if the getML engine is running.\n\n    Returns:\n        bool:\n            True if the getML engine is running and ready to accept\n            commands and False otherwise.\n\n    \"\"\"\n\n    cmd: Dict[str, str] = {}\n    cmd[\"type_\"] = \"is_alive\"\n    cmd[\"name_\"] = \"\"\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", comm.port))\n    except ConnectionRefusedError:\n        return False\n\n    comm.send_string(sock, json.dumps(cmd))\n\n    sock.close()\n\n    return True\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.list_projects","title":"<code>list_projects()</code>","text":"<p>List all projects on the getML engine.</p> <p>Returns:</p> Type Description <p>List[str]: Lists the name of all the projects.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_projects():\n    \"\"\"\n    List all projects on the getML engine.\n\n    Returns:\n        List[str]:\n            Lists the name of all the projects.\n    \"\"\"\n    return _list_projects_impl(running_only=False)\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.list_running_projects","title":"<code>list_running_projects()</code>","text":"<p>List all projects on the getML engine that are currently running.</p> <p>Returns:</p> Type Description <p>List[str]: Lists the name of all the projects currently running.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_running_projects():\n    \"\"\"\n    List all projects on the getML engine that are currently running.\n\n    Returns:\n        List[str]: Lists the name of all the projects currently running.\n    \"\"\"\n    return _list_projects_impl(running_only=True)\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.set_project","title":"<code>set_project(name)</code>","text":"<p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the engine, a new one will be created.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new project.</p> required Source code in <code>getml/engine/helpers.py</code> <pre><code>def set_project(name):\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the engine, a new one will\n    be created.\n\n    Args:\n        name (str):\n            Name of the new project.\n    \"\"\"\n    _set_project(name)\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down the getML engine.</p> Note <p>All changes applied to the <code>DataFrame</code> after calling their <code>save</code> method will be lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def shutdown():\n    \"\"\"Shuts down the getML engine.\n\n    Note:\n        All changes applied to the [`DataFrame`][getml.DataFrame]\n        after calling their [`save`][getml.DataFrame.save]\n        method will be lost.\n\n    \"\"\"\n    _shutdown()\n</code></pre>"},{"location":"reference/engine/helpers/#getml.engine.helpers.suspend_project","title":"<code>suspend_project(name)</code>","text":"<p>Suspends a project that is currently running.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of your project.</p> required Source code in <code>getml/engine/helpers.py</code> <pre><code>def suspend_project(name):\n    \"\"\"Suspends a project that is currently running.\n\n    Args:\n        name (str):\n            Name of your project.\n    \"\"\"\n    _suspend_project(name)\n</code></pre>"},{"location":"reference/engine/launch/","title":"Launch","text":""},{"location":"reference/engine/launch/#getml.engine.launch.launch","title":"<code>launch(allow_push_notifications=True, allow_remote_ips=False, home_directory=None, http_port=None, in_memory=True, install=False, launch_browser=True, log=False, project_directory=None, proxy_url=None, token=None)</code>","text":"<p>Launches the getML engine.</p> <p>Parameters:</p> Name Type Description Default <code>allow_push_notifications</code> <code>bool</code> <p>Whether you want to allow the getML monitor to send push notifications to your desktop.</p> <code>True</code> <code>allow_remote_ips</code> <code>bool</code> <p>Whether you want to allow remote IPs to access the http-port.</p> <code>False</code> <code>home_directory</code> <code>str</code> <p>The directory which should be treated as the home directory by getML. getML will create a hidden folder named '.getML' in said directory. All binaries will be installed there.</p> <code>None</code> <code>http_port</code> <code>int</code> <p>The local port of the getML monitor. This port can only be accessed from your local computer, unless you set <code>allow_remote_ips=True</code>.</p> <code>None</code> <code>in_memory</code> <code>bool</code> <p>Whether you want the engine to process everything in memory.</p> <code>True</code> <code>launch_browser</code> <code>bool</code> <p>Whether you want to automatically launch your browser.</p> <code>True</code> <code>log</code> <code>bool</code> <p>Whether you want the engine log to appear in the logfile (more detailed logging). The engine log also appears in the 'Log' page of the monitor.</p> <code>False</code> <code>project_directory</code> <code>str</code> <p>The directory in which to store all of your projects.</p> <code>None</code> <code>proxy_url</code> <code>str</code> <p>The URL of any proxy server that that redirects to the getML monitor.</p> <code>None</code> <code>tcp_port</code> <code>int</code> <p>Local TCP port which serves as the communication point for the engine. This port can only be accessed from your local computer.</p> required <code>token</code> <code>str</code> <p>The token used for authentication. Authentication is required when remote IPs are allowed to access the monitor. If authentication is required and no token is passed, a random hexcode will be generated as the token.</p> <code>None</code> Source code in <code>getml/engine/launch.py</code> <pre><code>def launch(\n    allow_push_notifications=True,\n    allow_remote_ips=False,\n    home_directory: Optional[str] = None,\n    http_port: Optional[int] = None,\n    in_memory=True,\n    install=False,\n    launch_browser=True,\n    log=False,\n    project_directory: Optional[str] = None,\n    proxy_url: Optional[str] = None,\n    token: Optional[str] = None,\n):\n    \"\"\"\n    Launches the getML engine.\n\n    Args:\n      allow_push_notifications (bool):\n        Whether you want to allow the getML monitor to send push notifications to your desktop.\n\n      allow_remote_ips (bool):\n        Whether you want to allow remote IPs to access the http-port.\n\n      home_directory (str, optional):\n        The directory which should be treated as the home directory by getML.\n        getML will create a hidden folder named '.getML' in said directory.\n        All binaries will be installed there.\n\n      http_port (int, optional):\n        The local port of the getML monitor.\n        This port can only be accessed from your local computer,\n        unless you set `allow_remote_ips=True`.\n\n      in_memory (bool):\n        Whether you want the engine to process everything in memory.\n\n      install (bool)\n        Reinstalls getML, even if it is already installed.\n\n      launch_browser (bool):\n        Whether you want to automatically launch your browser.\n\n      log (bool):\n        Whether you want the engine log to appear in the logfile (more detailed logging).\n        The engine log also appears in the 'Log' page of the monitor.\n\n      project_directory (str, optional):\n        The directory in which to store all of your projects.\n\n      proxy_url (str, optional):\n        The URL of any proxy server that that redirects to the getML monitor.\n\n      tcp_port (int, optional):\n        Local TCP port which serves as the communication point for the engine.\n        This port can only be accessed from your local computer.\n\n      token (str, optional):\n        The token used for authentication.\n        Authentication is required when remote IPs are allowed to access the monitor.\n        If authentication is required and no token is passed,\n        a random hexcode will be generated as the token.\"\"\"\n    if _is_monitor_alive():\n        print(\"getML engine is already running.\")\n        return\n    home_path = _make_home_path(home_directory)\n    binary_path = _find_binary(home_path)\n    log_path = _make_log_path(home_path)\n    logfile = open(str(log_path), \"w\", encoding=\"utf-8\")\n    cmd = _Options(\n        allow_push_notifications=allow_push_notifications,\n        allow_remote_ips=allow_remote_ips,\n        home_directory=str(home_path),\n        http_port=http_port,\n        in_memory=in_memory,\n        install=install,\n        launch_browser=launch_browser,\n        log=log,\n        project_directory=project_directory,\n        proxy_url=proxy_url,\n        token=token,\n    ).to_cmd(binary_path)\n    cwd = str(binary_path.parent)\n    print(f\"Launching {' '.join(cmd)} in {cwd}...\")\n    Popen(cmd, cwd=cwd, shell=False, stdout=logfile, stdin=logfile, stderr=logfile)\n    while not _is_monitor_alive():\n        sleep(0.1)\n    print(f\"Launched the getML engine. The log output will be stored in {log_path}.\")\n</code></pre>"},{"location":"reference/feature_learning/__init__/","title":"init","text":"<p>This module contains relational learning algorithms to learn features from relational data or time series.</p> Note <p>All feature learners need to be passed to <code>Pipeline</code>.</p>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.FastProp","title":"<code>FastProp</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Generates simple features based on propositionalization.</p> <p><code>FastProp</code> generates simple and easily interpretable features for relational data and time series. It is based on a propositionalization approach and has been optimized for speed and memory efficiency. <code>FastProp</code> generates a large number of features and selects the most relevant ones based on the pair-wise correlation with the target(s).</p> <p>It is recommended to combine <code>FastProp</code> with the <code>Mapping</code> and <code>Seasonal</code> preprocessors, which can drastically improve predictive accuracy.</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: FastProp.</p> <p>Attributes:</p> Name Type Description <code>aggregation</code> <code>List[str]</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables. Please note that you must also pass a value to max_lag.</p> <p>For more information please refer to Data Model Time Series. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_lag</code> <code>int</code> <p>Maximum number of steps taken into the past to form lag variables. The step size is determined by delta_t. Please note that you must also pass a value to delta_t.</p> <p>For more information please refer to Time Series. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>n_most_frequent</code> <code>int</code> <p><code>FastProp</code> can find the N most frequent categories in a categorical column and derive features from them. The parameter determines how many categories should be used. Range: [0, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>FastProp uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 2,000 samples are drawn from the population table. If the population table contains less than 2,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>@dataclass(repr=False)\nclass FastProp(_FeatureLearner):\n    \"\"\"\n    Generates simple features based on propositionalization.\n\n    [`FastProp`][getml.feature_learning.FastProp] generates simple and easily\n    interpretable features for relational data and time series. It is based on a\n    propositionalization approach and has been optimized for speed and memory efficiency.\n    [`FastProp`][getml.feature_learning.FastProp] generates a large number\n    of features and selects the most relevant ones based on the pair-wise correlation\n    with the target(s).\n\n    It is recommended to combine [`FastProp`][getml.feature_learning.FastProp] with\n    the [`Mapping`][getml.preprocessors.Mapping] and [`Seasonal`][getml.preprocessors.Seasonal]\n    preprocessors, which can drastically improve predictive accuracy.\n\n    For more information on the underlying feature learning algorithm, check\n    out the User guide: [FastProp][feature-engineering-algorithms-fastprop].\n\n    Attributes:\n        aggregation:\n            Mathematical operations used by the automated feature\n            learning algorithm to create new features.\n\n            Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables. Please note that you must also pass a value to\n            max_lag.\n\n            For more information please refer to\n            [Data Model Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_lag:\n            Maximum number of steps taken into the past to form lag variables. The\n            step size is determined by delta_t. Please note that you must also pass\n            a value to delta_t.\n\n            For more information please refer to\n            [Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        n_most_frequent:\n            [`FastProp`][getml.feature_learning.FastProp] can find the N most frequent\n            categories in a categorical column and derive features from them.\n            The parameter determines how many categories should be used.\n            Range: [0, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [0, $\\infty$]\n\n        sampling_factor:\n            FastProp uses a bootstrapping procedure (sampling with replacement) to train\n            each of the features. The sampling factor is proportional to the share of\n            the samples randomly drawn from the population table every time Multirel\n            generates a new feature. A lower sampling factor (but still greater than\n            0.0), will lead to less danger of overfitting, less complex statements and\n            faster training. When set to 1.0, roughly 2,000 samples are drawn from the\n            population table. If the population table contains less than 2,000 samples,\n            it will use standard bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    \"\"\"\n\n    agg_sets: ClassVar[_Aggregations] = fastprop_aggregations\n\n    aggregation: List[str] = field(\n        default_factory=lambda: fastprop_aggregations.Default\n    )\n    delta_t: float = 0.0\n    loss_function: Optional[str] = None\n    max_lag: int = 0\n    min_df: int = 30\n    n_most_frequent: int = 0\n    num_features: int = 200\n    num_threads: int = 0\n    sampling_factor: float = 1.0\n    silent: bool = True\n    vocab_size: int = 500\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing the parameters to validate.\n                params can hold the full set or a subset of the\n                parameters explained in\n                [`FastProp`][getml.feature_learning.FastProp].\n                If params is None, the\n                current set of parameters in the\n                instance dictionary will be validated.\n\n\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.FastProp.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>FastProp</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`FastProp`][getml.feature_learning.FastProp].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Fastboost","title":"<code>Fastboost</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Fastboost</code> automates feature learning for relational data and time series. The algorithm used is slightly simpler than <code>Relboost</code> and much faster.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>float</code> <p>During the training of Fastboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[Literal['CrossEntropyLoss', 'SquareLoss']]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_child_weights</code> <code>float</code> <p>Determines the minimum sum of the weights a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since Fastboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>subsample</code> <code>float</code> <p>Fastboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Fastboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, the number of samples drawn will be identical to the size of the population table. When set to 0.0, there will be no sampling at all. Range: [0, 1]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/fastboost.py</code> <pre><code>@dataclass(repr=False)\nclass Fastboost(_FeatureLearner):\n    \"\"\"\n    Feature learning based on Gradient Boosting.\n\n    [`Fastboost`][getml.feature_learning.Fastboost] automates feature learning\n    for relational data and time series. The algorithm used is slightly\n    simpler than [`Relboost`][getml.feature_learning.Relboost] and much faster.\n\n\n    Attributes:\n        gamma:\n            During the training of Fastboost, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0, $\\infty$]\n\n        min_child_weights:\n            Determines the minimum sum of the weights a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [$0$, $\\infty$]\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`Relboost`][getml.feature_learning.Relboost] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since Fastboost works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to a higher\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        subsample:\n            Fastboost uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            Fastboost generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, the number of samples drawn will\n            be identical to the size of the population table.\n            When set to 0.0, there will be no sampling at all.\n            Range: [0, 1]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    gamma: float = 0.0\n    loss_function: Optional[Literal[\"CrossEntropyLoss\", \"SquareLoss\"]] = None\n    max_depth: int = 5\n    min_child_weights: float = 1.0\n    num_features: int = 100\n    num_threads: int = 1\n    reg_lambda: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    subsample: float = 1.0\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params:\n                A dictionary containing the parameters to validate.\n                params can hold the full set or a subset of the\n                parameters explained in\n                [`Fastboost`][getml.feature_learning.Fastboost].\n                If params is None, the\n                current set of parameters in the\n                instance dictionary will be validated.\n\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_fastboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Fastboost.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>Fastboost</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/fastboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params:\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`Fastboost`][getml.feature_learning.Fastboost].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_fastboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Multirel","title":"<code>Multirel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Multi-Relational Decision Tree Learning.</p> <p><code>Multirel</code> automates feature learning for relational data and time series. It is based on an efficient variation of the Multi-Relational Decision Tree Learning (MRDTL).</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: Multirel.</p> <p>Attributes:</p> Name Type Description <code>aggregation</code> <code>List[_Aggregations]</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <code>allow_sets</code> <code>bool</code> <p>Multirel can summarize different categories into sets for producing conditions. When expressed as SQL statements these sets might look like this:</p> <pre><code>t2.category IN ( 'value_1', 'value_2', ... )\n</code></pre> <p>This can be very powerful, but it can also produce features that are hard to read and might be prone to overfitting when the <code>sampling_factor</code> is too low.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information please refer to :ref:<code>data_model_time_series</code>. Range: [0, \\(\\infty\\)]</p> <code>grid_factor</code> <code>float</code> <p>Multirel will try a grid of critical values for your numerical features. A higher <code>grid_factor</code> will lead to a larger number of critical values being considered. This can increase the training time, but also lead to more accurate features. Range: (0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_length</code> <code>int</code> <p>The maximum length a subcondition might have. Multirel will create conditions in the form</p> <pre><code>(condition 1.1 AND condition 1.2 AND condition 1.3 )\nOR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n...\n</code></pre> <p>Using this parameter you can set the maximum number of conditions allowed in the brackets. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>regularization</code> <code>float</code> <p>Most important regularization parameter for the quality of the features produced by Multirel. Higher values will lead to less complex features and less danger of overfitting. A <code>regularization</code> of 1.0 is very strong and allows no conditions. Range: [0, 1]</p> <code>round_robin</code> <code>bool</code> <p>If True, the Multirel picks a different <code>aggregation</code> every time a new feature is generated.</p> <code>sampling_factor</code> <code>float</code> <p>Multirel uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>share_aggregations</code> <code>float</code> <p>Every time a new feature is generated, the <code>aggregation</code> will be taken from a random subsample of possible aggregations and values to be aggregated. This parameter determines the size of that subsample. Only relevant when <code>round_robin</code> is False. Range: [0, 1]</p> <code>share_conditions</code> <code>float</code> <p>Every time a new column is tested for applying conditions, it might be skipped at random. This parameter determines the probability that a column will not be skipped. Range: [0, 1]</p> <code>shrinkage</code> <code>float</code> <p>Since Multirel works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. Higher values will lead to more danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>@dataclass(repr=False)\nclass Multirel(_FeatureLearner):\n    \"\"\"\nFeature learning based on Multi-Relational Decision Tree Learning.\n\n[`Multirel`][getml.feature_learning.Multirel] automates feature learning\nfor relational data and time series. It is based on an efficient\nvariation of the Multi-Relational Decision Tree Learning (MRDTL).\n\nFor more information on the underlying feature learning algorithm, check\nout the User guide: [Multirel][feature-engineering-algorithms-multirel].\n\n\nAttributes:\n    aggregation:\n        Mathematical operations used by the automated feature\n        learning algorithm to create new features.\n\n        Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n    allow_sets:\n        Multirel can summarize different categories into sets for\n        producing conditions. When expressed as SQL statements these\n        sets might look like this:\n\n\n\n            t2.category IN ( 'value_1', 'value_2', ... )\n\n        This can be very powerful, but it can also produce\n        features that are hard to read and might be prone to\n        overfitting when the `sampling_factor` is too low.\n\n    delta_t:\n        Frequency with which lag variables will be explored in a\n        time series setting. When set to 0.0, there will be no lag\n        variables.\n\n        For more information please refer to\n        :ref:`data_model_time_series`. Range: [0, $\\infty$]\n\n    grid_factor:\n        Multirel will try a grid of critical values for your\n        numerical features. A higher `grid_factor` will lead to a\n        larger number of critical values being considered. This\n        can increase the training time, but also lead to more\n        accurate features. Range: (0, $\\infty$]\n\n    loss_function:\n        Objective function used by the feature learning algorithm\n        to optimize your features. For regression problems use\n        [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n        classification problems use\n        [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n    max_length:\n        The maximum length a subcondition might have. Multirel\n        will create conditions in the form\n\n\n\n            (condition 1.1 AND condition 1.2 AND condition 1.3 )\n            OR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n            ...\n\n        Using this parameter you can set the maximum number of\n        conditions allowed in the brackets. Range: [0, $\\infty$]\n\n    min_df:\n        Only relevant for columns with role [`text`][getml.data.roles.text].\n        The minimum\n        number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n        given word is required to appear in to be included in the bag of words.\n        Range: [1, $\\infty$]\n\n    min_num_samples:\n        Determines the minimum number of samples a subcondition\n        should apply to in order for it to be considered. Higher\n        values lead to less complex statements and less danger of\n        overfitting. Range: [1, $\\infty$]\n\n    num_features:\n        Number of features generated by the feature learning\n        algorithm. Range: [1, $\\infty$]\n\n    num_subfeatures:\n        The number of subfeatures you would like to extract in a\n        subensemble (for snowflake data model only). See\n        [Snowflake Schema][data-model-snowflake-schema] for more\n        information. Range: [1, $\\infty$]\n\n    num_threads:\n        Number of threads used by the feature learning algorithm. If set to\n        zero or a negative value, the number of threads will be\n        determined automatically by the getML engine. Range:\n        [$0$, $\\infty$]\n\n    propositionalization:\n        The feature learner used for joins which are flagged to be\n        propositionalized (by setting a join's `relationship` parameter to\n        [`propositionalization`][getml.data.relationship.propositionalization])\n\n    regularization:\n        Most important regularization parameter for the quality of\n        the features produced by Multirel. Higher values will lead\n        to less complex features and less danger of overfitting. A\n        `regularization` of 1.0 is very strong and allows no\n        conditions. Range: [0, 1]\n\n    round_robin:\n        If True, the Multirel picks a different `aggregation`\n        every time a new feature is generated.\n\n    sampling_factor:\n        Multirel uses a bootstrapping procedure (sampling with\n        replacement) to train each of the features. The sampling\n        factor is proportional to the share of the samples\n        randomly drawn from the population table every time\n        Multirel generates a new feature. A lower sampling factor\n        (but still greater than 0.0), will lead to less danger of\n        overfitting, less complex statements and faster\n        training. When set to 1.0, roughly 20,000 samples are drawn\n        from the population table. If the population table\n        contains less than 20,000 samples, it will use standard\n        bagging. When set to 0.0, there will be no sampling at\n        all. Range: [0, $\\infty$]\n\n    seed:\n        Seed used for the random number generator that underlies\n        the sampling procedure to make the calculation\n        reproducible. Internally, a `seed` of None will be mapped to\n        5543. Range: [0, $\\infty$]\n\n    share_aggregations:\n        Every time a new feature is generated, the `aggregation`\n        will be taken from a random subsample of possible\n        aggregations and values to be aggregated. This parameter\n        determines the size of that subsample. Only relevant when\n        `round_robin` is False. Range: [0, 1]\n\n    share_conditions:\n        Every time a new column is tested for applying conditions,\n        it might be skipped at random. This parameter determines\n        the probability that a column will *not* be\n        skipped. Range: [0, 1]\n\n    shrinkage:\n        Since Multirel works using a gradient-boosting-like\n        algorithm, `shrinkage` (or learning rate) scales down the\n        weights and thus the impact of each new tree. This gives\n        more room for future ones to improve the overall\n        performance of the model in this greedy algorithm. Higher\n        values will lead to more danger of overfitting. Range: [0,\n        1]\n\n    silent:\n        Controls the logging during training.\n\n    vocab_size:\n        Determines the maximum number\n        of words that are extracted in total from [`text`][getml.data.roles.text]\n        columns. This can be interpreted as the maximum size of the bag of words.\n        Range: [0, $\\infty$]\n\nNote:\n    Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    agg_sets: ClassVar[_Aggregations] = multirel_aggregations\n\n    # ----------------------------------------------------------------\n\n    aggregation: List[_Aggregations] = field(\n        default_factory=lambda: multirel_aggregations.Default\n    )\n    allow_sets: bool = True\n    delta_t: float = 0.0\n    grid_factor: float = 1.0\n    loss_function: Optional[Union[CrossEntropyLoss, SquareLoss]] = None\n    max_length: int = 4\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 100\n    num_subfeatures: int = 5\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    regularization: float = 0.01\n    round_robin: bool = False\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    share_aggregations: float = 0.0\n    share_conditions: float = 1.0\n    shrinkage: float = 0.0\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        # ------------------------------------------------------------\n\n        _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Multirel.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.RelMT","title":"<code>RelMT</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on relational linear model trees.</p> <p><code>RelMT</code> automates feature learning for relational data and time series. It is based on a generalization of linear model trees to relational data, hence the name. A linear model tree is a decision tree with linear models on its leaves.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: RelMT.</p> <p>Attributes:</p> Name Type Description <code>allow_avg</code> <code>bool</code> <p>Whether to allow an AVG aggregation. Particularly for time series problems, AVG aggregations are not necessary and you can save some time by taking them out.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Data Model Time Series. Range: [0, \\(\\infty\\)]</p> <code>gamma</code> <code>float</code> <p>During the training of RelMT, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [-\\(\\infty\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>RelMT</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>RelMT uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time RelMT generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since RelMT works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to more danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>@dataclass(repr=False)\nclass RelMT(_FeatureLearner):\n    \"\"\"Feature learning based on relational linear model trees.\n\n    [`RelMT`][getml.feature_learning.RelMT] automates feature learning\n    for relational data and time series. It is based on a\n    generalization of linear model trees to relational data, hence\n    the name. A linear model tree is a decision tree\n    with linear models on its leaves.\n\n\n    For more information on the underlying feature learning\n    algorithm, check out the User Guide: [RelMT][feature-engineering-algorithms-relmt].\n\n\n    Attributes:\n        allow_avg:\n            Whether to allow an AVG aggregation. Particularly for time\n            series problems, AVG aggregations are not necessary and you\n            can save some time by taking them out.\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables.\n\n            For more information, please refer to [Data Model Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        gamma:\n            During the training of RelMT, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0,\n            $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        min_num_samples:\n            Determines the minimum number of samples a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_subfeatures:\n            The number of subfeatures you would like to extract in a\n            subensemble (for snowflake data model only). See\n            :ref:`data_model_snowflake_schema` for more\n            information. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [-$\\infty$, $\\infty$]\n\n        propositionalization:\n            The feature learner used for joins which are flagged to be\n            propositionalized (by setting a join's `relationship` parameter to\n            [`propositionalization`][getml.data.relationship.propositionalization])\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`RelMT`][getml.feature_learning.RelMT] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        sampling_factor:\n            RelMT uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            RelMT generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, roughly 20,000 samples are drawn\n            from the population table. If the population table\n            contains less than 20,000 samples, it will use standard\n            bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since RelMT works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to more\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    allow_avg: bool = True\n    delta_t: float = 0.0\n    gamma: float = 0.0\n    loss_function: Optional[str] = None\n    max_depth: int = 2\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 30\n    num_subfeatures: int = 30\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    reg_lambda: float = 0.0\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params[\"silent\"], bool):\n            raise TypeError(\"'silent' must be of type bool\")\n\n        # ------------------------------------------------------------\n\n        _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.RelMT.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Relboost","title":"<code>Relboost</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Relboost</code> automates feature learning for relational data and time series. It is based on a generalization of the XGBoost algorithm to relational data, hence the name.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: Relboost.</p> <p>Attributes:</p> Name Type Description <code>allow_null_weights</code> <code>bool</code> <p>Whether you want to allow <code>Relboost</code> to set weights to NULL.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to :ref:<code>data_model_time_series</code>. Range: [0, \\(\\infty\\)]</p> <code>gamma</code> <code>float</code> <p>During the training of Relboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>Relboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Relboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since Relboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>@dataclass(repr=False)\nclass Relboost(_FeatureLearner):\n    \"\"\"\n       Feature learning based on Gradient Boosting.\n\n    [`Relboost`][getml.feature_learning.Relboost] automates feature learning\n    for relational data and time series. It is based on a\n    generalization of the XGBoost algorithm to relational data, hence\n    the name.\n\n    For more information on the underlying feature learning\n    algorithm, check out the User Guide: [Relboost][feature-engineering-algorithms-relboost].\n\n    Attributes:\n        allow_null_weights:\n            Whether you want to allow\n            [`Relboost`][getml.feature_learning.Relboost] to set weights to\n            NULL.\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables.\n\n            For more information, please refer to\n            :ref:`data_model_time_series`. Range: [0, $\\infty$]\n\n        gamma:\n            During the training of Relboost, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0,\n            $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        min_num_samples:\n            Determines the minimum number of samples a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_subfeatures:\n            The number of subfeatures you would like to extract in a\n            subensemble (for snowflake data model only). See\n            :ref:`data_model_snowflake_schema` for more\n            information. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [$0$, $\\infty$]\n\n        propositionalization:\n            The feature learner used for joins which are flagged to be\n            propositionalized (by setting a join's `relationship` parameter to\n            [`propositionalization`][getml.data.relationship.propositionalization])\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`Relboost`][getml.feature_learning.Relboost] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        sampling_factor:\n            Relboost uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            Relboost generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, roughly 20,000 samples are drawn\n            from the population table. If the population table\n            contains less than 20,000 samples, it will use standard\n            bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since Relboost works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to a higher\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    allow_null_weights: bool = False\n    delta_t: float = 0.0\n    gamma: float = 0.0\n    loss_function: Optional[str] = None\n    max_depth: int = 3\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 100\n    num_subfeatures: int = 100\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    reg_lambda: float = 0.0\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                        f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                        )\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params[\"silent\"], bool):\n            raise TypeError(\"'silent' must be of type bool\")\n\n        # ------------------------------------------------------------\n\n        _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/__init__/#getml.feature_learning.Relboost.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                    )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/aggregations/","title":"Aggregations","text":"<p>This module contains all possible aggregations to be used with <code>Multirel</code>, <code>FastProp</code>, <code>Mapping</code>.</p> <p>Refer to the feature learning section in the user guide for details about how these aggregations are used in the context of feature learning.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Avg","title":"<code>Avg = 'AVG'</code>  <code>module-attribute</code>","text":"<p>Average value of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Count","title":"<code>Count = 'COUNT'</code>  <code>module-attribute</code>","text":"<p>Number of rows in a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.CountDistinct","title":"<code>CountDistinct = 'COUNT DISTINCT'</code>  <code>module-attribute</code>","text":"<p>Count function with distinct clause. This only counts unique elements.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.CountDistinctOverCount","title":"<code>CountDistinctOverCount = 'COUNT DISTINCT OVER COUNT'</code>  <code>module-attribute</code>","text":"<p>COUNT DISTINCT divided by COUNT. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.CountMinusCountDistinct","title":"<code>CountMinusCountDistinct = 'COUNT MINUS COUNT DISTINCT'</code>  <code>module-attribute</code>","text":"<p>Counts minus counts distinct. Substracts COUNT DISTINCT from COUNT.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1d","title":"<code>EWMA_1d = 'EWMA_1D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 1 day. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1h","title":"<code>EWMA_1h = 'EWMA_1H'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 1 hour. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1m","title":"<code>EWMA_1m = 'EWMA_1M'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 1 minute. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1s","title":"<code>EWMA_1s = 'EWMA_1S'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_30d","title":"<code>EWMA_30d = 'EWMA_30D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 30 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_365d","title":"<code>EWMA_365d = 'EWMA_365D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 365 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_7d","title":"<code>EWMA_7d = 'EWMA_7D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 7 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_90d","title":"<code>EWMA_90d = 'EWMA_90D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted moving average with a half-life of 90 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1d","title":"<code>EWMA_TREND_1d = 'EWMA_TREND_1D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 1 day. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1h","title":"<code>EWMA_TREND_1h = 'EWMA_TREND_1H'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 1 hour. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1m","title":"<code>EWMA_TREND_1m = 'EWMA_TREND_1M'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 1 minute. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1s","title":"<code>EWMA_TREND_1s = 'EWMA_TREND_1S'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_30d","title":"<code>EWMA_TREND_30d = 'EWMA_TREND_30D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 30 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_365d","title":"<code>EWMA_TREND_365d = 'EWMA_TREND_365D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 365 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_7d","title":"<code>EWMA_TREND_7d = 'EWMA_TREND_7D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 7 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_90d","title":"<code>EWMA_TREND_90d = 'EWMA_TREND_90D'</code>  <code>module-attribute</code>","text":"<p>Exponentially weighted trend with a half-life of 90 days. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.First","title":"<code>First = 'FIRST'</code>  <code>module-attribute</code>","text":"<p>First value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Kurtosis","title":"<code>Kurtosis = 'KURTOSIS'</code>  <code>module-attribute</code>","text":"<p>The kurtosis of a given column. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Last","title":"<code>Last = 'LAST'</code>  <code>module-attribute</code>","text":"<p>Last value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Max","title":"<code>Max = 'MAX'</code>  <code>module-attribute</code>","text":"<p>Largest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Median","title":"<code>Median = 'MEDIAN'</code>  <code>module-attribute</code>","text":"<p>Median of a given column</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Min","title":"<code>Min = 'MIN'</code>  <code>module-attribute</code>","text":"<p>Smallest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Mode","title":"<code>Mode = 'MODE'</code>  <code>module-attribute</code>","text":"<p>Most frequent value of a given column. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NumMax","title":"<code>NumMax = 'NUM MAX'</code>  <code>module-attribute</code>","text":"<p>The number of times we observe the maximum value. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NumMin","title":"<code>NumMin = 'NUM MIN'</code>  <code>module-attribute</code>","text":"<p>The number of times we observe the minimum value. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q1","title":"<code>Q1 = 'Q1'</code>  <code>module-attribute</code>","text":"<p>The 1%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q10","title":"<code>Q10 = 'Q10'</code>  <code>module-attribute</code>","text":"<p>The 10%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q25","title":"<code>Q25 = 'Q25'</code>  <code>module-attribute</code>","text":"<p>The 25%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q5","title":"<code>Q5 = 'Q5'</code>  <code>module-attribute</code>","text":"<p>The 5%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q75","title":"<code>Q75 = 'Q75'</code>  <code>module-attribute</code>","text":"<p>The 75%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q90","title":"<code>Q90 = 'Q90'</code>  <code>module-attribute</code>","text":"<p>The 90%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q95","title":"<code>Q95 = 'Q95'</code>  <code>module-attribute</code>","text":"<p>The 95%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q99","title":"<code>Q99 = 'Q99'</code>  <code>module-attribute</code>","text":"<p>The 99%-quantile. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Skew","title":"<code>Skew = 'SKEW'</code>  <code>module-attribute</code>","text":"<p>Skewness of a given column. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Stddev","title":"<code>Stddev = 'STDDEV'</code>  <code>module-attribute</code>","text":"<p>Standard deviation of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Sum","title":"<code>Sum = 'SUM'</code>  <code>module-attribute</code>","text":"<p>Total sum of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TimeSinceFirstMaximum","title":"<code>TimeSinceFirstMaximum = 'TIME SINCE FIRST MAXIMUM'</code>  <code>module-attribute</code>","text":"<p>The time difference between the first time we see the maximum value and the time stamp in the population table. If the maximum value is unique, then TIME SINCE FIRST MAXIMUM and TIME SINCE LAST MAXIMUM are identical. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TimeSinceFirstMinimum","title":"<code>TimeSinceFirstMinimum = 'TIME SINCE FIRST MINIMUM'</code>  <code>module-attribute</code>","text":"<p>The time difference between the first time we see the minimum value and the time stamp in the population table. If the minimum value is unique, then TIME SINCE FIRST MINIMUM and TIME SINCE LAST MINIMUM are identical. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TimeSinceLastMaximum","title":"<code>TimeSinceLastMaximum = 'TIME SINCE LAST MAXIMUM'</code>  <code>module-attribute</code>","text":"<p>The time difference between the last time we see the maximum value and the time stamp in the population table. If the maximum value is unique, then TIME SINCE FIRST MAXIMUM and TIME SINCE LAST MAXIMUM are identical. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TimeSinceLastMinimum","title":"<code>TimeSinceLastMinimum = 'TIME SINCE LAST MINIMUM'</code>  <code>module-attribute</code>","text":"<p>The time difference between the last time we see the minimum value and the time stamp in the population table. If the minimum value is unique, then TIME SINCE FIRST MINIMUM and TIME SINCE LAST MINIMUM are identical. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Trend","title":"<code>Trend = 'TREND'</code>  <code>module-attribute</code>","text":"<p>Extracts a linear trend from a variable over time and extrapolates this trend to the current time stamp. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Var","title":"<code>Var = 'VAR'</code>  <code>module-attribute</code>","text":"<p>Statistical variance of a given numerical column. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.VariationCoefficient","title":"<code>VariationCoefficient = 'VARIATION COEFFICIENT'</code>  <code>module-attribute</code>","text":"<p>VAR divided by MEAN. Please note that this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.fastprop","title":"<code>fastprop = _Aggregations(All=_all_aggregations, Default=[Avg, Count, CountDistinct, CountMinusCountDistinct, First, Last, Max, Median, Min, Mode, Stddev, Sum, Trend], Minimal=[Avg, Count, Max, Min, Sum])</code>  <code>module-attribute</code>","text":"<p>Set of default aggregations for <code>FastProp</code>. <code>All</code> contains all aggregations supported by FastProp, <code>Default</code> contains the subset of reasonable default aggregations, <code>Minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.mapping","title":"<code>mapping = _Aggregations(All=_mapping_subset, Default=[Avg], Minimal=[Avg])</code>  <code>module-attribute</code>","text":"<p>Set of default aggregations for <code>Mapping</code>. <code>All</code> contains all aggregations supported by the mapping preprocessor. <code>Default</code> and <code>Minimal</code> are identical and include only the AVG aggregation, which is the recommended setting for classification problems.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.multirel","title":"<code>multirel = _Aggregations(All=_multirel_subset, Default=[Avg, Count, Max, Min, Sum], Minimal=[Avg, Count, Sum])</code>  <code>module-attribute</code>","text":"<p>Set of default aggregations for <code>Multirel</code>. <code>All</code> contains all aggregations supported by Multirel, <code>Default</code> contains the subset of reasonable default aggregations, <code>Minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/fastboost/","title":"Fastboost","text":"<p>Feature learning based on fast gradient boosting.</p>"},{"location":"reference/feature_learning/fastboost/#getml.feature_learning.fastboost.Fastboost","title":"<code>Fastboost</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Fastboost</code> automates feature learning for relational data and time series. The algorithm used is slightly simpler than <code>Relboost</code> and much faster.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>float</code> <p>During the training of Fastboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[Literal['CrossEntropyLoss', 'SquareLoss']]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_child_weights</code> <code>float</code> <p>Determines the minimum sum of the weights a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since Fastboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>subsample</code> <code>float</code> <p>Fastboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Fastboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, the number of samples drawn will be identical to the size of the population table. When set to 0.0, there will be no sampling at all. Range: [0, 1]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/fastboost.py</code> <pre><code>@dataclass(repr=False)\nclass Fastboost(_FeatureLearner):\n    \"\"\"\n    Feature learning based on Gradient Boosting.\n\n    [`Fastboost`][getml.feature_learning.Fastboost] automates feature learning\n    for relational data and time series. The algorithm used is slightly\n    simpler than [`Relboost`][getml.feature_learning.Relboost] and much faster.\n\n\n    Attributes:\n        gamma:\n            During the training of Fastboost, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0, $\\infty$]\n\n        min_child_weights:\n            Determines the minimum sum of the weights a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [$0$, $\\infty$]\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`Relboost`][getml.feature_learning.Relboost] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since Fastboost works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to a higher\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        subsample:\n            Fastboost uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            Fastboost generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, the number of samples drawn will\n            be identical to the size of the population table.\n            When set to 0.0, there will be no sampling at all.\n            Range: [0, 1]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    gamma: float = 0.0\n    loss_function: Optional[Literal[\"CrossEntropyLoss\", \"SquareLoss\"]] = None\n    max_depth: int = 5\n    min_child_weights: float = 1.0\n    num_features: int = 100\n    num_threads: int = 1\n    reg_lambda: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    subsample: float = 1.0\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params:\n                A dictionary containing the parameters to validate.\n                params can hold the full set or a subset of the\n                parameters explained in\n                [`Fastboost`][getml.feature_learning.Fastboost].\n                If params is None, the\n                current set of parameters in the\n                instance dictionary will be validated.\n\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_fastboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/fastboost/#getml.feature_learning.fastboost.Fastboost.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>Fastboost</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/fastboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params:\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`Fastboost`][getml.feature_learning.Fastboost].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_fastboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/fastprop/","title":"Fastprop","text":"<p>Feature learning based on propositionalization.</p>"},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.fastprop.FastProp","title":"<code>FastProp</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Generates simple features based on propositionalization.</p> <p><code>FastProp</code> generates simple and easily interpretable features for relational data and time series. It is based on a propositionalization approach and has been optimized for speed and memory efficiency. <code>FastProp</code> generates a large number of features and selects the most relevant ones based on the pair-wise correlation with the target(s).</p> <p>It is recommended to combine <code>FastProp</code> with the <code>Mapping</code> and <code>Seasonal</code> preprocessors, which can drastically improve predictive accuracy.</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: FastProp.</p> <p>Attributes:</p> Name Type Description <code>aggregation</code> <code>List[str]</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables. Please note that you must also pass a value to max_lag.</p> <p>For more information please refer to Data Model Time Series. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_lag</code> <code>int</code> <p>Maximum number of steps taken into the past to form lag variables. The step size is determined by delta_t. Please note that you must also pass a value to delta_t.</p> <p>For more information please refer to Time Series. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>n_most_frequent</code> <code>int</code> <p><code>FastProp</code> can find the N most frequent categories in a categorical column and derive features from them. The parameter determines how many categories should be used. Range: [0, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>FastProp uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 2,000 samples are drawn from the population table. If the population table contains less than 2,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>@dataclass(repr=False)\nclass FastProp(_FeatureLearner):\n    \"\"\"\n    Generates simple features based on propositionalization.\n\n    [`FastProp`][getml.feature_learning.FastProp] generates simple and easily\n    interpretable features for relational data and time series. It is based on a\n    propositionalization approach and has been optimized for speed and memory efficiency.\n    [`FastProp`][getml.feature_learning.FastProp] generates a large number\n    of features and selects the most relevant ones based on the pair-wise correlation\n    with the target(s).\n\n    It is recommended to combine [`FastProp`][getml.feature_learning.FastProp] with\n    the [`Mapping`][getml.preprocessors.Mapping] and [`Seasonal`][getml.preprocessors.Seasonal]\n    preprocessors, which can drastically improve predictive accuracy.\n\n    For more information on the underlying feature learning algorithm, check\n    out the User guide: [FastProp][feature-engineering-algorithms-fastprop].\n\n    Attributes:\n        aggregation:\n            Mathematical operations used by the automated feature\n            learning algorithm to create new features.\n\n            Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables. Please note that you must also pass a value to\n            max_lag.\n\n            For more information please refer to\n            [Data Model Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_lag:\n            Maximum number of steps taken into the past to form lag variables. The\n            step size is determined by delta_t. Please note that you must also pass\n            a value to delta_t.\n\n            For more information please refer to\n            [Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        n_most_frequent:\n            [`FastProp`][getml.feature_learning.FastProp] can find the N most frequent\n            categories in a categorical column and derive features from them.\n            The parameter determines how many categories should be used.\n            Range: [0, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [0, $\\infty$]\n\n        sampling_factor:\n            FastProp uses a bootstrapping procedure (sampling with replacement) to train\n            each of the features. The sampling factor is proportional to the share of\n            the samples randomly drawn from the population table every time Multirel\n            generates a new feature. A lower sampling factor (but still greater than\n            0.0), will lead to less danger of overfitting, less complex statements and\n            faster training. When set to 1.0, roughly 2,000 samples are drawn from the\n            population table. If the population table contains less than 2,000 samples,\n            it will use standard bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    \"\"\"\n\n    agg_sets: ClassVar[_Aggregations] = fastprop_aggregations\n\n    aggregation: List[str] = field(\n        default_factory=lambda: fastprop_aggregations.Default\n    )\n    delta_t: float = 0.0\n    loss_function: Optional[str] = None\n    max_lag: int = 0\n    min_df: int = 30\n    n_most_frequent: int = 0\n    num_features: int = 200\n    num_threads: int = 0\n    sampling_factor: float = 1.0\n    silent: bool = True\n    vocab_size: int = 500\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing the parameters to validate.\n                params can hold the full set or a subset of the\n                parameters explained in\n                [`FastProp`][getml.feature_learning.FastProp].\n                If params is None, the\n                current set of parameters in the\n                instance dictionary will be validated.\n\n\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.fastprop.FastProp.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>FastProp</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`FastProp`][getml.feature_learning.FastProp].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/feature_learner/","title":"Feature learner","text":"<p>Base class. Should not ever be directly initialized!</p>"},{"location":"reference/feature_learning/loss_functions/","title":"Loss functions","text":"<p>Loss functions used by the feature learning algorithms.</p> <p>The getML Python API contains two different loss functions. We recommend using <code>SquareLoss</code> for regression problems and <code>CrossEntropyLoss</code> for classification problems.</p> <p>Please note that these loss functions will only be used by the feature learning algorithms and not by the <code>predictors</code>.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.CrossEntropyLoss","title":"<code>CrossEntropyLoss = 'CrossEntropyLoss'</code>  <code>module-attribute</code>","text":"<p>The cross entropy between two probability distributions \\(p(x)\\) and \\(q(x)\\) is a combination of the information contained in \\(p(x)\\) and the additional information stored in \\(q(x)\\) with respect to \\(p(x)\\). In technical terms: it is the entropy of \\(p(x)\\) plus the Kullback-Leibler divergence - a distance in probability space - from \\(q(x)\\) to \\(p(x)\\).</p> \\[ H(p,q) = H(p) + D_{KL}(p||q) \\] <p>For discrete probability distributions the cross entropy loss can be calculated by</p> \\[ H(p,q) = - \\sum_{x \\in X} p(x) \\log q(x) \\] <p>and for continuous probability distributions by</p> \\[ H(p,q) = - \\int_{X} p(x) \\log q(x) dx \\] <p>with \\(X\\) being the support of the samples and \\(p(x)\\) and \\(q(x)\\) being two discrete or continuous probability distributions over \\(X\\).</p> Note <p>Recommended loss function for classification problems.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.SquareLoss","title":"<code>SquareLoss = 'SquareLoss'</code>  <code>module-attribute</code>","text":"<p>The Square loss (aka mean squared error (MSE)) measures the loss by calculating the average of all squared deviations of the predictions \\(\\hat{y}\\) from the observed (given) outcomes \\(y\\). Depending on the context this measure is also known as mean squared error (MSE) or mean squared deviation (MSD).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2  \\] <p>with \\(n\\) being the number of samples, \\(y\\) the observed outcome, and \\(\\hat{y}\\) the estimate.</p> Note <p>Recommended loss function for regression problems.</p>"},{"location":"reference/feature_learning/multirel/","title":"Multirel","text":"<p>Feature learning based on Multi-Relational Decision Tree Learning.</p>"},{"location":"reference/feature_learning/multirel/#getml.feature_learning.multirel.Multirel","title":"<code>Multirel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Multi-Relational Decision Tree Learning.</p> <p><code>Multirel</code> automates feature learning for relational data and time series. It is based on an efficient variation of the Multi-Relational Decision Tree Learning (MRDTL).</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: Multirel.</p> <p>Attributes:</p> Name Type Description <code>aggregation</code> <code>List[_Aggregations]</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <code>allow_sets</code> <code>bool</code> <p>Multirel can summarize different categories into sets for producing conditions. When expressed as SQL statements these sets might look like this:</p> <pre><code>t2.category IN ( 'value_1', 'value_2', ... )\n</code></pre> <p>This can be very powerful, but it can also produce features that are hard to read and might be prone to overfitting when the <code>sampling_factor</code> is too low.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information please refer to :ref:<code>data_model_time_series</code>. Range: [0, \\(\\infty\\)]</p> <code>grid_factor</code> <code>float</code> <p>Multirel will try a grid of critical values for your numerical features. A higher <code>grid_factor</code> will lead to a larger number of critical values being considered. This can increase the training time, but also lead to more accurate features. Range: (0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_length</code> <code>int</code> <p>The maximum length a subcondition might have. Multirel will create conditions in the form</p> <pre><code>(condition 1.1 AND condition 1.2 AND condition 1.3 )\nOR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n...\n</code></pre> <p>Using this parameter you can set the maximum number of conditions allowed in the brackets. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>regularization</code> <code>float</code> <p>Most important regularization parameter for the quality of the features produced by Multirel. Higher values will lead to less complex features and less danger of overfitting. A <code>regularization</code> of 1.0 is very strong and allows no conditions. Range: [0, 1]</p> <code>round_robin</code> <code>bool</code> <p>If True, the Multirel picks a different <code>aggregation</code> every time a new feature is generated.</p> <code>sampling_factor</code> <code>float</code> <p>Multirel uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>share_aggregations</code> <code>float</code> <p>Every time a new feature is generated, the <code>aggregation</code> will be taken from a random subsample of possible aggregations and values to be aggregated. This parameter determines the size of that subsample. Only relevant when <code>round_robin</code> is False. Range: [0, 1]</p> <code>share_conditions</code> <code>float</code> <p>Every time a new column is tested for applying conditions, it might be skipped at random. This parameter determines the probability that a column will not be skipped. Range: [0, 1]</p> <code>shrinkage</code> <code>float</code> <p>Since Multirel works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. Higher values will lead to more danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>@dataclass(repr=False)\nclass Multirel(_FeatureLearner):\n    \"\"\"\nFeature learning based on Multi-Relational Decision Tree Learning.\n\n[`Multirel`][getml.feature_learning.Multirel] automates feature learning\nfor relational data and time series. It is based on an efficient\nvariation of the Multi-Relational Decision Tree Learning (MRDTL).\n\nFor more information on the underlying feature learning algorithm, check\nout the User guide: [Multirel][feature-engineering-algorithms-multirel].\n\n\nAttributes:\n    aggregation:\n        Mathematical operations used by the automated feature\n        learning algorithm to create new features.\n\n        Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n    allow_sets:\n        Multirel can summarize different categories into sets for\n        producing conditions. When expressed as SQL statements these\n        sets might look like this:\n\n\n\n            t2.category IN ( 'value_1', 'value_2', ... )\n\n        This can be very powerful, but it can also produce\n        features that are hard to read and might be prone to\n        overfitting when the `sampling_factor` is too low.\n\n    delta_t:\n        Frequency with which lag variables will be explored in a\n        time series setting. When set to 0.0, there will be no lag\n        variables.\n\n        For more information please refer to\n        :ref:`data_model_time_series`. Range: [0, $\\infty$]\n\n    grid_factor:\n        Multirel will try a grid of critical values for your\n        numerical features. A higher `grid_factor` will lead to a\n        larger number of critical values being considered. This\n        can increase the training time, but also lead to more\n        accurate features. Range: (0, $\\infty$]\n\n    loss_function:\n        Objective function used by the feature learning algorithm\n        to optimize your features. For regression problems use\n        [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n        classification problems use\n        [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n    max_length:\n        The maximum length a subcondition might have. Multirel\n        will create conditions in the form\n\n\n\n            (condition 1.1 AND condition 1.2 AND condition 1.3 )\n            OR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n            ...\n\n        Using this parameter you can set the maximum number of\n        conditions allowed in the brackets. Range: [0, $\\infty$]\n\n    min_df:\n        Only relevant for columns with role [`text`][getml.data.roles.text].\n        The minimum\n        number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n        given word is required to appear in to be included in the bag of words.\n        Range: [1, $\\infty$]\n\n    min_num_samples:\n        Determines the minimum number of samples a subcondition\n        should apply to in order for it to be considered. Higher\n        values lead to less complex statements and less danger of\n        overfitting. Range: [1, $\\infty$]\n\n    num_features:\n        Number of features generated by the feature learning\n        algorithm. Range: [1, $\\infty$]\n\n    num_subfeatures:\n        The number of subfeatures you would like to extract in a\n        subensemble (for snowflake data model only). See\n        [Snowflake Schema][data-model-snowflake-schema] for more\n        information. Range: [1, $\\infty$]\n\n    num_threads:\n        Number of threads used by the feature learning algorithm. If set to\n        zero or a negative value, the number of threads will be\n        determined automatically by the getML engine. Range:\n        [$0$, $\\infty$]\n\n    propositionalization:\n        The feature learner used for joins which are flagged to be\n        propositionalized (by setting a join's `relationship` parameter to\n        [`propositionalization`][getml.data.relationship.propositionalization])\n\n    regularization:\n        Most important regularization parameter for the quality of\n        the features produced by Multirel. Higher values will lead\n        to less complex features and less danger of overfitting. A\n        `regularization` of 1.0 is very strong and allows no\n        conditions. Range: [0, 1]\n\n    round_robin:\n        If True, the Multirel picks a different `aggregation`\n        every time a new feature is generated.\n\n    sampling_factor:\n        Multirel uses a bootstrapping procedure (sampling with\n        replacement) to train each of the features. The sampling\n        factor is proportional to the share of the samples\n        randomly drawn from the population table every time\n        Multirel generates a new feature. A lower sampling factor\n        (but still greater than 0.0), will lead to less danger of\n        overfitting, less complex statements and faster\n        training. When set to 1.0, roughly 20,000 samples are drawn\n        from the population table. If the population table\n        contains less than 20,000 samples, it will use standard\n        bagging. When set to 0.0, there will be no sampling at\n        all. Range: [0, $\\infty$]\n\n    seed:\n        Seed used for the random number generator that underlies\n        the sampling procedure to make the calculation\n        reproducible. Internally, a `seed` of None will be mapped to\n        5543. Range: [0, $\\infty$]\n\n    share_aggregations:\n        Every time a new feature is generated, the `aggregation`\n        will be taken from a random subsample of possible\n        aggregations and values to be aggregated. This parameter\n        determines the size of that subsample. Only relevant when\n        `round_robin` is False. Range: [0, 1]\n\n    share_conditions:\n        Every time a new column is tested for applying conditions,\n        it might be skipped at random. This parameter determines\n        the probability that a column will *not* be\n        skipped. Range: [0, 1]\n\n    shrinkage:\n        Since Multirel works using a gradient-boosting-like\n        algorithm, `shrinkage` (or learning rate) scales down the\n        weights and thus the impact of each new tree. This gives\n        more room for future ones to improve the overall\n        performance of the model in this greedy algorithm. Higher\n        values will lead to more danger of overfitting. Range: [0,\n        1]\n\n    silent:\n        Controls the logging during training.\n\n    vocab_size:\n        Determines the maximum number\n        of words that are extracted in total from [`text`][getml.data.roles.text]\n        columns. This can be interpreted as the maximum size of the bag of words.\n        Range: [0, $\\infty$]\n\nNote:\n    Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    agg_sets: ClassVar[_Aggregations] = multirel_aggregations\n\n    # ----------------------------------------------------------------\n\n    aggregation: List[_Aggregations] = field(\n        default_factory=lambda: multirel_aggregations.Default\n    )\n    allow_sets: bool = True\n    delta_t: float = 0.0\n    grid_factor: float = 1.0\n    loss_function: Optional[Union[CrossEntropyLoss, SquareLoss]] = None\n    max_length: int = 4\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 100\n    num_subfeatures: int = 5\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    regularization: float = 0.01\n    round_robin: bool = False\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    share_aggregations: float = 0.0\n    share_conditions: float = 1.0\n    shrinkage: float = 0.0\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        # ------------------------------------------------------------\n\n        _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/multirel/#getml.feature_learning.multirel.Multirel.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relboost/","title":"Relboost","text":"<p>Feature learning based on Gradient Boosting.</p>"},{"location":"reference/feature_learning/relboost/#getml.feature_learning.relboost.Relboost","title":"<code>Relboost</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Relboost</code> automates feature learning for relational data and time series. It is based on a generalization of the XGBoost algorithm to relational data, hence the name.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: Relboost.</p> <p>Attributes:</p> Name Type Description <code>allow_null_weights</code> <code>bool</code> <p>Whether you want to allow <code>Relboost</code> to set weights to NULL.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to :ref:<code>data_model_time_series</code>. Range: [0, \\(\\infty\\)]</p> <code>gamma</code> <code>float</code> <p>During the training of Relboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>Relboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Relboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since Relboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>@dataclass(repr=False)\nclass Relboost(_FeatureLearner):\n    \"\"\"\n       Feature learning based on Gradient Boosting.\n\n    [`Relboost`][getml.feature_learning.Relboost] automates feature learning\n    for relational data and time series. It is based on a\n    generalization of the XGBoost algorithm to relational data, hence\n    the name.\n\n    For more information on the underlying feature learning\n    algorithm, check out the User Guide: [Relboost][feature-engineering-algorithms-relboost].\n\n    Attributes:\n        allow_null_weights:\n            Whether you want to allow\n            [`Relboost`][getml.feature_learning.Relboost] to set weights to\n            NULL.\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables.\n\n            For more information, please refer to\n            :ref:`data_model_time_series`. Range: [0, $\\infty$]\n\n        gamma:\n            During the training of Relboost, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0,\n            $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        min_num_samples:\n            Determines the minimum number of samples a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_subfeatures:\n            The number of subfeatures you would like to extract in a\n            subensemble (for snowflake data model only). See\n            :ref:`data_model_snowflake_schema` for more\n            information. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [$0$, $\\infty$]\n\n        propositionalization:\n            The feature learner used for joins which are flagged to be\n            propositionalized (by setting a join's `relationship` parameter to\n            [`propositionalization`][getml.data.relationship.propositionalization])\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`Relboost`][getml.feature_learning.Relboost] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        sampling_factor:\n            Relboost uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            Relboost generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, roughly 20,000 samples are drawn\n            from the population table. If the population table\n            contains less than 20,000 samples, it will use standard\n            bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since Relboost works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to a higher\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    allow_null_weights: bool = False\n    delta_t: float = 0.0\n    gamma: float = 0.0\n    loss_function: Optional[str] = None\n    max_depth: int = 3\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 100\n    num_subfeatures: int = 100\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    reg_lambda: float = 0.0\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                        f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                        )\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params[\"silent\"], bool):\n            raise TypeError(\"'silent' must be of type bool\")\n\n        # ------------------------------------------------------------\n\n        _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relboost/#getml.feature_learning.relboost.Relboost.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                    )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relmt/","title":"Relmt","text":"<p>Feature learning based on Gradient Boosting.</p>"},{"location":"reference/feature_learning/relmt/#getml.feature_learning.relmt.RelMT","title":"<code>RelMT</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on relational linear model trees.</p> <p><code>RelMT</code> automates feature learning for relational data and time series. It is based on a generalization of linear model trees to relational data, hence the name. A linear model tree is a decision tree with linear models on its leaves.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: RelMT.</p> <p>Attributes:</p> Name Type Description <code>allow_avg</code> <code>bool</code> <p>Whether to allow an AVG aggregation. Particularly for time series problems, AVG aggregations are not necessary and you can save some time by taking them out.</p> <code>delta_t</code> <code>float</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Data Model Time Series. Range: [0, \\(\\infty\\)]</p> <code>gamma</code> <code>float</code> <p>During the training of RelMT, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <code>loss_function</code> <code>Optional[str]</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <code>max_depth</code> <code>int</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <code>min_df</code> <code>int</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <code>min_num_samples</code> <code>int</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <code>num_features</code> <code>int</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <code>num_subfeatures</code> <code>int</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \\(\\infty\\)]</p> <code>num_threads</code> <code>int</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [-\\(\\infty\\), \\(\\infty\\)]</p> <code>propositionalization</code> <code>FastProp</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>RelMT</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <code>sampling_factor</code> <code>float</code> <p>RelMT uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time RelMT generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>shrinkage</code> <code>float</code> <p>Since RelMT works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to more danger of overfitting. Range: [0, 1]</p> <code>silent</code> <code>bool</code> <p>Controls the logging during training.</p> <code>vocab_size</code> <code>int</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>@dataclass(repr=False)\nclass RelMT(_FeatureLearner):\n    \"\"\"Feature learning based on relational linear model trees.\n\n    [`RelMT`][getml.feature_learning.RelMT] automates feature learning\n    for relational data and time series. It is based on a\n    generalization of linear model trees to relational data, hence\n    the name. A linear model tree is a decision tree\n    with linear models on its leaves.\n\n\n    For more information on the underlying feature learning\n    algorithm, check out the User Guide: [RelMT][feature-engineering-algorithms-relmt].\n\n\n    Attributes:\n        allow_avg:\n            Whether to allow an AVG aggregation. Particularly for time\n            series problems, AVG aggregations are not necessary and you\n            can save some time by taking them out.\n\n        delta_t:\n            Frequency with which lag variables will be explored in a\n            time series setting. When set to 0.0, there will be no lag\n            variables.\n\n            For more information, please refer to [Data Model Time Series][data-model-time-series]. Range: [0, $\\infty$]\n\n        gamma:\n            During the training of RelMT, which is based on\n            gradient tree boosting, this value serves as the minimum\n            improvement in terms of the `loss_function` required for a\n            split of the tree to be applied. Larger `gamma` will lead\n            to fewer partitions of the tree and a more conservative\n            algorithm. Range: [0, $\\infty$]\n\n        loss_function:\n            Objective function used by the feature learning algorithm\n            to optimize your features. For regression problems use\n            [`SquareLoss`][getml.feature_learning.loss_functions.SquareLoss] and for\n            classification problems use\n            [`CrossEntropyLoss`][getml.feature_learning.loss_functions.CrossEntropyLoss].\n\n        max_depth:\n            Maximum depth of the trees generated during the gradient\n            tree boosting. Deeper trees will result in more complex\n            models and increase the risk of overfitting. Range: [0,\n            $\\infty$]\n\n        min_df:\n            Only relevant for columns with role [`text`][getml.data.roles.text].\n            The minimum\n            number of fields (i.e. rows) in [`text`][getml.data.roles.text] column a\n            given word is required to appear in to be included in the bag of words.\n            Range: [1, $\\infty$]\n\n        min_num_samples:\n            Determines the minimum number of samples a subcondition\n            should apply to in order for it to be considered. Higher\n            values lead to less complex statements and less danger of\n            overfitting. Range: [1, $\\infty$]\n\n        num_features:\n            Number of features generated by the feature learning\n            algorithm. Range: [1, $\\infty$]\n\n        num_subfeatures:\n            The number of subfeatures you would like to extract in a\n            subensemble (for snowflake data model only). See\n            :ref:`data_model_snowflake_schema` for more\n            information. Range: [1, $\\infty$]\n\n        num_threads:\n            Number of threads used by the feature learning algorithm. If set to\n            zero or a negative value, the number of threads will be\n            determined automatically by the getML engine. Range:\n            [-$\\infty$, $\\infty$]\n\n        propositionalization:\n            The feature learner used for joins which are flagged to be\n            propositionalized (by setting a join's `relationship` parameter to\n            [`propositionalization`][getml.data.relationship.propositionalization])\n\n        reg_lambda:\n            L2 regularization on the weights in the gradient boosting\n            routine. This is one of the most important hyperparameters\n            in the [`RelMT`][getml.feature_learning.RelMT] as it allows\n            for the most direct regularization. Larger values will\n            make the resulting model more conservative. Range: [0,\n            $\\infty$]\n\n        sampling_factor:\n            RelMT uses a bootstrapping procedure (sampling with\n            replacement) to train each of the features. The sampling\n            factor is proportional to the share of the samples\n            randomly drawn from the population table every time\n            RelMT generates a new feature. A lower sampling factor\n            (but still greater than 0.0), will lead to less danger of\n            overfitting, less complex statements and faster\n            training. When set to 1.0, roughly 20,000 samples are drawn\n            from the population table. If the population table\n            contains less than 20,000 samples, it will use standard\n            bagging. When set to 0.0, there will be no sampling at\n            all. Range: [0, $\\infty$]\n\n        seed:\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        shrinkage:\n            Since RelMT works using a gradient-boosting-like\n            algorithm, `shrinkage` (or learning rate) scales down the\n            weights and thus the impact of each new tree. This gives\n            more room for future ones to improve the overall\n            performance of the model in this greedy algorithm. It must\n            be between 0.0 and 1.0 with higher values leading to more\n            danger of overfitting. Range: [0, 1]\n\n        silent:\n            Controls the logging during training.\n\n        vocab_size:\n            Determines the maximum number\n            of words that are extracted in total from [`text`][getml.data.roles.text]\n            columns. This can be interpreted as the maximum size of the bag of words.\n            Range: [0, $\\infty$]\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    allow_avg: bool = True\n    delta_t: float = 0.0\n    gamma: float = 0.0\n    loss_function: Optional[str] = None\n    max_depth: int = 2\n    min_df: int = 30\n    min_num_samples: int = 1\n    num_features: int = 30\n    num_subfeatures: int = 30\n    num_threads: int = 0\n    propositionalization: FastProp = field(default_factory=FastProp)\n    reg_lambda: float = 0.0\n    sampling_factor: float = 1.0\n    seed: int = 5543\n    shrinkage: float = 0.1\n    silent: bool = True\n    vocab_size: int = 500\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        # ------------------------------------------------------------\n\n        for kkey in params:\n            if kkey not in type(self)._supported_params:\n                raise KeyError(\n                    f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n                )\n\n        # ------------------------------------------------------------\n\n        if not isinstance(params[\"silent\"], bool):\n            raise TypeError(\"'silent' must be of type bool\")\n\n        # ------------------------------------------------------------\n\n        _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relmt/#getml.feature_learning.relmt.RelMT.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/validation/","title":"Validation","text":"<p>Contains helper functions for validating the feature learning algorithms.</p>"},{"location":"reference/hyperopt/__init__/","title":"init","text":"<p>Automatically find the best parameters for</p> <ul> <li><code>Multirel</code></li> <li><code>Relboost</code></li> <li><code>RelMT</code></li> <li><code>FastProp</code></li> <li><code>FastBoost</code></li> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> </ul> Example <p>The easiest way to conduct a hyperparameter optimization is to use the built-in tuning routines. Note that these tuning routines usually take a day to complete unless we use very small data sets as we do in this example.</p> <p><pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_feature_learners(\n    pipeline=base_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=tuned_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre>     If you want to define the hyperparameter space and     the tuning routing yourself, this is how you     can do that:</p> <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.scores.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.GaussianHyperparameterSearch","title":"<code>GaussianHyperparameterSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Bayesian hyperparameter optimization using a Gaussian process.</p> <p>After a burn-in period, a Gaussian process is used to pick the most promising parameter combination to be evaluated next based on the knowledge gathered throughout previous evaluations. Accessing the quality of potential combinations will be done using the expected information (EI).</p> <p>Parameters:</p> Name Type Description Default <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> required <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful when constructing it since only the parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>rmse</code> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>100</code> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducable. Due to nature of the underlying algorithm, this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>5483</code> <code>ratio_iter</code> <code>float</code> <p>Ratio of the iterations used for the burn-in. For a <code>ratio_iter</code> of 1.0, all iterations will be spent in the burn-in period resulting in an equivalence of this class to <code>LatinHypercubeSearch</code> or <code>RandomSearch</code> - depending on <code>surrogate_burn_in_algorithm</code>. Range: [0, 1]</p> <p>As a rule of thumb at least 70 percent of the evaluations should be spent in the burn-in phase. The more comprehensive the exploration of the <code>param_space</code> during the burn-in, the less likely it is that the Gaussian process gets stuck in local minima.</p> <code>0.8</code> <code>optimization_algorithm</code> <code>string</code> <p>Determines the optimization algorithm used for the local search in the optimization of the expected information (EI). Must be from <code>optimization</code>.</p> <code>nelder_mead</code> <code>optimization_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw initial points in the burn-in period of the optimization of the expected information (EI). Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>optimization_burn_ins</code> <code>int</code> <p>Number of random evaluation points used during the burn-in of the minimization of the expected information (EI). After the surrogate model - the Gaussian process - was successfully fitted to the previous parameter combination, the algorithm is able to calculate the EI for a given point. In order to get to the next combination, the EI has to be maximized over the whole parameter space. Much like the GaussianProcess itself, this requires a burn-in phase. Range: [3, \\(\\infty\\)]</p> <code>500</code> <code>surrogate_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period. Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>gaussian_kernel</code> <code>string</code> <p>Specifies the 1-dimensional kernel of the Gaussian process which will be used along each dimension of the parameter space. All of the choices below will result in continuous sample paths and their main difference is the degree of smoothness of the results with 'exp' yielding the least and 'gauss' yielding the most smooth paths. Must be from <code>kernels</code>.</p> <code>matern52</code> <code>gaussian_optimization_algorithm</code> <code>string</code> <p>Determines the optimization algorithm used for the local search in the fitting of the Gaussian process to the previous parameter combinations. Must be from <code>optimization</code>.</p> <code>nelder_mead</code> <code>gaussian_optimization_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period of the optimization of the Gaussian process. Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>gaussian_optimization_burn_ins</code> <code>int</code> <p>Number of random evaluation points used during the burn-in of the fitting of the Gaussian process. Range: [3, \\(\\infty\\)]</p> <code>500</code> <code>early_stopping</code> <code>bool</code> <p>Whether you want to apply early stopping to the predictors.</p> <code>True</code> Note <p>A Gaussian hyperparameter search works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90%   of all iterations. During that burn-in phase, the hyperparameter   space is sampled more or less at random. You can control   this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a   Gaussian process on the hyperparameters with the <code>score</code> we want to   maximize or minimize as the predicted variable. Note that the   Gaussian process has hyperparameters itself, which are also optimized.   You can control this phase using <code>gaussian_kernel</code>,   <code>gaussian_optimization_algorithm</code>,   <code>gaussian_optimization_burn_in_algorithm</code> and   <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information   (EI), which is how much additional information it might get from   evaluating   a particular point in the hyperparameter space. The expected information   is to be maximized. The point in the hyperparameter space with   the maximum expected information is the next point that is actually   evaluated (meaning a new pipeline with these hyperparameters is trained).   You can control this phase using <code>optimization_algorithm</code>,   <code>optimization_burn_ins</code> and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space,   it starts evaluating hyperparameter combinations that are   particularly interesting.</p> </li> </ul> References <ul> <li>Carl Edward Rasmussen and Christopher K. I. Williams, MIT   Press, 2006 </li> <li>Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009   </li> </ul> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class GaussianHyperparameterSearch(_Hyperopt):\n    \"\"\"\n    Bayesian hyperparameter optimization using a Gaussian process.\n\n    After a burn-in period,\n    a Gaussian process is used to pick the most promising\n    parameter combination to be evaluated next based on the knowledge gathered\n    throughout previous evaluations. Accessing the quality of potential\n    combinations will be done using the expected information (EI).\n\n    Args:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful when\n            constructing it since only the parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducable. Due to nature of the underlying algorithm,\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        ratio_iter (float, optional):\n            Ratio of the iterations used for the burn-in.\n            For a `ratio_iter` of 1.0, all iterations will be\n            spent in the burn-in period resulting in an equivalence of\n            this class to\n            [`LatinHypercubeSearch`][getml.hyperopt.LatinHypercubeSearch] or\n            [`RandomSearch`][getml.hyperopt.RandomSearch] - depending on\n            `surrogate_burn_in_algorithm`. Range: [0, 1]\n\n            As a *rule of thumb* at least 70 percent of the evaluations\n            should be spent in the burn-in phase. The more comprehensive\n            the exploration of the `param_space` during the burn-in,\n            the less likely it is that the Gaussian process gets stuck in\n            local minima.\n\n        optimization_algorithm (string, optional):\n            Determines the optimization algorithm used for the local\n            search in the optimization of the expected information\n            (EI). Must be from\n            [`optimization`][getml.hyperopt.optimization].\n\n        optimization_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw initial points in the\n            burn-in period of the optimization of the expected\n            information (EI). Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        optimization_burn_ins (int, optional):\n            Number of random evaluation points used during the burn-in\n            of the minimization of the expected information (EI).\n            After the surrogate model - the Gaussian process - was\n            successfully fitted to the previous parameter combination,\n            the algorithm is able to calculate the EI for a given point. In\n            order to get to the next combination, the EI has to be\n            maximized over the whole parameter space. Much like the\n            GaussianProcess itself, this requires a burn-in phase.\n            Range: [3, $\\infty$]\n\n        surrogate_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw new parameter\n            combinations during the burn-in period.\n            Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        gaussian_kernel (string, optional):\n            Specifies the 1-dimensional kernel of the Gaussian process\n            which will be used along each dimension of the parameter\n            space. All of the choices below will result in continuous\n            sample paths and their main difference is the degree of\n            smoothness of the results with 'exp' yielding the least\n            and 'gauss' yielding the most smooth paths.\n            Must be from [`kernels`][getml.hyperopt.kernels].\n\n        gaussian_optimization_algorithm (string, optional):\n            Determines the optimization algorithm used for the local\n            search in the fitting of the Gaussian process to the\n            previous parameter combinations. Must be from\n            [`optimization`][getml.hyperopt.optimization].\n\n        gaussian_optimization_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw new parameter\n            combinations during the burn-in period of the optimization\n            of the Gaussian process.\n            Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        gaussian_optimization_burn_ins (int, optional):\n            Number of random evaluation points used during the burn-in\n            of the fitting of the Gaussian process. Range: [3,\n            $\\infty$]\n\n        early_stopping (bool, optional):\n            Whether you want to apply early stopping to the predictors.\n\n    Note:\n        A Gaussian hyperparameter search works like this:\n\n        - It begins with a burn-in phase, usually about 70% to 90%\n          of all iterations. During that burn-in phase, the hyperparameter\n          space is sampled more or less at random. You can control\n          this phase using ``ratio_iter`` and ``surrogate_burn_in_algorithm``.\n\n        - Once enough information has been collected, it fits a\n          Gaussian process on the hyperparameters with the ``score`` we want to\n          maximize or minimize as the predicted variable. Note that the\n          Gaussian process has hyperparameters itself, which are also optimized.\n          You can control this phase using ``gaussian_kernel``,\n          ``gaussian_optimization_algorithm``,\n          ``gaussian_optimization_burn_in_algorithm`` and\n          ``gaussian_optimization_burn_ins``.\n\n        - It then uses the Gaussian process to predict the expected information\n          (EI), which is how much additional information it might get from\n          evaluating\n          a particular point in the hyperparameter space. The expected information\n          is to be maximized. The point in the hyperparameter space with\n          the maximum expected information is the next point that is actually\n          evaluated (meaning a new pipeline with these hyperparameters is trained).\n          You can control this phase using ``optimization_algorithm``,\n          ``optimization_burn_ins`` and ``optimization_burn_in_algorithm``.\n\n        In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:\n\n        - At first, it picks random hyperparameter combinations.\n\n        - Once it has gained a better understanding of the hyperparameter space,\n          it starts evaluating hyperparameter combinations that are\n          particularly interesting.\n\n    References:\n        * [Carl Edward Rasmussen and Christopher K. I. Williams, MIT\n          Press, 2006 ](http://www.gaussianprocess.org/gpml/)\n        * [Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009\n          ](https://arxiv.org/pdf/cs/0611143.pdf)\n\n    Example:\n        ```python\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a GaussianHyperparameterSearch around the reference model\n\n        gaussian_search = hyperopt.GaussianHyperparameterSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        gaussian_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n\n        # ----------------\n\n        # We want 5 additional iterations.\n        gaussian_search.n_iter = 5\n\n        # We do not want another burn-in-phase,\n        # so we set ratio_iter to 0.\n        gaussian_search.ratio_iter = 0.0\n\n        # This widens the hyperparameter space.\n        gaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n        # This narrows the hyperparameter space.\n        gaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n        # This continues the hyperparameter search using the previous iterations as\n        # prior knowledge.\n        gaussian_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n\n        # ----------------\n\n        all_hyp = hyperopt.list_hyperopts()\n\n        best_pipeline = gaussian_search.best_pipeline\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        ratio_iter=0.80,\n        optimization_algorithm=nelder_mead,\n        optimization_burn_in_algorithm=latin_hypercube,\n        optimization_burn_ins=500,\n        surrogate_burn_in_algorithm=latin_hypercube,\n        gaussian_kernel=matern52,\n        gaussian_optimization_burn_in_algorithm=latin_hypercube,\n        gaussian_optimization_algorithm=nelder_mead,\n        gaussian_optimization_burn_ins=500,\n        gaussian_nugget=50,\n        early_stopping=True,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            ratio_iter=ratio_iter,\n            optimization_algorithm=optimization_algorithm,\n            optimization_burn_in_algorithm=optimization_burn_in_algorithm,\n            optimization_burn_ins=optimization_burn_ins,\n            surrogate_burn_in_algorithm=surrogate_burn_in_algorithm,\n            gaussian_kernel=gaussian_kernel,\n            gaussian_optimization_algorithm=gaussian_optimization_algorithm,\n            gaussian_optimization_burn_in_algorithm=gaussian_optimization_burn_in_algorithm,\n            gaussian_optimization_burn_ins=gaussian_optimization_burn_ins,\n            gaussian_nugget=gaussian_nugget,\n            early_stopping=early_stopping,\n        )\n\n        self._type = \"GaussianHyperparameterSearch\"\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict = copy.deepcopy(self.__dict__)\n        del obj_dict[\"pipeline\"]\n        del obj_dict[\"param_space\"]\n        del obj_dict[\"evaluations\"]\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.GaussianHyperparameterSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.LatinHypercubeSearch","title":"<code>LatinHypercubeSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Latin hypercube sampling of the hyperparameters.</p> <p>Uses a multidimensional, uniform cumulative distribution function to draw the random numbers from. For drawing <code>n_iter</code> samples, the distribution will be divided in <code>n_iter</code>*<code>n_iter</code> hypercubes of equal size (<code>n_iter</code> per dimension). <code>n_iter</code> of them will be selected in such a way only one per dimension is used and an independent and identically-distributed (iid) random number is drawn within the boundaries of the hypercube.</p> <p>A latin hypercube search can be seen as a compromise between a grid search, which iterates through the entire hyperparameter space, and a random search, which draws completely random samples from the hyperparameter space.</p> <p>Attributes:</p> Name Type Description <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a LatinHypercubeSearch around the reference model\n\nlatin_search = hyperopt.LatinHypercubeSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nlatin_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class LatinHypercubeSearch(_Hyperopt):\n    \"\"\"\n    Latin hypercube sampling of the hyperparameters.\n\n    Uses a multidimensional, uniform cumulative distribution function\n    to draw the random numbers from. For drawing `n_iter` samples,\n    the distribution will be divided in `n_iter`*`n_iter` hypercubes\n    of equal size (`n_iter` per dimension). `n_iter` of them will be\n    selected in such a way only one per dimension is used and an\n    independent and identically-distributed (iid) random number is\n    drawn within the boundaries of the hypercube.\n\n    A latin hypercube search can be seen as a compromise between\n    a grid search, which iterates through the entire hyperparameter\n    space, and a random search, which draws completely random samples\n    from the hyperparameter space.\n\n    Attributes:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful in\n            constructing it since only those parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Due to nature of the underlying algorithm\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            represents an unreproducible and is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n    Example:\n        ```python\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a LatinHypercubeSearch around the reference model\n\n        latin_search = hyperopt.LatinHypercubeSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        latin_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        **kwargs,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            **kwargs,\n        )\n\n        self._type = \"LatinHypercubeSearch\"\n\n        self.surrogate_burn_in_algorithm = latin_hypercube\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict = dict()\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        obj_dict[\"n_iter\"] = self.n_iter\n        obj_dict[\"seed\"] = self.seed\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n        if self.surrogate_burn_in_algorithm != latin_hypercube:\n            raise ValueError(\n                \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n            )\n\n        if self.ratio_iter != 1.0:\n            raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.LatinHypercubeSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != latin_hypercube:\n        raise ValueError(\n            \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n        )\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.RandomSearch","title":"<code>RandomSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Uniformly distributed sampling of the hyperparameters.</p> <p>During every iteration, a new set of hyperparameters is chosen at random by uniformly drawing a random value in between the lower and upper bound for each dimension of <code>param_space</code> independently.</p> <p>Attributes:</p> Name Type Description <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a RandomSearch around the reference model\n\nrandom_search = hyperopt.RandomSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nrandom_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class RandomSearch(_Hyperopt):\n    \"\"\"\n    Uniformly distributed sampling of the hyperparameters.\n\n    During every iteration, a new set of hyperparameters is chosen at random\n    by uniformly drawing a random value in between the lower and upper\n    bound for each dimension of `param_space` independently.\n\n    Attributes:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful in\n            constructing it since only those parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Due to nature of the underlying algorithm\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            represents an unreproducible and is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n    Example:\n        ```python\n\n\n\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a RandomSearch around the reference model\n\n        random_search = hyperopt.RandomSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        random_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        **kwargs,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            **kwargs,\n        )\n\n        self._type = \"RandomSearch\"\n\n        self.surrogate_burn_in_algorithm = random\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict: Dict[str, Any] = {}\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        obj_dict[\"n_iter\"] = self.n_iter\n        obj_dict[\"seed\"] = self.seed\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n        if self.surrogate_burn_in_algorithm != random:\n            raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n        if self.ratio_iter != 1.0:\n            raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.RandomSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != random:\n        raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.delete","title":"<code>delete(name)</code>","text":"<p>If a hyperopt named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the hyperopt.</p> required Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def delete(name):\n    \"\"\"\n    If a hyperopt named 'name' exists, it is deleted.\n\n    Args:\n        name (str): The name of the hyperopt.\n    \"\"\"\n\n    if not exists(name):\n        return\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Hyperopt.delete\"\n    cmd[\"name_\"] = name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.exists","title":"<code>exists(name)</code>","text":"<p>Determines whether a hyperopt exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the hyperopt.</p> required <p>Returns:</p> Type Description <p>A boolean indicating whether a hyperopt named name exists.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def exists(name):\n    \"\"\"Determines whether a hyperopt exists.\n\n    Args:\n        name (str): The name of the hyperopt.\n\n    Returns:\n        A boolean indicating whether a hyperopt named *name* exists.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    return name in list_hyperopts()\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.list_hyperopts","title":"<code>list_hyperopts()</code>","text":"<p>Lists all hyperparameter optimization objects present in the engine.</p> <p>Note that this function only lists hyperopts which are part of the current project. See <code>set_project</code> for changing projects.</p> <p>To subsequently load one of them, use <code>load_hyperopt</code>.</p> <p>Returns:</p> Type Description <p>list containing the names of all hyperopts.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def list_hyperopts():\n    \"\"\"Lists all hyperparameter optimization objects present in the engine.\n\n    Note that this function only lists hyperopts which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects.\n\n    To subsequently load one of them, use\n    [`load_hyperopt`][getml.hyperopt.load_hyperopt].\n\n    Returns:\n        list containing the names of all hyperopts.\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_hyperopts\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.tune_feature_learners","title":"<code>tune_feature_learners(pipeline, container, train='train', validation='validation', n_iter=0, score=None, num_threads=0)</code>","text":"<p>A high-level interface for optimizing the feature learners of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of feature learners (from <code>feature_learning</code>) of a given pipeline by breaking each feature learner's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>container</code> <code>[`Container`][getml.data.Container]</code> <p>The data container used for the hyperparameter tuning.</p> required <code>train</code> <code>str</code> <p>The name of the subset in 'container' used for training.</p> <code>'train'</code> <code>validation</code> <code>str</code> <p>The name of the subset in 'container' used for validation.</p> <code>'validation'</code> <code>n_iter</code> <code>int</code> <p>The number of iterations.</p> <code>0</code> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>None</code> <code>num_threads</code> <code>int</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <code>0</code> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> <p>Returns:</p> Type Description <p>A <code>Pipeline</code> containing tuned versions</p> <p>of the feature learners.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_feature_learners(\n    pipeline,\n    container,\n    train=\"train\",\n    validation=\"validation\",\n    n_iter=0,\n    score=None,\n    num_threads=0,\n):\n    \"\"\"\n    A high-level interface for optimizing the feature learners of a\n    [`Pipeline`][getml.pipelines.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of feature learners\n    (from [`feature_learning`][getml.feature_learning]) of a given pipeline by breaking each\n    feature learner's hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer\n    to tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container ([`Container`][getml.data.Container]):\n            The data container used for the hyperparameter tuning.\n\n        train (str, optional):\n            The name of the subset in 'container' used for training.\n\n        validation (str, optional):\n            The name of the subset in 'container' used for validation.\n\n        n_iter (int, optional):\n            The number of iterations.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads (int, optional):\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n\n\n            tuned_pipeline = getml.hyperopt.tune_predictors(\n                pipeline=base_pipeline,\n                container=container)\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] containing tuned versions\n        of the feature learners.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_feature_learners = []\n\n    for feature_learner in pipeline.feature_learners:\n        tuned_pipeline = _tune_feature_learner(\n            feature_learner=feature_learner,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.feature_learners) == 1\n        ), \"Expected exactly one feature learner, got \" + str(\n            len(tuned_pipeline.feature_learners)\n        )\n\n        tuned_feature_learners.append(tuned_pipeline.feature_learners[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        tuned_feature_learners,\n        copy.deepcopy(pipeline.predictors),\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/__init__/#getml.hyperopt.tune_predictors","title":"<code>tune_predictors(pipeline, container, train='train', validation='validation', n_iter=0, score=None, num_threads=0)</code>","text":"<p>A high-level interface for optimizing the predictors of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of predictors (from <code>getml.predictors</code>) of a given pipeline by breaking each predictor's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>container</code> <code>[`Container`][getml.data.Container]</code> <p>The data container used for the hyperparameter tuning.</p> required <code>train</code> <code>str</code> <p>The name of the subset in 'container' used for training.</p> <code>'train'</code> <code>validation</code> <code>str</code> <p>The name of the subset in 'container' used for validation.</p> <code>'validation'</code> <code>n_iter</code> <code>int</code> <p>The number of iterations.</p> <code>0</code> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>None</code> <code>num_threads</code> <code>int</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <code>0</code> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> <p>Returns:</p> Type Description <p>A <code>Pipeline</code> containing tuned</p> <p>predictors.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_predictors(\n    pipeline: getml.pipeline.Pipeline,\n    container: getml.data.Container,\n    train=\"train\",\n    validation=\"validation\",\n    n_iter=0,\n    score=None,\n    num_threads=0,\n):\n    \"\"\"\n    A high-level interface for optimizing the predictors of a\n    [`Pipeline`][getml.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of predictors (from\n    `getml.predictors`) of a given pipeline by breaking each predictor's\n    hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer to\n    tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container ([`Container`][getml.data.Container]):\n            The data container used for the hyperparameter tuning.\n\n        train (str, optional):\n            The name of the subset in 'container' used for training.\n\n        validation (str, optional):\n            The name of the subset in 'container' used for validation.\n\n        n_iter (int, optional):\n            The number of iterations.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads (int, optional):\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n\n\n            tuned_pipeline = getml.hyperopt.tune_predictors(\n                pipeline=base_pipeline,\n                container=container)\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] containing tuned\n        predictors.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_predictors = []\n\n    for predictor in pipeline.predictors:\n        tuned_pipeline = _tune_predictor(\n            predictor=predictor,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.predictors) == 1\n        ), \"Expected exactly one predictor, got \" + str(len(tuned_pipeline.predictors))\n\n        tuned_predictors.append(tuned_pipeline.predictors[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        copy.deepcopy(pipeline.feature_learners),\n        tuned_predictors,\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/burn_in/","title":"Burn in","text":"<p>Collection of burn-in algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/burn_in/#getml.hyperopt.burn_in.latin_hypercube","title":"<code>latin_hypercube = 'latinHypercube'</code>  <code>module-attribute</code>","text":"<p>Samples from the hyperparameter space almost randomly, but ensures that the different draws are sufficiently different from each other.</p>"},{"location":"reference/hyperopt/burn_in/#getml.hyperopt.burn_in.random","title":"<code>random = 'random'</code>  <code>module-attribute</code>","text":"<p>Samples from the hyperparameter space at random.</p>"},{"location":"reference/hyperopt/helpers/","title":"Helpers","text":"<p>Lists all hyperparameter optimization objects present in the engine.</p>"},{"location":"reference/hyperopt/helpers/#getml.hyperopt.helpers.delete","title":"<code>delete(name)</code>","text":"<p>If a hyperopt named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the hyperopt.</p> required Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def delete(name):\n    \"\"\"\n    If a hyperopt named 'name' exists, it is deleted.\n\n    Args:\n        name (str): The name of the hyperopt.\n    \"\"\"\n\n    if not exists(name):\n        return\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Hyperopt.delete\"\n    cmd[\"name_\"] = name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/hyperopt/helpers/#getml.hyperopt.helpers.exists","title":"<code>exists(name)</code>","text":"<p>Determines whether a hyperopt exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the hyperopt.</p> required <p>Returns:</p> Type Description <p>A boolean indicating whether a hyperopt named name exists.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def exists(name):\n    \"\"\"Determines whether a hyperopt exists.\n\n    Args:\n        name (str): The name of the hyperopt.\n\n    Returns:\n        A boolean indicating whether a hyperopt named *name* exists.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    return name in list_hyperopts()\n</code></pre>"},{"location":"reference/hyperopt/helpers/#getml.hyperopt.helpers.list_hyperopts","title":"<code>list_hyperopts()</code>","text":"<p>Lists all hyperparameter optimization objects present in the engine.</p> <p>Note that this function only lists hyperopts which are part of the current project. See <code>set_project</code> for changing projects.</p> <p>To subsequently load one of them, use <code>load_hyperopt</code>.</p> <p>Returns:</p> Type Description <p>list containing the names of all hyperopts.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def list_hyperopts():\n    \"\"\"Lists all hyperparameter optimization objects present in the engine.\n\n    Note that this function only lists hyperopts which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects.\n\n    To subsequently load one of them, use\n    [`load_hyperopt`][getml.hyperopt.load_hyperopt].\n\n    Returns:\n        list containing the names of all hyperopts.\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_hyperopts\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/","title":"Hyperopt","text":"<p>Contains hyperparameter optimization routines.</p>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.GaussianHyperparameterSearch","title":"<code>GaussianHyperparameterSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Bayesian hyperparameter optimization using a Gaussian process.</p> <p>After a burn-in period, a Gaussian process is used to pick the most promising parameter combination to be evaluated next based on the knowledge gathered throughout previous evaluations. Accessing the quality of potential combinations will be done using the expected information (EI).</p> <p>Parameters:</p> Name Type Description Default <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> required <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful when constructing it since only the parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>rmse</code> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>100</code> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducable. Due to nature of the underlying algorithm, this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <code>5483</code> <code>ratio_iter</code> <code>float</code> <p>Ratio of the iterations used for the burn-in. For a <code>ratio_iter</code> of 1.0, all iterations will be spent in the burn-in period resulting in an equivalence of this class to <code>LatinHypercubeSearch</code> or <code>RandomSearch</code> - depending on <code>surrogate_burn_in_algorithm</code>. Range: [0, 1]</p> <p>As a rule of thumb at least 70 percent of the evaluations should be spent in the burn-in phase. The more comprehensive the exploration of the <code>param_space</code> during the burn-in, the less likely it is that the Gaussian process gets stuck in local minima.</p> <code>0.8</code> <code>optimization_algorithm</code> <code>string</code> <p>Determines the optimization algorithm used for the local search in the optimization of the expected information (EI). Must be from <code>optimization</code>.</p> <code>nelder_mead</code> <code>optimization_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw initial points in the burn-in period of the optimization of the expected information (EI). Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>optimization_burn_ins</code> <code>int</code> <p>Number of random evaluation points used during the burn-in of the minimization of the expected information (EI). After the surrogate model - the Gaussian process - was successfully fitted to the previous parameter combination, the algorithm is able to calculate the EI for a given point. In order to get to the next combination, the EI has to be maximized over the whole parameter space. Much like the GaussianProcess itself, this requires a burn-in phase. Range: [3, \\(\\infty\\)]</p> <code>500</code> <code>surrogate_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period. Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>gaussian_kernel</code> <code>string</code> <p>Specifies the 1-dimensional kernel of the Gaussian process which will be used along each dimension of the parameter space. All of the choices below will result in continuous sample paths and their main difference is the degree of smoothness of the results with 'exp' yielding the least and 'gauss' yielding the most smooth paths. Must be from <code>kernels</code>.</p> <code>matern52</code> <code>gaussian_optimization_algorithm</code> <code>string</code> <p>Determines the optimization algorithm used for the local search in the fitting of the Gaussian process to the previous parameter combinations. Must be from <code>optimization</code>.</p> <code>nelder_mead</code> <code>gaussian_optimization_burn_in_algorithm</code> <code>string</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period of the optimization of the Gaussian process. Must be from <code>burn_in</code>.</p> <code>latin_hypercube</code> <code>gaussian_optimization_burn_ins</code> <code>int</code> <p>Number of random evaluation points used during the burn-in of the fitting of the Gaussian process. Range: [3, \\(\\infty\\)]</p> <code>500</code> <code>early_stopping</code> <code>bool</code> <p>Whether you want to apply early stopping to the predictors.</p> <code>True</code> Note <p>A Gaussian hyperparameter search works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90%   of all iterations. During that burn-in phase, the hyperparameter   space is sampled more or less at random. You can control   this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a   Gaussian process on the hyperparameters with the <code>score</code> we want to   maximize or minimize as the predicted variable. Note that the   Gaussian process has hyperparameters itself, which are also optimized.   You can control this phase using <code>gaussian_kernel</code>,   <code>gaussian_optimization_algorithm</code>,   <code>gaussian_optimization_burn_in_algorithm</code> and   <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information   (EI), which is how much additional information it might get from   evaluating   a particular point in the hyperparameter space. The expected information   is to be maximized. The point in the hyperparameter space with   the maximum expected information is the next point that is actually   evaluated (meaning a new pipeline with these hyperparameters is trained).   You can control this phase using <code>optimization_algorithm</code>,   <code>optimization_burn_ins</code> and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space,   it starts evaluating hyperparameter combinations that are   particularly interesting.</p> </li> </ul> References <ul> <li>Carl Edward Rasmussen and Christopher K. I. Williams, MIT   Press, 2006 </li> <li>Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009   </li> </ul> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class GaussianHyperparameterSearch(_Hyperopt):\n    \"\"\"\n    Bayesian hyperparameter optimization using a Gaussian process.\n\n    After a burn-in period,\n    a Gaussian process is used to pick the most promising\n    parameter combination to be evaluated next based on the knowledge gathered\n    throughout previous evaluations. Accessing the quality of potential\n    combinations will be done using the expected information (EI).\n\n    Args:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful when\n            constructing it since only the parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducable. Due to nature of the underlying algorithm,\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n        ratio_iter (float, optional):\n            Ratio of the iterations used for the burn-in.\n            For a `ratio_iter` of 1.0, all iterations will be\n            spent in the burn-in period resulting in an equivalence of\n            this class to\n            [`LatinHypercubeSearch`][getml.hyperopt.LatinHypercubeSearch] or\n            [`RandomSearch`][getml.hyperopt.RandomSearch] - depending on\n            `surrogate_burn_in_algorithm`. Range: [0, 1]\n\n            As a *rule of thumb* at least 70 percent of the evaluations\n            should be spent in the burn-in phase. The more comprehensive\n            the exploration of the `param_space` during the burn-in,\n            the less likely it is that the Gaussian process gets stuck in\n            local minima.\n\n        optimization_algorithm (string, optional):\n            Determines the optimization algorithm used for the local\n            search in the optimization of the expected information\n            (EI). Must be from\n            [`optimization`][getml.hyperopt.optimization].\n\n        optimization_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw initial points in the\n            burn-in period of the optimization of the expected\n            information (EI). Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        optimization_burn_ins (int, optional):\n            Number of random evaluation points used during the burn-in\n            of the minimization of the expected information (EI).\n            After the surrogate model - the Gaussian process - was\n            successfully fitted to the previous parameter combination,\n            the algorithm is able to calculate the EI for a given point. In\n            order to get to the next combination, the EI has to be\n            maximized over the whole parameter space. Much like the\n            GaussianProcess itself, this requires a burn-in phase.\n            Range: [3, $\\infty$]\n\n        surrogate_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw new parameter\n            combinations during the burn-in period.\n            Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        gaussian_kernel (string, optional):\n            Specifies the 1-dimensional kernel of the Gaussian process\n            which will be used along each dimension of the parameter\n            space. All of the choices below will result in continuous\n            sample paths and their main difference is the degree of\n            smoothness of the results with 'exp' yielding the least\n            and 'gauss' yielding the most smooth paths.\n            Must be from [`kernels`][getml.hyperopt.kernels].\n\n        gaussian_optimization_algorithm (string, optional):\n            Determines the optimization algorithm used for the local\n            search in the fitting of the Gaussian process to the\n            previous parameter combinations. Must be from\n            [`optimization`][getml.hyperopt.optimization].\n\n        gaussian_optimization_burn_in_algorithm (string, optional):\n            Specifies the algorithm used to draw new parameter\n            combinations during the burn-in period of the optimization\n            of the Gaussian process.\n            Must be from [`burn_in`][getml.hyperopt.burn_in].\n\n        gaussian_optimization_burn_ins (int, optional):\n            Number of random evaluation points used during the burn-in\n            of the fitting of the Gaussian process. Range: [3,\n            $\\infty$]\n\n        early_stopping (bool, optional):\n            Whether you want to apply early stopping to the predictors.\n\n    Note:\n        A Gaussian hyperparameter search works like this:\n\n        - It begins with a burn-in phase, usually about 70% to 90%\n          of all iterations. During that burn-in phase, the hyperparameter\n          space is sampled more or less at random. You can control\n          this phase using ``ratio_iter`` and ``surrogate_burn_in_algorithm``.\n\n        - Once enough information has been collected, it fits a\n          Gaussian process on the hyperparameters with the ``score`` we want to\n          maximize or minimize as the predicted variable. Note that the\n          Gaussian process has hyperparameters itself, which are also optimized.\n          You can control this phase using ``gaussian_kernel``,\n          ``gaussian_optimization_algorithm``,\n          ``gaussian_optimization_burn_in_algorithm`` and\n          ``gaussian_optimization_burn_ins``.\n\n        - It then uses the Gaussian process to predict the expected information\n          (EI), which is how much additional information it might get from\n          evaluating\n          a particular point in the hyperparameter space. The expected information\n          is to be maximized. The point in the hyperparameter space with\n          the maximum expected information is the next point that is actually\n          evaluated (meaning a new pipeline with these hyperparameters is trained).\n          You can control this phase using ``optimization_algorithm``,\n          ``optimization_burn_ins`` and ``optimization_burn_in_algorithm``.\n\n        In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:\n\n        - At first, it picks random hyperparameter combinations.\n\n        - Once it has gained a better understanding of the hyperparameter space,\n          it starts evaluating hyperparameter combinations that are\n          particularly interesting.\n\n    References:\n        * [Carl Edward Rasmussen and Christopher K. I. Williams, MIT\n          Press, 2006 ](http://www.gaussianprocess.org/gpml/)\n        * [Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009\n          ](https://arxiv.org/pdf/cs/0611143.pdf)\n\n    Example:\n        ```python\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a GaussianHyperparameterSearch around the reference model\n\n        gaussian_search = hyperopt.GaussianHyperparameterSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        gaussian_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n\n        # ----------------\n\n        # We want 5 additional iterations.\n        gaussian_search.n_iter = 5\n\n        # We do not want another burn-in-phase,\n        # so we set ratio_iter to 0.\n        gaussian_search.ratio_iter = 0.0\n\n        # This widens the hyperparameter space.\n        gaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n        # This narrows the hyperparameter space.\n        gaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n        # This continues the hyperparameter search using the previous iterations as\n        # prior knowledge.\n        gaussian_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n\n        # ----------------\n\n        all_hyp = hyperopt.list_hyperopts()\n\n        best_pipeline = gaussian_search.best_pipeline\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        ratio_iter=0.80,\n        optimization_algorithm=nelder_mead,\n        optimization_burn_in_algorithm=latin_hypercube,\n        optimization_burn_ins=500,\n        surrogate_burn_in_algorithm=latin_hypercube,\n        gaussian_kernel=matern52,\n        gaussian_optimization_burn_in_algorithm=latin_hypercube,\n        gaussian_optimization_algorithm=nelder_mead,\n        gaussian_optimization_burn_ins=500,\n        gaussian_nugget=50,\n        early_stopping=True,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            ratio_iter=ratio_iter,\n            optimization_algorithm=optimization_algorithm,\n            optimization_burn_in_algorithm=optimization_burn_in_algorithm,\n            optimization_burn_ins=optimization_burn_ins,\n            surrogate_burn_in_algorithm=surrogate_burn_in_algorithm,\n            gaussian_kernel=gaussian_kernel,\n            gaussian_optimization_algorithm=gaussian_optimization_algorithm,\n            gaussian_optimization_burn_in_algorithm=gaussian_optimization_burn_in_algorithm,\n            gaussian_optimization_burn_ins=gaussian_optimization_burn_ins,\n            gaussian_nugget=gaussian_nugget,\n            early_stopping=early_stopping,\n        )\n\n        self._type = \"GaussianHyperparameterSearch\"\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict = copy.deepcopy(self.__dict__)\n        del obj_dict[\"pipeline\"]\n        del obj_dict[\"param_space\"]\n        del obj_dict[\"evaluations\"]\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.GaussianHyperparameterSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.LatinHypercubeSearch","title":"<code>LatinHypercubeSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Latin hypercube sampling of the hyperparameters.</p> <p>Uses a multidimensional, uniform cumulative distribution function to draw the random numbers from. For drawing <code>n_iter</code> samples, the distribution will be divided in <code>n_iter</code>*<code>n_iter</code> hypercubes of equal size (<code>n_iter</code> per dimension). <code>n_iter</code> of them will be selected in such a way only one per dimension is used and an independent and identically-distributed (iid) random number is drawn within the boundaries of the hypercube.</p> <p>A latin hypercube search can be seen as a compromise between a grid search, which iterates through the entire hyperparameter space, and a random search, which draws completely random samples from the hyperparameter space.</p> <p>Attributes:</p> Name Type Description <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a LatinHypercubeSearch around the reference model\n\nlatin_search = hyperopt.LatinHypercubeSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nlatin_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class LatinHypercubeSearch(_Hyperopt):\n    \"\"\"\n    Latin hypercube sampling of the hyperparameters.\n\n    Uses a multidimensional, uniform cumulative distribution function\n    to draw the random numbers from. For drawing `n_iter` samples,\n    the distribution will be divided in `n_iter`*`n_iter` hypercubes\n    of equal size (`n_iter` per dimension). `n_iter` of them will be\n    selected in such a way only one per dimension is used and an\n    independent and identically-distributed (iid) random number is\n    drawn within the boundaries of the hypercube.\n\n    A latin hypercube search can be seen as a compromise between\n    a grid search, which iterates through the entire hyperparameter\n    space, and a random search, which draws completely random samples\n    from the hyperparameter space.\n\n    Attributes:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful in\n            constructing it since only those parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Due to nature of the underlying algorithm\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            represents an unreproducible and is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n    Example:\n        ```python\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a LatinHypercubeSearch around the reference model\n\n        latin_search = hyperopt.LatinHypercubeSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        latin_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        **kwargs,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            **kwargs,\n        )\n\n        self._type = \"LatinHypercubeSearch\"\n\n        self.surrogate_burn_in_algorithm = latin_hypercube\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict = dict()\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        obj_dict[\"n_iter\"] = self.n_iter\n        obj_dict[\"seed\"] = self.seed\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n        if self.surrogate_burn_in_algorithm != latin_hypercube:\n            raise ValueError(\n                \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n            )\n\n        if self.ratio_iter != 1.0:\n            raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.LatinHypercubeSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != latin_hypercube:\n        raise ValueError(\n            \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n        )\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.RandomSearch","title":"<code>RandomSearch</code>","text":"<p>               Bases: <code>_Hyperopt</code></p> <p>Uniformly distributed sampling of the hyperparameters.</p> <p>During every iteration, a new set of hyperparameters is chosen at random by uniformly drawing a random value in between the lower and upper bound for each dimension of <code>param_space</code> independently.</p> <p>Attributes:</p> Name Type Description <code>param_space</code> <code>dict</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>n_iter</code> <code>int</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <code>seed</code> <code>int</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a RandomSearch around the reference model\n\nrandom_search = hyperopt.RandomSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nrandom_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>class RandomSearch(_Hyperopt):\n    \"\"\"\n    Uniformly distributed sampling of the hyperparameters.\n\n    During every iteration, a new set of hyperparameters is chosen at random\n    by uniformly drawing a random value in between the lower and upper\n    bound for each dimension of `param_space` independently.\n\n    Attributes:\n        param_space (dict):\n            Dictionary containing numerical arrays of length two\n            holding the lower and upper bounds of all parameters which\n            will be altered in `pipeline` during the hyperparameter\n            optimization.\n\n            If we have two feature learners and one predictor,\n            the hyperparameter space might look like this:\n\n\n\n                param_space = {\n                    \"feature_learners\": [\n                        {\n                            \"num_features\": [10, 50],\n                        },\n                        {\n                            \"max_depth\": [1, 10],\n                            \"min_num_samples\": [100, 500],\n                            \"num_features\": [10, 50],\n                            \"reg_lambda\": [0.0, 0.1],\n                            \"shrinkage\": [0.01, 0.4]\n                        }],\n                    \"predictors\": [\n                        {\n                            \"reg_lambda\": [0.0, 10.0]\n                        }\n                    ]\n                }\n\n            If we only want to optimize the predictor, then\n            we can leave out the feature learners.\n\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. Be careful in\n            constructing it since only those parameters present in\n            `param_space` will be overwritten. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        n_iter (int, optional):\n            Number of iterations in the hyperparameter optimization\n            and thus the number of parameter combinations to draw and\n            evaluate. Range: [1, $\\infty$]\n\n        seed (int, optional):\n            Seed used for the random number generator that underlies\n            the sampling procedure to make the calculation\n            reproducible. Due to nature of the underlying algorithm\n            this is only the case if the fit is done without\n            multithreading. To reflect this, a `seed` of None\n            represents an unreproducible and is only allowed to be set\n            to an actual integer if both ``num_threads`` and\n            ``n_jobs`` instance variables of the ``predictor`` and\n            ``feature_selector`` in `model` - if they are instances of\n            either [`XGBoostRegressor`][getml.predictors.XGBoostRegressor] or\n            [`XGBoostClassifier`][getml.predictors.XGBoostClassifier] - are set to\n            1. Internally, a `seed` of None will be mapped to\n            5543. Range: [0, $\\infty$]\n\n    Example:\n        ```python\n\n\n\n        from getml import data\n        from getml import datasets\n        from getml import engine\n        from getml import feature_learning\n        from getml.feature_learning import aggregations\n        from getml.feature_learning import loss_functions\n        from getml import hyperopt\n        from getml import pipeline\n        from getml import predictors\n\n        # ----------------\n\n        engine.set_project(\"examples\")\n\n        # ----------------\n\n        population_table, peripheral_table = datasets.make_numerical()\n\n        # ----------------\n        # Construct placeholders\n\n        population_placeholder = data.Placeholder(\"POPULATION\")\n        peripheral_placeholder = data.Placeholder(\"PERIPHERAL\")\n        population_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe1 = feature_learning.Multirel(\n            aggregation=[\n                aggregations.Count,\n                aggregations.Sum\n            ],\n            loss_function=loss_functions.SquareLoss,\n            num_features=10,\n            share_aggregations=1.0,\n            max_length=1,\n            num_threads=0\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        fe2 = feature_learning.Relboost(\n            loss_function=loss_functions.SquareLoss,\n            num_features=10\n        )\n\n        # ----------------\n        # Base model - any parameters not included\n        # in param_space will be taken from this.\n\n        predictor = predictors.LinearRegression()\n\n        # ----------------\n\n        pipe = pipeline.Pipeline(\n            population=population_placeholder,\n            peripheral=[peripheral_placeholder],\n            feature_learners=[fe1, fe2],\n            predictors=[predictor]\n        )\n\n        # ----------------\n        # Build a hyperparameter space.\n        # We have two feature learners and one\n        # predictor, so this is how we must\n        # construct our hyperparameter space.\n        # If we only wanted to optimize the predictor,\n        # we could just leave out the feature_learners.\n\n        param_space = {\n            \"feature_learners\": [\n                {\n                    \"num_features\": [10, 50],\n                },\n                {\n                    \"max_depth\": [1, 10],\n                    \"min_num_samples\": [100, 500],\n                    \"num_features\": [10, 50],\n                    \"reg_lambda\": [0.0, 0.1],\n                    \"shrinkage\": [0.01, 0.4]\n                }],\n            \"predictors\": [\n                {\n                    \"reg_lambda\": [0.0, 10.0]\n                }\n            ]\n        }\n\n        # ----------------\n        # Wrap a RandomSearch around the reference model\n\n        random_search = hyperopt.RandomSearch(\n            pipeline=pipe,\n            param_space=param_space,\n            n_iter=30,\n            score=pipeline.metrics.rsquared\n        )\n\n        random_search.fit(\n            population_table_training=population_table,\n            population_table_validation=population_table,\n            peripheral_tables=[peripheral_table]\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    def __init__(\n        self,\n        param_space: Dict[str, Any],\n        pipeline: Pipeline,\n        score=metrics.rmse,\n        n_iter=100,\n        seed=5483,\n        **kwargs,\n    ):\n        super().__init__(\n            param_space=param_space,\n            pipeline=pipeline,\n            score=score,\n            n_iter=n_iter,\n            seed=seed,\n            **kwargs,\n        )\n\n        self._type = \"RandomSearch\"\n\n        self.surrogate_burn_in_algorithm = random\n\n        self.validate()\n\n    # ----------------------------------------------------------------\n\n    def __str__(self):\n        obj_dict: Dict[str, Any] = {}\n        obj_dict[\"type\"] = self.type\n        obj_dict[\"score\"] = self.score\n        obj_dict[\"n_iter\"] = self.n_iter\n        obj_dict[\"seed\"] = self.seed\n        sig = _SignatureFormatter(data=obj_dict)\n        return sig._format()\n\n    # ------------------------------------------------------------\n\n    def validate(self):\n        \"\"\"\n        Validate the parameters of the hyperparameter optimization.\n        \"\"\"\n        _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n        if self.surrogate_burn_in_algorithm != random:\n            raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n        if self.ratio_iter != 1.0:\n            raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/hyperopt/#getml.hyperopt.hyperopt.RandomSearch.validate","title":"<code>validate()</code>","text":"<p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self):\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != random:\n        raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/kernels/","title":"Kernels","text":"<p>Collection of kernel functions to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/kernels/#getml.hyperopt.kernels.exp","title":"<code>exp = 'exp'</code>  <code>module-attribute</code>","text":"<p>An exponential kernel yielding non-differentiable sample paths.</p>"},{"location":"reference/hyperopt/kernels/#getml.hyperopt.kernels.gauss","title":"<code>gauss = 'gauss'</code>  <code>module-attribute</code>","text":"<p>A Gaussian kernel yielding analytic (infinitely--differentiable) sample paths.</p>"},{"location":"reference/hyperopt/kernels/#getml.hyperopt.kernels.matern32","title":"<code>matern32 = 'matern32'</code>  <code>module-attribute</code>","text":"<p>A Mat\u00e9rn 3/2 kernel yielding once-differentiable sample paths.</p>"},{"location":"reference/hyperopt/kernels/#getml.hyperopt.kernels.matern52","title":"<code>matern52 = 'matern52'</code>  <code>module-attribute</code>","text":"<p>A Mat\u00e9rn 5/2 kernel yielding twice-differentiable sample paths.</p>"},{"location":"reference/hyperopt/load_hyperopt/","title":"Load hyperopt","text":"<p>Loads a hyperparameter optimization object from the getML engine into Python.</p>"},{"location":"reference/hyperopt/load_hyperopt/#getml.hyperopt.load_hyperopt.load_hyperopt","title":"<code>load_hyperopt(name)</code>","text":"<p>Loads a hyperparameter optimization object from the getML engine into Python.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the hyperopt to be loaded.</p> required <p>Returns:</p> Type Description <p>A <code>GaussianHyperparameterSearch</code> that is a handler for the pipeline signified by name.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/load_hyperopt.py</code> <pre><code>def load_hyperopt(name):\n    \"\"\"Loads a hyperparameter optimization object from the getML engine into Python.\n\n    Args:\n        name (str):\n            The name of the hyperopt to be loaded.\n\n    Returns:\n        A [`GaussianHyperparameterSearch`][getml.hyperopt.GaussianHyperparameterSearch] that is a handler for the pipeline signified by name.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n    # This will be overwritten by .refresh(...) anyway\n    dummy_pipeline = _make_dummy(\"123456\")\n\n    dummy_param_space = {\"predictors\": [{\"reg_lambda\": [0.0, 1.0]}]}\n\n    json_obj = _get_json_obj(name)\n\n    if json_obj[\"type_\"] == \"GaussianHyperparameterSearch\":\n        return GaussianHyperparameterSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"LatinHypercubeSearch\":\n        return LatinHypercubeSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"RandomSearch\":\n        return RandomSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    raise ValueError(\"Unknown type: '\" + json_obj[\"type_\"] + \"'!\")\n</code></pre>"},{"location":"reference/hyperopt/optimization/","title":"Optimization","text":"<p>Collection of optimization algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/optimization/#getml.hyperopt.optimization.bfgs","title":"<code>bfgs = 'bfgs'</code>  <code>module-attribute</code>","text":"<p>Broyden-Fletcher-Goldbarb-Shanno optimization algorithm.</p> <p>The BFGS algorithm is a quasi-Newton method that requires the function to be differentiable.</p>"},{"location":"reference/hyperopt/optimization/#getml.hyperopt.optimization.nelder_mead","title":"<code>nelder_mead = 'nelderMead'</code>  <code>module-attribute</code>","text":"<p>Nelder-Mead optimization algorithm.</p> <p>Nelder-Mead is a direct search method that does not require functions to be differentiable.</p>"},{"location":"reference/hyperopt/tuning/","title":"Tuning","text":"<p>Tuning routines simplify the hyperparameter optimizations and are the recommended way of tuning hyperparameters.</p>"},{"location":"reference/hyperopt/tuning/#getml.hyperopt.tuning.tune_feature_learners","title":"<code>tune_feature_learners(pipeline, container, train='train', validation='validation', n_iter=0, score=None, num_threads=0)</code>","text":"<p>A high-level interface for optimizing the feature learners of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of feature learners (from <code>feature_learning</code>) of a given pipeline by breaking each feature learner's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>container</code> <code>[`Container`][getml.data.Container]</code> <p>The data container used for the hyperparameter tuning.</p> required <code>train</code> <code>str</code> <p>The name of the subset in 'container' used for training.</p> <code>'train'</code> <code>validation</code> <code>str</code> <p>The name of the subset in 'container' used for validation.</p> <code>'validation'</code> <code>n_iter</code> <code>int</code> <p>The number of iterations.</p> <code>0</code> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>None</code> <code>num_threads</code> <code>int</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <code>0</code> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> <p>Returns:</p> Type Description <p>A <code>Pipeline</code> containing tuned versions</p> <p>of the feature learners.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_feature_learners(\n    pipeline,\n    container,\n    train=\"train\",\n    validation=\"validation\",\n    n_iter=0,\n    score=None,\n    num_threads=0,\n):\n    \"\"\"\n    A high-level interface for optimizing the feature learners of a\n    [`Pipeline`][getml.pipelines.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of feature learners\n    (from [`feature_learning`][getml.feature_learning]) of a given pipeline by breaking each\n    feature learner's hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer\n    to tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container ([`Container`][getml.data.Container]):\n            The data container used for the hyperparameter tuning.\n\n        train (str, optional):\n            The name of the subset in 'container' used for training.\n\n        validation (str, optional):\n            The name of the subset in 'container' used for validation.\n\n        n_iter (int, optional):\n            The number of iterations.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads (int, optional):\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n\n\n            tuned_pipeline = getml.hyperopt.tune_predictors(\n                pipeline=base_pipeline,\n                container=container)\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] containing tuned versions\n        of the feature learners.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_feature_learners = []\n\n    for feature_learner in pipeline.feature_learners:\n        tuned_pipeline = _tune_feature_learner(\n            feature_learner=feature_learner,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.feature_learners) == 1\n        ), \"Expected exactly one feature learner, got \" + str(\n            len(tuned_pipeline.feature_learners)\n        )\n\n        tuned_feature_learners.append(tuned_pipeline.feature_learners[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        tuned_feature_learners,\n        copy.deepcopy(pipeline.predictors),\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/tuning/#getml.hyperopt.tuning.tune_predictors","title":"<code>tune_predictors(pipeline, container, train='train', validation='validation', n_iter=0, score=None, num_threads=0)</code>","text":"<p>A high-level interface for optimizing the predictors of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of predictors (from <code>getml.predictors</code>) of a given pipeline by breaking each predictor's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>[`Pipeline`][getml.Pipeline]</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> required <code>container</code> <code>[`Container`][getml.data.Container]</code> <p>The data container used for the hyperparameter tuning.</p> required <code>train</code> <code>str</code> <p>The name of the subset in 'container' used for training.</p> <code>'train'</code> <code>validation</code> <code>str</code> <p>The name of the subset in 'container' used for validation.</p> <code>'validation'</code> <code>n_iter</code> <code>int</code> <p>The number of iterations.</p> <code>0</code> <code>score</code> <code>str</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <code>None</code> <code>num_threads</code> <code>int</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <code>0</code> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> <p>Returns:</p> Type Description <p>A <code>Pipeline</code> containing tuned</p> <p>predictors.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_predictors(\n    pipeline: getml.pipeline.Pipeline,\n    container: getml.data.Container,\n    train=\"train\",\n    validation=\"validation\",\n    n_iter=0,\n    score=None,\n    num_threads=0,\n):\n    \"\"\"\n    A high-level interface for optimizing the predictors of a\n    [`Pipeline`][getml.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of predictors (from\n    `getml.predictors`) of a given pipeline by breaking each predictor's\n    hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer to\n    tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline ([`Pipeline`][getml.Pipeline]):\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container ([`Container`][getml.data.Container]):\n            The data container used for the hyperparameter tuning.\n\n        train (str, optional):\n            The name of the subset in 'container' used for training.\n\n        validation (str, optional):\n            The name of the subset in 'container' used for validation.\n\n        n_iter (int, optional):\n            The number of iterations.\n\n        score (str, optional):\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads (int, optional):\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n\n\n            tuned_pipeline = getml.hyperopt.tune_predictors(\n                pipeline=base_pipeline,\n                container=container)\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] containing tuned\n        predictors.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_predictors = []\n\n    for predictor in pipeline.predictors:\n        tuned_pipeline = _tune_predictor(\n            predictor=predictor,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.predictors) == 1\n        ), \"Expected exactly one predictor, got \" + str(len(tuned_pipeline.predictors))\n\n        tuned_predictors.append(tuned_pipeline.predictors[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        copy.deepcopy(pipeline.feature_learners),\n        tuned_predictors,\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/validation/","title":"Validation","text":"<p>Checks both the types and values of the <code>kwargs</code> and raises an exception is something is off.</p>"},{"location":"reference/pipeline/__init__/","title":"init","text":"<p>Contains handlers for all steps involved in a data science project after data preparation:</p> <ul> <li>Automated feature learning</li> <li>Automated feature selection</li> <li>Training and evaluation of machine learning (ML) algorithms</li> <li>Deployment of the fitted models</li> </ul> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>: Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <p><pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the __repr__ method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns","title":"<code>Columns</code>","text":"<p>Container which holds a pipeline's columns. These include the columns for which importance can be calculated, such as the ones with <code>roles</code> as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Columns can be accessed by name, index or with a NumPy array. The container supports slicing and is sort- and filterable. Further, the container holds global methods to request columns' importances and apply a column selection to data frames provided to the pipeline.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_columns = my_pipeline.columns\n\nfirst_column = my_pipeline.columns[0]\n\nall_but_last_10_columns = my_pipeline.columns[:-10]\n\nimportant_columns = [column for column in my_pipeline.columns if\ncolumn.importance &gt; 0.1]\n\nnames, importances = my_pipeline.columns.importances()\n\n# Drops all categorical and numerical columns that are not # in the\ntop 20%. new_container = my_pipeline.columns.select(\n    container, share_selected_columns=0.2,\n)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>class Columns:\n    \"\"\"\n    Container which holds a pipeline's columns. These include the columns for\n    which importance can be calculated, such as the ones with\n    [`roles`][getml.data.roles] as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Columns can be accessed by name, index or with a NumPy array. The container\n    supports slicing and is sort- and filterable. Further, the container holds\n    global methods to request columns' importances and apply a column selection\n    to data frames provided to the pipeline.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Columns.filter] you can also use python list\n        comprehensions for filtering.\n\n    Example:\n        ```python\n        all_my_columns = my_pipeline.columns\n\n        first_column = my_pipeline.columns[0]\n\n        all_but_last_10_columns = my_pipeline.columns[:-10]\n\n        important_columns = [column for column in my_pipeline.columns if\n        column.importance &gt; 0.1]\n\n        names, importances = my_pipeline.columns.importances()\n\n        # Drops all categorical and numerical columns that are not # in the\n        top 20%. new_container = my_pipeline.columns.select(\n            container, share_selected_columns=0.2,\n        )\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(\n        self,\n        pipeline: str,\n        targets: Sequence[str],\n        peripheral: Sequence[Placeholder],\n        data: Optional[Sequence[Column]] = None,\n    ) -&gt; None:\n        if not isinstance(pipeline, str):\n            raise ValueError(\"'pipeline' must be a str.\")\n\n        if not _is_typed_list(targets, str):\n            raise TypeError(\"'targets' must be a list of str.\")\n\n        self.pipeline = pipeline\n\n        self.targets = targets\n\n        self.peripheral = peripheral\n\n        self.peripheral_names = [p.name for p in self.peripheral]\n\n        if data is not None:\n            self.data = data\n        else:\n            self._load_columns()\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Column]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[str, int, slice, Union[NDArray[np.int_], NDArray[np.bool_]]]\n    ) -&gt; Union[Column, Columns, List[Column]]:\n        if not self.data:\n            raise AttributeError(\"Columns container not fully initialised.\")\n\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            columns_subset = self.data[key]\n            return self._make_columns(columns_subset)\n        if isinstance(key, str):\n            if key in self.names:\n                return [column for column in self.data if column.name == key][0]\n            raise AttributeError(f\"No Column with name: {key}\")\n        if isinstance(key, np.ndarray):\n            columns_subset = np.array(self.data)[key].tolist()\n            return list(columns_subset)\n        raise TypeError(\n            \"Columns can only be indexed by: int, slices, or str,\"\n            f\" not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _get_column_importances(\n        self, target_num: int, sort: bool\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.column_importances\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        descriptions = np.asarray(json_obj[\"column_descriptions_\"])\n        importances = np.asarray(json_obj[\"column_importances_\"])\n\n        if hasattr(self, \"data\"):\n            indices = np.asarray(\n                [\n                    column.index\n                    for column in self.data\n                    if column.target == self.targets[target_num]\n                    and column.index &lt; len(importances)\n                ]\n            )\n\n            descriptions = descriptions[indices]\n            importances = importances[indices]\n\n        if not sort:\n            return descriptions, importances\n\n        indices = np.argsort(importances)[::-1]\n\n        return (descriptions[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        rows = [\n            [\n                column.name,\n                column.marker,\n                column.table,\n                column.importance,\n                column.target,\n            ]\n            for column in self.data\n        ]\n\n        headers = [\n            [\n                \"name\",\n                \"marker\",\n                \"table\",\n                \"importance\",\n                \"target\",\n            ]\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _load_columns(self) -&gt; None:\n        \"\"\"\n        Loads the actual column data from the engine.\n        \"\"\"\n        columns = []\n\n        for target_num, target in enumerate(self.targets):\n            descriptions, importances = self._get_column_importances(\n                target_num=target_num, sort=False\n            )\n\n            columns.extend(\n                [\n                    Column(\n                        index=index,\n                        name=description.get(\"name_\"),\n                        marker=description.get(\"marker_\"),\n                        table=description.get(\"table_\"),\n                        importance=importances[index],\n                        target=target,\n                    )\n                    for index, description in enumerate(descriptions)\n                ]\n            )\n\n        self.data = columns\n\n    # ----------------------------------------------------------------\n\n    def _make_columns(self, data: Sequence[Column]) -&gt; Columns:\n        \"\"\"\n        A factory to construct a :class:`getml.pipeline.Columns` container from a list\n        of :class:`getml.pipeline.Column`s.\n        \"\"\"\n        return Columns(self.pipeline, self.targets, self.peripheral, data)\n\n    # ----------------------------------------------------------------\n\n    def _pivot(self, field: str) -&gt; Any:\n        \"\"\"\n        Pivots the data for a given field. Returns a list of values of the field's type.\n        \"\"\"\n        return [getattr(column, field) for column in self.data]\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n        \"\"\"\n        Filters the columns container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            A container of filtered Columns.\n\n        Example:\n            ```python\n            important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n            peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n            ```\n        \"\"\"\n        columns_filtered = [column for column in self.data if conditional(column)]\n        return self._make_columns(columns_filtered)\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the column importances.\n\n        Column importances extend the idea of column importances\n        to the columns originally inserted into the pipeline.\n        Each column is assigned an importance value that measures\n        its contribution to the predictive performance. All\n        columns importances add up to 1.\n\n        The importances can be calculated for columns with\n        [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n        [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n        The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n        [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n        [`unused_float`][getml.data.roles.unused_float] and\n        [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the columns.\n            The second array contains their importances. By definition, all importances add up to 1.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        descriptions, importances = self._get_column_importances(\n            target_num=target_num, sort=sort\n        )\n\n        # ------------------------------------------------------------\n\n        names = np.asarray(\n            [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n        )\n\n        # ------------------------------------------------------------\n\n        return names, importances\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s columns.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [column.name for column in self.data]\n\n    # ----------------------------------------------------------------\n\n    def select(\n        self, container: Container, share_selected_columns: float = 0.5\n    ) -&gt; Container:\n        \"\"\"\n        Returns a new data container with all insufficiently important columns dropped.\n\n        Args:\n            container ([`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]):\n                The container containing the data you want to use.\n\n            share_selected_columns: The share of columns\n                to keep. Must be between 0.0 and 1.0.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if isinstance(container, (StarSchema, TimeSeries)):\n            data = self.select(\n                container.container, share_selected_columns=share_selected_columns\n            )\n            new_container = deepcopy(container)\n            new_container._container = data\n            return new_container\n\n        # ------------------------------------------------------------\n\n        if not isinstance(container, Container):\n            raise TypeError(\n                \"'container' must be a getml.data.Container, \"\n                + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n            )\n\n        if not isinstance(share_selected_columns, numbers.Real):\n            raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n        if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n            raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n        # ------------------------------------------------------------\n\n        descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n        # ------------------------------------------------------------\n\n        num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n        keep_columns = descriptions[:num_keep]\n\n        # ------------------------------------------------------------\n\n        subsets = {\n            k: _drop(v, keep_columns, k, POPULATION)\n            for (k, v) in container.subsets.items()\n        }\n\n        peripheral = {\n            k: _drop(v, keep_columns, k, PERIPHERAL)\n            for (k, v) in container.peripheral.items()\n        }\n\n        # ------------------------------------------------------------\n\n        new_container = Container(**subsets)\n        new_container.add(**peripheral)\n        new_container.freeze()\n\n        # ------------------------------------------------------------\n\n        return new_container\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[Callable[[Column], Any]] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Columns:\n        \"\"\"\n        Sorts the Columns container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - table(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n            [`columns`][getml.pipeline.columns]:\n                A container of sorted columns.\n\n        Example:\n            ```python\n            by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            columns_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_columns(columns_sorted)\n\n        if by is None:\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.name, reverse=reverse\n            )\n            columns_sorted.sort(key=lambda column: column.target)\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.name, reverse=reverse\n            )\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"tables?$\", string=by):\n            columns_sorted = sorted(\n                self.data,\n                key=lambda column: column.table,\n            )\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.importance, reverse=reverse\n            )\n            return self._make_columns(columns_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n        names, markers, tables, importances, targets = (\n            self._pivot(field)\n            for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n        )\n\n        data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n        data_frame[\"name\"] = names\n\n        data_frame[\"marker\"] = markers\n\n        data_frame[\"table\"] = tables\n\n        data_frame[\"importance\"] = importances\n\n        data_frame[\"target\"] = targets\n\n        return data_frame\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.names","title":"<code>names: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s columns.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the columns container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Columns</code> <p>A container of filtered Columns.</p> Example <pre><code>important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\nperipheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n    \"\"\"\n    Filters the columns container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        A container of filtered Columns.\n\n    Example:\n        ```python\n        important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n        peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    columns_filtered = [column for column in self.data if conditional(column)]\n    return self._make_columns(columns_filtered)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the data for the column importances.</p> <p>Column importances extend the idea of column importances to the columns originally inserted into the pipeline. Each column is assigned an importance value that measures its contribution to the predictive performance. All columns importances add up to 1.</p> <p>The importances can be calculated for columns with <code>roles</code> such as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the columns.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the column importances.\n\n    Column importances extend the idea of column importances\n    to the columns originally inserted into the pipeline.\n    Each column is assigned an importance value that measures\n    its contribution to the predictive performance. All\n    columns importances add up to 1.\n\n    The importances can be calculated for columns with\n    [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the columns.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    descriptions, importances = self._get_column_importances(\n        target_num=target_num, sort=sort\n    )\n\n    # ------------------------------------------------------------\n\n    names = np.asarray(\n        [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n    )\n\n    # ------------------------------------------------------------\n\n    return names, importances\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.select","title":"<code>select(container, share_selected_columns=0.5)</code>","text":"<p>Returns a new data container with all insufficiently important columns dropped.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>[`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]</code> <p>The container containing the data you want to use.</p> required <code>share_selected_columns</code> <code>float</code> <p>The share of columns to keep. Must be between 0.0 and 1.0.</p> <code>0.5</code> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def select(\n    self, container: Container, share_selected_columns: float = 0.5\n) -&gt; Container:\n    \"\"\"\n    Returns a new data container with all insufficiently important columns dropped.\n\n    Args:\n        container ([`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]):\n            The container containing the data you want to use.\n\n        share_selected_columns: The share of columns\n            to keep. Must be between 0.0 and 1.0.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        data = self.select(\n            container.container, share_selected_columns=share_selected_columns\n        )\n        new_container = deepcopy(container)\n        new_container._container = data\n        return new_container\n\n    # ------------------------------------------------------------\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a getml.data.Container, \"\n            + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n        )\n\n    if not isinstance(share_selected_columns, numbers.Real):\n        raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n    if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n        raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n    # ------------------------------------------------------------\n\n    descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n    # ------------------------------------------------------------\n\n    num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n    keep_columns = descriptions[:num_keep]\n\n    # ------------------------------------------------------------\n\n    subsets = {\n        k: _drop(v, keep_columns, k, POPULATION)\n        for (k, v) in container.subsets.items()\n    }\n\n    peripheral = {\n        k: _drop(v, keep_columns, k, PERIPHERAL)\n        for (k, v) in container.peripheral.items()\n    }\n\n    # ------------------------------------------------------------\n\n    new_container = Container(**subsets)\n    new_container.add(**peripheral)\n    new_container.freeze()\n\n    # ------------------------------------------------------------\n\n    return new_container\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Columns container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - table(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Columns</code> <p><code>columns</code>: A container of sorted columns.</p> Example <pre><code>by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns:\n    \"\"\"\n    Sorts the Columns container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - table(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n        [`columns`][getml.pipeline.columns]:\n            A container of sorted columns.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        columns_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_columns(columns_sorted)\n\n    if by is None:\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        columns_sorted.sort(key=lambda column: column.target)\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"tables?$\", string=by):\n        columns_sorted = sorted(\n            self.data,\n            key=lambda column: column.table,\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.importance, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Columns.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the columns in a pandas data frame.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n    names, markers, tables, importances, targets = (\n        self._pivot(field)\n        for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n    )\n\n    data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n    data_frame[\"name\"] = names\n\n    data_frame[\"marker\"] = markers\n\n    data_frame[\"table\"] = tables\n\n    data_frame[\"importance\"] = importances\n\n    data_frame[\"target\"] = targets\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features","title":"<code>Features</code>","text":"<p>Container which holds a pipeline's features. Features can be accessed by name, index or with a numpy array. The container supports slicing and is sort- and filterable.</p> <p>Further, the container holds global methods to request features' importances, correlations and their respective transpiled sql representation.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_features = my_pipeline.features\n\nfirst_feature = my_pipeline.features[0]\n\nsecond_feature = my_pipeline.features[\"feature_1_2\"]\n\nall_but_last_10_features = my_pipeline.features[:-10]\n\nimportant_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\nnames, importances = my_pipeline.features.importances()\n\nnames, correlations = my_pipeline.features.correlations()\n\nsql_code = my_pipeline.features.to_sql()\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>class Features:\n    \"\"\"\n    Container which holds a pipeline's features. Features can be accessed\n    by name, index or with a numpy array. The container supports slicing and\n    is sort- and filterable.\n\n    Further, the container holds global methods to request features' importances,\n    correlations and their respective transpiled sql representation.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Features.filter] you can also use python list\n        comprehensions for filtering.\n\n    Example:\n        ```python\n        all_my_features = my_pipeline.features\n\n        first_feature = my_pipeline.features[0]\n\n        second_feature = my_pipeline.features[\"feature_1_2\"]\n\n        all_but_last_10_features = my_pipeline.features[:-10]\n\n        important_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\n        names, importances = my_pipeline.features.importances()\n\n        names, correlations = my_pipeline.features.correlations()\n\n        sql_code = my_pipeline.features.to_sql()\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(\n        self,\n        pipeline: str,\n        targets: Sequence[str],\n        data: Optional[Sequence[Feature]] = None,\n    ) -&gt; None:\n        if not isinstance(pipeline, str):\n            raise ValueError(\"'pipeline' must be a str.\")\n\n        if not _is_typed_list(targets, str):\n            raise TypeError(\"'targets' must be a list of str.\")\n\n        self.pipeline = pipeline\n\n        self.targets = targets\n\n        if data is None:\n            self.data = self._load_features()\n\n        else:\n            self.data = list(data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[int, slice, str, NDArray[np.int_]]\n    ) -&gt; Union[Feature, Features, Sequence[Feature]]:\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            return self._make_features(self.data[key])\n        if isinstance(key, str):\n            if key in self.names:\n                return [feature for feature in self.data if feature.name == key][0]\n            raise AttributeError(f\"No Feature with name: {key}\")\n        if isinstance(key, np.ndarray):\n            features_subset = np.array(self.data)[key].tolist()\n            return features_subset\n        raise TypeError(\n            f\"Features can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Feature]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def _pivot(self, field: str) -&gt; List[Any]:\n        \"\"\"\n        Pivots the data for a given field. Returns a list of values of the field's type.\n        \"\"\"\n        return [getattr(feature, field) for feature in self.data]\n\n    # ----------------------------------------------------------------\n\n    def _load_features(self) -&gt; List[Feature]:\n        \"\"\"\n        Loads the actual feature data from the engine.\n        \"\"\"\n        features = []\n\n        for target_num, target in enumerate(self.targets):\n            names = self.correlations(target_num, sort=False)[0].tolist()\n            indices = range(len(names))\n            correlations = _attach_empty(\n                self.correlations(target_num, sort=False)[1].tolist(),\n                len(names),\n                np.NaN,\n            )\n            importances = _attach_empty(\n                self.importances(target_num, sort=False)[1].tolist(), len(names), np.NaN\n            )\n            sql_transpilations = _attach_empty(\n                self.to_sql(subfeatures=False).code[:-1], len(names), \"\"\n            )\n\n            features.extend(\n                [\n                    Feature(\n                        index=index,\n                        name=names[index],\n                        pipeline=self.pipeline,\n                        target=target,\n                        targets=self.targets,\n                        importance=importances[index],\n                        correlation=correlations[index],\n                        sql=SQLString(sql_transpilations[index]),\n                    )\n                    for index in indices\n                ]\n            )\n        return features\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        rows = [\n            [\n                feature.target,\n                feature.name,\n                feature.correlation,\n                feature.importance,\n            ]\n            for feature in self.data\n        ]\n\n        headers = [[\"target\", \"name\", \"correlation\", \"importance\"]]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _make_features(self, data: Sequence[Feature]) -&gt; Features:\n        \"\"\"\n        A factory to construct a `Features` container from a list of\n        sole `Feature`s.\n        \"\"\"\n        return Features(self.pipeline, self.targets, data)\n\n    # ----------------------------------------------------------------\n\n    def _to_pandas(self) -&gt; pd.DataFrame:\n        names, correlations, importances, sql, target = (\n            self._pivot(field)\n            for field in [\"name\", \"correlation\", \"importance\", \"sql\", \"target\"]\n        )\n\n        data_frame = pd.DataFrame(index=range(len(names)))\n\n        data_frame[\"names\"] = names\n\n        data_frame[\"correlations\"] = correlations\n\n        data_frame[\"importances\"] = importances\n\n        data_frame[\"target\"] = target\n\n        data_frame[\"sql\"] = sql\n\n        return data_frame\n\n    # ----------------------------------------------------------------\n\n    @property\n    def correlation(self) -&gt; List[float]:\n        \"\"\"\n        Holds the correlations of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the correlations.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"correlation\")\n\n    # ------------------------------------------------------------\n\n    def correlations(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the feature correlations,\n        as displayed in the getML monitor.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the features.\n            The second array contains the correlations with the target.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        names = np.asarray(json_obj[\"feature_names_\"])\n        correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n        assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n        if hasattr(self, \"data\"):\n            indices = np.asarray(\n                [\n                    feature.index\n                    for feature in self.data\n                    if feature.target == self.targets[target_num]\n                    and feature.index &lt; len(correlations)\n                ]\n            )\n\n            names = names[indices]\n            correlations = correlations[indices]\n\n        if not sort:\n            return names, correlations\n\n        indices = np.argsort(np.abs(correlations))[::-1]\n\n        return (names[indices], correlations[indices])\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n        \"\"\"\n         Filters the Features container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            [`Features`][getml.pipeline.Features]:\n                A container of filtered Features.\n\n        Example:\n            ```python\n            important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n            correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n            ```\n        \"\"\"\n        features_filtered = [feature for feature in self.data if conditional(feature)]\n        return Features(self.pipeline, self.targets, data=features_filtered)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def importance(self) -&gt; List[float]:\n        \"\"\"\n         Holds the correlations of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the correlations.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"importance\")\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the feature importances,\n        as displayed in the getML monitor.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns\n            The first array contains the names of the features.\n            The second array contains their importances. By definition, all importances add up to 1.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.feature_importances\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        names = np.asarray(json_obj[\"feature_names_\"])\n        importances = np.asarray(json_obj[\"feature_importances_\"])\n\n        if hasattr(self, \"data\"):\n            assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n            indices = np.asarray(\n                [\n                    feature.index\n                    for feature in self.data\n                    if feature.target == self.targets[target_num]\n                    and feature.index &lt; len(importances)\n                ]\n            )\n\n            names = names[indices]\n            importances = importances[indices]\n\n        if not sort:\n            return names, importances\n\n        assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n        indices = np.argsort(importances)[::-1]\n\n        return (names[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def name(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"name\")\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"name\")\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[\n            Callable[\n                [Feature],\n                Union[\n                    float,\n                    int,\n                    str,\n                ],\n            ]\n        ] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Features:\n        \"\"\"\n        Sorts the Features container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - correlation(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n            [`Features`][getml.pipeline.Features]:\n                A container of sorted Features.\n\n        Example:\n            ```python\n            by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n            by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            features_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_features(features_sorted)\n\n        else:\n            if by is None:\n                features_sorted = sorted(\n                    self.data, key=lambda feature: feature.index, reverse=reverse\n                )\n                features_sorted.sort(key=lambda feature: feature.target)\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"names?$\", string=by):\n                features_sorted = sorted(\n                    self.data, key=lambda feature: feature.name, reverse=reverse\n                )\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"correlations?$\", string=by):\n                reverse = True if descending is None else descending\n                features_sorted = sorted(\n                    self.data,\n                    key=lambda feature: abs(feature.correlation),\n                    reverse=reverse,\n                )\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"importances?$\", string=by):\n                reverse = True if descending is None else descending\n                features_sorted = sorted(\n                    self.data,\n                    key=lambda feature: feature.importance,\n                    reverse=reverse,\n                )\n                return self._make_features(features_sorted)\n\n            raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns all information related to the features in a pandas data frame.\n        \"\"\"\n\n        return self._to_pandas()\n\n    # ----------------------------------------------------------------\n\n    def to_sql(\n        self,\n        targets: bool = True,\n        subfeatures: bool = True,\n        dialect: str = sqlite3,\n        schema: Optional[str] = None,\n        nchar_categorical: int = 128,\n        nchar_join_key: int = 128,\n        nchar_text: int = 4096,\n        size_threshold: Optional[int] = 50000,\n    ) -&gt; SQLCode:\n        \"\"\"\n        Returns SQL statements visualizing the features.\n\n        Args:\n            targets (boolean):\n                Whether you want to include the target columns\n                in the main table.\n\n            subfeatures (boolean):\n                Whether you want to include the code for the\n                subfeatures of a snowflake schema.\n\n            dialect (string):\n                The SQL dialect to use. Must be from\n                [`dialect`][getml.pipeline.dialect]. Please\n                note that not all dialects are supported\n                in the getML community edition.\n\n            schema (string, optional):\n                The schema in which to wrap all generated tables and\n                indices. None for no schema. Not applicable to all dialects.\n                For the BigQuery and MySQL dialects, the schema is identical\n                to the database ID.\n\n            nchar_categorical (int):\n                The maximum number of characters used in the\n                VARCHAR for categorical columns. Not applicable\n                to all dialects.\n\n            nchar_join_key (int):\n                The maximum number of characters used in the\n                VARCHAR for join keys. Not applicable\n                to all dialects.\n\n            nchar_text (int):\n                The maximum number of characters used in the\n                VARCHAR for text columns. Not applicable\n                to all dialects.\n\n            size_threshold (int, optional):\n                The maximum number of characters to display\n                in a single feature. Displaying extremely\n                complicated features can crash your iPython\n                notebook or lead to unexpectedly high memory\n                consumption, which is why a reasonable\n                upper limit is advantageous. Set to None\n                for no upper limit.\n\n        Examples:\n            ```python\n            my_pipeline.features.to_sql()\n            ```\n        Returns:\n            [`SQLCode`][getml.pipeline.SQLCode]\n                Object representing the features.\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can hold trained\n            features which can be returned as SQL statements.\n\n        Note:\n            The getML community edition only supports\n            transpilation to human-readable SQL. Passing\n            'sqlite3' will also produce human-readable SQL.\n\n        \"\"\"\n\n        if not isinstance(targets, bool):\n            raise TypeError(\"'targets' must be a bool!\")\n\n        if not isinstance(subfeatures, bool):\n            raise TypeError(\"'subfeatures' must be a bool!\")\n\n        if not isinstance(dialect, str):\n            raise TypeError(\"'dialect' must be a string!\")\n\n        if not isinstance(nchar_categorical, int):\n            raise TypeError(\"'nchar_categorical' must be an int!\")\n\n        if not isinstance(nchar_join_key, int):\n            raise TypeError(\"'nchar_join_key' must be an int!\")\n\n        if not isinstance(nchar_text, int):\n            raise TypeError(\"'nchar_text' must be an int!\")\n\n        if dialect not in _all_dialects:\n            raise ValueError(\n                \"'dialect' must from getml.pipeline.dialect, \"\n                + \"meaning that is must be one of the following: \"\n                + str(_all_dialects)\n                + \".\"\n            )\n\n        if size_threshold is not None and not isinstance(size_threshold, int):\n            raise TypeError(\"'size_threshold' must be an int or None!\")\n\n        if size_threshold is not None and size_threshold &lt;= 0:\n            raise ValueError(\"'size_threshold' must be a positive number!\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.to_sql\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"targets_\"] = targets\n        cmd[\"subfeatures_\"] = subfeatures\n        cmd[\"dialect_\"] = dialect\n        cmd[\"schema_\"] = schema or \"\"\n        cmd[\"nchar_categorical_\"] = nchar_categorical\n        cmd[\"nchar_join_key_\"] = nchar_join_key\n        cmd[\"nchar_text_\"] = nchar_text\n\n        if size_threshold is not None:\n            cmd[\"size_threshold_\"] = size_threshold\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            sql = comm.recv_string(sock)\n\n        return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.correlation","title":"<code>correlation: List[float]</code>  <code>property</code>","text":"<p>Holds the correlations of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[float]</code> <p><code>list</code> containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.importance","title":"<code>importance: List[float]</code>  <code>property</code>","text":"<p>Holds the correlations of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[float]</code> <p><code>list</code> containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.name","title":"<code>name: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.names","title":"<code>names: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.correlations","title":"<code>correlations(target_num=0, sort=True)</code>","text":"<p>Returns the data for the feature correlations, as displayed in the getML monitor.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains the correlations with the target.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def correlations(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature correlations,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains the correlations with the target.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n    assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n    if hasattr(self, \"data\"):\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(correlations)\n            ]\n        )\n\n        names = names[indices]\n        correlations = correlations[indices]\n\n    if not sort:\n        return names, correlations\n\n    indices = np.argsort(np.abs(correlations))[::-1]\n\n    return (names[indices], correlations[indices])\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the Features container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Features</code> <p><code>Features</code>: A container of filtered Features.</p> Example <pre><code>important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\ncorrelated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n    \"\"\"\n     Filters the Features container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        [`Features`][getml.pipeline.Features]:\n            A container of filtered Features.\n\n    Example:\n        ```python\n        important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n        correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n        ```\n    \"\"\"\n    features_filtered = [feature for feature in self.data if conditional(feature)]\n    return Features(self.pipeline, self.targets, data=features_filtered)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the data for the feature importances, as displayed in the getML monitor.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns     The first array contains the names of the features.     The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature importances,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns\n        The first array contains the names of the features.\n        The second array contains their importances. By definition, all importances add up to 1.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_importances\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    importances = np.asarray(json_obj[\"feature_importances_\"])\n\n    if hasattr(self, \"data\"):\n        assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(importances)\n            ]\n        )\n\n        names = names[indices]\n        importances = importances[indices]\n\n    if not sort:\n        return names, importances\n\n    assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Features container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - correlation(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> Return <p><code>Features</code>:     A container of sorted Features.</p> Example <pre><code>by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\nby_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[\n            [Feature],\n            Union[\n                float,\n                int,\n                str,\n            ],\n        ]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features:\n    \"\"\"\n    Sorts the Features container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - correlation(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n        [`Features`][getml.pipeline.Features]:\n            A container of sorted Features.\n\n    Example:\n        ```python\n        by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n        by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        features_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_features(features_sorted)\n\n    else:\n        if by is None:\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.index, reverse=reverse\n            )\n            features_sorted.sort(key=lambda feature: feature.target)\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.name, reverse=reverse\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"correlations?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: abs(feature.correlation),\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: feature.importance,\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the features in a pandas data frame.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the features in a pandas data frame.\n    \"\"\"\n\n    return self._to_pandas()\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Features.to_sql","title":"<code>to_sql(targets=True, subfeatures=True, dialect=sqlite3, schema=None, nchar_categorical=128, nchar_join_key=128, nchar_text=4096, size_threshold=50000)</code>","text":"<p>Returns SQL statements visualizing the features.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>boolean</code> <p>Whether you want to include the target columns in the main table.</p> <code>True</code> <code>subfeatures</code> <code>boolean</code> <p>Whether you want to include the code for the subfeatures of a snowflake schema.</p> <code>True</code> <code>dialect</code> <code>string</code> <p>The SQL dialect to use. Must be from <code>dialect</code>. Please note that not all dialects are supported in the getML community edition.</p> <code>sqlite3</code> <code>schema</code> <code>string</code> <p>The schema in which to wrap all generated tables and indices. None for no schema. Not applicable to all dialects. For the BigQuery and MySQL dialects, the schema is identical to the database ID.</p> <code>None</code> <code>nchar_categorical</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for categorical columns. Not applicable to all dialects.</p> <code>128</code> <code>nchar_join_key</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for join keys. Not applicable to all dialects.</p> <code>128</code> <code>nchar_text</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for text columns. Not applicable to all dialects.</p> <code>4096</code> <code>size_threshold</code> <code>int</code> <p>The maximum number of characters to display in a single feature. Displaying extremely complicated features can crash your iPython notebook or lead to unexpectedly high memory consumption, which is why a reasonable upper limit is advantageous. Set to None for no upper limit.</p> <code>50000</code> <p>Examples:</p> <pre><code>my_pipeline.features.to_sql()\n</code></pre> <p>Returns:     <code>SQLCode</code>         Object representing the features.</p> Note <p>Only fitted pipelines (<code>fit</code>) can hold trained features which can be returned as SQL statements.</p> Note <p>The getML community edition only supports transpilation to human-readable SQL. Passing 'sqlite3' will also produce human-readable SQL.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_sql(\n    self,\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode:\n    \"\"\"\n    Returns SQL statements visualizing the features.\n\n    Args:\n        targets (boolean):\n            Whether you want to include the target columns\n            in the main table.\n\n        subfeatures (boolean):\n            Whether you want to include the code for the\n            subfeatures of a snowflake schema.\n\n        dialect (string):\n            The SQL dialect to use. Must be from\n            [`dialect`][getml.pipeline.dialect]. Please\n            note that not all dialects are supported\n            in the getML community edition.\n\n        schema (string, optional):\n            The schema in which to wrap all generated tables and\n            indices. None for no schema. Not applicable to all dialects.\n            For the BigQuery and MySQL dialects, the schema is identical\n            to the database ID.\n\n        nchar_categorical (int):\n            The maximum number of characters used in the\n            VARCHAR for categorical columns. Not applicable\n            to all dialects.\n\n        nchar_join_key (int):\n            The maximum number of characters used in the\n            VARCHAR for join keys. Not applicable\n            to all dialects.\n\n        nchar_text (int):\n            The maximum number of characters used in the\n            VARCHAR for text columns. Not applicable\n            to all dialects.\n\n        size_threshold (int, optional):\n            The maximum number of characters to display\n            in a single feature. Displaying extremely\n            complicated features can crash your iPython\n            notebook or lead to unexpectedly high memory\n            consumption, which is why a reasonable\n            upper limit is advantageous. Set to None\n            for no upper limit.\n\n    Examples:\n        ```python\n        my_pipeline.features.to_sql()\n        ```\n    Returns:\n        [`SQLCode`][getml.pipeline.SQLCode]\n            Object representing the features.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can hold trained\n        features which can be returned as SQL statements.\n\n    Note:\n        The getML community edition only supports\n        transpilation to human-readable SQL. Passing\n        'sqlite3' will also produce human-readable SQL.\n\n    \"\"\"\n\n    if not isinstance(targets, bool):\n        raise TypeError(\"'targets' must be a bool!\")\n\n    if not isinstance(subfeatures, bool):\n        raise TypeError(\"'subfeatures' must be a bool!\")\n\n    if not isinstance(dialect, str):\n        raise TypeError(\"'dialect' must be a string!\")\n\n    if not isinstance(nchar_categorical, int):\n        raise TypeError(\"'nchar_categorical' must be an int!\")\n\n    if not isinstance(nchar_join_key, int):\n        raise TypeError(\"'nchar_join_key' must be an int!\")\n\n    if not isinstance(nchar_text, int):\n        raise TypeError(\"'nchar_text' must be an int!\")\n\n    if dialect not in _all_dialects:\n        raise ValueError(\n            \"'dialect' must from getml.pipeline.dialect, \"\n            + \"meaning that is must be one of the following: \"\n            + str(_all_dialects)\n            + \".\"\n        )\n\n    if size_threshold is not None and not isinstance(size_threshold, int):\n        raise TypeError(\"'size_threshold' must be an int or None!\")\n\n    if size_threshold is not None and size_threshold &lt;= 0:\n        raise ValueError(\"'size_threshold' must be a positive number!\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.to_sql\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"targets_\"] = targets\n    cmd[\"subfeatures_\"] = subfeatures\n    cmd[\"dialect_\"] = dialect\n    cmd[\"schema_\"] = schema or \"\"\n    cmd[\"nchar_categorical_\"] = nchar_categorical\n    cmd[\"nchar_join_key_\"] = nchar_join_key\n    cmd[\"nchar_text_\"] = nchar_text\n\n    if size_threshold is not None:\n        cmd[\"size_threshold_\"] = size_threshold\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        sql = comm.recv_string(sock)\n\n    return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>A Pipeline is the main class for feature learning and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>Optional[DataModel]</code> <p>Abstract representation of the data_model, which defines the abstract relationships between the tables. Required for the feature learners.</p> <code>None</code> <code>peripheral</code> <code>Optional[List[Placeholder]]</code> <p>Abstract representations of the additional tables used to augment the information provided in <code>population</code>. These have to be the same objects that were <code>join</code> ed onto the <code>population</code> <code>Placeholder</code>. Their order determines the order of the peripheral <code>DataFrame</code> passed to the 'peripheral_tables' argument in <code>check</code>, <code>fit</code>, <code>predict</code>, <code>score</code>, and <code>transform</code>, if you pass the data frames as a list. If you omit the peripheral placeholders, they will be inferred from the data model and ordered alphabetically.</p> <code>None</code> <code>preprocessors</code> <code>Optional[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter, List[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter]]]]</code> <p>The preprocessor(s) to be used. Must be from <code>preprocessors</code>. A single preprocessor does not have to be wrapped in a list.</p> <code>None</code> <code>feature_learners</code> <code>Optional[Union[Union[Fastboost, FastProp, Multirel, Relboost, RelMT], List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]]]]</code> <p>The feature learner(s) to be used. Must be from <code>feature_learning</code>. A single feature learner does not have to be wrapped in a list.</p> <code>None</code> <code>feature_selectors</code> <code>Optional[Union[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor], List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> <p>Predictor(s) used to select the best features. Must be from <code>predictors</code>. A single feature selector does not have to be wrapped in a list. Make sure to also set share_selected_features.</p> <code>None</code> <code>predictors</code> <code>Optional[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor, List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> <p>Predictor(s) used to generate the predictions. If more than one predictor is passed, the predictions generated will be averaged. Must be from <code>predictors</code>. A single predictor does not have to be wrapped in a list.</p> <code>None</code> <code>loss_function</code> <code>Optional[str]</code> <p>The loss function to use for the feature learners.</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>Tags exist to help you organize your pipelines. You can add any tags that help you remember what you were trying to do.</p> <code>None</code> <code>include_categorical</code> <code>bool</code> <p>Whether you want to pass categorical columns in the population table to the predictor.</p> <code>False</code> <code>share_selected_features</code> <code>float</code> <p>The share of features you want the feature selection to keep. When set to 0.0, then all features will be kept.</p> <code>0.5</code> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>:</p> <p>Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> <p>Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the __repr__ method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    A Pipeline is the main class for feature learning and prediction.\n\n    Args:\n        data_model:\n            Abstract representation of the data_model,\n            which defines the abstract relationships between the tables.\n            Required for the feature learners.\n\n        peripheral:\n            Abstract representations of the additional tables used to\n            augment the information provided in `population`. These\n            have to be the same objects that were\n            [`join`][getml.data.Placeholder.join] ed onto the\n            `population` [`Placeholder`][getml.data.Placeholder].\n            Their order determines the order of the\n            peripheral [`DataFrame`][getml.DataFrame] passed to\n            the 'peripheral_tables' argument in\n            [`check`][getml.Pipeline.check],\n            [`fit`][getml.Pipeline.fit],\n            [`predict`][getml.Pipeline.predict],\n            [`score`][getml.Pipeline.score], and\n            [`transform`][getml.Pipeline.transform], if you\n            pass the data frames as a list.\n            If you omit the peripheral placeholders, they will\n            be inferred from the data model and ordered\n            alphabetically.\n\n        preprocessors:\n            The preprocessor(s) to be used.\n            Must be from [`preprocessors`][getml.preprocessors].\n            A single preprocessor does not have to be wrapped in a list.\n\n        feature_learners:\n            The feature learner(s) to be used.\n            Must be from [`feature_learning`][getml.feature_learning].\n            A single feature learner does not have to be wrapped\n            in a list.\n\n        feature_selectors:\n            Predictor(s) used to select the best features.\n            Must be from [`predictors`][getml.predictors].\n            A single feature selector does not have to be wrapped\n            in a list.\n            Make sure to also set *share_selected_features*.\n\n        predictors:\n            Predictor(s) used to generate the predictions.\n            If more than one predictor is passed, the predictions\n            generated will be averaged.\n            Must be from [`predictors`][getml.predictors].\n            A single predictor does not have to be wrapped\n            in a list.\n\n        loss_function:\n            The loss function to use for the feature learners.\n\n        tags: Tags exist to help you organize your pipelines.\n            You can add any tags that help you remember what you were\n            trying to do.\n\n        include_categorical:\n            Whether you want to pass categorical columns\n            in the population table to the predictor.\n\n        share_selected_features:\n            The share of features you want the feature\n            selection to keep. When set to 0.0, then all features will be kept.\n\n    Example:\n        We assume that you have already set up your\n        preprocessors (refer to [`preprocessors`][getml.preprocessors]),\n        your feature learners (refer to [`feature_learning`][getml.feature_learning])\n        as well as your feature selectors and predictors\n        (refer to [`predictors`][getml.predictors], which can be used\n        for prediction and feature selection).\n\n        You might also want to refer to\n        [`DataFrame`][getml.DataFrame], [`View`][getml.data.View],\n        [`DataModel`][getml.data.DataModel], [`Container`][getml.data.Container],\n        [`Placeholder`][getml.data.Placeholder] and\n        [`StarSchema`][getml.data.StarSchema].\n\n        If you want to create features for a time series problem,\n        the easiest way to do so is to use the [`TimeSeries`][getml.data.TimeSeries]\n        abstraction.\n\n        Note that this example is taken from the\n        [robot notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/robot.ipynb).\n\n        ```python\n        # All rows before row 10500 will be used for training.\n        split = getml.data.split.time(data_all, \"rowid\", test=10500)\n\n        time_series = getml.data.TimeSeries(\n            population=data_all,\n            time_stamps=\"rowid\",\n            split=split,\n            lagged_targets=False,\n            memory=30,\n        )\n\n        pipe = getml.Pipeline(\n            data_model=time_series.data_model,\n            feature_learners=[...],\n            predictors=...\n        )\n\n        pipe.check(time_series.train)\n\n        pipe.fit(time_series.train)\n\n        pipe.score(time_series.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # TimeSeries, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new,\n        )\n\n        # Add the data as a peripheral table, for the\n        # self-join.\n        container.add(population=population_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n\n    Example:\n        If your data can be organized in a simple star schema,\n        you can use [`StarSchema`][getml.data.StarSchema].\n        [`StarSchema`][getml.data.StarSchema] unifies\n        [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel]:\n\n        Note that this example is taken from the\n        [loans notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n\n        ```python\n\n        # First, we insert our data into a StarSchema.\n        # population_train and population_test are either\n        # DataFrames or Views. The population table\n        # defines the statistical population of your\n        # machine learning problem and contains the\n        # target variables.\n        star_schema = getml.data.StarSchema(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views.\n        # Because this is a star schema,\n        # all joins take place on the population\n        # table.\n        star_schema.join(\n            trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        star_schema.join(\n            order,\n            on=\"account_id\",\n        )\n\n        star_schema.join(\n            meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=star_schema.data_model,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # Now, we pass the actual data.\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(star_schema.train)\n\n        pipe.fit(star_schema.train)\n\n        pipe.score(star_schema.test)\n        ```\n\n    Example:\n        [`StarSchema`][getml.data.StarSchema] is simpler,\n        but cannot be used for more complex data models.\n        The general approach is to use\n        [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel]:\n\n        ```python\n\n        # First, we insert our data into a Container.\n        # population_train and population_test are either\n        # DataFrames or Views.\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views. They are given\n        # aliases, so we can refer to them in the\n        # DataModel.\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # Freezing makes the container immutable.\n        # This is not required, but often a good idea.\n        container.freeze()\n\n        # The abstract data model is constructed\n        # using the DataModel class. A data model\n        # does not contain any actual data. It just\n        # defines the abstract relational structure.\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        dm.add(getml.data.to_placeholder(\n            meta=meta,\n            order=order,\n            trans=trans)\n        )\n\n        dm.population.join(\n            dm.trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        dm.population.join(\n            dm.order,\n            on=\"account_id\",\n        )\n\n        dm.population.join(\n            dm.meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=dm,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(container.train)\n\n        pipe.fit(container.train)\n\n        pipe.score(container.test)\n        ```\n\n        Technically, you don't actually have to use a\n        [`Container`][getml.data.Container]. You might as well do this\n        (in fact, a [`Container`][getml.data.Container] is just\n        syntactic sugar for this approach):\n\n        ```python\n\n        pipe.check(\n            population_train,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n\n        pipe.fit(\n            population_train,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n\n        pipe.score(\n            population_test,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n        ```\n        Or you could even do this. The order of the peripheral tables\n        can be inferred from the \\_\\_repr\\_\\_ method of the pipeline,\n        and it is usually in alphabetical order.\n\n        ```python\n        pipe.check(\n            population_train,\n            [meta, order, trans],\n        )\n\n        pipe.fit(\n            population_train,\n            [meta, order, trans],\n        )\n\n        pipe.score(\n            population_test,\n            [meta, order, trans],\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        data_model: Optional[DataModel] = None,\n        peripheral: Optional[List[Placeholder]] = None,\n        preprocessors: Optional[\n            Union[\n                CategoryTrimmer,\n                EmailDomain,\n                Imputation,\n                Mapping,\n                Seasonal,\n                Substring,\n                TextFieldSplitter,\n                List[\n                    Union[\n                        CategoryTrimmer,\n                        EmailDomain,\n                        Imputation,\n                        Mapping,\n                        Seasonal,\n                        Substring,\n                        TextFieldSplitter,\n                    ]\n                ],\n            ],\n        ] = None,\n        feature_learners: Optional[\n            Union[\n                Union[Fastboost, FastProp, Multirel, Relboost, RelMT],\n                List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]],\n            ]\n        ] = None,\n        feature_selectors: Optional[\n            Union[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ],\n                List[\n                    Union[\n                        LinearRegression,\n                        LogisticRegression,\n                        XGBoostClassifier,\n                        XGBoostRegressor,\n                        ScaleGBMClassifier,\n                        ScaleGBMRegressor,\n                    ]\n                ],\n            ],\n        ] = None,\n        predictors: Optional[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n                List[\n                    Union[\n                        LinearRegression,\n                        LogisticRegression,\n                        XGBoostClassifier,\n                        XGBoostRegressor,\n                        ScaleGBMClassifier,\n                        ScaleGBMRegressor,\n                    ]\n                ],\n            ]\n        ] = None,\n        loss_function: Optional[str] = None,\n        tags: Optional[list[str]] = None,\n        include_categorical: bool = False,\n        share_selected_features: float = 0.5,\n    ) -&gt; None:\n        data_model = data_model or DataModel(\"population\")\n\n        if not isinstance(data_model, DataModel):\n            raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n        peripheral = peripheral or _infer_peripheral(data_model.population)\n\n        preprocessors = preprocessors or []\n\n        feature_learners = feature_learners or []\n\n        feature_selectors = feature_selectors or []\n\n        predictors = predictors or []\n\n        tags = tags or []\n\n        if not isinstance(preprocessors, list):\n            preprocessors = [preprocessors]\n\n        if not isinstance(feature_learners, list):\n            feature_learners = [feature_learners]\n\n        if not isinstance(feature_selectors, list):\n            feature_selectors = [feature_selectors]\n\n        if not isinstance(predictors, list):\n            predictors = [predictors]\n\n        if not isinstance(peripheral, list):\n            peripheral = [peripheral]\n\n        if not isinstance(tags, list):\n            tags = [tags]\n\n        self._id: str = NOT_FITTED\n\n        self.type = \"Pipeline\"\n\n        loss_function = (\n            loss_function\n            or (\n                [fl.loss_function for fl in feature_learners if fl.loss_function]\n                or [\"SquareLoss\"]\n            )[0]\n        )\n\n        feature_learners = [\n            _handle_loss_function(fl, loss_function) for fl in feature_learners\n        ]\n\n        self.data_model = data_model\n        self.feature_learners = feature_learners\n        self.feature_selectors = feature_selectors\n        self.include_categorical = include_categorical\n        self.loss_function = loss_function\n        self.peripheral = peripheral\n        self.predictors = predictors\n        self.preprocessors = preprocessors\n        self.share_selected_features = share_selected_features\n        self.tags = Tags(tags)\n\n        self._metadata: Optional[AllMetadata] = None\n\n        self._scores: Dict[str, Any] = {}\n\n        self._targets: List[str] = []\n\n        setattr(type(self), \"_supported_params\", list(self.__dict__.keys()))\n\n        self._validate()\n\n    # ----------------------------------------------------------------\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Pipeline):\n            raise TypeError(\"A Pipeline can only be compared to another Pipeline\")\n\n        if len(set(self.__dict__.keys())) != len(set(other.__dict__.keys())):\n            return False\n\n        for kkey in self.__dict__:\n            if kkey not in other.__dict__:\n                return False\n\n            # Take special care when comparing numbers.\n            if isinstance(self.__dict__[kkey], numbers.Real):\n                if not np.isclose(self.__dict__[kkey], other.__dict__[kkey]):\n                    return False\n\n            elif self.__dict__[kkey] != other.__dict__[kkey]:\n                return False\n\n        return True\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        obj_dict = self._make_object_dict()\n\n        sig = _SignatureFormatter(data=obj_dict)\n        repr_str = sig._format()\n\n        if self.fitted:\n            url = self._make_url()\n            repr_str += \"\\n\\nurl: \" + url if url else \"\"\n\n        return repr_str\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        obj_dict = self._make_object_dict()\n\n        sig = _SignatureFormatter(data=obj_dict)\n        repr_str = sig._format()\n        html = f\"&lt;pre&gt;{repr_str}&lt;/pre&gt;\"\n\n        if self.fitted:\n            url = self._make_url()\n            html += (\n                (\n                    \"&lt;br&gt;&lt;pre&gt;\"\n                    + \"url: &lt;a href='\"\n                    + url\n                    + '\\' target=\"_blank\"&gt;'\n                    + url\n                    + \"&lt;/a&gt;\"\n                    + \"&lt;/pre&gt;\"\n                )\n                if url\n                else \"\"\n            )\n\n        return html\n\n    # ------------------------------------------------------------\n\n    def _check_classification_or_regression(self) -&gt; bool:\n        \"\"\"\n        Checks whether there are inconsistencies in the algorithms used\n        (mixing classification and regression algorithms).\n        \"\"\"\n\n        all_classifiers = all(\n            [\n                elem.loss_function in _classification_loss\n                for elem in self.feature_learners\n            ]\n        )\n\n        all_classifiers = all_classifiers and all(\n            [elem.type in _classification_types for elem in self.feature_selectors]\n        )\n\n        all_classifiers = all_classifiers and all(\n            [elem.type in _classification_types for elem in self.predictors]\n        )\n\n        all_regressors = all(\n            [\n                elem.loss_function not in _classification_loss\n                for elem in self.feature_learners\n            ]\n        )\n\n        all_regressors = all_regressors and all(\n            [elem.type not in _classification_types for elem in self.feature_selectors]\n        )\n\n        all_regressors = all_regressors and all(\n            [elem.type not in _classification_types for elem in self.predictors]\n        )\n\n        if not all_classifiers and not all_regressors:\n            raise ValueError(\n                \"\"\"You are mixing classification and regression\n                                algorithms. Please make sure that your feature learning\n                                algorithms consistently have classification loss functions\n                                (like CrossEntropyLoss) or consistently have regression\n                                loss functions (like SquareLoss). Also make sure that your\n                                feature selectors and predictors are consistently classifiers\n                                (like XGBoostClassifier or LogisticRegression) or consistently\n                                regressors (like XGBoostRegressor or LinearRegression).\n                             \"\"\"\n            )\n\n        return all_classifiers\n\n    # ------------------------------------------------------------\n\n    def _check_whether_fitted(self) -&gt; None:\n        if not self.fitted:\n            raise ValueError(\"Pipeline has not been fitted!\")\n\n    # ------------------------------------------------------------\n\n    def _close(self, sock: socket.socket) -&gt; None:\n        if not isinstance(sock, socket.socket):\n            raise TypeError(\"'sock' must be a socket.\")\n\n        cmd = dict()\n        cmd[\"type_\"] = self.type + \".close\"\n        cmd[\"name_\"] = self.id\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    # ------------------------------------------------------------\n\n    def _get_latest_score(self, score: str) -&gt; List[float]:\n        nan_ = [np.nan] * len(self.targets)\n\n        if score not in _all_metrics:\n            raise AttributeError(f\"Not a valid score name: {score}\")\n\n        if not self.scored:\n            return nan_\n\n        if self.is_classification and score not in _classification_metrics:\n            return nan_\n\n        if self.is_regression and score in _classification_metrics:\n            return nan_\n\n        return self._scores[score]\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Expresses the pipeline in a form the engine can understand.\n        \"\"\"\n\n        cmd = dict()\n\n        self_dict = self.__dict__\n\n        cmd[\"name_\"] = self.id\n\n        for key, value in self_dict.items():\n            cmd[key + \"_\"] = value\n\n        del cmd[\"_id_\"]\n        del cmd[\"_metadata_\"]\n        del cmd[\"_scores_\"]\n        del cmd[\"_targets_\"]\n\n        return cmd\n\n    # ----------------------------------------------------------------\n\n    def _make_object_dict(self) -&gt; Dict[str, Any]:\n        obj_dict = copy.deepcopy(self.__dict__)\n\n        obj_dict[\"data_model\"] = self.data_model.population.name\n\n        obj_dict[\"peripheral\"] = [elem.name for elem in self.peripheral]\n\n        obj_dict[\"preprocessors\"] = [elem.type for elem in self.preprocessors]\n\n        obj_dict[\"feature_learners\"] = [elem.type for elem in self.feature_learners]\n\n        obj_dict[\"feature_selectors\"] = [elem.type for elem in self.feature_selectors]\n\n        obj_dict[\"predictors\"] = [elem.type for elem in self.predictors]\n\n        return obj_dict\n\n    # ----------------------------------------------------------------\n\n    def _make_score_history(self) -&gt; List[Union[ClassificationScore, RegressionScore]]:\n        scores: List[Dict[str, Any]] = self._scores[\"history\"]\n        scores = [_replace_with_nan_maybe(score) for score in scores]\n\n        if self.is_classification:\n            return [\n                ClassificationScore(\n                    date_time=datetime.strptime(\n                        score.get(\"date_time\", \"\"), \"%Y-%m-%d %H:%M:%S\"\n                    ),\n                    set_used=score.get(\"set_used\", \"\"),\n                    target=target,\n                    accuracy=score.get(accuracy, [np.nan])[target_num],\n                    auc=score.get(auc, [np.nan])[target_num],\n                    cross_entropy=score.get(cross_entropy, [np.nan])[target_num],\n                )\n                for score in scores\n                for target_num, target in enumerate(self.targets)\n            ]\n\n        return [\n            RegressionScore(\n                date_time=datetime.strptime(\n                    score.get(\"date_time\", \"\"), \"%Y-%m-%d %H:%M:%S\"\n                ),\n                set_used=score.get(\"set_used\", \"\"),\n                target=target,\n                mae=score.get(mae, [np.nan])[target_num],\n                rmse=score.get(rmse, [np.nan])[target_num],\n                rsquared=score.get(rsquared, [np.nan])[target_num],\n            )\n            for score in scores\n            for target_num, target in enumerate(self.targets)\n        ]\n\n    # ----------------------------------------------------------------\n\n    def _make_url(self) -&gt; Optional[str]:\n        url = comm._monitor_url()\n        if not url:\n            return None\n        url += \"getpipeline/\" + comm._get_project_name() + \"/\" + self.id + \"/0/\"\n        return url\n\n    # ----------------------------------------------------------------\n\n    def _parse_cmd(self, json_obj: Dict[str, Any]) -&gt; \"Pipeline\":\n        ptype = json_obj[\"type_\"]\n\n        del json_obj[\"type_\"]\n\n        if ptype != \"Pipeline\":\n            raise ValueError(\"Expected type 'Pipeline', got '\" + ptype + \"'.\")\n\n        preprocessors = [\n            _parse_preprocessor(elem) for elem in json_obj[\"preprocessors_\"]\n        ]\n\n        del json_obj[\"preprocessors_\"]\n\n        feature_learners = [_parse_fe(elem) for elem in json_obj[\"feature_learners_\"]]\n\n        del json_obj[\"feature_learners_\"]\n\n        feature_selectors = [\n            _parse_pred(elem) for elem in json_obj[\"feature_selectors_\"]\n        ]\n\n        del json_obj[\"feature_selectors_\"]\n\n        predictors = [_parse_pred(elem) for elem in json_obj[\"predictors_\"]]\n\n        del json_obj[\"predictors_\"]\n\n        data_model = _decode_data_model(json_obj[\"data_model_\"])\n\n        del json_obj[\"data_model_\"]\n\n        peripheral = [_decode_placeholder(elem) for elem in json_obj[\"peripheral_\"]]\n\n        del json_obj[\"peripheral_\"]\n\n        id_ = json_obj[\"name_\"]\n\n        del json_obj[\"name_\"]\n\n        kwargs = _remove_trailing_underscores(json_obj)\n\n        self.__init__(  # type: ignore\n            data_model=data_model,\n            peripheral=peripheral,\n            preprocessors=preprocessors,\n            feature_learners=feature_learners,\n            feature_selectors=feature_selectors,\n            predictors=predictors,\n            **kwargs,\n        )\n\n        self._id = id_\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    def _parse_json_obj(self, all_json_objs: Dict[str, Any]) -&gt; \"Pipeline\":\n        obj = all_json_objs[\"obj\"]\n\n        scores = all_json_objs[\"scores\"]\n\n        targets = all_json_objs[\"targets\"]\n\n        self._parse_cmd(obj)\n\n        scores = _remove_trailing_underscores(scores)\n        scores = _replace_with_nan_maybe(scores)\n\n        self._scores = scores\n\n        self._targets = targets\n\n        peripheral_metadata = [\n            _parse_metadata(m) for m in all_json_objs[\"peripheral_metadata\"]\n        ]\n        population_metadata = _parse_metadata(all_json_objs[\"population_metadata\"])\n\n        self._metadata = AllMetadata(\n            peripheral=peripheral_metadata,\n            population=population_metadata,\n        )\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    def _save(self) -&gt; None:\n        \"\"\"\n        Saves the pipeline as a JSON file.\n        \"\"\"\n\n        cmd = dict()\n        cmd[\"type_\"] = self.type + \".save\"\n        cmd[\"name_\"] = self.id\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def _send(self, additional_tags: Optional[List[str]] = None) -&gt; \"Pipeline\":\n        self._validate()\n\n        self._id = _make_id()\n\n        cmd = self._getml_deserialize()\n\n        if additional_tags is not None:\n            cmd[\"tags_\"] += additional_tags\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def _transform(\n        self,\n        peripheral_data_frames: Sequence[Union[DataFrame, View]],\n        population_data_frame: Union[DataFrame, View],\n        sock: socket.socket,\n        score: bool = False,\n        predict: bool = False,\n        df_name: str = \"\",\n        table_name: str = \"\",\n    ) -&gt; Union[NDArray[np.float_], None]:\n        _check_df_types(population_data_frame, peripheral_data_frames)\n\n        if not isinstance(sock, socket.socket):\n            raise TypeError(\"'sock' must be a socket.\")\n\n        if not isinstance(score, bool):\n            raise TypeError(\"'score' must be of type bool\")\n\n        if not isinstance(predict, bool):\n            raise TypeError(\"'predict' must be of type bool\")\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        if not isinstance(df_name, str):\n            raise TypeError(\"'df_name' must be of type str\")\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n\n        cmd[\"score_\"] = score\n        cmd[\"predict_\"] = predict\n\n        cmd[\"peripheral_dfs_\"] = [\n            df._getml_deserialize() for df in peripheral_data_frames\n        ]\n        cmd[\"population_df_\"] = population_data_frame._getml_deserialize()\n\n        cmd[\"df_name_\"] = df_name\n        cmd[\"table_name_\"] = table_name\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.log(sock)\n\n        if msg == \"Success!\":\n            if table_name == \"\" and df_name == \"\" and not score:\n                yhat = comm.recv_float_matrix(sock)\n            else:\n                yhat = None\n        else:\n            comm.engine_exception_handler(msg)\n\n        print()\n\n        return yhat\n\n    # ----------------------------------------------------------------\n\n    @property\n    def accuracy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the accuracy of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.accuracy\n\n    # ----------------------------------------------------------------\n\n    @property\n    def auc(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the auc of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.auc\n\n    # ----------------------------------------------------------------\n\n    def check(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Dict[str, Union[DataFrame, View]],\n                Sequence[Union[DataFrame, View]],\n            ]\n        ] = None,\n    ) -&gt; Optional[Issues]:\n        \"\"\"\n        Checks the validity of the data model.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n        \"\"\"\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        temp = copy.deepcopy(self)\n\n        temp._send()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = temp.type + \".check\"\n        cmd[\"name_\"] = temp.id\n\n        cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n        cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            print(\"Checking data model...\")\n            msg = comm.log(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            print()\n            issues = Issues(comm.recv_issues(sock))\n            if len(issues) == 0:\n                print(\"OK.\")\n            else:\n                print(\n                    f\"The pipeline check generated {len(issues.info)} \"\n                    + f\"issues labeled INFO and {len(issues.warnings)} \"\n                    + \"issues labeled WARNING.\"\n                )\n\n        temp.delete()\n\n        return None if len(issues) == 0 else issues\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self) -&gt; Columns:\n        \"\"\"\n        [`Columns`][getml.pipeline.Columns] object that\n        can be used to handle information about the original\n        columns utilized by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Columns(self.id, self.targets, self.peripheral)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def cross_entropy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the cross entropy of the latest scoring\n        run (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.cross_entropy\n\n    # ----------------------------------------------------------------\n\n    def delete(self) -&gt; None:\n        \"\"\"\n        Deletes the pipeline from the engine.\n\n        Note:\n            Caution: You can not undo this action!\n        \"\"\"\n        self._check_whether_fitted()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".delete\"\n        cmd[\"name_\"] = self.id\n        cmd[\"mem_only_\"] = False\n\n        comm.send(cmd)\n\n        self._id = NOT_FITTED\n\n    # ------------------------------------------------------------\n\n    def deploy(self, deploy: bool) -&gt; None:\n        \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n        See [deployment][deployment] for details.\n\n        Args:\n            deploy (bool): If `True`, the deployment of the pipeline\n                will be triggered.\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if not isinstance(deploy, bool):\n            raise TypeError(\"'deploy' must be of type bool\")\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".deploy\"\n        cmd[\"name_\"] = self.id\n        cmd[\"deploy_\"] = deploy\n\n        comm.send(cmd)\n\n        self._save()\n\n    # ------------------------------------------------------------\n\n    @property\n    def features(self) -&gt; Features:\n        \"\"\"\n        [`Features`][getml.pipeline.Features] object that\n        can be used to handle the features generated\n        by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Features(self.id, self.targets)\n\n    # ------------------------------------------------------------\n\n    def fit(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n        check: bool = True,\n    ) -&gt; \"Pipeline\":\n        \"\"\"Trains the feature learning algorithms, feature selectors\n        and predictors.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            validation_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If you are passing a subset, that subset\n                must be derived from the same container as *population_table*.\n\n                Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n                and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n            check (bool):\n                Whether you want to check the data model before fitting. The checks are\n                equivalent to the checks run by [`check`][getml.Pipeline.check].\n        \"\"\"\n\n        additional_tags = (\n            [\"container-\" + population_table.container_id]\n            if isinstance(population_table, data.Subset)\n            else []\n        )\n\n        if (\n            isinstance(population_table, data.Subset)\n            and isinstance(validation_table, data.Subset)\n            and validation_table.container_id != population_table.container_id\n        ):\n            raise ValueError(\n                \"The subset used for validation must be from the same container \"\n                + \"as the subset used for training.\"\n            )\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        if isinstance(validation_table, data.Subset):\n            validation_table = validation_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        if check:\n            warnings = self.check(population_table, peripheral_tables)\n            if warnings:\n                print(\"To see the issues in full, run .check() on the pipeline.\")\n                print()\n\n        self._send(additional_tags)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = self.type + \".fit\"\n        cmd[\"name_\"] = self.id\n\n        cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n        cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n        if validation_table is not None:\n            cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n\n            begin = time.time()\n\n            msg = comm.log(sock)\n\n            end = time.time()\n\n            if \"Trained\" in msg:\n                print()\n                print(msg)\n                _print_time_taken(begin, end, \"Time taken: \")\n            else:\n                comm.engine_exception_handler(msg)\n\n        self._save()\n\n        return self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def fitted(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline has already been fitted.\n        \"\"\"\n        return self._id != NOT_FITTED\n\n    # ----------------------------------------------------------------\n\n    @property\n    def mae(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the mae of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.mae\n\n    # ------------------------------------------------------------\n\n    @property\n    def plots(self) -&gt; Plots:\n        \"\"\"\n        [`Plots`][getml.pipeline.Plots] object that\n        can be used to generate plots like an ROC\n        curve or a lift curve.\n        \"\"\"\n        self._check_whether_fitted()\n        return Plots(self.id)\n\n    # ------------------------------------------------------------\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"\n        ID of the pipeline. This is used to uniquely identify\n        the pipeline on the engine.\n        \"\"\"\n        return self._id\n\n    # ------------------------------------------------------------\n\n    @property\n    def is_classification(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline can used for classification problems.\n        \"\"\"\n        return self._check_classification_or_regression()\n\n    # ------------------------------------------------------------\n\n    @property\n    def is_regression(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline can used for regression problems.\n        \"\"\"\n        return not self.is_classification\n\n    # ------------------------------------------------------------\n\n    @property\n    def metadata(self) -&gt; Optional[AllMetadata]:\n        \"\"\"\n        Contains information on the data frames\n        that were passed to .fit(...). The roles\n        contained therein can be directly passed\n        to existing data frames to correctly reassign\n        the roles of existing columns. If the pipeline\n        has not been fitted, this is None.\n        \"\"\"\n        return self._metadata\n\n    # ------------------------------------------------------------\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Returns the ID of the pipeline. The name property is\n        kept for backward compatibility.\n        \"\"\"\n        return self._id\n\n    # ------------------------------------------------------------\n\n    def predict(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        table_name: str = \"\",\n    ) -&gt; Union[NDArray[np.float_], None]:\n        \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n        Returns the predictions generated by the pipeline based on\n        `population_table` and `peripheral_tables` or writes them into\n        a data base named `table_name`.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            table_name (str, optional):\n                If not an empty string, the resulting predictions will\n                be written into a table in a [`database`][getml.database].\n                Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n        Returns:\n            Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can be used for\n            prediction.\n\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            y_hat = self._transform(\n                peripheral_tables,\n                population_table,\n                sock,\n                predict=True,\n                table_name=table_name,\n            )\n\n        return y_hat\n\n    # ------------------------------------------------------------\n\n    def refresh(self) -&gt; \"Pipeline\":\n        \"\"\"Reloads the pipeline from the engine.\n\n        This discards all local changes you have made since the\n        last time you called [`fit`][getml.Pipeline.fit].\n\n        Returns:\n            [`Pipeline`][getml.Pipeline]:\n                Current instance\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".refresh\"\n        cmd[\"name_\"] = self.id\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n        if msg[0] != \"{\":\n            comm.engine_exception_handler(msg)\n\n        json_obj = json.loads(msg)\n\n        self._parse_json_obj(json_obj)\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rmse(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the rmse of the latest scoring run\n        (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.rmse\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rsquared(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the rsquared of the latest scoring run\n        (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.rsquared\n\n    # ----------------------------------------------------------------\n\n    def score(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n    ) -&gt; Scores:\n        \"\"\"Calculates the performance of the ``predictor``.\n\n        Returns different scores calculated on `population_table` and\n        `peripheral_tables`.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can be\n            scored.\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n\n            self._transform(\n                peripheral_tables, population_table, sock, predict=True, score=True\n            )\n\n            msg = comm.recv_string(sock)\n\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n            scores = comm.recv_string(sock)\n\n            scores = json.loads(scores)\n\n        self.refresh()\n\n        self._save()\n\n        return self.scores\n\n    # ----------------------------------------------------------------\n\n    @property\n    def scores(self) -&gt; Scores:\n        \"\"\"\n        Contains all scores generated by [`score`][getml.Pipeline.score]\n\n        Returns:\n            [`Scores`][getml.pipeline.Scores]:\n                A container that holds the scores for the pipeline.\n\n        \"\"\"\n        self._check_whether_fitted()\n\n        scores = self._make_score_history()\n\n        latest = {score: self._get_latest_score(score) for score in _all_metrics}\n\n        return Scores(scores, latest)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def scored(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline has been scored.\n        \"\"\"\n        if self._scores is None:\n            return False\n        return len(self._scores) &gt; 1\n\n    # ----------------------------------------------------------------\n\n    @property\n    def tables(self) -&gt; Tables:\n        \"\"\"\n        [`Tables`][getml.pipeline.Tables] object that\n        can be used to handle information about the original\n        tables utilized by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Tables(self.targets, self.columns)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def targets(self) -&gt; List[str]:\n        \"\"\"\n        Contains the names of the targets used for this pipeline.\n        \"\"\"\n        self._check_whether_fitted()\n        return copy.deepcopy(self._targets)\n\n    # ----------------------------------------------------------------\n\n    def transform(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        df_name: str = \"\",\n        table_name: str = \"\",\n    ) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n        \"\"\"Translates new data into the trained features.\n\n        Transforms the data passed in `population_table` and\n        `peripheral_tables` into features, which can be inserted into\n        machine learning models.\n\n        Example:\n            By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n            ```python\n            my_features_array = pipe.transform()\n            ```\n            You can also export your features as a [`DataFrame`][getml.DataFrame]\n            by providing the `df_name` argument:\n            ```python\n            my_features_df = pipe.transform(df_name=\"my_features\")\n            ```\n            Or you can write the results directly into a database:\n            ```python\n            getml.database.connect_odbc(...)\n            pipe.transform(table_name=\"MY_FEATURES\")\n            ```\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            df_name (str, optional):\n                If not an empty string, the resulting features will be\n                written into a newly created DataFrame.\n\n            table_name (str, optional):\n                If not an empty string, the resulting features will\n                be written into a table in a [`database`][getml.database].\n                Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can transform\n            data into features.\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            y_hat = self._transform(\n                peripheral_tables,\n                population_table,\n                sock,\n                df_name=df_name,\n                table_name=table_name,\n            )\n\n        if df_name != \"\":\n            return data.DataFrame(name=df_name).refresh()\n\n        return y_hat\n\n    # ----------------------------------------------------------------\n\n    def _validate(self) -&gt; None:\n        if not isinstance(self.id, str):\n            raise TypeError(\"'name' must be of type str\")\n\n        if not isinstance(self.data_model, DataModel):\n            raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n        if not _is_typed_list(self.peripheral, data.Placeholder):\n            raise TypeError(\n                \"'peripheral' must be either a getml.data.Placeholder or a list thereof\"\n            )\n\n        if not _is_subclass_list(self.preprocessors, _Preprocessor):\n            raise TypeError(\"'preprocessor' must be a list of _Preprocessor.\")\n\n        if not _is_subclass_list(self.feature_learners, _FeatureLearner):\n            raise TypeError(\"'feature_learners' must be a list of _FeatureLearners.\")\n\n        if not _is_subclass_list(self.feature_selectors, _Predictor):\n            raise TypeError(\n                \"'feature_selectors' must be a list of getml.predictors._Predictors.\"\n            )\n\n        if not _is_subclass_list(self.predictors, _Predictor):\n            raise TypeError(\n                \"'predictors' must be a list of getml.predictors._Predictors.\"\n            )\n\n        if not isinstance(self.include_categorical, bool):\n            raise TypeError(\"'include_categorical' must be a bool!\")\n\n        if not isinstance(self.share_selected_features, numbers.Real):\n            raise TypeError(\"'share_selected_features' must be number!\")\n\n        if not _is_typed_list(self.tags, str):\n            raise TypeError(\"'tags' must be a list of str.\")\n\n        if self.type != \"Pipeline\":\n            raise ValueError(\"'type' must be 'Pipeline'\")\n\n        for kkey in self.__dict__:\n            if kkey not in Pipeline._supported_params:  # pylint: disable=E1101\n                raise KeyError(\n                    \"\"\"Instance variable [\"\"\"\n                    + kkey\n                    + \"\"\"]\n                       is not supported in Pipeline.\"\"\"\n                )\n\n        for elem in self.feature_learners:\n            elem.validate()\n\n        for elem in self.feature_selectors:\n            elem.validate()\n\n        for elem in self.predictors:\n            elem.validate()\n\n        self._check_classification_or_regression()\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.accuracy","title":"<code>accuracy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the accuracy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.auc","title":"<code>auc: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the auc of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.columns","title":"<code>columns: Columns</code>  <code>property</code>","text":"<p><code>Columns</code> object that can be used to handle information about the original columns utilized by the feature learners.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.cross_entropy","title":"<code>cross_entropy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the cross entropy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.features","title":"<code>features: Features</code>  <code>property</code>","text":"<p><code>Features</code> object that can be used to handle the features generated by the feature learners.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.fitted","title":"<code>fitted: bool</code>  <code>property</code>","text":"<p>Whether the pipeline has already been fitted.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>ID of the pipeline. This is used to uniquely identify the pipeline on the engine.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.is_classification","title":"<code>is_classification: bool</code>  <code>property</code>","text":"<p>Whether the pipeline can used for classification problems.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.is_regression","title":"<code>is_regression: bool</code>  <code>property</code>","text":"<p>Whether the pipeline can used for regression problems.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.mae","title":"<code>mae: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the mae of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.metadata","title":"<code>metadata: Optional[AllMetadata]</code>  <code>property</code>","text":"<p>Contains information on the data frames that were passed to .fit(...). The roles contained therein can be directly passed to existing data frames to correctly reassign the roles of existing columns. If the pipeline has not been fitted, this is None.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Returns the ID of the pipeline. The name property is kept for backward compatibility.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.plots","title":"<code>plots: Plots</code>  <code>property</code>","text":"<p><code>Plots</code> object that can be used to generate plots like an ROC curve or a lift curve.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.rmse","title":"<code>rmse: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the rmse of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.rsquared","title":"<code>rsquared: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the rsquared of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.scored","title":"<code>scored: bool</code>  <code>property</code>","text":"<p>Whether the pipeline has been scored.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.scores","title":"<code>scores: Scores</code>  <code>property</code>","text":"<p>Contains all scores generated by <code>score</code></p> <p>Returns:</p> Type Description <code>Scores</code> <p><code>Scores</code>: A container that holds the scores for the pipeline.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.tables","title":"<code>tables: Tables</code>  <code>property</code>","text":"<p><code>Tables</code> object that can be used to handle information about the original tables utilized by the feature learners.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.targets","title":"<code>targets: List[str]</code>  <code>property</code>","text":"<p>Contains the names of the targets used for this pipeline.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.check","title":"<code>check(population_table, peripheral_tables=None)</code>","text":"<p>Checks the validity of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def check(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]:\n    \"\"\"\n    Checks the validity of the data model.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    \"\"\"\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    temp = copy.deepcopy(self)\n\n    temp._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = temp.type + \".check\"\n    cmd[\"name_\"] = temp.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        print(\"Checking data model...\")\n        msg = comm.log(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        print()\n        issues = Issues(comm.recv_issues(sock))\n        if len(issues) == 0:\n            print(\"OK.\")\n        else:\n            print(\n                f\"The pipeline check generated {len(issues.info)} \"\n                + f\"issues labeled INFO and {len(issues.warnings)} \"\n                + \"issues labeled WARNING.\"\n            )\n\n    temp.delete()\n\n    return None if len(issues) == 0 else issues\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.delete","title":"<code>delete()</code>","text":"<p>Deletes the pipeline from the engine.</p> Note <p>Caution: You can not undo this action!</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes the pipeline from the engine.\n\n    Note:\n        Caution: You can not undo this action!\n    \"\"\"\n    self._check_whether_fitted()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".delete\"\n    cmd[\"name_\"] = self.id\n    cmd[\"mem_only_\"] = False\n\n    comm.send(cmd)\n\n    self._id = NOT_FITTED\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.deploy","title":"<code>deploy(deploy)</code>","text":"<p>Allows a fitted pipeline to be addressable via an HTTP request. See deployment for details.</p> <p>Parameters:</p> Name Type Description Default <code>deploy</code> <code>bool</code> <p>If <code>True</code>, the deployment of the pipeline will be triggered.</p> required Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def deploy(self, deploy: bool) -&gt; None:\n    \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n    See [deployment][deployment] for details.\n\n    Args:\n        deploy (bool): If `True`, the deployment of the pipeline\n            will be triggered.\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if not isinstance(deploy, bool):\n        raise TypeError(\"'deploy' must be of type bool\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".deploy\"\n    cmd[\"name_\"] = self.id\n    cmd[\"deploy_\"] = deploy\n\n    comm.send(cmd)\n\n    self._save()\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.fit","title":"<code>fit(population_table, peripheral_tables=None, validation_table=None, check=True)</code>","text":"<p>Trains the feature learning algorithms, feature selectors and predictors.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>validation_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable. If you are passing a subset, that subset must be derived from the same container as population_table.</p> <p>Only used for early stopping in <code>XGBoostClassifier</code> and <code>XGBoostRegressor</code>.</p> <code>None</code> <code>check</code> <code>bool</code> <p>Whether you want to check the data model before fitting. The checks are equivalent to the checks run by <code>check</code>.</p> <code>True</code> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def fit(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n    check: bool = True,\n) -&gt; \"Pipeline\":\n    \"\"\"Trains the feature learning algorithms, feature selectors\n    and predictors.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        validation_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If you are passing a subset, that subset\n            must be derived from the same container as *population_table*.\n\n            Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n            and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n        check (bool):\n            Whether you want to check the data model before fitting. The checks are\n            equivalent to the checks run by [`check`][getml.Pipeline.check].\n    \"\"\"\n\n    additional_tags = (\n        [\"container-\" + population_table.container_id]\n        if isinstance(population_table, data.Subset)\n        else []\n    )\n\n    if (\n        isinstance(population_table, data.Subset)\n        and isinstance(validation_table, data.Subset)\n        and validation_table.container_id != population_table.container_id\n    ):\n        raise ValueError(\n            \"The subset used for validation must be from the same container \"\n            + \"as the subset used for training.\"\n        )\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    if isinstance(validation_table, data.Subset):\n        validation_table = validation_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if check:\n        warnings = self.check(population_table, peripheral_tables)\n        if warnings:\n            print(\"To see the issues in full, run .check() on the pipeline.\")\n            print()\n\n    self._send(additional_tags)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = self.type + \".fit\"\n    cmd[\"name_\"] = self.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    if validation_table is not None:\n        cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        begin = time.time()\n\n        msg = comm.log(sock)\n\n        end = time.time()\n\n        if \"Trained\" in msg:\n            print()\n            print(msg)\n            _print_time_taken(begin, end, \"Time taken: \")\n        else:\n            comm.engine_exception_handler(msg)\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.predict","title":"<code>predict(population_table, peripheral_tables=None, table_name='')</code>","text":"<p>Forecasts on new, unseen data using the trained <code>predictor</code>.</p> <p>Returns the predictions generated by the pipeline based on <code>population_table</code> and <code>peripheral_tables</code> or writes them into a data base named <code>table_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>If not an empty string, the resulting predictions will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[NDArray[float_], None]</code> <p>Resulting predictions provided in an array of the (number of rows in <code>population_table</code>, number of targets in <code>population_table</code>).</p> Note <p>Only fitted pipelines (<code>fit</code>) can be used for prediction.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def predict(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[np.float_], None]:\n    \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n    Returns the predictions generated by the pipeline based on\n    `population_table` and `peripheral_tables` or writes them into\n    a data base named `table_name`.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        table_name (str, optional):\n            If not an empty string, the resulting predictions will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be used for\n        prediction.\n\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            predict=True,\n            table_name=table_name,\n        )\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.refresh","title":"<code>refresh()</code>","text":"<p>Reloads the pipeline from the engine.</p> <p>This discards all local changes you have made since the last time you called <code>fit</code>.</p> <p>Returns:</p> Type Description <code>'Pipeline'</code> <p><code>Pipeline</code>: Current instance</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def refresh(self) -&gt; \"Pipeline\":\n    \"\"\"Reloads the pipeline from the engine.\n\n    This discards all local changes you have made since the\n    last time you called [`fit`][getml.Pipeline.fit].\n\n    Returns:\n        [`Pipeline`][getml.Pipeline]:\n            Current instance\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".refresh\"\n    cmd[\"name_\"] = self.id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    json_obj = json.loads(msg)\n\n    self._parse_json_obj(json_obj)\n\n    return self\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.score","title":"<code>score(population_table, peripheral_tables=None)</code>","text":"<p>Calculates the performance of the <code>predictor</code>.</p> <p>Returns different scores calculated on <code>population_table</code> and <code>peripheral_tables</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> Note <p>Only fitted pipelines (<code>fit</code>) can be scored.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def score(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores:\n    \"\"\"Calculates the performance of the ``predictor``.\n\n    Returns different scores calculated on `population_table` and\n    `peripheral_tables`.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be\n        scored.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        self._transform(\n            peripheral_tables, population_table, sock, predict=True, score=True\n        )\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        scores = comm.recv_string(sock)\n\n        scores = json.loads(scores)\n\n    self.refresh()\n\n    self._save()\n\n    return self.scores\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Pipeline.transform","title":"<code>transform(population_table, peripheral_tables=None, df_name='', table_name='')</code>","text":"<p>Translates new data into the trained features.</p> <p>Transforms the data passed in <code>population_table</code> and <code>peripheral_tables</code> into features, which can be inserted into machine learning models.</p> Example <p>By default, <code>transform</code> returns a <code>ndarray</code>: <pre><code>my_features_array = pipe.transform()\n</code></pre> You can also export your features as a <code>DataFrame</code> by providing the <code>df_name</code> argument: <pre><code>my_features_df = pipe.transform(df_name=\"my_features\")\n</code></pre> Or you can write the results directly into a database: <pre><code>getml.database.connect_odbc(...)\npipe.transform(table_name=\"MY_FEATURES\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>df_name</code> <code>str</code> <p>If not an empty string, the resulting features will be written into a newly created DataFrame.</p> <code>''</code> <code>table_name</code> <code>str</code> <p>If not an empty string, the resulting features will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <code>''</code> Note <p>Only fitted pipelines (<code>fit</code>) can transform data into features.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def transform(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n    \"\"\"Translates new data into the trained features.\n\n    Transforms the data passed in `population_table` and\n    `peripheral_tables` into features, which can be inserted into\n    machine learning models.\n\n    Example:\n        By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n        ```python\n        my_features_array = pipe.transform()\n        ```\n        You can also export your features as a [`DataFrame`][getml.DataFrame]\n        by providing the `df_name` argument:\n        ```python\n        my_features_df = pipe.transform(df_name=\"my_features\")\n        ```\n        Or you can write the results directly into a database:\n        ```python\n        getml.database.connect_odbc(...)\n        pipe.transform(table_name=\"MY_FEATURES\")\n        ```\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        df_name (str, optional):\n            If not an empty string, the resulting features will be\n            written into a newly created DataFrame.\n\n        table_name (str, optional):\n            If not an empty string, the resulting features will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can transform\n        data into features.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            df_name=df_name,\n            table_name=table_name,\n        )\n\n    if df_name != \"\":\n        return data.DataFrame(name=df_name).refresh()\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Plots","title":"<code>Plots</code>","text":"<p>Custom class for handling the plots generated by the pipeline.</p> Example <pre><code>recall, precision = my_pipeline.plots.precision_recall_curve()\nfpr, tpr = my_pipeline.plots.roc_curve()\n</code></pre> Source code in <code>getml/pipeline/plots.py</code> <pre><code>class Plots:\n    \"\"\"\n    Custom class for handling the\n    plots generated by the pipeline.\n\n    Example:\n        ```python\n        recall, precision = my_pipeline.plots.precision_recall_curve()\n        fpr, tpr = my_pipeline.plots.roc_curve()\n        ```\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, name: str) -&gt; None:\n\n        if not isinstance(name, str):\n            raise ValueError(\"'name' must be a str.\")\n\n        self.name = name\n\n    # ------------------------------------------------------------\n\n    def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the lift curve, as displayed in the getML monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the proportion of samples, usually displayed on the x-axis.\n            The second array is the lift, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.lift_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n\n    # ------------------------------------------------------------\n\n    def precision_recall_curve(\n        self, target_num: int = 0\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the precision-recall curve, as displayed in the getML\n        monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n            The second array is the precision, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n\n    # ------------------------------------------------------------\n\n    def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the ROC curve, as displayed in the getML monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the false positive rate, usually displayed on the x-axis.\n            The second array is the true positive rate, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.roc_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Plots.lift_curve","title":"<code>lift_curve(target_num=0)</code>","text":"<p>Returns the data for the lift curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the proportion of samples, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the lift, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the lift curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the proportion of samples, usually displayed on the x-axis.\n        The second array is the lift, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.lift_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Plots.precision_recall_curve","title":"<code>precision_recall_curve(target_num=0)</code>","text":"<p>Returns the data for the precision-recall curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the precision, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def precision_recall_curve(\n    self, target_num: int = 0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the precision-recall curve, as displayed in the getML\n    monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n        The second array is the precision, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Plots.roc_curve","title":"<code>roc_curve(target_num=0)</code>","text":"<p>Returns the data for the ROC curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the false positive rate, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the true positive rate, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the ROC curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the false positive rate, usually displayed on the x-axis.\n        The second array is the true positive rate, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.roc_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.SQLCode","title":"<code>SQLCode</code>","text":"<p>Custom class for handling the SQL code of the features generated by the pipeline.</p> Example <pre><code>sql_code = my_pipeline.features.to_sql()\n\n# You can access individual features\n# by index.\nfeature_1_1 = sql_code[0]\n\n# You can also access them by name.\nfeature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n# You can also type the name of\n# a table or column to find all\n# features related to that table\n# or column.\nfeatures = sql_code.find(\"SOME_TABLE\")\n\n# HINT: The generated SQL code always\n# escapes table and column names using\n# quotation marks. So if you want exact\n# matching, you can do this:\nfeatures = sql_code.find('\"SOME_TABLE\"')\n</code></pre> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>class SQLCode:\n    \"\"\"\n    Custom class for handling the SQL code of the\n    features generated by the pipeline.\n\n    Example:\n        ```python\n        sql_code = my_pipeline.features.to_sql()\n\n        # You can access individual features\n        # by index.\n        feature_1_1 = sql_code[0]\n\n        # You can also access them by name.\n        feature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n        # You can also type the name of\n        # a table or column to find all\n        # features related to that table\n        # or column.\n        features = sql_code.find(\"SOME_TABLE\")\n\n        # HINT: The generated SQL code always\n        # escapes table and column names using\n        # quotation marks. So if you want exact\n        # matching, you can do this:\n        features = sql_code.find('\"SOME_TABLE\"')\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        code: Sequence[Union[str, SQLString]],\n        dialect: str = sqlite3,\n    ) -&gt; None:\n\n        if not _is_typed_list(code, str):\n            raise TypeError(\"'code' must be a list of str.\")\n\n        self.code = [SQLString(elem) for elem in code]\n\n        self.dialect = dialect\n\n        self.tables = [\n            _edit_table_name(table_name)\n            for table_name in re.findall(_table_pattern(self.dialect), \"\".join(code))\n        ]\n\n    def __getitem__(self, key: Union[int, slice, str]) -&gt; Union[SQLCode, SQLString]:\n\n        if isinstance(key, int):\n            return self.code[key]\n\n        if isinstance(key, slice):\n            return SQLCode(self.code[key], self.dialect)\n\n        if isinstance(key, str):\n            if key.upper() in self.tables:\n                return self.find(_drop_table(self.dialect, key))[0]\n            return SQLString(\"\")\n\n        raise TypeError(\n            \"Features can only be indexed by: int, slices, \"\n            f\"or str, not {type(key).__name__}\"\n        )\n\n    def __iter__(self) -&gt; Iterator[SQLString]:\n        yield from self.code\n\n    def __len__(self) -&gt; int:\n        return len(self.code)\n\n    def __repr__(self) -&gt; str:\n        return \"\\n\\n\\n\".join(self.code)\n\n    def _repr_markdown_(self) -&gt; str:\n        return \"```sql\\n\" + self.__repr__() + \"\\n```\"\n\n    def find(self, keyword: str) -&gt; SQLCode:\n        \"\"\"\n        Returns the SQLCode for all features\n        containing the keyword.\n\n        Args:\n            keyword (str): The keyword to be found.\n        \"\"\"\n        if not isinstance(keyword, str):\n            raise TypeError(\"'keyword' must be a str.\")\n\n        return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n\n    def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n        \"\"\"\n        Saves the SQL code to a file.\n\n        Args:\n            fname (str):\n                The name of the file or folder (if `split` is True)\n                in which you want to save the features.\n\n            split (bool):\n                If True, the code will be split into multiple files, one for\n                each feature and saved into a folder `fname`.\n\n            remove (bool):\n                If True, the existing SQL files in `fname` folder generated\n                previously with the save method will be removed.\n        \"\"\"\n        if not split:\n            with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n                sqlfile.write(str(self))\n            return\n\n        directory = Path(fname)\n\n        if directory.exists():\n            iter_dir = os.listdir(fname)\n\n            pattern = \"^\\d{4}.*\\_.*\\.sql$\"\n\n            exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n            if not remove and exist_files_path:\n                print(f\"The following files already exist in the directory ({fname}):\")\n                for fp in np.sort(exist_files_path):\n                    print(fp)\n                print(\"Please set 'remove=True' to remove them.\")\n                return\n\n            if remove and exist_files_path:\n                for fp in exist_files_path:\n                    os.remove(fname + \"/\" + fp)\n\n        directory.mkdir(exist_ok=True)\n\n        for index, code in enumerate(self.code, 1):\n            match = re.search(_table_pattern(self.dialect), str(code))\n            name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n            name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n            file_path = directory / f\"{index:04d}_{name}.sql\"\n            with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n                sqlfile.write(str(code))\n\n    def to_str(self) -&gt; str:\n        \"\"\"\n        Returns a raw string representation of the SQL code.\n        \"\"\"\n        return str(self)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.SQLCode.find","title":"<code>find(keyword)</code>","text":"<p>Returns the SQLCode for all features containing the keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to be found.</p> required Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def find(self, keyword: str) -&gt; SQLCode:\n    \"\"\"\n    Returns the SQLCode for all features\n    containing the keyword.\n\n    Args:\n        keyword (str): The keyword to be found.\n    \"\"\"\n    if not isinstance(keyword, str):\n        raise TypeError(\"'keyword' must be a str.\")\n\n    return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.SQLCode.save","title":"<code>save(fname, split=True, remove=False)</code>","text":"<p>Saves the SQL code to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the file or folder (if <code>split</code> is True) in which you want to save the features.</p> required <code>split</code> <code>bool</code> <p>If True, the code will be split into multiple files, one for each feature and saved into a folder <code>fname</code>.</p> <code>True</code> <code>remove</code> <code>bool</code> <p>If True, the existing SQL files in <code>fname</code> folder generated previously with the save method will be removed.</p> <code>False</code> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n    \"\"\"\n    Saves the SQL code to a file.\n\n    Args:\n        fname (str):\n            The name of the file or folder (if `split` is True)\n            in which you want to save the features.\n\n        split (bool):\n            If True, the code will be split into multiple files, one for\n            each feature and saved into a folder `fname`.\n\n        remove (bool):\n            If True, the existing SQL files in `fname` folder generated\n            previously with the save method will be removed.\n    \"\"\"\n    if not split:\n        with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(self))\n        return\n\n    directory = Path(fname)\n\n    if directory.exists():\n        iter_dir = os.listdir(fname)\n\n        pattern = \"^\\d{4}.*\\_.*\\.sql$\"\n\n        exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n        if not remove and exist_files_path:\n            print(f\"The following files already exist in the directory ({fname}):\")\n            for fp in np.sort(exist_files_path):\n                print(fp)\n            print(\"Please set 'remove=True' to remove them.\")\n            return\n\n        if remove and exist_files_path:\n            for fp in exist_files_path:\n                os.remove(fname + \"/\" + fp)\n\n    directory.mkdir(exist_ok=True)\n\n    for index, code in enumerate(self.code, 1):\n        match = re.search(_table_pattern(self.dialect), str(code))\n        name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n        name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n        file_path = directory / f\"{index:04d}_{name}.sql\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(code))\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.SQLCode.to_str","title":"<code>to_str()</code>","text":"<p>Returns a raw string representation of the SQL code.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"\n    Returns a raw string representation of the SQL code.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores","title":"<code>Scores</code>","text":"<p>Container which holds the history of all scores associated with a given pipeline. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>class Scores:\n    \"\"\"\n    Container which holds the history of all scores associated with a given pipeline.\n    The container supports slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data: Sequence[Score], latest: Dict[str, List[float]]) -&gt; None:\n        self._latest = latest\n\n        self.is_classification = all(\n            isinstance(score, ClassificationScore) for score in data\n        )\n\n        self.is_regression = not self.is_classification\n\n        self.data = data\n\n        self.sets_used = [score.set_used for score in data]\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key: Union[int, slice, str]):\n        if isinstance(key, int):\n            return self.data[key]\n\n        if isinstance(key, slice):\n            scores_subset = self.data[key]\n            return Scores(scores_subset, self._latest)\n\n        if isinstance(key, str):\n            # allow to access latest scores via their name for backward compatibility\n            if key in _all_metrics:\n                return self._latest[key]\n\n            scores_subset = [score for score in self.data if score.set_used == key]\n\n            return Scores(scores_subset, self._latest)\n\n        raise TypeError(\n            f\"Scores can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self):\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        headers = [\"date time\", \"set used\", \"target\"]\n        if self.is_classification:\n            headers += [\"accuracy\", \"auc\", \"cross entropy\"]\n        if self.is_regression:\n            headers += [\"mae\", \"rmse\", \"rsquared\"]\n\n        rows = [list(vars(score).values()) for score in self.data]\n\n        return _Formatter([headers], rows)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def accuracy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `accuracy` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[accuracy])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def auc(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `auc` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[auc])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def cross_entropy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `cross entropy` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[cross_entropy])\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n        \"\"\"\n        Filters the scores container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            [`Scores`][getml.pipeline.Scores]:\n                A container of filtered scores.\n\n        Example:\n            ```python\n            from datetime import datetime, timedelta\n            one_week_ago = datetime.today() - timedelta(days=7)\n            scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n            ```\n        \"\"\"\n        scores_filtered = [score for score in self.data if conditional(score)]\n\n        return Scores(scores_filtered, self._latest)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def mae(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `mae` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[mae])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rmse(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `rmse` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[rmse])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rsquared(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `rsquared` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[rsquared])\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n    ) -&gt; Scores:\n        \"\"\"\n        Sorts the scores container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n            [`Scores`][getml.pipeline.Scores]:\n                A container of sorted scores.\n\n        Example:\n            ```python\n            by_auc = pipe.scores.sort(key=lambda score: score.auc)\n            most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n            ```\n        \"\"\"\n\n        scores_sorted = sorted(self.data, key=key, reverse=descending)\n        return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.accuracy","title":"<code>accuracy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>accuracy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.auc","title":"<code>auc: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>auc</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.cross_entropy","title":"<code>cross_entropy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>cross entropy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.mae","title":"<code>mae: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>mae</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.rmse","title":"<code>rmse: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>rmse</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.rsquared","title":"<code>rsquared: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>rsquared</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the scores container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Scores</code> <p><code>Scores</code>: A container of filtered scores.</p> Example <pre><code>from datetime import datetime, timedelta\none_week_ago = datetime.today() - timedelta(days=7)\nscores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n    \"\"\"\n    Filters the scores container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        [`Scores`][getml.pipeline.Scores]:\n            A container of filtered scores.\n\n    Example:\n        ```python\n        from datetime import datetime, timedelta\n        one_week_ago = datetime.today() - timedelta(days=7)\n        scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n        ```\n    \"\"\"\n    scores_filtered = [score for score in self.data if conditional(score)]\n\n    return Scores(scores_filtered, self._latest)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Scores.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the scores container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p><code>Scores</code>:     A container of sorted scores.</p> Example <pre><code>by_auc = pipe.scores.sort(key=lambda score: score.auc)\nmost_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def sort(\n    self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n) -&gt; Scores:\n    \"\"\"\n    Sorts the scores container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n        [`Scores`][getml.pipeline.Scores]:\n            A container of sorted scores.\n\n    Example:\n        ```python\n        by_auc = pipe.scores.sort(key=lambda score: score.auc)\n        most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n        ```\n    \"\"\"\n\n    scores_sorted = sorted(self.data, key=key, reverse=descending)\n    return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables","title":"<code>Tables</code>","text":"<p>This container holds a pipeline's tables. These tables are build from the columns for which importances can be calculated. The motivation behind this container is to determine which tables are more important than others.</p> <p>Tables can be accessed by name, index or with a NumPy array. The container supports slicing and can be sorted and filtered. Further, the container holds global methods to request tables' importances.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> <p>Examples:</p> <pre><code>all_my_tables = my_pipeline.tables\nfirst_table = my_pipeline.tables[0]\nall_but_last_10_tables = my_pipeline.tables[:-10]\nimportant_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\nnames, importances = my_pipeline.tables.importances()\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>class Tables:\n    \"\"\"\n    This container holds a pipeline's tables. These tables are build from the\n    columns for which importances can be calculated. The motivation behind this\n    container is to determine which tables are more important than others.\n\n    Tables can be accessed by name, index or with a NumPy array. The container\n    supports slicing and can be sorted and filtered. Further, the container\n    holds global methods to request tables' importances.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Tables.filter] you can also use python list\n        comprehensions for filtering.\n\n    Examples:\n        ```python\n        all_my_tables = my_pipeline.tables\n        first_table = my_pipeline.tables[0]\n        all_but_last_10_tables = my_pipeline.tables[:-10]\n        important_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\n        names, importances = my_pipeline.tables.importances()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        targets: Sequence[str],\n        columns: Columns,\n        data: Optional[Sequence[Table]] = None,\n    ) -&gt; None:\n        self._targets = targets\n        self._columns = columns\n\n        if data is not None:\n            self.data = data\n\n        else:\n            self._load_tables()\n\n        if not (targets and columns) and not data:\n            raise ValueError(\n                \"Missing required arguments. Either provide `targets` &amp; \"\n                \"`columns` or else provide `data`.\"\n            )\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[str, int, slice, Union[NDArray[np.int_], NDArray[np.bool_]]]\n    ) -&gt; Union[Table, Tables, list[Table]]:\n        if not self.data:\n            raise AttributeError(\"Tables container not fully initialized.\")\n\n        if isinstance(key, int):\n            return self.data[key]\n\n        if isinstance(key, slice):\n            tables_subset = self.data[key]\n            return self._make_tables(tables_subset)\n\n        if isinstance(key, str):\n            if key in self.names:\n                return [table for table in self.data if table.name == key][0]\n            raise AttributeError(f\"No Table with name: {key}\")\n\n        if isinstance(key, np.ndarray):\n            tables_subset = np.array(self.data)[key].tolist()\n            return self._make_tables(tables_subset)\n\n        raise TypeError(\n            \"Columns can only be indexed by: int, slices, str or np.ndarray,\"\n            f\" not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Table]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        headers = [[\"name\", \"importance\", \"target\", \"marker\"]]\n\n        rows = [\n            [\n                table.name,\n                table.importance,\n                table.target,\n                table.marker,\n            ]\n            for table in self.data\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _load_tables(self) -&gt; None:\n        \"\"\"\n        Gets tables data from columns\n        \"\"\"\n\n        tables = []\n\n        for table_target in self._targets:\n            importances: dict[str, float] = {\n                column.table: 0.0\n                for column in self._columns\n                if column.target == table_target\n            }\n\n            targets: dict[str, str] = {}\n            markers: dict[str, str] = {}\n\n            for column in self._columns:\n                if column.target == table_target:\n                    importances[column.table] += column.importance\n                    targets[column.table] = column.target\n                    markers[column.table] = column.marker\n\n            tables_zip = zip(\n                importances.keys(),\n                importances.values(),\n                targets.values(),\n                markers.values(),\n            )\n\n            for name, importance, target, marker in tables_zip:\n                tables.append(\n                    Table(\n                        name=name, importance=importance, target=target, marker=marker\n                    )\n                )\n\n        self.data = tables\n\n    # ----------------------------------------------------------------\n\n    def _make_tables(self, data: Sequence[Table]) -&gt; Tables:\n        \"\"\"\n        A factory to construct a [`Tables`][getml.pipeline.Tables] container\n        from a list of [`Table`][getml.pipeline.Table]s.\n        \"\"\"\n\n        return Tables(self._targets, self._columns, data=data)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n        \"\"\"\n        Filters the tables container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered tables.\n\n        Example:\n            ```python\n            important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n            peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n            ```\n        \"\"\"\n        tables_filtered = [table for table in self.data if conditional(table)]\n        return self._make_tables(tables_filtered)\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the importances of tables.\n\n        Table importances are calculated by summing up the importances of the\n        columns belonging to the tables. Each column is assigned an importance\n        value that measures its contribution to the predictive performance. For\n        each target, the importances add up to 1.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances. (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the tables.\n            The second array contains their importances. By definition, all importances add up to 1.\n        \"\"\"\n\n        target_name = self._targets[target_num]\n\n        names = np.empty(0, dtype=str)\n        importances = np.empty(0, dtype=float)\n\n        for table in self.data:\n            if table.target == target_name:\n                names = np.append(names, table.name)\n                importances = np.append(importances, table.importance)\n\n        if not sort:\n            return names, importances\n\n        indices = np.argsort(importances)[::-1]\n\n        return (names[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; list[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s tables.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [table.name for table in self.data]\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[Callable[[Table], Any]] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Tables:\n        \"\"\"\n        Sorts the Tables container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n                A container of sorted tables.\n\n        Example:\n            ```python\n            by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            tables_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_tables(tables_sorted)\n\n        if by is None:\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.name, reverse=reverse\n            )\n            tables_sorted.sort(key=lambda table: table.target)\n            return self._make_tables(tables_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.name, reverse=reverse\n            )\n            return self._make_tables(tables_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.importance, reverse=reverse\n            )\n            return self._make_tables(tables_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    @property\n    def targets(self) -&gt; list[str]:\n        \"\"\"\n        Holds the targets of a [`Pipeline`][getml.Pipeline]'s tables.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [table.target for table in self.data]\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns all information related to the tables in a pandas DataFrame.\n        \"\"\"\n\n        data_frame = pd.DataFrame()\n\n        for i, table in enumerate(self.data):\n            data_frame.loc[i, \"name\"] = table.name\n            data_frame.loc[i, \"importance\"] = table.importance\n            data_frame.loc[i, \"target\"] = table.target\n            data_frame.loc[i, \"marker\"] = table.marker\n\n        return data_frame\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.names","title":"<code>names: list[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s tables.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.targets","title":"<code>targets: list[str]</code>  <code>property</code>","text":"<p>Holds the targets of a <code>Pipeline</code>'s tables.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the tables container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Tables</code> <p>A container of filtered tables.</p> Example <pre><code>important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\nperipheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n    \"\"\"\n    Filters the tables container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered tables.\n\n    Example:\n        ```python\n        important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n        peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    tables_filtered = [table for table in self.data if conditional(table)]\n    return self._make_tables(tables_filtered)\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the importances of tables.</p> <p>Table importances are calculated by summing up the importances of the columns belonging to the tables. Each column is assigned an importance value that measures its contribution to the predictive performance. For each target, the importances add up to 1.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the tables.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the importances of tables.\n\n    Table importances are calculated by summing up the importances of the\n    columns belonging to the tables. Each column is assigned an importance\n    value that measures its contribution to the predictive performance. For\n    each target, the importances add up to 1.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances. (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the tables.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    target_name = self._targets[target_num]\n\n    names = np.empty(0, dtype=str)\n    importances = np.empty(0, dtype=float)\n\n    for table in self.data:\n        if table.target == target_name:\n            names = np.append(names, table.name)\n            importances = np.append(importances, table.importance)\n\n    if not sort:\n        return names, importances\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Tables container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tables</code> <p>A container of sorted tables.</p> Example <pre><code>by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables:\n    \"\"\"\n    Sorts the Tables container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted tables.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        tables_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_tables(tables_sorted)\n\n    if by is None:\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        tables_sorted.sort(key=lambda table: table.target)\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.importance, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.Tables.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the tables in a pandas DataFrame.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the tables in a pandas DataFrame.\n    \"\"\"\n\n    data_frame = pd.DataFrame()\n\n    for i, table in enumerate(self.data):\n        data_frame.loc[i, \"name\"] = table.name\n        data_frame.loc[i, \"importance\"] = table.importance\n        data_frame.loc[i, \"target\"] = table.target\n        data_frame.loc[i, \"marker\"] = table.marker\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.delete","title":"<code>delete(name)</code>","text":"<p>If a pipeline named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline.</p> required Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a pipeline named 'name' exists, it is deleted.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        _make_dummy(name).delete()\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.exists","title":"<code>exists(name)</code>","text":"<p>Returns true if a pipeline named 'name' exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline.</p> required Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"\n    Returns true if a pipeline named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_pipelines = list_pipelines()\n\n    return name in all_pipelines\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.list_pipelines","title":"<code>list_pipelines()</code>","text":"<p>Lists all pipelines present in the engine.</p> <p>Note that this function only lists pipelines which are part of the current project. See <code>set_project</code> for changing projects and <code>pipelines</code> for more details about the lifecycles of the pipelines.</p> <p>To subsequently load one of them, use <code>load</code>.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list containing the names of all pipelines.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def list_pipelines() -&gt; List[str]:\n    \"\"\"Lists all pipelines present in the engine.\n\n    Note that this function only lists pipelines which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects and [`pipelines`][getml.pipeline] for more details about\n    the lifecycles of the pipelines.\n\n    To subsequently load one of them, use\n    [`load`][getml.pipeline.load].\n\n    Returns:\n        list containing the names of all pipelines.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"list_pipelines\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/pipeline/__init__/#getml.pipeline.load","title":"<code>load(name)</code>","text":"<p>Loads a pipeline from the getML engine into Python.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline to be loaded.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>A <code>Pipeline</code> that is a handler</p> <code>Pipeline</code> <p>for the pipeline signified by name.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def load(name: str) -&gt; Pipeline:\n    \"\"\"Loads a pipeline from the getML engine into Python.\n\n    Args:\n        name: The name of the pipeline to be loaded.\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] that is a handler\n        for the pipeline signified by name.\n    \"\"\"\n\n    return _make_dummy(name).refresh()\n</code></pre>"},{"location":"reference/pipeline/column/","title":"Column","text":"<p>Class representing metadata on a column used by the features of a pipeline.</p>"},{"location":"reference/pipeline/column/#getml.pipeline.column.Column","title":"<code>Column</code>  <code>dataclass</code>","text":"<p>Dataclass that holds data about a single column.</p> Source code in <code>getml/pipeline/column.py</code> <pre><code>@dataclass\nclass Column:\n    \"\"\"\n    Dataclass that holds data about a single column.\n    \"\"\"\n\n    index: int\n    name: str\n    marker: str\n    table: str\n    target: str\n    importance: float = np.nan\n</code></pre>"},{"location":"reference/pipeline/columns/","title":"Columns","text":"<p>Custom class for handling the columns of a pipeline.</p>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns","title":"<code>Columns</code>","text":"<p>Container which holds a pipeline's columns. These include the columns for which importance can be calculated, such as the ones with <code>roles</code> as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Columns can be accessed by name, index or with a NumPy array. The container supports slicing and is sort- and filterable. Further, the container holds global methods to request columns' importances and apply a column selection to data frames provided to the pipeline.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_columns = my_pipeline.columns\n\nfirst_column = my_pipeline.columns[0]\n\nall_but_last_10_columns = my_pipeline.columns[:-10]\n\nimportant_columns = [column for column in my_pipeline.columns if\ncolumn.importance &gt; 0.1]\n\nnames, importances = my_pipeline.columns.importances()\n\n# Drops all categorical and numerical columns that are not # in the\ntop 20%. new_container = my_pipeline.columns.select(\n    container, share_selected_columns=0.2,\n)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>class Columns:\n    \"\"\"\n    Container which holds a pipeline's columns. These include the columns for\n    which importance can be calculated, such as the ones with\n    [`roles`][getml.data.roles] as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Columns can be accessed by name, index or with a NumPy array. The container\n    supports slicing and is sort- and filterable. Further, the container holds\n    global methods to request columns' importances and apply a column selection\n    to data frames provided to the pipeline.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Columns.filter] you can also use python list\n        comprehensions for filtering.\n\n    Example:\n        ```python\n        all_my_columns = my_pipeline.columns\n\n        first_column = my_pipeline.columns[0]\n\n        all_but_last_10_columns = my_pipeline.columns[:-10]\n\n        important_columns = [column for column in my_pipeline.columns if\n        column.importance &gt; 0.1]\n\n        names, importances = my_pipeline.columns.importances()\n\n        # Drops all categorical and numerical columns that are not # in the\n        top 20%. new_container = my_pipeline.columns.select(\n            container, share_selected_columns=0.2,\n        )\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(\n        self,\n        pipeline: str,\n        targets: Sequence[str],\n        peripheral: Sequence[Placeholder],\n        data: Optional[Sequence[Column]] = None,\n    ) -&gt; None:\n        if not isinstance(pipeline, str):\n            raise ValueError(\"'pipeline' must be a str.\")\n\n        if not _is_typed_list(targets, str):\n            raise TypeError(\"'targets' must be a list of str.\")\n\n        self.pipeline = pipeline\n\n        self.targets = targets\n\n        self.peripheral = peripheral\n\n        self.peripheral_names = [p.name for p in self.peripheral]\n\n        if data is not None:\n            self.data = data\n        else:\n            self._load_columns()\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Column]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[str, int, slice, Union[NDArray[np.int_], NDArray[np.bool_]]]\n    ) -&gt; Union[Column, Columns, List[Column]]:\n        if not self.data:\n            raise AttributeError(\"Columns container not fully initialised.\")\n\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            columns_subset = self.data[key]\n            return self._make_columns(columns_subset)\n        if isinstance(key, str):\n            if key in self.names:\n                return [column for column in self.data if column.name == key][0]\n            raise AttributeError(f\"No Column with name: {key}\")\n        if isinstance(key, np.ndarray):\n            columns_subset = np.array(self.data)[key].tolist()\n            return list(columns_subset)\n        raise TypeError(\n            \"Columns can only be indexed by: int, slices, or str,\"\n            f\" not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _get_column_importances(\n        self, target_num: int, sort: bool\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.column_importances\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        descriptions = np.asarray(json_obj[\"column_descriptions_\"])\n        importances = np.asarray(json_obj[\"column_importances_\"])\n\n        if hasattr(self, \"data\"):\n            indices = np.asarray(\n                [\n                    column.index\n                    for column in self.data\n                    if column.target == self.targets[target_num]\n                    and column.index &lt; len(importances)\n                ]\n            )\n\n            descriptions = descriptions[indices]\n            importances = importances[indices]\n\n        if not sort:\n            return descriptions, importances\n\n        indices = np.argsort(importances)[::-1]\n\n        return (descriptions[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        rows = [\n            [\n                column.name,\n                column.marker,\n                column.table,\n                column.importance,\n                column.target,\n            ]\n            for column in self.data\n        ]\n\n        headers = [\n            [\n                \"name\",\n                \"marker\",\n                \"table\",\n                \"importance\",\n                \"target\",\n            ]\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _load_columns(self) -&gt; None:\n        \"\"\"\n        Loads the actual column data from the engine.\n        \"\"\"\n        columns = []\n\n        for target_num, target in enumerate(self.targets):\n            descriptions, importances = self._get_column_importances(\n                target_num=target_num, sort=False\n            )\n\n            columns.extend(\n                [\n                    Column(\n                        index=index,\n                        name=description.get(\"name_\"),\n                        marker=description.get(\"marker_\"),\n                        table=description.get(\"table_\"),\n                        importance=importances[index],\n                        target=target,\n                    )\n                    for index, description in enumerate(descriptions)\n                ]\n            )\n\n        self.data = columns\n\n    # ----------------------------------------------------------------\n\n    def _make_columns(self, data: Sequence[Column]) -&gt; Columns:\n        \"\"\"\n        A factory to construct a :class:`getml.pipeline.Columns` container from a list\n        of :class:`getml.pipeline.Column`s.\n        \"\"\"\n        return Columns(self.pipeline, self.targets, self.peripheral, data)\n\n    # ----------------------------------------------------------------\n\n    def _pivot(self, field: str) -&gt; Any:\n        \"\"\"\n        Pivots the data for a given field. Returns a list of values of the field's type.\n        \"\"\"\n        return [getattr(column, field) for column in self.data]\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n        \"\"\"\n        Filters the columns container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            A container of filtered Columns.\n\n        Example:\n            ```python\n            important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n            peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n            ```\n        \"\"\"\n        columns_filtered = [column for column in self.data if conditional(column)]\n        return self._make_columns(columns_filtered)\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the column importances.\n\n        Column importances extend the idea of column importances\n        to the columns originally inserted into the pipeline.\n        Each column is assigned an importance value that measures\n        its contribution to the predictive performance. All\n        columns importances add up to 1.\n\n        The importances can be calculated for columns with\n        [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n        [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n        The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n        [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n        [`unused_float`][getml.data.roles.unused_float] and\n        [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the columns.\n            The second array contains their importances. By definition, all importances add up to 1.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        descriptions, importances = self._get_column_importances(\n            target_num=target_num, sort=sort\n        )\n\n        # ------------------------------------------------------------\n\n        names = np.asarray(\n            [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n        )\n\n        # ------------------------------------------------------------\n\n        return names, importances\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s columns.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [column.name for column in self.data]\n\n    # ----------------------------------------------------------------\n\n    def select(\n        self, container: Container, share_selected_columns: float = 0.5\n    ) -&gt; Container:\n        \"\"\"\n        Returns a new data container with all insufficiently important columns dropped.\n\n        Args:\n            container ([`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]):\n                The container containing the data you want to use.\n\n            share_selected_columns: The share of columns\n                to keep. Must be between 0.0 and 1.0.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if isinstance(container, (StarSchema, TimeSeries)):\n            data = self.select(\n                container.container, share_selected_columns=share_selected_columns\n            )\n            new_container = deepcopy(container)\n            new_container._container = data\n            return new_container\n\n        # ------------------------------------------------------------\n\n        if not isinstance(container, Container):\n            raise TypeError(\n                \"'container' must be a getml.data.Container, \"\n                + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n            )\n\n        if not isinstance(share_selected_columns, numbers.Real):\n            raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n        if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n            raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n        # ------------------------------------------------------------\n\n        descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n        # ------------------------------------------------------------\n\n        num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n        keep_columns = descriptions[:num_keep]\n\n        # ------------------------------------------------------------\n\n        subsets = {\n            k: _drop(v, keep_columns, k, POPULATION)\n            for (k, v) in container.subsets.items()\n        }\n\n        peripheral = {\n            k: _drop(v, keep_columns, k, PERIPHERAL)\n            for (k, v) in container.peripheral.items()\n        }\n\n        # ------------------------------------------------------------\n\n        new_container = Container(**subsets)\n        new_container.add(**peripheral)\n        new_container.freeze()\n\n        # ------------------------------------------------------------\n\n        return new_container\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[Callable[[Column], Any]] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Columns:\n        \"\"\"\n        Sorts the Columns container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - table(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n            [`columns`][getml.pipeline.columns]:\n                A container of sorted columns.\n\n        Example:\n            ```python\n            by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            columns_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_columns(columns_sorted)\n\n        if by is None:\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.name, reverse=reverse\n            )\n            columns_sorted.sort(key=lambda column: column.target)\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.name, reverse=reverse\n            )\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"tables?$\", string=by):\n            columns_sorted = sorted(\n                self.data,\n                key=lambda column: column.table,\n            )\n            return self._make_columns(columns_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            columns_sorted = sorted(\n                self.data, key=lambda column: column.importance, reverse=reverse\n            )\n            return self._make_columns(columns_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n        names, markers, tables, importances, targets = (\n            self._pivot(field)\n            for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n        )\n\n        data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n        data_frame[\"name\"] = names\n\n        data_frame[\"marker\"] = markers\n\n        data_frame[\"table\"] = tables\n\n        data_frame[\"importance\"] = importances\n\n        data_frame[\"target\"] = targets\n\n        return data_frame\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.names","title":"<code>names: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s columns.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the columns container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Columns</code> <p>A container of filtered Columns.</p> Example <pre><code>important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\nperipheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n    \"\"\"\n    Filters the columns container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        A container of filtered Columns.\n\n    Example:\n        ```python\n        important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n        peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    columns_filtered = [column for column in self.data if conditional(column)]\n    return self._make_columns(columns_filtered)\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the data for the column importances.</p> <p>Column importances extend the idea of column importances to the columns originally inserted into the pipeline. Each column is assigned an importance value that measures its contribution to the predictive performance. All columns importances add up to 1.</p> <p>The importances can be calculated for columns with <code>roles</code> such as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the columns.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the column importances.\n\n    Column importances extend the idea of column importances\n    to the columns originally inserted into the pipeline.\n    Each column is assigned an importance value that measures\n    its contribution to the predictive performance. All\n    columns importances add up to 1.\n\n    The importances can be calculated for columns with\n    [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the columns.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    descriptions, importances = self._get_column_importances(\n        target_num=target_num, sort=sort\n    )\n\n    # ------------------------------------------------------------\n\n    names = np.asarray(\n        [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n    )\n\n    # ------------------------------------------------------------\n\n    return names, importances\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.select","title":"<code>select(container, share_selected_columns=0.5)</code>","text":"<p>Returns a new data container with all insufficiently important columns dropped.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>[`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]</code> <p>The container containing the data you want to use.</p> required <code>share_selected_columns</code> <code>float</code> <p>The share of columns to keep. Must be between 0.0 and 1.0.</p> <code>0.5</code> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def select(\n    self, container: Container, share_selected_columns: float = 0.5\n) -&gt; Container:\n    \"\"\"\n    Returns a new data container with all insufficiently important columns dropped.\n\n    Args:\n        container ([`Container`][getml.data.Container] or [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries]):\n            The container containing the data you want to use.\n\n        share_selected_columns: The share of columns\n            to keep. Must be between 0.0 and 1.0.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        data = self.select(\n            container.container, share_selected_columns=share_selected_columns\n        )\n        new_container = deepcopy(container)\n        new_container._container = data\n        return new_container\n\n    # ------------------------------------------------------------\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a getml.data.Container, \"\n            + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n        )\n\n    if not isinstance(share_selected_columns, numbers.Real):\n        raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n    if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n        raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n    # ------------------------------------------------------------\n\n    descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n    # ------------------------------------------------------------\n\n    num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n    keep_columns = descriptions[:num_keep]\n\n    # ------------------------------------------------------------\n\n    subsets = {\n        k: _drop(v, keep_columns, k, POPULATION)\n        for (k, v) in container.subsets.items()\n    }\n\n    peripheral = {\n        k: _drop(v, keep_columns, k, PERIPHERAL)\n        for (k, v) in container.peripheral.items()\n    }\n\n    # ------------------------------------------------------------\n\n    new_container = Container(**subsets)\n    new_container.add(**peripheral)\n    new_container.freeze()\n\n    # ------------------------------------------------------------\n\n    return new_container\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Columns container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - table(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Columns</code> <p><code>columns</code>: A container of sorted columns.</p> Example <pre><code>by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns:\n    \"\"\"\n    Sorts the Columns container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - table(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n        [`columns`][getml.pipeline.columns]:\n            A container of sorted columns.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        columns_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_columns(columns_sorted)\n\n    if by is None:\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        columns_sorted.sort(key=lambda column: column.target)\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"tables?$\", string=by):\n        columns_sorted = sorted(\n            self.data,\n            key=lambda column: column.table,\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.importance, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.columns.Columns.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the columns in a pandas data frame.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n    names, markers, tables, importances, targets = (\n        self._pivot(field)\n        for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n    )\n\n    data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n    data_frame[\"name\"] = names\n\n    data_frame[\"marker\"] = markers\n\n    data_frame[\"table\"] = tables\n\n    data_frame[\"importance\"] = importances\n\n    data_frame[\"target\"] = targets\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/dialect/","title":"Dialect","text":"<p>SQL dialects that can be used for the generated code.</p> <p>One way to productionize a <code>Pipeline</code> is to transpile its features to production-ready SQL code. This SQL code can be run on standard cloud infrastructure. Please also refer to <code>SQLCode</code>.</p> Example <pre><code>sql_code = my_pipeline.features.to_sql(\n    getml.pipeline.dialect.spark_sql)\n\n# Creates a folder called \"my_pipeline\"\n# which contains the SQL scripts.\nsql_code.save(\"my_pipeline\")\n</code></pre>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.bigquery","title":"<code>bigquery = _all_dialects[0]</code>  <code>module-attribute</code>","text":"<p>BigQuery is a proprietary database system used by the Google Cloud.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.human_readable_sql","title":"<code>human_readable_sql = _all_dialects[1]</code>  <code>module-attribute</code>","text":"<p>SQL that is not meant to be executed, but for interpretation by humans.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.mysql","title":"<code>mysql = _all_dialects[2]</code>  <code>module-attribute</code>","text":"<p>MySQL and its fork MariaDB are among the most popular open-source database systems.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.postgres","title":"<code>postgres = _all_dialects[3]</code>  <code>module-attribute</code>","text":"<p>The PostgreSQL or postgres dialect is a popular SQL dialect used by PostgreSQL and its many derivatives like Redshift or Greenplum.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.spark_sql","title":"<code>spark_sql = _all_dialects[4]</code>  <code>module-attribute</code>","text":"<p>Spark SQL is the SQL dialect used by Apache Spark.</p> <p>Apache Spark is an open-source, distributed, in-memory engine for large-scale data processing and a popular choice for producutionizing machine learning pipelines.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.sqlite3","title":"<code>sqlite3 = _all_dialects[5]</code>  <code>module-attribute</code>","text":"<p>SQLite3 is a light-weight and widely used database system.</p> <p>It is recommended for live prediction systems or when the amount of data handled is unlikely to be too large.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.tsql","title":"<code>tsql = _all_dialects[6]</code>  <code>module-attribute</code>","text":"<p>TSQL or Transact-SQL is the dialect used by most Microsoft databases.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/feature/","title":"Feature","text":"<p>Custom representing a sole feature.</p>"},{"location":"reference/pipeline/feature/#getml.pipeline.feature.Feature","title":"<code>Feature</code>  <code>dataclass</code>","text":"<p>Dataclass that holds data about a single feature.</p> Source code in <code>getml/pipeline/feature.py</code> <pre><code>@dataclass\nclass Feature:\n    \"\"\"\n    Dataclass that holds data about a single feature.\n    \"\"\"\n\n    index: int\n    name: str\n    pipeline: str\n    target: str\n    targets: Sequence[str]\n    importance: float\n    correlation: float\n    sql: SQLString\n</code></pre>"},{"location":"reference/pipeline/features/","title":"Features","text":"<p>Container for the features associated with a pipeline.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features","title":"<code>Features</code>","text":"<p>Container which holds a pipeline's features. Features can be accessed by name, index or with a numpy array. The container supports slicing and is sort- and filterable.</p> <p>Further, the container holds global methods to request features' importances, correlations and their respective transpiled sql representation.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_features = my_pipeline.features\n\nfirst_feature = my_pipeline.features[0]\n\nsecond_feature = my_pipeline.features[\"feature_1_2\"]\n\nall_but_last_10_features = my_pipeline.features[:-10]\n\nimportant_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\nnames, importances = my_pipeline.features.importances()\n\nnames, correlations = my_pipeline.features.correlations()\n\nsql_code = my_pipeline.features.to_sql()\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>class Features:\n    \"\"\"\n    Container which holds a pipeline's features. Features can be accessed\n    by name, index or with a numpy array. The container supports slicing and\n    is sort- and filterable.\n\n    Further, the container holds global methods to request features' importances,\n    correlations and their respective transpiled sql representation.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Features.filter] you can also use python list\n        comprehensions for filtering.\n\n    Example:\n        ```python\n        all_my_features = my_pipeline.features\n\n        first_feature = my_pipeline.features[0]\n\n        second_feature = my_pipeline.features[\"feature_1_2\"]\n\n        all_but_last_10_features = my_pipeline.features[:-10]\n\n        important_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\n        names, importances = my_pipeline.features.importances()\n\n        names, correlations = my_pipeline.features.correlations()\n\n        sql_code = my_pipeline.features.to_sql()\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(\n        self,\n        pipeline: str,\n        targets: Sequence[str],\n        data: Optional[Sequence[Feature]] = None,\n    ) -&gt; None:\n        if not isinstance(pipeline, str):\n            raise ValueError(\"'pipeline' must be a str.\")\n\n        if not _is_typed_list(targets, str):\n            raise TypeError(\"'targets' must be a list of str.\")\n\n        self.pipeline = pipeline\n\n        self.targets = targets\n\n        if data is None:\n            self.data = self._load_features()\n\n        else:\n            self.data = list(data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[int, slice, str, NDArray[np.int_]]\n    ) -&gt; Union[Feature, Features, Sequence[Feature]]:\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            return self._make_features(self.data[key])\n        if isinstance(key, str):\n            if key in self.names:\n                return [feature for feature in self.data if feature.name == key][0]\n            raise AttributeError(f\"No Feature with name: {key}\")\n        if isinstance(key, np.ndarray):\n            features_subset = np.array(self.data)[key].tolist()\n            return features_subset\n        raise TypeError(\n            f\"Features can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Feature]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def _pivot(self, field: str) -&gt; List[Any]:\n        \"\"\"\n        Pivots the data for a given field. Returns a list of values of the field's type.\n        \"\"\"\n        return [getattr(feature, field) for feature in self.data]\n\n    # ----------------------------------------------------------------\n\n    def _load_features(self) -&gt; List[Feature]:\n        \"\"\"\n        Loads the actual feature data from the engine.\n        \"\"\"\n        features = []\n\n        for target_num, target in enumerate(self.targets):\n            names = self.correlations(target_num, sort=False)[0].tolist()\n            indices = range(len(names))\n            correlations = _attach_empty(\n                self.correlations(target_num, sort=False)[1].tolist(),\n                len(names),\n                np.NaN,\n            )\n            importances = _attach_empty(\n                self.importances(target_num, sort=False)[1].tolist(), len(names), np.NaN\n            )\n            sql_transpilations = _attach_empty(\n                self.to_sql(subfeatures=False).code[:-1], len(names), \"\"\n            )\n\n            features.extend(\n                [\n                    Feature(\n                        index=index,\n                        name=names[index],\n                        pipeline=self.pipeline,\n                        target=target,\n                        targets=self.targets,\n                        importance=importances[index],\n                        correlation=correlations[index],\n                        sql=SQLString(sql_transpilations[index]),\n                    )\n                    for index in indices\n                ]\n            )\n        return features\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        rows = [\n            [\n                feature.target,\n                feature.name,\n                feature.correlation,\n                feature.importance,\n            ]\n            for feature in self.data\n        ]\n\n        headers = [[\"target\", \"name\", \"correlation\", \"importance\"]]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _make_features(self, data: Sequence[Feature]) -&gt; Features:\n        \"\"\"\n        A factory to construct a `Features` container from a list of\n        sole `Feature`s.\n        \"\"\"\n        return Features(self.pipeline, self.targets, data)\n\n    # ----------------------------------------------------------------\n\n    def _to_pandas(self) -&gt; pd.DataFrame:\n        names, correlations, importances, sql, target = (\n            self._pivot(field)\n            for field in [\"name\", \"correlation\", \"importance\", \"sql\", \"target\"]\n        )\n\n        data_frame = pd.DataFrame(index=range(len(names)))\n\n        data_frame[\"names\"] = names\n\n        data_frame[\"correlations\"] = correlations\n\n        data_frame[\"importances\"] = importances\n\n        data_frame[\"target\"] = target\n\n        data_frame[\"sql\"] = sql\n\n        return data_frame\n\n    # ----------------------------------------------------------------\n\n    @property\n    def correlation(self) -&gt; List[float]:\n        \"\"\"\n        Holds the correlations of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the correlations.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"correlation\")\n\n    # ------------------------------------------------------------\n\n    def correlations(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the feature correlations,\n        as displayed in the getML monitor.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the features.\n            The second array contains the correlations with the target.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        names = np.asarray(json_obj[\"feature_names_\"])\n        correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n        assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n        if hasattr(self, \"data\"):\n            indices = np.asarray(\n                [\n                    feature.index\n                    for feature in self.data\n                    if feature.target == self.targets[target_num]\n                    and feature.index &lt; len(correlations)\n                ]\n            )\n\n            names = names[indices]\n            correlations = correlations[indices]\n\n        if not sort:\n            return names, correlations\n\n        indices = np.argsort(np.abs(correlations))[::-1]\n\n        return (names[indices], correlations[indices])\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n        \"\"\"\n         Filters the Features container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            [`Features`][getml.pipeline.Features]:\n                A container of filtered Features.\n\n        Example:\n            ```python\n            important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n            correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n            ```\n        \"\"\"\n        features_filtered = [feature for feature in self.data if conditional(feature)]\n        return Features(self.pipeline, self.targets, data=features_filtered)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def importance(self) -&gt; List[float]:\n        \"\"\"\n         Holds the correlations of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the correlations.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"importance\")\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the data for the feature importances,\n        as displayed in the getML monitor.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances.\n                (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns\n            The first array contains the names of the features.\n            The second array contains their importances. By definition, all importances add up to 1.\n\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.feature_importances\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        names = np.asarray(json_obj[\"feature_names_\"])\n        importances = np.asarray(json_obj[\"feature_importances_\"])\n\n        if hasattr(self, \"data\"):\n            assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n            indices = np.asarray(\n                [\n                    feature.index\n                    for feature in self.data\n                    if feature.target == self.targets[target_num]\n                    and feature.index &lt; len(importances)\n                ]\n            )\n\n            names = names[indices]\n            importances = importances[indices]\n\n        if not sort:\n            return names, importances\n\n        assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n        indices = np.argsort(importances)[::-1]\n\n        return (names[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def name(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"name\")\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; List[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s features.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return self._pivot(\"name\")\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[\n            Callable[\n                [Feature],\n                Union[\n                    float,\n                    int,\n                    str,\n                ],\n            ]\n        ] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Features:\n        \"\"\"\n        Sorts the Features container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - correlation(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n            [`Features`][getml.pipeline.Features]:\n                A container of sorted Features.\n\n        Example:\n            ```python\n            by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n            by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            features_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_features(features_sorted)\n\n        else:\n            if by is None:\n                features_sorted = sorted(\n                    self.data, key=lambda feature: feature.index, reverse=reverse\n                )\n                features_sorted.sort(key=lambda feature: feature.target)\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"names?$\", string=by):\n                features_sorted = sorted(\n                    self.data, key=lambda feature: feature.name, reverse=reverse\n                )\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"correlations?$\", string=by):\n                reverse = True if descending is None else descending\n                features_sorted = sorted(\n                    self.data,\n                    key=lambda feature: abs(feature.correlation),\n                    reverse=reverse,\n                )\n                return self._make_features(features_sorted)\n\n            if re.match(pattern=\"importances?$\", string=by):\n                reverse = True if descending is None else descending\n                features_sorted = sorted(\n                    self.data,\n                    key=lambda feature: feature.importance,\n                    reverse=reverse,\n                )\n                return self._make_features(features_sorted)\n\n            raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns all information related to the features in a pandas data frame.\n        \"\"\"\n\n        return self._to_pandas()\n\n    # ----------------------------------------------------------------\n\n    def to_sql(\n        self,\n        targets: bool = True,\n        subfeatures: bool = True,\n        dialect: str = sqlite3,\n        schema: Optional[str] = None,\n        nchar_categorical: int = 128,\n        nchar_join_key: int = 128,\n        nchar_text: int = 4096,\n        size_threshold: Optional[int] = 50000,\n    ) -&gt; SQLCode:\n        \"\"\"\n        Returns SQL statements visualizing the features.\n\n        Args:\n            targets (boolean):\n                Whether you want to include the target columns\n                in the main table.\n\n            subfeatures (boolean):\n                Whether you want to include the code for the\n                subfeatures of a snowflake schema.\n\n            dialect (string):\n                The SQL dialect to use. Must be from\n                [`dialect`][getml.pipeline.dialect]. Please\n                note that not all dialects are supported\n                in the getML community edition.\n\n            schema (string, optional):\n                The schema in which to wrap all generated tables and\n                indices. None for no schema. Not applicable to all dialects.\n                For the BigQuery and MySQL dialects, the schema is identical\n                to the database ID.\n\n            nchar_categorical (int):\n                The maximum number of characters used in the\n                VARCHAR for categorical columns. Not applicable\n                to all dialects.\n\n            nchar_join_key (int):\n                The maximum number of characters used in the\n                VARCHAR for join keys. Not applicable\n                to all dialects.\n\n            nchar_text (int):\n                The maximum number of characters used in the\n                VARCHAR for text columns. Not applicable\n                to all dialects.\n\n            size_threshold (int, optional):\n                The maximum number of characters to display\n                in a single feature. Displaying extremely\n                complicated features can crash your iPython\n                notebook or lead to unexpectedly high memory\n                consumption, which is why a reasonable\n                upper limit is advantageous. Set to None\n                for no upper limit.\n\n        Examples:\n            ```python\n            my_pipeline.features.to_sql()\n            ```\n        Returns:\n            [`SQLCode`][getml.pipeline.SQLCode]\n                Object representing the features.\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can hold trained\n            features which can be returned as SQL statements.\n\n        Note:\n            The getML community edition only supports\n            transpilation to human-readable SQL. Passing\n            'sqlite3' will also produce human-readable SQL.\n\n        \"\"\"\n\n        if not isinstance(targets, bool):\n            raise TypeError(\"'targets' must be a bool!\")\n\n        if not isinstance(subfeatures, bool):\n            raise TypeError(\"'subfeatures' must be a bool!\")\n\n        if not isinstance(dialect, str):\n            raise TypeError(\"'dialect' must be a string!\")\n\n        if not isinstance(nchar_categorical, int):\n            raise TypeError(\"'nchar_categorical' must be an int!\")\n\n        if not isinstance(nchar_join_key, int):\n            raise TypeError(\"'nchar_join_key' must be an int!\")\n\n        if not isinstance(nchar_text, int):\n            raise TypeError(\"'nchar_text' must be an int!\")\n\n        if dialect not in _all_dialects:\n            raise ValueError(\n                \"'dialect' must from getml.pipeline.dialect, \"\n                + \"meaning that is must be one of the following: \"\n                + str(_all_dialects)\n                + \".\"\n            )\n\n        if size_threshold is not None and not isinstance(size_threshold, int):\n            raise TypeError(\"'size_threshold' must be an int or None!\")\n\n        if size_threshold is not None and size_threshold &lt;= 0:\n            raise ValueError(\"'size_threshold' must be a positive number!\")\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.to_sql\"\n        cmd[\"name_\"] = self.pipeline\n\n        cmd[\"targets_\"] = targets\n        cmd[\"subfeatures_\"] = subfeatures\n        cmd[\"dialect_\"] = dialect\n        cmd[\"schema_\"] = schema or \"\"\n        cmd[\"nchar_categorical_\"] = nchar_categorical\n        cmd[\"nchar_join_key_\"] = nchar_join_key\n        cmd[\"nchar_text_\"] = nchar_text\n\n        if size_threshold is not None:\n            cmd[\"size_threshold_\"] = size_threshold\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            sql = comm.recv_string(sock)\n\n        return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.correlation","title":"<code>correlation: List[float]</code>  <code>property</code>","text":"<p>Holds the correlations of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[float]</code> <p><code>list</code> containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.importance","title":"<code>importance: List[float]</code>  <code>property</code>","text":"<p>Holds the correlations of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[float]</code> <p><code>list</code> containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.name","title":"<code>name: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.names","title":"<code>names: List[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s features.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.correlations","title":"<code>correlations(target_num=0, sort=True)</code>","text":"<p>Returns the data for the feature correlations, as displayed in the getML monitor.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains the correlations with the target.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def correlations(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature correlations,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains the correlations with the target.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n    assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n    if hasattr(self, \"data\"):\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(correlations)\n            ]\n        )\n\n        names = names[indices]\n        correlations = correlations[indices]\n\n    if not sort:\n        return names, correlations\n\n    indices = np.argsort(np.abs(correlations))[::-1]\n\n    return (names[indices], correlations[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the Features container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Features</code> <p><code>Features</code>: A container of filtered Features.</p> Example <pre><code>important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\ncorrelated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n    \"\"\"\n     Filters the Features container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        [`Features`][getml.pipeline.Features]:\n            A container of filtered Features.\n\n    Example:\n        ```python\n        important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n        correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n        ```\n    \"\"\"\n    features_filtered = [feature for feature in self.data if conditional(feature)]\n    return Features(self.pipeline, self.targets, data=features_filtered)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the data for the feature importances, as displayed in the getML monitor.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns     The first array contains the names of the features.     The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature importances,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns\n        The first array contains the names of the features.\n        The second array contains their importances. By definition, all importances add up to 1.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_importances\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    importances = np.asarray(json_obj[\"feature_importances_\"])\n\n    if hasattr(self, \"data\"):\n        assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(importances)\n            ]\n        )\n\n        names = names[indices]\n        importances = importances[indices]\n\n    if not sort:\n        return names, importances\n\n    assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Features container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - correlation(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> Return <p><code>Features</code>:     A container of sorted Features.</p> Example <pre><code>by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\nby_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[\n            [Feature],\n            Union[\n                float,\n                int,\n                str,\n            ],\n        ]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features:\n    \"\"\"\n    Sorts the Features container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - correlation(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n        [`Features`][getml.pipeline.Features]:\n            A container of sorted Features.\n\n    Example:\n        ```python\n        by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n        by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        features_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_features(features_sorted)\n\n    else:\n        if by is None:\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.index, reverse=reverse\n            )\n            features_sorted.sort(key=lambda feature: feature.target)\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.name, reverse=reverse\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"correlations?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: abs(feature.correlation),\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: feature.importance,\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the features in a pandas data frame.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the features in a pandas data frame.\n    \"\"\"\n\n    return self._to_pandas()\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.features.Features.to_sql","title":"<code>to_sql(targets=True, subfeatures=True, dialect=sqlite3, schema=None, nchar_categorical=128, nchar_join_key=128, nchar_text=4096, size_threshold=50000)</code>","text":"<p>Returns SQL statements visualizing the features.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>boolean</code> <p>Whether you want to include the target columns in the main table.</p> <code>True</code> <code>subfeatures</code> <code>boolean</code> <p>Whether you want to include the code for the subfeatures of a snowflake schema.</p> <code>True</code> <code>dialect</code> <code>string</code> <p>The SQL dialect to use. Must be from <code>dialect</code>. Please note that not all dialects are supported in the getML community edition.</p> <code>sqlite3</code> <code>schema</code> <code>string</code> <p>The schema in which to wrap all generated tables and indices. None for no schema. Not applicable to all dialects. For the BigQuery and MySQL dialects, the schema is identical to the database ID.</p> <code>None</code> <code>nchar_categorical</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for categorical columns. Not applicable to all dialects.</p> <code>128</code> <code>nchar_join_key</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for join keys. Not applicable to all dialects.</p> <code>128</code> <code>nchar_text</code> <code>int</code> <p>The maximum number of characters used in the VARCHAR for text columns. Not applicable to all dialects.</p> <code>4096</code> <code>size_threshold</code> <code>int</code> <p>The maximum number of characters to display in a single feature. Displaying extremely complicated features can crash your iPython notebook or lead to unexpectedly high memory consumption, which is why a reasonable upper limit is advantageous. Set to None for no upper limit.</p> <code>50000</code> <p>Examples:</p> <pre><code>my_pipeline.features.to_sql()\n</code></pre> <p>Returns:     <code>SQLCode</code>         Object representing the features.</p> Note <p>Only fitted pipelines (<code>fit</code>) can hold trained features which can be returned as SQL statements.</p> Note <p>The getML community edition only supports transpilation to human-readable SQL. Passing 'sqlite3' will also produce human-readable SQL.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_sql(\n    self,\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode:\n    \"\"\"\n    Returns SQL statements visualizing the features.\n\n    Args:\n        targets (boolean):\n            Whether you want to include the target columns\n            in the main table.\n\n        subfeatures (boolean):\n            Whether you want to include the code for the\n            subfeatures of a snowflake schema.\n\n        dialect (string):\n            The SQL dialect to use. Must be from\n            [`dialect`][getml.pipeline.dialect]. Please\n            note that not all dialects are supported\n            in the getML community edition.\n\n        schema (string, optional):\n            The schema in which to wrap all generated tables and\n            indices. None for no schema. Not applicable to all dialects.\n            For the BigQuery and MySQL dialects, the schema is identical\n            to the database ID.\n\n        nchar_categorical (int):\n            The maximum number of characters used in the\n            VARCHAR for categorical columns. Not applicable\n            to all dialects.\n\n        nchar_join_key (int):\n            The maximum number of characters used in the\n            VARCHAR for join keys. Not applicable\n            to all dialects.\n\n        nchar_text (int):\n            The maximum number of characters used in the\n            VARCHAR for text columns. Not applicable\n            to all dialects.\n\n        size_threshold (int, optional):\n            The maximum number of characters to display\n            in a single feature. Displaying extremely\n            complicated features can crash your iPython\n            notebook or lead to unexpectedly high memory\n            consumption, which is why a reasonable\n            upper limit is advantageous. Set to None\n            for no upper limit.\n\n    Examples:\n        ```python\n        my_pipeline.features.to_sql()\n        ```\n    Returns:\n        [`SQLCode`][getml.pipeline.SQLCode]\n            Object representing the features.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can hold trained\n        features which can be returned as SQL statements.\n\n    Note:\n        The getML community edition only supports\n        transpilation to human-readable SQL. Passing\n        'sqlite3' will also produce human-readable SQL.\n\n    \"\"\"\n\n    if not isinstance(targets, bool):\n        raise TypeError(\"'targets' must be a bool!\")\n\n    if not isinstance(subfeatures, bool):\n        raise TypeError(\"'subfeatures' must be a bool!\")\n\n    if not isinstance(dialect, str):\n        raise TypeError(\"'dialect' must be a string!\")\n\n    if not isinstance(nchar_categorical, int):\n        raise TypeError(\"'nchar_categorical' must be an int!\")\n\n    if not isinstance(nchar_join_key, int):\n        raise TypeError(\"'nchar_join_key' must be an int!\")\n\n    if not isinstance(nchar_text, int):\n        raise TypeError(\"'nchar_text' must be an int!\")\n\n    if dialect not in _all_dialects:\n        raise ValueError(\n            \"'dialect' must from getml.pipeline.dialect, \"\n            + \"meaning that is must be one of the following: \"\n            + str(_all_dialects)\n            + \".\"\n        )\n\n    if size_threshold is not None and not isinstance(size_threshold, int):\n        raise TypeError(\"'size_threshold' must be an int or None!\")\n\n    if size_threshold is not None and size_threshold &lt;= 0:\n        raise ValueError(\"'size_threshold' must be a positive number!\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.to_sql\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"targets_\"] = targets\n    cmd[\"subfeatures_\"] = subfeatures\n    cmd[\"dialect_\"] = dialect\n    cmd[\"schema_\"] = schema or \"\"\n    cmd[\"nchar_categorical_\"] = nchar_categorical\n    cmd[\"nchar_join_key_\"] = nchar_join_key\n    cmd[\"nchar_text_\"] = nchar_text\n\n    if size_threshold is not None:\n        cmd[\"size_threshold_\"] = size_threshold\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        sql = comm.recv_string(sock)\n\n    return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/helpers/","title":"Helpers","text":"<p>Collection of helper functions not intended to be used by the end-user.</p>"},{"location":"reference/pipeline/helpers/#getml.pipeline.helpers.PERIPHERAL","title":"<code>PERIPHERAL = '[PERIPHERAL]'</code>  <code>module-attribute</code>","text":"<p>Peripheral marker - the names of the population and peripheral tables may overlap, so markers are necessary.</p>"},{"location":"reference/pipeline/helpers/#getml.pipeline.helpers.POPULATION","title":"<code>POPULATION = '[POPULATION]'</code>  <code>module-attribute</code>","text":"<p>Population marker - the names of the population and peripheral tables may overlap, so markers are necessary.</p>"},{"location":"reference/pipeline/helpers2/","title":"Helpers2","text":"<p>Helper functions that depend on Pipeline.</p>"},{"location":"reference/pipeline/helpers2/#getml.pipeline.helpers2.delete","title":"<code>delete(name)</code>","text":"<p>If a pipeline named 'name' exists, it is deleted.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline.</p> required Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a pipeline named 'name' exists, it is deleted.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        _make_dummy(name).delete()\n</code></pre>"},{"location":"reference/pipeline/helpers2/#getml.pipeline.helpers2.exists","title":"<code>exists(name)</code>","text":"<p>Returns true if a pipeline named 'name' exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline.</p> required Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"\n    Returns true if a pipeline named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_pipelines = list_pipelines()\n\n    return name in all_pipelines\n</code></pre>"},{"location":"reference/pipeline/helpers2/#getml.pipeline.helpers2.list_pipelines","title":"<code>list_pipelines()</code>","text":"<p>Lists all pipelines present in the engine.</p> <p>Note that this function only lists pipelines which are part of the current project. See <code>set_project</code> for changing projects and <code>pipelines</code> for more details about the lifecycles of the pipelines.</p> <p>To subsequently load one of them, use <code>load</code>.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list containing the names of all pipelines.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def list_pipelines() -&gt; List[str]:\n    \"\"\"Lists all pipelines present in the engine.\n\n    Note that this function only lists pipelines which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects and [`pipelines`][getml.pipeline] for more details about\n    the lifecycles of the pipelines.\n\n    To subsequently load one of them, use\n    [`load`][getml.pipeline.load].\n\n    Returns:\n        list containing the names of all pipelines.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"list_pipelines\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/pipeline/helpers2/#getml.pipeline.helpers2.load","title":"<code>load(name)</code>","text":"<p>Loads a pipeline from the getML engine into Python.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the pipeline to be loaded.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>A <code>Pipeline</code> that is a handler</p> <code>Pipeline</code> <p>for the pipeline signified by name.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def load(name: str) -&gt; Pipeline:\n    \"\"\"Loads a pipeline from the getML engine into Python.\n\n    Args:\n        name: The name of the pipeline to be loaded.\n\n    Returns:\n        A [`Pipeline`][getml.Pipeline] that is a handler\n        for the pipeline signified by name.\n    \"\"\"\n\n    return _make_dummy(name).refresh()\n</code></pre>"},{"location":"reference/pipeline/issues/","title":"Issues","text":"<p>A helper class meant to represent the warnings generated by the check method of the pipeline.</p>"},{"location":"reference/pipeline/issues/#getml.pipeline.issues.Issues","title":"<code>Issues</code>","text":"<p>A helper class meant to represent the warnings generated by the check method of the pipeline.</p> Source code in <code>getml/pipeline/issues.py</code> <pre><code>class Issues:\n    \"\"\"\n    A helper class meant to represent the\n    warnings generated by the check method of\n    the pipeline.\n    \"\"\"\n\n    def __init__(self, data: Sequence[_Issue]) -&gt; None:\n        self.data = data\n\n    def __iter__(self):\n        yield from self.data\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    def _format(self) -&gt; _Formatter:\n        headers = [\"type\", \"label\", \"message\"]\n        rows = [[w.warning_type, w.label, w.message] for w in self.data]\n        return _Formatter([headers], rows)\n\n    @property\n    def info(self) -&gt; \"Issues\":\n        \"\"\"\n        Returns a new container with all warnings\n        labeled INFO.\n        \"\"\"\n        return Issues([d for d in self.data if d.warning_type == \"INFO\"])\n\n    @property\n    def warnings(self) -&gt; \"Issues\":\n        \"\"\"\n        Returns a new container with all warnings\n        labeled WARNING.\n        \"\"\"\n        return Issues([d for d in self.data if d.warning_type == \"WARNING\"])\n</code></pre>"},{"location":"reference/pipeline/issues/#getml.pipeline.issues.Issues.info","title":"<code>info: Issues</code>  <code>property</code>","text":"<p>Returns a new container with all warnings labeled INFO.</p>"},{"location":"reference/pipeline/issues/#getml.pipeline.issues.Issues.warnings","title":"<code>warnings: Issues</code>  <code>property</code>","text":"<p>Returns a new container with all warnings labeled WARNING.</p>"},{"location":"reference/pipeline/metadata/","title":"Metadata","text":"<p>Contains the metadata related to the data frames that were originally passed to .fit(...).</p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata","title":"<code>AllMetadata</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to all the data frames that were originally passed to .fit(...).</p> Source code in <code>getml/pipeline/metadata.py</code> <pre><code>class AllMetadata(NamedTuple):\n    \"\"\"\n    Contains the metadata related\n    to all the data frames that\n    were originally passed to .fit(...).\n    \"\"\"\n\n    peripheral: List[Metadata]\n    population: Metadata\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata","title":"<code>Metadata</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to a data frame that were originally passed to .fit(...).</p> Source code in <code>getml/pipeline/metadata.py</code> <pre><code>class Metadata(NamedTuple):\n    \"\"\"\n    Contains the metadata related\n    to a data frame that\n    were originally passed to .fit(...).\n    \"\"\"\n\n    name: str\n    roles: Roles\n</code></pre>"},{"location":"reference/pipeline/metrics/","title":"Metrics","text":"<p>Signifies different scoring methods.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.accuracy","title":"<code>accuracy = _all_metrics[1]</code>  <code>module-attribute</code>","text":"<p>Accuracy - measures the share of accurate predictions as of total samples in the testing set.</p> <p>Used for classification problems.</p> \\[ accuracy = \\frac{number \\; of \\; correct \\; predictions}{number \\; of \\; all \\; predictions} \\] <p>The number of correct predictions depends on the threshold used: For instance, we could interpret all predictions for which the probability is greater than 0.5 as a positive and all others as a negative. But we do not have to use a threshold of 0.5 - we might as well use any other threshold. Which threshold we choose will impact the calculated accuracy.</p> <p>When calculating the accuracy, the value returned is the accuracy returned by the best threshold.</p> <p>Even though accuracy is the most intuitive way to measure a classification algorithm, it can also be very misleading when the samples are very skewed. For instance, if only 2% of the samples are positive, a predictor that always predicts negative outcomes will have an accuracy of 98%. This sounds very good to the layman, but the predictor in this example actually has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.auc","title":"<code>auc = _all_metrics[0]</code>  <code>module-attribute</code>","text":"<p>Area under the curve - refers to the area under the receiver operating characteristic (ROC) curve.</p> <p>Used for classification problems.</p> <p>When handling a classification problem, the ROC curve maps the relationship between two conflicting goals:</p> <p>On the hand, we want a high true positive rate. The true positive rate, sometimes referred to as recall, measures the share of true positive predictions over all positives:</p> \\[ TPR = \\frac{number \\; of \\; true \\; positives}{number \\; of \\; all \\; positives} \\] <p>In other words, we want our classification algorithm to \"catch\" as many positives as possible.</p> <p>On the other hand, we also want a low false positive rate (FPR). The false positive rate measures the share of false positives over all negatives.</p> \\[ FPR = \\frac{number \\; of \\; false \\; positives}{number \\; of \\; all \\; negatives} \\] <p>In other words, we want as few \"false alarms\" as possible.</p> <p>However, unless we have a perfect classifier, these two goals conflict with each other.</p> <p>The ROC curve maps the TPR against the FPR. We now measure the area under said curve (AUC). A higher AUC implies that the trade-off between TPR and FPR is more beneficial. A perfect model would have an AUC of 1. An AUC of 0.5 implies that the model has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.cross_entropy","title":"<code>cross_entropy = _all_metrics[2]</code>  <code>module-attribute</code>","text":"<p>Cross entropy, also referred to as log-loss, is a measure of the likelihood of the classification model.</p> <p>Used for classification problems.</p> <p>Mathematically speaking, cross-entropy for a binary classification problem is defined as follows:</p> \\[ cross \\; entropy = - \\frac{1}{N} \\sum_{i}^{N} (y_i \\log p_i + (1 - y_i) \\log(1 - p_i), \\] <p>where \\(p_i\\) is the probability of a positive outcome as predicted by the classification algorithm and \\(y_i\\) is the target value, which is 1 for a positive outcome and 0 otherwise.</p> <p>There are several ways to justify the use of cross entropy to evaluate classification algorithms. But the most intuitive way is to think of it as a measure of likelihood. When we have a classification algorithm that gives us probabilities, we would like to know how likely it is that we observe a particular state of the world given the probabilities.</p> <p>We can calculate this likelihood as follows:</p> \\[ likelihood = \\prod_{i}^{N} (p_i^{y_i} * (1 - p_i)^{1 - y_i}). \\] <p>(Recall that \\(y_i\\) can only be 0 or 1.)</p> <p>If we take the logarithm of the likelihood as defined above, divide by \\(N\\) and then multiply by <code>-1</code> (because we want lower to mean better and 0 to mean perfect), the outcome will be cross entropy.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.mae","title":"<code>mae = _all_metrics[3]</code>  <code>module-attribute</code>","text":"<p>Mean Absolute Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ MAE = \\frac{\\sum_{i=1}^n | \\mathbf{y}_i - \\mathbf{\\hat{y}}_i |}{n}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rmse","title":"<code>rmse = _all_metrics[4]</code>  <code>module-attribute</code>","text":"<p>Root Mean Squared Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^n ( \\mathbf{y}_i - \\mathbf{\\hat{y}}_i )^2}{n}}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rsquared","title":"<code>rsquared = _all_metrics[5]</code>  <code>module-attribute</code>","text":"<p>\\(R^{2}\\) - squared correlation coefficient between predictions and targets.</p> <p>Used for regression problems.</p> <p>\\(R^{2}\\) is defined as follows:</p> \\[ R^{2} = \\frac{(\\sum_{i=1}^n ( y_i - \\bar{y_i} ) *  ( \\hat{y_i} - \\bar{\\hat{y_i}} ))^2 }{\\sum_{i=1}^n ( y_i - \\bar{y_i} )^2 \\sum_{i=1}^n ( \\hat{y_i} - \\bar{\\hat{y_i}} )^2 }, \\] <p>where \\(y_i\\) are the true values, \\(\\hat{y_i}\\) are the predictions and \\(\\bar{...}\\) denotes the mean operator.</p> <p>An \\(R^{2}\\) of 1 implies perfect correlation between the predictions and the targets and an \\(R^{2}\\) of 0 implies no correlation at all.</p>"},{"location":"reference/pipeline/pipeline/","title":"Pipeline","text":"<p>This submodule contains the Pipeline, which is the main class for feature learning and prediction.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>A Pipeline is the main class for feature learning and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>Optional[DataModel]</code> <p>Abstract representation of the data_model, which defines the abstract relationships between the tables. Required for the feature learners.</p> <code>None</code> <code>peripheral</code> <code>Optional[List[Placeholder]]</code> <p>Abstract representations of the additional tables used to augment the information provided in <code>population</code>. These have to be the same objects that were <code>join</code> ed onto the <code>population</code> <code>Placeholder</code>. Their order determines the order of the peripheral <code>DataFrame</code> passed to the 'peripheral_tables' argument in <code>check</code>, <code>fit</code>, <code>predict</code>, <code>score</code>, and <code>transform</code>, if you pass the data frames as a list. If you omit the peripheral placeholders, they will be inferred from the data model and ordered alphabetically.</p> <code>None</code> <code>preprocessors</code> <code>Optional[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter, List[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter]]]]</code> <p>The preprocessor(s) to be used. Must be from <code>preprocessors</code>. A single preprocessor does not have to be wrapped in a list.</p> <code>None</code> <code>feature_learners</code> <code>Optional[Union[Union[Fastboost, FastProp, Multirel, Relboost, RelMT], List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]]]]</code> <p>The feature learner(s) to be used. Must be from <code>feature_learning</code>. A single feature learner does not have to be wrapped in a list.</p> <code>None</code> <code>feature_selectors</code> <code>Optional[Union[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor], List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> <p>Predictor(s) used to select the best features. Must be from <code>predictors</code>. A single feature selector does not have to be wrapped in a list. Make sure to also set share_selected_features.</p> <code>None</code> <code>predictors</code> <code>Optional[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor, List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> <p>Predictor(s) used to generate the predictions. If more than one predictor is passed, the predictions generated will be averaged. Must be from <code>predictors</code>. A single predictor does not have to be wrapped in a list.</p> <code>None</code> <code>loss_function</code> <code>Optional[str]</code> <p>The loss function to use for the feature learners.</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>Tags exist to help you organize your pipelines. You can add any tags that help you remember what you were trying to do.</p> <code>None</code> <code>include_categorical</code> <code>bool</code> <p>Whether you want to pass categorical columns in the population table to the predictor.</p> <code>False</code> <code>share_selected_features</code> <code>float</code> <p>The share of features you want the feature selection to keep. When set to 0.0, then all features will be kept.</p> <code>0.5</code> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>:</p> <p>Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> <p>Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the __repr__ method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>class Pipeline:\n    \"\"\"\n    A Pipeline is the main class for feature learning and prediction.\n\n    Args:\n        data_model:\n            Abstract representation of the data_model,\n            which defines the abstract relationships between the tables.\n            Required for the feature learners.\n\n        peripheral:\n            Abstract representations of the additional tables used to\n            augment the information provided in `population`. These\n            have to be the same objects that were\n            [`join`][getml.data.Placeholder.join] ed onto the\n            `population` [`Placeholder`][getml.data.Placeholder].\n            Their order determines the order of the\n            peripheral [`DataFrame`][getml.DataFrame] passed to\n            the 'peripheral_tables' argument in\n            [`check`][getml.Pipeline.check],\n            [`fit`][getml.Pipeline.fit],\n            [`predict`][getml.Pipeline.predict],\n            [`score`][getml.Pipeline.score], and\n            [`transform`][getml.Pipeline.transform], if you\n            pass the data frames as a list.\n            If you omit the peripheral placeholders, they will\n            be inferred from the data model and ordered\n            alphabetically.\n\n        preprocessors:\n            The preprocessor(s) to be used.\n            Must be from [`preprocessors`][getml.preprocessors].\n            A single preprocessor does not have to be wrapped in a list.\n\n        feature_learners:\n            The feature learner(s) to be used.\n            Must be from [`feature_learning`][getml.feature_learning].\n            A single feature learner does not have to be wrapped\n            in a list.\n\n        feature_selectors:\n            Predictor(s) used to select the best features.\n            Must be from [`predictors`][getml.predictors].\n            A single feature selector does not have to be wrapped\n            in a list.\n            Make sure to also set *share_selected_features*.\n\n        predictors:\n            Predictor(s) used to generate the predictions.\n            If more than one predictor is passed, the predictions\n            generated will be averaged.\n            Must be from [`predictors`][getml.predictors].\n            A single predictor does not have to be wrapped\n            in a list.\n\n        loss_function:\n            The loss function to use for the feature learners.\n\n        tags: Tags exist to help you organize your pipelines.\n            You can add any tags that help you remember what you were\n            trying to do.\n\n        include_categorical:\n            Whether you want to pass categorical columns\n            in the population table to the predictor.\n\n        share_selected_features:\n            The share of features you want the feature\n            selection to keep. When set to 0.0, then all features will be kept.\n\n    Example:\n        We assume that you have already set up your\n        preprocessors (refer to [`preprocessors`][getml.preprocessors]),\n        your feature learners (refer to [`feature_learning`][getml.feature_learning])\n        as well as your feature selectors and predictors\n        (refer to [`predictors`][getml.predictors], which can be used\n        for prediction and feature selection).\n\n        You might also want to refer to\n        [`DataFrame`][getml.DataFrame], [`View`][getml.data.View],\n        [`DataModel`][getml.data.DataModel], [`Container`][getml.data.Container],\n        [`Placeholder`][getml.data.Placeholder] and\n        [`StarSchema`][getml.data.StarSchema].\n\n        If you want to create features for a time series problem,\n        the easiest way to do so is to use the [`TimeSeries`][getml.data.TimeSeries]\n        abstraction.\n\n        Note that this example is taken from the\n        [robot notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/robot.ipynb).\n\n        ```python\n        # All rows before row 10500 will be used for training.\n        split = getml.data.split.time(data_all, \"rowid\", test=10500)\n\n        time_series = getml.data.TimeSeries(\n            population=data_all,\n            time_stamps=\"rowid\",\n            split=split,\n            lagged_targets=False,\n            memory=30,\n        )\n\n        pipe = getml.Pipeline(\n            data_model=time_series.data_model,\n            feature_learners=[...],\n            predictors=...\n        )\n\n        pipe.check(time_series.train)\n\n        pipe.fit(time_series.train)\n\n        pipe.score(time_series.test)\n\n        # To generate predictions on new data,\n        # it is sufficient to use a Container.\n        # You don't have to recreate the entire\n        # TimeSeries, because the abstract data model\n        # is stored in the pipeline.\n        container = getml.data.Container(\n            population=population_new,\n        )\n\n        # Add the data as a peripheral table, for the\n        # self-join.\n        container.add(population=population_new)\n\n        predictions = pipe.predict(container.full)\n        ```\n\n    Example:\n        If your data can be organized in a simple star schema,\n        you can use [`StarSchema`][getml.data.StarSchema].\n        [`StarSchema`][getml.data.StarSchema] unifies\n        [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel]:\n\n        Note that this example is taken from the\n        [loans notebook ](https://nbviewer.getml.com/github/getml/getml-demo/blob/master/loans.ipynb).\n\n        ```python\n\n        # First, we insert our data into a StarSchema.\n        # population_train and population_test are either\n        # DataFrames or Views. The population table\n        # defines the statistical population of your\n        # machine learning problem and contains the\n        # target variables.\n        star_schema = getml.data.StarSchema(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views.\n        # Because this is a star schema,\n        # all joins take place on the population\n        # table.\n        star_schema.join(\n            trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        star_schema.join(\n            order,\n            on=\"account_id\",\n        )\n\n        star_schema.join(\n            meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=star_schema.data_model,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # Now, we pass the actual data.\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(star_schema.train)\n\n        pipe.fit(star_schema.train)\n\n        pipe.score(star_schema.test)\n        ```\n\n    Example:\n        [`StarSchema`][getml.data.StarSchema] is simpler,\n        but cannot be used for more complex data models.\n        The general approach is to use\n        [`Container`][getml.data.Container] and [`DataModel`][getml.data.DataModel]:\n\n        ```python\n\n        # First, we insert our data into a Container.\n        # population_train and population_test are either\n        # DataFrames or Views.\n        container = getml.data.Container(\n            train=population_train,\n            test=population_test\n        )\n\n        # meta, order and trans are either\n        # DataFrames or Views. They are given\n        # aliases, so we can refer to them in the\n        # DataModel.\n        container.add(\n            meta=meta,\n            order=order,\n            trans=trans\n        )\n\n        # Freezing makes the container immutable.\n        # This is not required, but often a good idea.\n        container.freeze()\n\n        # The abstract data model is constructed\n        # using the DataModel class. A data model\n        # does not contain any actual data. It just\n        # defines the abstract relational structure.\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        dm.add(getml.data.to_placeholder(\n            meta=meta,\n            order=order,\n            trans=trans)\n        )\n\n        dm.population.join(\n            dm.trans,\n            on=\"account_id\",\n            time_stamps=(\"date_loan\", \"date\")\n        )\n\n        dm.population.join(\n            dm.order,\n            on=\"account_id\",\n        )\n\n        dm.population.join(\n            dm.meta,\n            on=\"account_id\",\n        )\n\n        # Now you can insert your data model,\n        # your preprocessors, feature learners,\n        # feature selectors and predictors\n        # into the pipeline.\n        # Note that the pipeline only knows\n        # the abstract data model, but hasn't\n        # seen the actual data yet.\n        pipe = getml.Pipeline(\n            data_model=dm,\n            preprocessors=[mapping],\n            feature_learners=[fast_prop],\n            feature_selectors=[feature_selector],\n            predictors=predictor,\n        )\n\n        # This passes 'population_train' and the\n        # peripheral tables (meta, order and trans)\n        # to the pipeline.\n        pipe.check(container.train)\n\n        pipe.fit(container.train)\n\n        pipe.score(container.test)\n        ```\n\n        Technically, you don't actually have to use a\n        [`Container`][getml.data.Container]. You might as well do this\n        (in fact, a [`Container`][getml.data.Container] is just\n        syntactic sugar for this approach):\n\n        ```python\n\n        pipe.check(\n            population_train,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n\n        pipe.fit(\n            population_train,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n\n        pipe.score(\n            population_test,\n            {\"meta\": meta, \"order\": order, \"trans\": trans},\n        )\n        ```\n        Or you could even do this. The order of the peripheral tables\n        can be inferred from the \\_\\_repr\\_\\_ method of the pipeline,\n        and it is usually in alphabetical order.\n\n        ```python\n        pipe.check(\n            population_train,\n            [meta, order, trans],\n        )\n\n        pipe.fit(\n            population_train,\n            [meta, order, trans],\n        )\n\n        pipe.score(\n            population_test,\n            [meta, order, trans],\n        )\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        data_model: Optional[DataModel] = None,\n        peripheral: Optional[List[Placeholder]] = None,\n        preprocessors: Optional[\n            Union[\n                CategoryTrimmer,\n                EmailDomain,\n                Imputation,\n                Mapping,\n                Seasonal,\n                Substring,\n                TextFieldSplitter,\n                List[\n                    Union[\n                        CategoryTrimmer,\n                        EmailDomain,\n                        Imputation,\n                        Mapping,\n                        Seasonal,\n                        Substring,\n                        TextFieldSplitter,\n                    ]\n                ],\n            ],\n        ] = None,\n        feature_learners: Optional[\n            Union[\n                Union[Fastboost, FastProp, Multirel, Relboost, RelMT],\n                List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]],\n            ]\n        ] = None,\n        feature_selectors: Optional[\n            Union[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ],\n                List[\n                    Union[\n                        LinearRegression,\n                        LogisticRegression,\n                        XGBoostClassifier,\n                        XGBoostRegressor,\n                        ScaleGBMClassifier,\n                        ScaleGBMRegressor,\n                    ]\n                ],\n            ],\n        ] = None,\n        predictors: Optional[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n                List[\n                    Union[\n                        LinearRegression,\n                        LogisticRegression,\n                        XGBoostClassifier,\n                        XGBoostRegressor,\n                        ScaleGBMClassifier,\n                        ScaleGBMRegressor,\n                    ]\n                ],\n            ]\n        ] = None,\n        loss_function: Optional[str] = None,\n        tags: Optional[list[str]] = None,\n        include_categorical: bool = False,\n        share_selected_features: float = 0.5,\n    ) -&gt; None:\n        data_model = data_model or DataModel(\"population\")\n\n        if not isinstance(data_model, DataModel):\n            raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n        peripheral = peripheral or _infer_peripheral(data_model.population)\n\n        preprocessors = preprocessors or []\n\n        feature_learners = feature_learners or []\n\n        feature_selectors = feature_selectors or []\n\n        predictors = predictors or []\n\n        tags = tags or []\n\n        if not isinstance(preprocessors, list):\n            preprocessors = [preprocessors]\n\n        if not isinstance(feature_learners, list):\n            feature_learners = [feature_learners]\n\n        if not isinstance(feature_selectors, list):\n            feature_selectors = [feature_selectors]\n\n        if not isinstance(predictors, list):\n            predictors = [predictors]\n\n        if not isinstance(peripheral, list):\n            peripheral = [peripheral]\n\n        if not isinstance(tags, list):\n            tags = [tags]\n\n        self._id: str = NOT_FITTED\n\n        self.type = \"Pipeline\"\n\n        loss_function = (\n            loss_function\n            or (\n                [fl.loss_function for fl in feature_learners if fl.loss_function]\n                or [\"SquareLoss\"]\n            )[0]\n        )\n\n        feature_learners = [\n            _handle_loss_function(fl, loss_function) for fl in feature_learners\n        ]\n\n        self.data_model = data_model\n        self.feature_learners = feature_learners\n        self.feature_selectors = feature_selectors\n        self.include_categorical = include_categorical\n        self.loss_function = loss_function\n        self.peripheral = peripheral\n        self.predictors = predictors\n        self.preprocessors = preprocessors\n        self.share_selected_features = share_selected_features\n        self.tags = Tags(tags)\n\n        self._metadata: Optional[AllMetadata] = None\n\n        self._scores: Dict[str, Any] = {}\n\n        self._targets: List[str] = []\n\n        setattr(type(self), \"_supported_params\", list(self.__dict__.keys()))\n\n        self._validate()\n\n    # ----------------------------------------------------------------\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Pipeline):\n            raise TypeError(\"A Pipeline can only be compared to another Pipeline\")\n\n        if len(set(self.__dict__.keys())) != len(set(other.__dict__.keys())):\n            return False\n\n        for kkey in self.__dict__:\n            if kkey not in other.__dict__:\n                return False\n\n            # Take special care when comparing numbers.\n            if isinstance(self.__dict__[kkey], numbers.Real):\n                if not np.isclose(self.__dict__[kkey], other.__dict__[kkey]):\n                    return False\n\n            elif self.__dict__[kkey] != other.__dict__[kkey]:\n                return False\n\n        return True\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        obj_dict = self._make_object_dict()\n\n        sig = _SignatureFormatter(data=obj_dict)\n        repr_str = sig._format()\n\n        if self.fitted:\n            url = self._make_url()\n            repr_str += \"\\n\\nurl: \" + url if url else \"\"\n\n        return repr_str\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        obj_dict = self._make_object_dict()\n\n        sig = _SignatureFormatter(data=obj_dict)\n        repr_str = sig._format()\n        html = f\"&lt;pre&gt;{repr_str}&lt;/pre&gt;\"\n\n        if self.fitted:\n            url = self._make_url()\n            html += (\n                (\n                    \"&lt;br&gt;&lt;pre&gt;\"\n                    + \"url: &lt;a href='\"\n                    + url\n                    + '\\' target=\"_blank\"&gt;'\n                    + url\n                    + \"&lt;/a&gt;\"\n                    + \"&lt;/pre&gt;\"\n                )\n                if url\n                else \"\"\n            )\n\n        return html\n\n    # ------------------------------------------------------------\n\n    def _check_classification_or_regression(self) -&gt; bool:\n        \"\"\"\n        Checks whether there are inconsistencies in the algorithms used\n        (mixing classification and regression algorithms).\n        \"\"\"\n\n        all_classifiers = all(\n            [\n                elem.loss_function in _classification_loss\n                for elem in self.feature_learners\n            ]\n        )\n\n        all_classifiers = all_classifiers and all(\n            [elem.type in _classification_types for elem in self.feature_selectors]\n        )\n\n        all_classifiers = all_classifiers and all(\n            [elem.type in _classification_types for elem in self.predictors]\n        )\n\n        all_regressors = all(\n            [\n                elem.loss_function not in _classification_loss\n                for elem in self.feature_learners\n            ]\n        )\n\n        all_regressors = all_regressors and all(\n            [elem.type not in _classification_types for elem in self.feature_selectors]\n        )\n\n        all_regressors = all_regressors and all(\n            [elem.type not in _classification_types for elem in self.predictors]\n        )\n\n        if not all_classifiers and not all_regressors:\n            raise ValueError(\n                \"\"\"You are mixing classification and regression\n                                algorithms. Please make sure that your feature learning\n                                algorithms consistently have classification loss functions\n                                (like CrossEntropyLoss) or consistently have regression\n                                loss functions (like SquareLoss). Also make sure that your\n                                feature selectors and predictors are consistently classifiers\n                                (like XGBoostClassifier or LogisticRegression) or consistently\n                                regressors (like XGBoostRegressor or LinearRegression).\n                             \"\"\"\n            )\n\n        return all_classifiers\n\n    # ------------------------------------------------------------\n\n    def _check_whether_fitted(self) -&gt; None:\n        if not self.fitted:\n            raise ValueError(\"Pipeline has not been fitted!\")\n\n    # ------------------------------------------------------------\n\n    def _close(self, sock: socket.socket) -&gt; None:\n        if not isinstance(sock, socket.socket):\n            raise TypeError(\"'sock' must be a socket.\")\n\n        cmd = dict()\n        cmd[\"type_\"] = self.type + \".close\"\n        cmd[\"name_\"] = self.id\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    # ------------------------------------------------------------\n\n    def _get_latest_score(self, score: str) -&gt; List[float]:\n        nan_ = [np.nan] * len(self.targets)\n\n        if score not in _all_metrics:\n            raise AttributeError(f\"Not a valid score name: {score}\")\n\n        if not self.scored:\n            return nan_\n\n        if self.is_classification and score not in _classification_metrics:\n            return nan_\n\n        if self.is_regression and score in _classification_metrics:\n            return nan_\n\n        return self._scores[score]\n\n    # ------------------------------------------------------------\n\n    def _getml_deserialize(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Expresses the pipeline in a form the engine can understand.\n        \"\"\"\n\n        cmd = dict()\n\n        self_dict = self.__dict__\n\n        cmd[\"name_\"] = self.id\n\n        for key, value in self_dict.items():\n            cmd[key + \"_\"] = value\n\n        del cmd[\"_id_\"]\n        del cmd[\"_metadata_\"]\n        del cmd[\"_scores_\"]\n        del cmd[\"_targets_\"]\n\n        return cmd\n\n    # ----------------------------------------------------------------\n\n    def _make_object_dict(self) -&gt; Dict[str, Any]:\n        obj_dict = copy.deepcopy(self.__dict__)\n\n        obj_dict[\"data_model\"] = self.data_model.population.name\n\n        obj_dict[\"peripheral\"] = [elem.name for elem in self.peripheral]\n\n        obj_dict[\"preprocessors\"] = [elem.type for elem in self.preprocessors]\n\n        obj_dict[\"feature_learners\"] = [elem.type for elem in self.feature_learners]\n\n        obj_dict[\"feature_selectors\"] = [elem.type for elem in self.feature_selectors]\n\n        obj_dict[\"predictors\"] = [elem.type for elem in self.predictors]\n\n        return obj_dict\n\n    # ----------------------------------------------------------------\n\n    def _make_score_history(self) -&gt; List[Union[ClassificationScore, RegressionScore]]:\n        scores: List[Dict[str, Any]] = self._scores[\"history\"]\n        scores = [_replace_with_nan_maybe(score) for score in scores]\n\n        if self.is_classification:\n            return [\n                ClassificationScore(\n                    date_time=datetime.strptime(\n                        score.get(\"date_time\", \"\"), \"%Y-%m-%d %H:%M:%S\"\n                    ),\n                    set_used=score.get(\"set_used\", \"\"),\n                    target=target,\n                    accuracy=score.get(accuracy, [np.nan])[target_num],\n                    auc=score.get(auc, [np.nan])[target_num],\n                    cross_entropy=score.get(cross_entropy, [np.nan])[target_num],\n                )\n                for score in scores\n                for target_num, target in enumerate(self.targets)\n            ]\n\n        return [\n            RegressionScore(\n                date_time=datetime.strptime(\n                    score.get(\"date_time\", \"\"), \"%Y-%m-%d %H:%M:%S\"\n                ),\n                set_used=score.get(\"set_used\", \"\"),\n                target=target,\n                mae=score.get(mae, [np.nan])[target_num],\n                rmse=score.get(rmse, [np.nan])[target_num],\n                rsquared=score.get(rsquared, [np.nan])[target_num],\n            )\n            for score in scores\n            for target_num, target in enumerate(self.targets)\n        ]\n\n    # ----------------------------------------------------------------\n\n    def _make_url(self) -&gt; Optional[str]:\n        url = comm._monitor_url()\n        if not url:\n            return None\n        url += \"getpipeline/\" + comm._get_project_name() + \"/\" + self.id + \"/0/\"\n        return url\n\n    # ----------------------------------------------------------------\n\n    def _parse_cmd(self, json_obj: Dict[str, Any]) -&gt; \"Pipeline\":\n        ptype = json_obj[\"type_\"]\n\n        del json_obj[\"type_\"]\n\n        if ptype != \"Pipeline\":\n            raise ValueError(\"Expected type 'Pipeline', got '\" + ptype + \"'.\")\n\n        preprocessors = [\n            _parse_preprocessor(elem) for elem in json_obj[\"preprocessors_\"]\n        ]\n\n        del json_obj[\"preprocessors_\"]\n\n        feature_learners = [_parse_fe(elem) for elem in json_obj[\"feature_learners_\"]]\n\n        del json_obj[\"feature_learners_\"]\n\n        feature_selectors = [\n            _parse_pred(elem) for elem in json_obj[\"feature_selectors_\"]\n        ]\n\n        del json_obj[\"feature_selectors_\"]\n\n        predictors = [_parse_pred(elem) for elem in json_obj[\"predictors_\"]]\n\n        del json_obj[\"predictors_\"]\n\n        data_model = _decode_data_model(json_obj[\"data_model_\"])\n\n        del json_obj[\"data_model_\"]\n\n        peripheral = [_decode_placeholder(elem) for elem in json_obj[\"peripheral_\"]]\n\n        del json_obj[\"peripheral_\"]\n\n        id_ = json_obj[\"name_\"]\n\n        del json_obj[\"name_\"]\n\n        kwargs = _remove_trailing_underscores(json_obj)\n\n        self.__init__(  # type: ignore\n            data_model=data_model,\n            peripheral=peripheral,\n            preprocessors=preprocessors,\n            feature_learners=feature_learners,\n            feature_selectors=feature_selectors,\n            predictors=predictors,\n            **kwargs,\n        )\n\n        self._id = id_\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    def _parse_json_obj(self, all_json_objs: Dict[str, Any]) -&gt; \"Pipeline\":\n        obj = all_json_objs[\"obj\"]\n\n        scores = all_json_objs[\"scores\"]\n\n        targets = all_json_objs[\"targets\"]\n\n        self._parse_cmd(obj)\n\n        scores = _remove_trailing_underscores(scores)\n        scores = _replace_with_nan_maybe(scores)\n\n        self._scores = scores\n\n        self._targets = targets\n\n        peripheral_metadata = [\n            _parse_metadata(m) for m in all_json_objs[\"peripheral_metadata\"]\n        ]\n        population_metadata = _parse_metadata(all_json_objs[\"population_metadata\"])\n\n        self._metadata = AllMetadata(\n            peripheral=peripheral_metadata,\n            population=population_metadata,\n        )\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    def _save(self) -&gt; None:\n        \"\"\"\n        Saves the pipeline as a JSON file.\n        \"\"\"\n\n        cmd = dict()\n        cmd[\"type_\"] = self.type + \".save\"\n        cmd[\"name_\"] = self.id\n\n        comm.send(cmd)\n\n    # ------------------------------------------------------------\n\n    def _send(self, additional_tags: Optional[List[str]] = None) -&gt; \"Pipeline\":\n        self._validate()\n\n        self._id = _make_id()\n\n        cmd = self._getml_deserialize()\n\n        if additional_tags is not None:\n            cmd[\"tags_\"] += additional_tags\n\n        comm.send(cmd)\n\n        return self\n\n    # ------------------------------------------------------------\n\n    def _transform(\n        self,\n        peripheral_data_frames: Sequence[Union[DataFrame, View]],\n        population_data_frame: Union[DataFrame, View],\n        sock: socket.socket,\n        score: bool = False,\n        predict: bool = False,\n        df_name: str = \"\",\n        table_name: str = \"\",\n    ) -&gt; Union[NDArray[np.float_], None]:\n        _check_df_types(population_data_frame, peripheral_data_frames)\n\n        if not isinstance(sock, socket.socket):\n            raise TypeError(\"'sock' must be a socket.\")\n\n        if not isinstance(score, bool):\n            raise TypeError(\"'score' must be of type bool\")\n\n        if not isinstance(predict, bool):\n            raise TypeError(\"'predict' must be of type bool\")\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        if not isinstance(df_name, str):\n            raise TypeError(\"'df_name' must be of type str\")\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n\n        cmd[\"score_\"] = score\n        cmd[\"predict_\"] = predict\n\n        cmd[\"peripheral_dfs_\"] = [\n            df._getml_deserialize() for df in peripheral_data_frames\n        ]\n        cmd[\"population_df_\"] = population_data_frame._getml_deserialize()\n\n        cmd[\"df_name_\"] = df_name\n        cmd[\"table_name_\"] = table_name\n\n        comm.send_string(sock, json.dumps(cmd))\n\n        msg = comm.log(sock)\n\n        if msg == \"Success!\":\n            if table_name == \"\" and df_name == \"\" and not score:\n                yhat = comm.recv_float_matrix(sock)\n            else:\n                yhat = None\n        else:\n            comm.engine_exception_handler(msg)\n\n        print()\n\n        return yhat\n\n    # ----------------------------------------------------------------\n\n    @property\n    def accuracy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the accuracy of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.accuracy\n\n    # ----------------------------------------------------------------\n\n    @property\n    def auc(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the auc of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.auc\n\n    # ----------------------------------------------------------------\n\n    def check(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Dict[str, Union[DataFrame, View]],\n                Sequence[Union[DataFrame, View]],\n            ]\n        ] = None,\n    ) -&gt; Optional[Issues]:\n        \"\"\"\n        Checks the validity of the data model.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n        \"\"\"\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        temp = copy.deepcopy(self)\n\n        temp._send()\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = temp.type + \".check\"\n        cmd[\"name_\"] = temp.id\n\n        cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n        cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            print(\"Checking data model...\")\n            msg = comm.log(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            print()\n            issues = Issues(comm.recv_issues(sock))\n            if len(issues) == 0:\n                print(\"OK.\")\n            else:\n                print(\n                    f\"The pipeline check generated {len(issues.info)} \"\n                    + f\"issues labeled INFO and {len(issues.warnings)} \"\n                    + \"issues labeled WARNING.\"\n                )\n\n        temp.delete()\n\n        return None if len(issues) == 0 else issues\n\n    # ------------------------------------------------------------\n\n    @property\n    def columns(self) -&gt; Columns:\n        \"\"\"\n        [`Columns`][getml.pipeline.Columns] object that\n        can be used to handle information about the original\n        columns utilized by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Columns(self.id, self.targets, self.peripheral)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def cross_entropy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the cross entropy of the latest scoring\n        run (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.cross_entropy\n\n    # ----------------------------------------------------------------\n\n    def delete(self) -&gt; None:\n        \"\"\"\n        Deletes the pipeline from the engine.\n\n        Note:\n            Caution: You can not undo this action!\n        \"\"\"\n        self._check_whether_fitted()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".delete\"\n        cmd[\"name_\"] = self.id\n        cmd[\"mem_only_\"] = False\n\n        comm.send(cmd)\n\n        self._id = NOT_FITTED\n\n    # ------------------------------------------------------------\n\n    def deploy(self, deploy: bool) -&gt; None:\n        \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n        See [deployment][deployment] for details.\n\n        Args:\n            deploy (bool): If `True`, the deployment of the pipeline\n                will be triggered.\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if not isinstance(deploy, bool):\n            raise TypeError(\"'deploy' must be of type bool\")\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".deploy\"\n        cmd[\"name_\"] = self.id\n        cmd[\"deploy_\"] = deploy\n\n        comm.send(cmd)\n\n        self._save()\n\n    # ------------------------------------------------------------\n\n    @property\n    def features(self) -&gt; Features:\n        \"\"\"\n        [`Features`][getml.pipeline.Features] object that\n        can be used to handle the features generated\n        by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Features(self.id, self.targets)\n\n    # ------------------------------------------------------------\n\n    def fit(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n        check: bool = True,\n    ) -&gt; \"Pipeline\":\n        \"\"\"Trains the feature learning algorithms, feature selectors\n        and predictors.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            validation_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If you are passing a subset, that subset\n                must be derived from the same container as *population_table*.\n\n                Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n                and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n            check (bool):\n                Whether you want to check the data model before fitting. The checks are\n                equivalent to the checks run by [`check`][getml.Pipeline.check].\n        \"\"\"\n\n        additional_tags = (\n            [\"container-\" + population_table.container_id]\n            if isinstance(population_table, data.Subset)\n            else []\n        )\n\n        if (\n            isinstance(population_table, data.Subset)\n            and isinstance(validation_table, data.Subset)\n            and validation_table.container_id != population_table.container_id\n        ):\n            raise ValueError(\n                \"The subset used for validation must be from the same container \"\n                + \"as the subset used for training.\"\n            )\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        if isinstance(validation_table, data.Subset):\n            validation_table = validation_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        if check:\n            warnings = self.check(population_table, peripheral_tables)\n            if warnings:\n                print(\"To see the issues in full, run .check() on the pipeline.\")\n                print()\n\n        self._send(additional_tags)\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = self.type + \".fit\"\n        cmd[\"name_\"] = self.id\n\n        cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n        cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n        if validation_table is not None:\n            cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n\n            begin = time.time()\n\n            msg = comm.log(sock)\n\n            end = time.time()\n\n            if \"Trained\" in msg:\n                print()\n                print(msg)\n                _print_time_taken(begin, end, \"Time taken: \")\n            else:\n                comm.engine_exception_handler(msg)\n\n        self._save()\n\n        return self.refresh()\n\n    # ------------------------------------------------------------\n\n    @property\n    def fitted(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline has already been fitted.\n        \"\"\"\n        return self._id != NOT_FITTED\n\n    # ----------------------------------------------------------------\n\n    @property\n    def mae(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the mae of the latest scoring run (the\n        last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.mae\n\n    # ------------------------------------------------------------\n\n    @property\n    def plots(self) -&gt; Plots:\n        \"\"\"\n        [`Plots`][getml.pipeline.Plots] object that\n        can be used to generate plots like an ROC\n        curve or a lift curve.\n        \"\"\"\n        self._check_whether_fitted()\n        return Plots(self.id)\n\n    # ------------------------------------------------------------\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"\n        ID of the pipeline. This is used to uniquely identify\n        the pipeline on the engine.\n        \"\"\"\n        return self._id\n\n    # ------------------------------------------------------------\n\n    @property\n    def is_classification(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline can used for classification problems.\n        \"\"\"\n        return self._check_classification_or_regression()\n\n    # ------------------------------------------------------------\n\n    @property\n    def is_regression(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline can used for regression problems.\n        \"\"\"\n        return not self.is_classification\n\n    # ------------------------------------------------------------\n\n    @property\n    def metadata(self) -&gt; Optional[AllMetadata]:\n        \"\"\"\n        Contains information on the data frames\n        that were passed to .fit(...). The roles\n        contained therein can be directly passed\n        to existing data frames to correctly reassign\n        the roles of existing columns. If the pipeline\n        has not been fitted, this is None.\n        \"\"\"\n        return self._metadata\n\n    # ------------------------------------------------------------\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Returns the ID of the pipeline. The name property is\n        kept for backward compatibility.\n        \"\"\"\n        return self._id\n\n    # ------------------------------------------------------------\n\n    def predict(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        table_name: str = \"\",\n    ) -&gt; Union[NDArray[np.float_], None]:\n        \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n        Returns the predictions generated by the pipeline based on\n        `population_table` and `peripheral_tables` or writes them into\n        a data base named `table_name`.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            table_name (str, optional):\n                If not an empty string, the resulting predictions will\n                be written into a table in a [`database`][getml.database].\n                Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n        Returns:\n            Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can be used for\n            prediction.\n\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        if not isinstance(table_name, str):\n            raise TypeError(\"'table_name' must be of type str\")\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            y_hat = self._transform(\n                peripheral_tables,\n                population_table,\n                sock,\n                predict=True,\n                table_name=table_name,\n            )\n\n        return y_hat\n\n    # ------------------------------------------------------------\n\n    def refresh(self) -&gt; \"Pipeline\":\n        \"\"\"Reloads the pipeline from the engine.\n\n        This discards all local changes you have made since the\n        last time you called [`fit`][getml.Pipeline.fit].\n\n        Returns:\n            [`Pipeline`][getml.Pipeline]:\n                Current instance\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".refresh\"\n        cmd[\"name_\"] = self.id\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n        if msg[0] != \"{\":\n            comm.engine_exception_handler(msg)\n\n        json_obj = json.loads(msg)\n\n        self._parse_json_obj(json_obj)\n\n        return self\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rmse(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the rmse of the latest scoring run\n        (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.rmse\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rsquared(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the rsquared of the latest scoring run\n        (the last time `.score()` was called) on the pipeline.\n\n        For programmatic access use [`metrics`][getml.pipeline.metrics].\n        \"\"\"\n        return self.scores.rsquared\n\n    # ----------------------------------------------------------------\n\n    def score(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n    ) -&gt; Scores:\n        \"\"\"Calculates the performance of the ``predictor``.\n\n        Returns different scores calculated on `population_table` and\n        `peripheral_tables`.\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can be\n            scored.\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n\n            self._transform(\n                peripheral_tables, population_table, sock, predict=True, score=True\n            )\n\n            msg = comm.recv_string(sock)\n\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n\n            scores = comm.recv_string(sock)\n\n            scores = json.loads(scores)\n\n        self.refresh()\n\n        self._save()\n\n        return self.scores\n\n    # ----------------------------------------------------------------\n\n    @property\n    def scores(self) -&gt; Scores:\n        \"\"\"\n        Contains all scores generated by [`score`][getml.Pipeline.score]\n\n        Returns:\n            [`Scores`][getml.pipeline.Scores]:\n                A container that holds the scores for the pipeline.\n\n        \"\"\"\n        self._check_whether_fitted()\n\n        scores = self._make_score_history()\n\n        latest = {score: self._get_latest_score(score) for score in _all_metrics}\n\n        return Scores(scores, latest)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def scored(self) -&gt; bool:\n        \"\"\"\n        Whether the pipeline has been scored.\n        \"\"\"\n        if self._scores is None:\n            return False\n        return len(self._scores) &gt; 1\n\n    # ----------------------------------------------------------------\n\n    @property\n    def tables(self) -&gt; Tables:\n        \"\"\"\n        [`Tables`][getml.pipeline.Tables] object that\n        can be used to handle information about the original\n        tables utilized by the feature learners.\n        \"\"\"\n        self._check_whether_fitted()\n        return Tables(self.targets, self.columns)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def targets(self) -&gt; List[str]:\n        \"\"\"\n        Contains the names of the targets used for this pipeline.\n        \"\"\"\n        self._check_whether_fitted()\n        return copy.deepcopy(self._targets)\n\n    # ----------------------------------------------------------------\n\n    def transform(\n        self,\n        population_table: Union[DataFrame, View, data.Subset],\n        peripheral_tables: Optional[\n            Union[\n                Sequence[Union[DataFrame, View]],\n                Dict[str, Union[DataFrame, View]],\n            ]\n        ] = None,\n        df_name: str = \"\",\n        table_name: str = \"\",\n    ) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n        \"\"\"Translates new data into the trained features.\n\n        Transforms the data passed in `population_table` and\n        `peripheral_tables` into features, which can be inserted into\n        machine learning models.\n\n        Example:\n            By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n            ```python\n            my_features_array = pipe.transform()\n            ```\n            You can also export your features as a [`DataFrame`][getml.DataFrame]\n            by providing the `df_name` argument:\n            ```python\n            my_features_df = pipe.transform(df_name=\"my_features\")\n            ```\n            Or you can write the results directly into a database:\n            ```python\n            getml.database.connect_odbc(...)\n            pipe.transform(table_name=\"MY_FEATURES\")\n            ```\n\n        Args:\n            population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n                Main table containing the target variable(s) and\n                corresponding to the ``population``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable.\n\n            peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n                Additional tables corresponding to the ``peripheral``\n                [`Placeholder`][getml.data.Placeholder] instance\n                variable. If passed as a list, the order needs to\n                match the order of the corresponding placeholders passed\n                to ``peripheral``.\n\n                If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n                the peripheral tables from that subset will be used. If you use\n                a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n                or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n                a [`Subset`][getml.data.Subset].\n\n            df_name (str, optional):\n                If not an empty string, the resulting features will be\n                written into a newly created DataFrame.\n\n            table_name (str, optional):\n                If not an empty string, the resulting features will\n                be written into a table in a [`database`][getml.database].\n                Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n        Note:\n            Only fitted pipelines\n            ([`fit`][getml.Pipeline.fit]) can transform\n            data into features.\n\n        \"\"\"\n\n        self._check_whether_fitted()\n\n        if isinstance(population_table, data.Subset):\n            peripheral_tables = population_table.peripheral\n            population_table = population_table.population\n\n        peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n        _check_df_types(population_table, peripheral_tables)\n\n        self._validate()\n\n        cmd: Dict[str, Any] = {}\n        cmd[\"type_\"] = self.type + \".transform\"\n        cmd[\"name_\"] = self.id\n        cmd[\"http_request_\"] = False\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Found!\":\n                comm.engine_exception_handler(msg)\n            y_hat = self._transform(\n                peripheral_tables,\n                population_table,\n                sock,\n                df_name=df_name,\n                table_name=table_name,\n            )\n\n        if df_name != \"\":\n            return data.DataFrame(name=df_name).refresh()\n\n        return y_hat\n\n    # ----------------------------------------------------------------\n\n    def _validate(self) -&gt; None:\n        if not isinstance(self.id, str):\n            raise TypeError(\"'name' must be of type str\")\n\n        if not isinstance(self.data_model, DataModel):\n            raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n        if not _is_typed_list(self.peripheral, data.Placeholder):\n            raise TypeError(\n                \"'peripheral' must be either a getml.data.Placeholder or a list thereof\"\n            )\n\n        if not _is_subclass_list(self.preprocessors, _Preprocessor):\n            raise TypeError(\"'preprocessor' must be a list of _Preprocessor.\")\n\n        if not _is_subclass_list(self.feature_learners, _FeatureLearner):\n            raise TypeError(\"'feature_learners' must be a list of _FeatureLearners.\")\n\n        if not _is_subclass_list(self.feature_selectors, _Predictor):\n            raise TypeError(\n                \"'feature_selectors' must be a list of getml.predictors._Predictors.\"\n            )\n\n        if not _is_subclass_list(self.predictors, _Predictor):\n            raise TypeError(\n                \"'predictors' must be a list of getml.predictors._Predictors.\"\n            )\n\n        if not isinstance(self.include_categorical, bool):\n            raise TypeError(\"'include_categorical' must be a bool!\")\n\n        if not isinstance(self.share_selected_features, numbers.Real):\n            raise TypeError(\"'share_selected_features' must be number!\")\n\n        if not _is_typed_list(self.tags, str):\n            raise TypeError(\"'tags' must be a list of str.\")\n\n        if self.type != \"Pipeline\":\n            raise ValueError(\"'type' must be 'Pipeline'\")\n\n        for kkey in self.__dict__:\n            if kkey not in Pipeline._supported_params:  # pylint: disable=E1101\n                raise KeyError(\n                    \"\"\"Instance variable [\"\"\"\n                    + kkey\n                    + \"\"\"]\n                       is not supported in Pipeline.\"\"\"\n                )\n\n        for elem in self.feature_learners:\n            elem.validate()\n\n        for elem in self.feature_selectors:\n            elem.validate()\n\n        for elem in self.predictors:\n            elem.validate()\n\n        self._check_classification_or_regression()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.accuracy","title":"<code>accuracy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the accuracy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.auc","title":"<code>auc: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the auc of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.columns","title":"<code>columns: Columns</code>  <code>property</code>","text":"<p><code>Columns</code> object that can be used to handle information about the original columns utilized by the feature learners.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.cross_entropy","title":"<code>cross_entropy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the cross entropy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.features","title":"<code>features: Features</code>  <code>property</code>","text":"<p><code>Features</code> object that can be used to handle the features generated by the feature learners.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.fitted","title":"<code>fitted: bool</code>  <code>property</code>","text":"<p>Whether the pipeline has already been fitted.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>ID of the pipeline. This is used to uniquely identify the pipeline on the engine.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.is_classification","title":"<code>is_classification: bool</code>  <code>property</code>","text":"<p>Whether the pipeline can used for classification problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.is_regression","title":"<code>is_regression: bool</code>  <code>property</code>","text":"<p>Whether the pipeline can used for regression problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.mae","title":"<code>mae: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the mae of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.metadata","title":"<code>metadata: Optional[AllMetadata]</code>  <code>property</code>","text":"<p>Contains information on the data frames that were passed to .fit(...). The roles contained therein can be directly passed to existing data frames to correctly reassign the roles of existing columns. If the pipeline has not been fitted, this is None.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Returns the ID of the pipeline. The name property is kept for backward compatibility.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.plots","title":"<code>plots: Plots</code>  <code>property</code>","text":"<p><code>Plots</code> object that can be used to generate plots like an ROC curve or a lift curve.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.rmse","title":"<code>rmse: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the rmse of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.rsquared","title":"<code>rsquared: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the rsquared of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.scored","title":"<code>scored: bool</code>  <code>property</code>","text":"<p>Whether the pipeline has been scored.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.scores","title":"<code>scores: Scores</code>  <code>property</code>","text":"<p>Contains all scores generated by <code>score</code></p> <p>Returns:</p> Type Description <code>Scores</code> <p><code>Scores</code>: A container that holds the scores for the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.tables","title":"<code>tables: Tables</code>  <code>property</code>","text":"<p><code>Tables</code> object that can be used to handle information about the original tables utilized by the feature learners.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.targets","title":"<code>targets: List[str]</code>  <code>property</code>","text":"<p>Contains the names of the targets used for this pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.check","title":"<code>check(population_table, peripheral_tables=None)</code>","text":"<p>Checks the validity of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def check(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]:\n    \"\"\"\n    Checks the validity of the data model.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    \"\"\"\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    temp = copy.deepcopy(self)\n\n    temp._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = temp.type + \".check\"\n    cmd[\"name_\"] = temp.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        print(\"Checking data model...\")\n        msg = comm.log(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        print()\n        issues = Issues(comm.recv_issues(sock))\n        if len(issues) == 0:\n            print(\"OK.\")\n        else:\n            print(\n                f\"The pipeline check generated {len(issues.info)} \"\n                + f\"issues labeled INFO and {len(issues.warnings)} \"\n                + \"issues labeled WARNING.\"\n            )\n\n    temp.delete()\n\n    return None if len(issues) == 0 else issues\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.delete","title":"<code>delete()</code>","text":"<p>Deletes the pipeline from the engine.</p> Note <p>Caution: You can not undo this action!</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes the pipeline from the engine.\n\n    Note:\n        Caution: You can not undo this action!\n    \"\"\"\n    self._check_whether_fitted()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".delete\"\n    cmd[\"name_\"] = self.id\n    cmd[\"mem_only_\"] = False\n\n    comm.send(cmd)\n\n    self._id = NOT_FITTED\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.deploy","title":"<code>deploy(deploy)</code>","text":"<p>Allows a fitted pipeline to be addressable via an HTTP request. See deployment for details.</p> <p>Parameters:</p> Name Type Description Default <code>deploy</code> <code>bool</code> <p>If <code>True</code>, the deployment of the pipeline will be triggered.</p> required Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def deploy(self, deploy: bool) -&gt; None:\n    \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n    See [deployment][deployment] for details.\n\n    Args:\n        deploy (bool): If `True`, the deployment of the pipeline\n            will be triggered.\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if not isinstance(deploy, bool):\n        raise TypeError(\"'deploy' must be of type bool\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".deploy\"\n    cmd[\"name_\"] = self.id\n    cmd[\"deploy_\"] = deploy\n\n    comm.send(cmd)\n\n    self._save()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.fit","title":"<code>fit(population_table, peripheral_tables=None, validation_table=None, check=True)</code>","text":"<p>Trains the feature learning algorithms, feature selectors and predictors.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>validation_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable. If you are passing a subset, that subset must be derived from the same container as population_table.</p> <p>Only used for early stopping in <code>XGBoostClassifier</code> and <code>XGBoostRegressor</code>.</p> <code>None</code> <code>check</code> <code>bool</code> <p>Whether you want to check the data model before fitting. The checks are equivalent to the checks run by <code>check</code>.</p> <code>True</code> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def fit(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n    check: bool = True,\n) -&gt; \"Pipeline\":\n    \"\"\"Trains the feature learning algorithms, feature selectors\n    and predictors.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        validation_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If you are passing a subset, that subset\n            must be derived from the same container as *population_table*.\n\n            Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n            and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n        check (bool):\n            Whether you want to check the data model before fitting. The checks are\n            equivalent to the checks run by [`check`][getml.Pipeline.check].\n    \"\"\"\n\n    additional_tags = (\n        [\"container-\" + population_table.container_id]\n        if isinstance(population_table, data.Subset)\n        else []\n    )\n\n    if (\n        isinstance(population_table, data.Subset)\n        and isinstance(validation_table, data.Subset)\n        and validation_table.container_id != population_table.container_id\n    ):\n        raise ValueError(\n            \"The subset used for validation must be from the same container \"\n            + \"as the subset used for training.\"\n        )\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    if isinstance(validation_table, data.Subset):\n        validation_table = validation_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if check:\n        warnings = self.check(population_table, peripheral_tables)\n        if warnings:\n            print(\"To see the issues in full, run .check() on the pipeline.\")\n            print()\n\n    self._send(additional_tags)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = self.type + \".fit\"\n    cmd[\"name_\"] = self.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    if validation_table is not None:\n        cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        begin = time.time()\n\n        msg = comm.log(sock)\n\n        end = time.time()\n\n        if \"Trained\" in msg:\n            print()\n            print(msg)\n            _print_time_taken(begin, end, \"Time taken: \")\n        else:\n            comm.engine_exception_handler(msg)\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.predict","title":"<code>predict(population_table, peripheral_tables=None, table_name='')</code>","text":"<p>Forecasts on new, unseen data using the trained <code>predictor</code>.</p> <p>Returns the predictions generated by the pipeline based on <code>population_table</code> and <code>peripheral_tables</code> or writes them into a data base named <code>table_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>If not an empty string, the resulting predictions will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[NDArray[float_], None]</code> <p>Resulting predictions provided in an array of the (number of rows in <code>population_table</code>, number of targets in <code>population_table</code>).</p> Note <p>Only fitted pipelines (<code>fit</code>) can be used for prediction.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def predict(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[np.float_], None]:\n    \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n    Returns the predictions generated by the pipeline based on\n    `population_table` and `peripheral_tables` or writes them into\n    a data base named `table_name`.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        table_name (str, optional):\n            If not an empty string, the resulting predictions will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be used for\n        prediction.\n\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            predict=True,\n            table_name=table_name,\n        )\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.refresh","title":"<code>refresh()</code>","text":"<p>Reloads the pipeline from the engine.</p> <p>This discards all local changes you have made since the last time you called <code>fit</code>.</p> <p>Returns:</p> Type Description <code>'Pipeline'</code> <p><code>Pipeline</code>: Current instance</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def refresh(self) -&gt; \"Pipeline\":\n    \"\"\"Reloads the pipeline from the engine.\n\n    This discards all local changes you have made since the\n    last time you called [`fit`][getml.Pipeline.fit].\n\n    Returns:\n        [`Pipeline`][getml.Pipeline]:\n            Current instance\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".refresh\"\n    cmd[\"name_\"] = self.id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    json_obj = json.loads(msg)\n\n    self._parse_json_obj(json_obj)\n\n    return self\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.score","title":"<code>score(population_table, peripheral_tables=None)</code>","text":"<p>Calculates the performance of the <code>predictor</code>.</p> <p>Returns different scores calculated on <code>population_table</code> and <code>peripheral_tables</code>.</p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> Note <p>Only fitted pipelines (<code>fit</code>) can be scored.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def score(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores:\n    \"\"\"Calculates the performance of the ``predictor``.\n\n    Returns different scores calculated on `population_table` and\n    `peripheral_tables`.\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be\n        scored.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        self._transform(\n            peripheral_tables, population_table, sock, predict=True, score=True\n        )\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        scores = comm.recv_string(sock)\n\n        scores = json.loads(scores)\n\n    self.refresh()\n\n    self._save()\n\n    return self.scores\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.pipeline.Pipeline.transform","title":"<code>transform(population_table, peripheral_tables=None, df_name='', table_name='')</code>","text":"<p>Translates new data into the trained features.</p> <p>Transforms the data passed in <code>population_table</code> and <code>peripheral_tables</code> into features, which can be inserted into machine learning models.</p> Example <p>By default, <code>transform</code> returns a <code>ndarray</code>: <pre><code>my_features_array = pipe.transform()\n</code></pre> You can also export your features as a <code>DataFrame</code> by providing the <code>df_name</code> argument: <pre><code>my_features_df = pipe.transform(df_name=\"my_features\")\n</code></pre> Or you can write the results directly into a database: <pre><code>getml.database.connect_odbc(...)\npipe.transform(table_name=\"MY_FEATURES\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>population_table</code> <code>[`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> required <code>peripheral_tables</code> <code>List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View]</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <code>None</code> <code>df_name</code> <code>str</code> <p>If not an empty string, the resulting features will be written into a newly created DataFrame.</p> <code>''</code> <code>table_name</code> <code>str</code> <p>If not an empty string, the resulting features will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <code>''</code> Note <p>Only fitted pipelines (<code>fit</code>) can transform data into features.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def transform(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n    \"\"\"Translates new data into the trained features.\n\n    Transforms the data passed in `population_table` and\n    `peripheral_tables` into features, which can be inserted into\n    machine learning models.\n\n    Example:\n        By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n        ```python\n        my_features_array = pipe.transform()\n        ```\n        You can also export your features as a [`DataFrame`][getml.DataFrame]\n        by providing the `df_name` argument:\n        ```python\n        my_features_df = pipe.transform(df_name=\"my_features\")\n        ```\n        Or you can write the results directly into a database:\n        ```python\n        getml.database.connect_odbc(...)\n        pipe.transform(table_name=\"MY_FEATURES\")\n        ```\n\n    Args:\n        population_table ([`DataFrame`][getml.DataFrame], [`View`][getml.View] or [`Subset`][getml.data.Subset]):\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables (List[[`DataFrame`][getml.DataFrame] or [`View`][getml.View]], dict, [`DataFrame`][getml.DataFrame] or [`View`][getml.View], optional):\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        df_name (str, optional):\n            If not an empty string, the resulting features will be\n            written into a newly created DataFrame.\n\n        table_name (str, optional):\n            If not an empty string, the resulting features will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can transform\n        data into features.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            df_name=df_name,\n            table_name=table_name,\n        )\n\n    if df_name != \"\":\n        return data.DataFrame(name=df_name).refresh()\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/plots/","title":"Plots","text":"<p>Custom class for handling the plots of a pipeline.</p>"},{"location":"reference/pipeline/plots/#getml.pipeline.plots.Plots","title":"<code>Plots</code>","text":"<p>Custom class for handling the plots generated by the pipeline.</p> Example <pre><code>recall, precision = my_pipeline.plots.precision_recall_curve()\nfpr, tpr = my_pipeline.plots.roc_curve()\n</code></pre> Source code in <code>getml/pipeline/plots.py</code> <pre><code>class Plots:\n    \"\"\"\n    Custom class for handling the\n    plots generated by the pipeline.\n\n    Example:\n        ```python\n        recall, precision = my_pipeline.plots.precision_recall_curve()\n        fpr, tpr = my_pipeline.plots.roc_curve()\n        ```\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, name: str) -&gt; None:\n\n        if not isinstance(name, str):\n            raise ValueError(\"'name' must be a str.\")\n\n        self.name = name\n\n    # ------------------------------------------------------------\n\n    def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the lift curve, as displayed in the getML monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the proportion of samples, usually displayed on the x-axis.\n            The second array is the lift, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.lift_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n\n    # ------------------------------------------------------------\n\n    def precision_recall_curve(\n        self, target_num: int = 0\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the precision-recall curve, as displayed in the getML\n        monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n            The second array is the precision, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n\n    # ------------------------------------------------------------\n\n    def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the data for the ROC curve, as displayed in the getML monitor.\n\n        This requires that you call\n        [`score`][getml.Pipeline.score] first. The data used\n        for the curve will always be the data from the *last* time\n        you called [`score`][getml.Pipeline.score].\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to plot the lift\n                curve. (Pipelines can have more than one target.)\n\n        Returns:\n            The first array is the false positive rate, usually displayed on the x-axis.\n            The second array is the true positive rate, usually displayed on the y-axis.\n        \"\"\"\n\n        cmd: Dict[str, Any] = {}\n\n        cmd[\"type_\"] = \"Pipeline.roc_curve\"\n        cmd[\"name_\"] = self.name\n\n        cmd[\"target_num_\"] = target_num\n\n        with comm.send_and_get_socket(cmd) as sock:\n            msg = comm.recv_string(sock)\n            if msg != \"Success!\":\n                comm.engine_exception_handler(msg)\n            msg = comm.recv_string(sock)\n\n        json_obj = json.loads(msg)\n\n        return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.plots.Plots.lift_curve","title":"<code>lift_curve(target_num=0)</code>","text":"<p>Returns the data for the lift curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the proportion of samples, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the lift, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the lift curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the proportion of samples, usually displayed on the x-axis.\n        The second array is the lift, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.lift_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.plots.Plots.precision_recall_curve","title":"<code>precision_recall_curve(target_num=0)</code>","text":"<p>Returns the data for the precision-recall curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the precision, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def precision_recall_curve(\n    self, target_num: int = 0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the precision-recall curve, as displayed in the getML\n    monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n        The second array is the precision, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.plots.Plots.roc_curve","title":"<code>roc_curve(target_num=0)</code>","text":"<p>Returns the data for the ROC curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The first array is the false positive rate, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the true positive rate, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the ROC curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the false positive rate, usually displayed on the x-axis.\n        The second array is the true positive rate, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.roc_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/score/","title":"Score","text":""},{"location":"reference/pipeline/score/#getml.pipeline.score.ClassificationScore","title":"<code>ClassificationScore</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a classification pipeline.</p> Source code in <code>getml/pipeline/score.py</code> <pre><code>@dataclass\nclass ClassificationScore(Score):\n    \"\"\"\n    Dataclass that holds data of a scoring run for a classification pipeline.\n    \"\"\"\n\n    accuracy: float\n    auc: float\n    cross_entropy: float\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.date_time:%Y-%m-%d %H:%M:%S} {self.set_used} {self.target} {self.accuracy} {self.auc} {self.cross_entropy}\"\n</code></pre>"},{"location":"reference/pipeline/score/#getml.pipeline.score.RegressionScore","title":"<code>RegressionScore</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a regression pipeline.</p> Source code in <code>getml/pipeline/score.py</code> <pre><code>@dataclass\nclass RegressionScore(Score):\n    \"\"\"\n    Dataclass that holds data of a scoring run for a regression pipeline.\n    \"\"\"\n\n    mae: float\n    rmse: float\n    rsquared: float\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.date_time:%Y-%m-%d %H:%M:%S} {self.set_used} {self.target} {self.mae} {self.rmse} {self.rsquared}\"\n</code></pre>"},{"location":"reference/pipeline/scores_container/","title":"Scores container","text":"<p>A container for storing a pipeline's scoring history.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores","title":"<code>Scores</code>","text":"<p>Container which holds the history of all scores associated with a given pipeline. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>class Scores:\n    \"\"\"\n    Container which holds the history of all scores associated with a given pipeline.\n    The container supports slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data: Sequence[Score], latest: Dict[str, List[float]]) -&gt; None:\n        self._latest = latest\n\n        self.is_classification = all(\n            isinstance(score, ClassificationScore) for score in data\n        )\n\n        self.is_regression = not self.is_classification\n\n        self.data = data\n\n        self.sets_used = [score.set_used for score in data]\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key: Union[int, slice, str]):\n        if isinstance(key, int):\n            return self.data[key]\n\n        if isinstance(key, slice):\n            scores_subset = self.data[key]\n            return Scores(scores_subset, self._latest)\n\n        if isinstance(key, str):\n            # allow to access latest scores via their name for backward compatibility\n            if key in _all_metrics:\n                return self._latest[key]\n\n            scores_subset = [score for score in self.data if score.set_used == key]\n\n            return Scores(scores_subset, self._latest)\n\n        raise TypeError(\n            f\"Scores can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self):\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        headers = [\"date time\", \"set used\", \"target\"]\n        if self.is_classification:\n            headers += [\"accuracy\", \"auc\", \"cross entropy\"]\n        if self.is_regression:\n            headers += [\"mae\", \"rmse\", \"rsquared\"]\n\n        rows = [list(vars(score).values()) for score in self.data]\n\n        return _Formatter([headers], rows)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def accuracy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `accuracy` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[accuracy])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def auc(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `auc` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[auc])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def cross_entropy(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `cross entropy` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[cross_entropy])\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n        \"\"\"\n        Filters the scores container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n            [`Scores`][getml.pipeline.Scores]:\n                A container of filtered scores.\n\n        Example:\n            ```python\n            from datetime import datetime, timedelta\n            one_week_ago = datetime.today() - timedelta(days=7)\n            scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n            ```\n        \"\"\"\n        scores_filtered = [score for score in self.data if conditional(score)]\n\n        return Scores(scores_filtered, self._latest)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def mae(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `mae` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[mae])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rmse(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `rmse` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[rmse])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def rsquared(self) -&gt; Union[float, List[float]]:\n        \"\"\"\n        A convenience wrapper to retrieve the `rsquared` from the latest scoring run.\n        \"\"\"\n        return _unlist_maybe(self._latest[rsquared])\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n    ) -&gt; Scores:\n        \"\"\"\n        Sorts the scores container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n            [`Scores`][getml.pipeline.Scores]:\n                A container of sorted scores.\n\n        Example:\n            ```python\n            by_auc = pipe.scores.sort(key=lambda score: score.auc)\n            most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n            ```\n        \"\"\"\n\n        scores_sorted = sorted(self.data, key=key, reverse=descending)\n        return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.accuracy","title":"<code>accuracy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>accuracy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.auc","title":"<code>auc: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>auc</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.cross_entropy","title":"<code>cross_entropy: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>cross entropy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.mae","title":"<code>mae: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>mae</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.rmse","title":"<code>rmse: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>rmse</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.rsquared","title":"<code>rsquared: Union[float, List[float]]</code>  <code>property</code>","text":"<p>A convenience wrapper to retrieve the <code>rsquared</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the scores container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Scores</code> <p><code>Scores</code>: A container of filtered scores.</p> Example <pre><code>from datetime import datetime, timedelta\none_week_ago = datetime.today() - timedelta(days=7)\nscores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n    \"\"\"\n    Filters the scores container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        [`Scores`][getml.pipeline.Scores]:\n            A container of filtered scores.\n\n    Example:\n        ```python\n        from datetime import datetime, timedelta\n        one_week_ago = datetime.today() - timedelta(days=7)\n        scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n        ```\n    \"\"\"\n    scores_filtered = [score for score in self.data if conditional(score)]\n\n    return Scores(scores_filtered, self._latest)\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.scores_container.Scores.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the scores container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p><code>Scores</code>:     A container of sorted scores.</p> Example <pre><code>by_auc = pipe.scores.sort(key=lambda score: score.auc)\nmost_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def sort(\n    self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n) -&gt; Scores:\n    \"\"\"\n    Sorts the scores container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n        [`Scores`][getml.pipeline.Scores]:\n            A container of sorted scores.\n\n    Example:\n        ```python\n        by_auc = pipe.scores.sort(key=lambda score: score.auc)\n        most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n        ```\n    \"\"\"\n\n    scores_sorted = sorted(self.data, key=key, reverse=descending)\n    return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/sql_code/","title":"Sql code","text":"<p>Custom class for handling the SQL code of the features.</p>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.sql_code.SQLCode","title":"<code>SQLCode</code>","text":"<p>Custom class for handling the SQL code of the features generated by the pipeline.</p> Example <pre><code>sql_code = my_pipeline.features.to_sql()\n\n# You can access individual features\n# by index.\nfeature_1_1 = sql_code[0]\n\n# You can also access them by name.\nfeature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n# You can also type the name of\n# a table or column to find all\n# features related to that table\n# or column.\nfeatures = sql_code.find(\"SOME_TABLE\")\n\n# HINT: The generated SQL code always\n# escapes table and column names using\n# quotation marks. So if you want exact\n# matching, you can do this:\nfeatures = sql_code.find('\"SOME_TABLE\"')\n</code></pre> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>class SQLCode:\n    \"\"\"\n    Custom class for handling the SQL code of the\n    features generated by the pipeline.\n\n    Example:\n        ```python\n        sql_code = my_pipeline.features.to_sql()\n\n        # You can access individual features\n        # by index.\n        feature_1_1 = sql_code[0]\n\n        # You can also access them by name.\n        feature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n        # You can also type the name of\n        # a table or column to find all\n        # features related to that table\n        # or column.\n        features = sql_code.find(\"SOME_TABLE\")\n\n        # HINT: The generated SQL code always\n        # escapes table and column names using\n        # quotation marks. So if you want exact\n        # matching, you can do this:\n        features = sql_code.find('\"SOME_TABLE\"')\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        code: Sequence[Union[str, SQLString]],\n        dialect: str = sqlite3,\n    ) -&gt; None:\n\n        if not _is_typed_list(code, str):\n            raise TypeError(\"'code' must be a list of str.\")\n\n        self.code = [SQLString(elem) for elem in code]\n\n        self.dialect = dialect\n\n        self.tables = [\n            _edit_table_name(table_name)\n            for table_name in re.findall(_table_pattern(self.dialect), \"\".join(code))\n        ]\n\n    def __getitem__(self, key: Union[int, slice, str]) -&gt; Union[SQLCode, SQLString]:\n\n        if isinstance(key, int):\n            return self.code[key]\n\n        if isinstance(key, slice):\n            return SQLCode(self.code[key], self.dialect)\n\n        if isinstance(key, str):\n            if key.upper() in self.tables:\n                return self.find(_drop_table(self.dialect, key))[0]\n            return SQLString(\"\")\n\n        raise TypeError(\n            \"Features can only be indexed by: int, slices, \"\n            f\"or str, not {type(key).__name__}\"\n        )\n\n    def __iter__(self) -&gt; Iterator[SQLString]:\n        yield from self.code\n\n    def __len__(self) -&gt; int:\n        return len(self.code)\n\n    def __repr__(self) -&gt; str:\n        return \"\\n\\n\\n\".join(self.code)\n\n    def _repr_markdown_(self) -&gt; str:\n        return \"```sql\\n\" + self.__repr__() + \"\\n```\"\n\n    def find(self, keyword: str) -&gt; SQLCode:\n        \"\"\"\n        Returns the SQLCode for all features\n        containing the keyword.\n\n        Args:\n            keyword (str): The keyword to be found.\n        \"\"\"\n        if not isinstance(keyword, str):\n            raise TypeError(\"'keyword' must be a str.\")\n\n        return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n\n    def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n        \"\"\"\n        Saves the SQL code to a file.\n\n        Args:\n            fname (str):\n                The name of the file or folder (if `split` is True)\n                in which you want to save the features.\n\n            split (bool):\n                If True, the code will be split into multiple files, one for\n                each feature and saved into a folder `fname`.\n\n            remove (bool):\n                If True, the existing SQL files in `fname` folder generated\n                previously with the save method will be removed.\n        \"\"\"\n        if not split:\n            with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n                sqlfile.write(str(self))\n            return\n\n        directory = Path(fname)\n\n        if directory.exists():\n            iter_dir = os.listdir(fname)\n\n            pattern = \"^\\d{4}.*\\_.*\\.sql$\"\n\n            exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n            if not remove and exist_files_path:\n                print(f\"The following files already exist in the directory ({fname}):\")\n                for fp in np.sort(exist_files_path):\n                    print(fp)\n                print(\"Please set 'remove=True' to remove them.\")\n                return\n\n            if remove and exist_files_path:\n                for fp in exist_files_path:\n                    os.remove(fname + \"/\" + fp)\n\n        directory.mkdir(exist_ok=True)\n\n        for index, code in enumerate(self.code, 1):\n            match = re.search(_table_pattern(self.dialect), str(code))\n            name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n            name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n            file_path = directory / f\"{index:04d}_{name}.sql\"\n            with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n                sqlfile.write(str(code))\n\n    def to_str(self) -&gt; str:\n        \"\"\"\n        Returns a raw string representation of the SQL code.\n        \"\"\"\n        return str(self)\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.sql_code.SQLCode.find","title":"<code>find(keyword)</code>","text":"<p>Returns the SQLCode for all features containing the keyword.</p> <p>Parameters:</p> Name Type Description Default <code>keyword</code> <code>str</code> <p>The keyword to be found.</p> required Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def find(self, keyword: str) -&gt; SQLCode:\n    \"\"\"\n    Returns the SQLCode for all features\n    containing the keyword.\n\n    Args:\n        keyword (str): The keyword to be found.\n    \"\"\"\n    if not isinstance(keyword, str):\n        raise TypeError(\"'keyword' must be a str.\")\n\n    return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.sql_code.SQLCode.save","title":"<code>save(fname, split=True, remove=False)</code>","text":"<p>Saves the SQL code to a file.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>The name of the file or folder (if <code>split</code> is True) in which you want to save the features.</p> required <code>split</code> <code>bool</code> <p>If True, the code will be split into multiple files, one for each feature and saved into a folder <code>fname</code>.</p> <code>True</code> <code>remove</code> <code>bool</code> <p>If True, the existing SQL files in <code>fname</code> folder generated previously with the save method will be removed.</p> <code>False</code> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n    \"\"\"\n    Saves the SQL code to a file.\n\n    Args:\n        fname (str):\n            The name of the file or folder (if `split` is True)\n            in which you want to save the features.\n\n        split (bool):\n            If True, the code will be split into multiple files, one for\n            each feature and saved into a folder `fname`.\n\n        remove (bool):\n            If True, the existing SQL files in `fname` folder generated\n            previously with the save method will be removed.\n    \"\"\"\n    if not split:\n        with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(self))\n        return\n\n    directory = Path(fname)\n\n    if directory.exists():\n        iter_dir = os.listdir(fname)\n\n        pattern = \"^\\d{4}.*\\_.*\\.sql$\"\n\n        exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n        if not remove and exist_files_path:\n            print(f\"The following files already exist in the directory ({fname}):\")\n            for fp in np.sort(exist_files_path):\n                print(fp)\n            print(\"Please set 'remove=True' to remove them.\")\n            return\n\n        if remove and exist_files_path:\n            for fp in exist_files_path:\n                os.remove(fname + \"/\" + fp)\n\n    directory.mkdir(exist_ok=True)\n\n    for index, code in enumerate(self.code, 1):\n        match = re.search(_table_pattern(self.dialect), str(code))\n        name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n        name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n        file_path = directory / f\"{index:04d}_{name}.sql\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(code))\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.sql_code.SQLCode.to_str","title":"<code>to_str()</code>","text":"<p>Returns a raw string representation of the SQL code.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"\n    Returns a raw string representation of the SQL code.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/pipeline/sql_string/","title":"Sql string","text":"<p>Custom str type that holds SQL Source code.</p>"},{"location":"reference/pipeline/sql_string/#getml.pipeline.sql_string.SQLString","title":"<code>SQLString</code>","text":"<p>               Bases: <code>str</code></p> <p>A custom string type that handles the representation of SQL code strings.</p> Source code in <code>getml/pipeline/sql_string.py</code> <pre><code>class SQLString(str):\n    \"\"\"\n    A custom string type that handles the representation of SQL code strings.\n    \"\"\"\n\n    def _repr_markdown_(self) -&gt; str:\n        return \"```sql\\n\" + self + \"\\n```\"\n</code></pre>"},{"location":"reference/pipeline/table/","title":"Table","text":"<p>Contains class representing data for a table of a pipeline.</p>"},{"location":"reference/pipeline/table/#getml.pipeline.table.Table","title":"<code>Table</code>  <code>dataclass</code>","text":"<p>A dataclass that holds data about a single table.</p> Source code in <code>getml/pipeline/table.py</code> <pre><code>@dataclass\nclass Table:\n    \"\"\"\n    A dataclass that holds data about a single table.\n    \"\"\"\n\n    name: str\n    importance: float\n    target: str\n    marker: str\n</code></pre>"},{"location":"reference/pipeline/tables/","title":"Tables","text":"<p>Contains custom class for handling the tables of a pipeline.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables","title":"<code>Tables</code>","text":"<p>This container holds a pipeline's tables. These tables are build from the columns for which importances can be calculated. The motivation behind this container is to determine which tables are more important than others.</p> <p>Tables can be accessed by name, index or with a NumPy array. The container supports slicing and can be sorted and filtered. Further, the container holds global methods to request tables' importances.</p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> <p>Examples:</p> <pre><code>all_my_tables = my_pipeline.tables\nfirst_table = my_pipeline.tables[0]\nall_but_last_10_tables = my_pipeline.tables[:-10]\nimportant_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\nnames, importances = my_pipeline.tables.importances()\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>class Tables:\n    \"\"\"\n    This container holds a pipeline's tables. These tables are build from the\n    columns for which importances can be calculated. The motivation behind this\n    container is to determine which tables are more important than others.\n\n    Tables can be accessed by name, index or with a NumPy array. The container\n    supports slicing and can be sorted and filtered. Further, the container\n    holds global methods to request tables' importances.\n\n    Note:\n        The container is an iterable. So, in addition to\n        [`filter`][getml.pipeline.Tables.filter] you can also use python list\n        comprehensions for filtering.\n\n    Examples:\n        ```python\n        all_my_tables = my_pipeline.tables\n        first_table = my_pipeline.tables[0]\n        all_but_last_10_tables = my_pipeline.tables[:-10]\n        important_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\n        names, importances = my_pipeline.tables.importances()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        targets: Sequence[str],\n        columns: Columns,\n        data: Optional[Sequence[Table]] = None,\n    ) -&gt; None:\n        self._targets = targets\n        self._columns = columns\n\n        if data is not None:\n            self.data = data\n\n        else:\n            self._load_tables()\n\n        if not (targets and columns) and not data:\n            raise ValueError(\n                \"Missing required arguments. Either provide `targets` &amp; \"\n                \"`columns` or else provide `data`.\"\n            )\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(\n        self, key: Union[str, int, slice, Union[NDArray[np.int_], NDArray[np.bool_]]]\n    ) -&gt; Union[Table, Tables, list[Table]]:\n        if not self.data:\n            raise AttributeError(\"Tables container not fully initialized.\")\n\n        if isinstance(key, int):\n            return self.data[key]\n\n        if isinstance(key, slice):\n            tables_subset = self.data[key]\n            return self._make_tables(tables_subset)\n\n        if isinstance(key, str):\n            if key in self.names:\n                return [table for table in self.data if table.name == key][0]\n            raise AttributeError(f\"No Table with name: {key}\")\n\n        if isinstance(key, np.ndarray):\n            tables_subset = np.array(self.data)[key].tolist()\n            return self._make_tables(tables_subset)\n\n        raise TypeError(\n            \"Columns can only be indexed by: int, slices, str or np.ndarray,\"\n            f\" not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __iter__(self) -&gt; Iterator[Table]:\n        yield from self.data\n\n    # ----------------------------------------------------------------\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        return self._format()._render_string()\n\n    # ------------------------------------------------------------\n\n    def _repr_html_(self) -&gt; str:\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _format(self) -&gt; _Formatter:\n        headers = [[\"name\", \"importance\", \"target\", \"marker\"]]\n\n        rows = [\n            [\n                table.name,\n                table.importance,\n                table.target,\n                table.marker,\n            ]\n            for table in self.data\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def _load_tables(self) -&gt; None:\n        \"\"\"\n        Gets tables data from columns\n        \"\"\"\n\n        tables = []\n\n        for table_target in self._targets:\n            importances: dict[str, float] = {\n                column.table: 0.0\n                for column in self._columns\n                if column.target == table_target\n            }\n\n            targets: dict[str, str] = {}\n            markers: dict[str, str] = {}\n\n            for column in self._columns:\n                if column.target == table_target:\n                    importances[column.table] += column.importance\n                    targets[column.table] = column.target\n                    markers[column.table] = column.marker\n\n            tables_zip = zip(\n                importances.keys(),\n                importances.values(),\n                targets.values(),\n                markers.values(),\n            )\n\n            for name, importance, target, marker in tables_zip:\n                tables.append(\n                    Table(\n                        name=name, importance=importance, target=target, marker=marker\n                    )\n                )\n\n        self.data = tables\n\n    # ----------------------------------------------------------------\n\n    def _make_tables(self, data: Sequence[Table]) -&gt; Tables:\n        \"\"\"\n        A factory to construct a [`Tables`][getml.pipeline.Tables] container\n        from a list of [`Table`][getml.pipeline.Table]s.\n        \"\"\"\n\n        return Tables(self._targets, self._columns, data=data)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n        \"\"\"\n        Filters the tables container.\n\n        Args:\n            conditional (callable, optional):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered tables.\n\n        Example:\n            ```python\n            important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n            peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n            ```\n        \"\"\"\n        tables_filtered = [table for table in self.data if conditional(table)]\n        return self._make_tables(tables_filtered)\n\n    # ----------------------------------------------------------------\n\n    def importances(\n        self, target_num: int = 0, sort: bool = True\n    ) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n        \"\"\"\n        Returns the importances of tables.\n\n        Table importances are calculated by summing up the importances of the\n        columns belonging to the tables. Each column is assigned an importance\n        value that measures its contribution to the predictive performance. For\n        each target, the importances add up to 1.\n\n        Args:\n            target_num (int):\n                Indicates for which target you want to view the\n                importances. (Pipelines can have more than one target.)\n\n            sort (bool):\n                Whether you want the results to be sorted.\n\n        Returns:\n            The first array contains the names of the tables.\n            The second array contains their importances. By definition, all importances add up to 1.\n        \"\"\"\n\n        target_name = self._targets[target_num]\n\n        names = np.empty(0, dtype=str)\n        importances = np.empty(0, dtype=float)\n\n        for table in self.data:\n            if table.target == target_name:\n                names = np.append(names, table.name)\n                importances = np.append(importances, table.importance)\n\n        if not sort:\n            return names, importances\n\n        indices = np.argsort(importances)[::-1]\n\n        return (names[indices], importances[indices])\n\n    # ----------------------------------------------------------------\n\n    @property\n    def names(self) -&gt; list[str]:\n        \"\"\"\n        Holds the names of a [`Pipeline`][getml.Pipeline]'s tables.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [table.name for table in self.data]\n\n    # ----------------------------------------------------------------\n\n    def sort(\n        self,\n        by: Optional[str] = None,\n        key: Optional[Callable[[Table], Any]] = None,\n        descending: Optional[bool] = None,\n    ) -&gt; Tables:\n        \"\"\"\n        Sorts the Tables container. If no arguments are provided the\n        container is sorted by target and name.\n\n        Args:\n            by (str, optional):\n                The name of field to sort by. Possible fields:\n                    - name(s)\n                    - importances(s)\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n                A container of sorted tables.\n\n        Example:\n            ```python\n            by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n            ```\n        \"\"\"\n\n        reverse = False if descending is None else descending\n\n        if (by is not None) and (key is not None):\n            raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n        if key is not None:\n            tables_sorted = sorted(self.data, key=key, reverse=reverse)\n            return self._make_tables(tables_sorted)\n\n        if by is None:\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.name, reverse=reverse\n            )\n            tables_sorted.sort(key=lambda table: table.target)\n            return self._make_tables(tables_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.name, reverse=reverse\n            )\n            return self._make_tables(tables_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            tables_sorted = sorted(\n                self.data, key=lambda table: table.importance, reverse=reverse\n            )\n            return self._make_tables(tables_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n\n    # ----------------------------------------------------------------\n\n    @property\n    def targets(self) -&gt; list[str]:\n        \"\"\"\n        Holds the targets of a [`Pipeline`][getml.Pipeline]'s tables.\n\n        Returns:\n            `list` containing the names.\n\n        Note:\n            The order corresponds to the current sorting of the container.\n        \"\"\"\n        return [table.target for table in self.data]\n\n    # ----------------------------------------------------------------\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns all information related to the tables in a pandas DataFrame.\n        \"\"\"\n\n        data_frame = pd.DataFrame()\n\n        for i, table in enumerate(self.data):\n            data_frame.loc[i, \"name\"] = table.name\n            data_frame.loc[i, \"importance\"] = table.importance\n            data_frame.loc[i, \"target\"] = table.target\n            data_frame.loc[i, \"marker\"] = table.marker\n\n        return data_frame\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.names","title":"<code>names: list[str]</code>  <code>property</code>","text":"<p>Holds the names of a <code>Pipeline</code>'s tables.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.targets","title":"<code>targets: list[str]</code>  <code>property</code>","text":"<p>Holds the targets of a <code>Pipeline</code>'s tables.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p><code>list</code> containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the tables container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <code>Tables</code> <p>A container of filtered tables.</p> Example <pre><code>important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\nperipheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n    \"\"\"\n    Filters the tables container.\n\n    Args:\n        conditional (callable, optional):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered tables.\n\n    Example:\n        ```python\n        important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n        peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    tables_filtered = [table for table in self.data if conditional(table)]\n    return self._make_tables(tables_filtered)\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.importances","title":"<code>importances(target_num=0, sort=True)</code>","text":"<p>Returns the importances of tables.</p> <p>Table importances are calculated by summing up the importances of the columns belonging to the tables. Each column is assigned an importance value that measures its contribution to the predictive performance. For each target, the importances add up to 1.</p> <p>Parameters:</p> Name Type Description Default <code>target_num</code> <code>int</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <code>0</code> <code>sort</code> <code>bool</code> <p>Whether you want the results to be sorted.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArray[str_]</code> <p>The first array contains the names of the tables.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the importances of tables.\n\n    Table importances are calculated by summing up the importances of the\n    columns belonging to the tables. Each column is assigned an importance\n    value that measures its contribution to the predictive performance. For\n    each target, the importances add up to 1.\n\n    Args:\n        target_num (int):\n            Indicates for which target you want to view the\n            importances. (Pipelines can have more than one target.)\n\n        sort (bool):\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the tables.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    target_name = self._targets[target_num]\n\n    names = np.empty(0, dtype=str)\n    importances = np.empty(0, dtype=float)\n\n    for table in self.data:\n        if table.target == target_name:\n            names = np.append(names, table.name)\n            importances = np.append(importances, table.importance)\n\n    if not sort:\n        return names, importances\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.sort","title":"<code>sort(by=None, key=None, descending=None)</code>","text":"<p>Sorts the Tables container. If no arguments are provided the container is sorted by target and name.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>The name of field to sort by. Possible fields:     - name(s)     - importances(s)</p> <code>None</code> <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> <code>None</code> <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tables</code> <p>A container of sorted tables.</p> Example <pre><code>by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables:\n    \"\"\"\n    Sorts the Tables container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by (str, optional):\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - importances(s)\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted tables.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        tables_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_tables(tables_sorted)\n\n    if by is None:\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        tables_sorted.sort(key=lambda table: table.target)\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.importance, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.tables.Tables.to_pandas","title":"<code>to_pandas()</code>","text":"<p>Returns all information related to the tables in a pandas DataFrame.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the tables in a pandas DataFrame.\n    \"\"\"\n\n    data_frame = pd.DataFrame()\n\n    for i, table in enumerate(self.data):\n        data_frame.loc[i, \"name\"] = table.name\n        data_frame.loc[i, \"importance\"] = table.importance\n        data_frame.loc[i, \"target\"] = table.target\n        data_frame.loc[i, \"marker\"] = table.marker\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/tags/","title":"Tags","text":"<p>A small lists-type class that allows to search for arbitrary substrings within its items.</p>"},{"location":"reference/pipeline/tags/#getml.pipeline.tags.Tags","title":"<code>Tags</code>","text":"<p>               Bases: <code>list</code></p> <p>A small lists-type class that allows to search for arbitrary substrings within its items.</p> Source code in <code>getml/pipeline/tags.py</code> <pre><code>class Tags(list):\n    \"\"\"\n    A small lists-type class that allows to search for arbitrary substrings within its items.\n    \"\"\"\n\n    def __contains__(self, substr: object) -&gt; bool:\n        if not isinstance(substr, str):\n            raise ValueError(\"Tags can only contain strings.\")\n        return any(substr in tag for tag in self)\n</code></pre>"},{"location":"reference/predictors/__init__/","title":"init","text":"<p>This module contains machine learning algorithms to learn and predict on the generated features.</p> <p>The predictor classes defined in this module serve two purposes. First, a predictor can be used as a <code>feature_selector</code> in <code>Pipeline</code> to only select the best features generated during the automated feature learning and to get rid of any redundancies. Second, by using it as a <code>predictor</code>, it will be trained on the features of the supplied data set and used to predict to unknown results. Every time a new data set is passed to the <code>predict</code> method of one of the models, the raw relational data is interpreted in the data model, which was provided during the construction of the model, transformed into features using the trained feature learning algorithm, and, finally, its target will be predicted using the trained predictor.</p> <p>The algorithms can be grouped according to their finesse and whether you want to use them for a classification or regression problem.</p> simple sophisticated regression <code>LinearRegression</code> <code>XGBoostRegressor</code> classification <code>LogisticRegression</code> <code>XGBoostClassifier</code> Note <p>All predictors need to be passed to <code>Pipeline</code>.</p>"},{"location":"reference/predictors/__init__/#getml.predictors.LinearRegression","title":"<code>LinearRegression</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for regression problems.</p> <p>Learns a simple linear relationship using ordinary least squares (OLS) regression:</p> \\[ \\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ... \\] <p>The weights are optimized by minimizing the squared loss of the predictions \\(\\hat{y}\\) w.r.t. the target \\(y\\).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2 \\] <p>Linear regressions can be trained arithmetically or numerically. Training arithmetically is more accurate, but suffers worse scalability.</p> <p>If you decide to pass categorical features to the <code>LinearRegression</code>, it will be trained numerically. Otherwise, it will be trained arithmetically.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate used for training numerically (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <code>0.9</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <code>1e-10</code> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>@dataclass(repr=False)\nclass LinearRegression(_Predictor):\n    \"\"\"\nSimple predictor for regression problems.\n\nLearns a simple linear relationship using ordinary least squares (OLS)\nregression:\n\n$$\n\\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ...\n$$\n\nThe weights are optimized by minimizing the squared loss of the\npredictions $\\hat{y}$ w.r.t. the [target][annotating-data-target] $y$.\n\n$$\nL(y,\\hat{y}) = \\\\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2\n$$\n\nLinear regressions can be trained arithmetically or numerically.\nTraining arithmetically is more accurate, but suffers worse\nscalability.\n\nIf you decide to pass [categorical features][annotating-data-categorical] to the\n[`LinearRegression`][getml.predictors.LinearRegression], it will be trained\nnumerically. Otherwise, it will be trained arithmetically.\n\nArgs:\n    learning_rate (float, optional):\n        The learning rate used for training numerically (only\n        relevant when categorical features are included). Range:\n        (0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization parameter. Range: [0, $\\infty$]\n\n\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    learning_rate: float = 0.9\n    reg_lambda: float = 1e-10\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If nothing is passed,\n                the default parameters will be validated.\n\n        Example:\n            ```python\n            l = getml.predictors.LinearRegression()\n            l.learning_rate = 8.1\n            l.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.LinearRegression.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If nothing is passed, the default parameters will be validated.</p> <code>None</code> Example <pre><code>l = getml.predictors.LinearRegression()\nl.learning_rate = 8.1\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If nothing is passed,\n            the default parameters will be validated.\n\n    Example:\n        ```python\n        l = getml.predictors.LinearRegression()\n        l.learning_rate = 8.1\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.LogisticRegression","title":"<code>LogisticRegression</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for classification problems.</p> <p>Learns a simple linear relationship using the sigmoid function:</p> \\[ \\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...) \\] <p>\\(\\sigma\\) denotes the sigmoid function:</p> \\[ \\sigma(z) = \\frac{1}{1 + exp(-z)} \\] <p>The weights are optimized by minimizing the cross entropy loss of the predictions \\(\\hat{y}\\) w.r.t. the targets \\(y\\).</p> \\[ L(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y}) \\] <p>Logistic regressions are always trained numerically.</p> <p>If you decide to pass categorical features: <code>annotating_roles_categorical</code> to the <code>LogisticRegression</code>, it will be trained using the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm. Otherwise, it will be trained using adaptive moments (Adam). BFGS is more accurate, but less scalable than Adam.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate used for the Adaptive Moments algorithm (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <code>0.9</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <code>1e-10</code> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>@dataclass(repr=False)\nclass LogisticRegression(_Predictor):\n    \"\"\"Simple predictor for classification problems.\n\nLearns a simple linear relationship using the sigmoid function:\n\n$$\n\\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...)\n$$\n\n$\\sigma$ denotes the sigmoid function:\n\n$$\n\\sigma(z) = \\\\frac{1}{1 + exp(-z)}\n$$\n\nThe weights are optimized by minimizing the cross entropy loss of\nthe predictions $\\hat{y}$ w.r.t. the [targets][annotating-data-target] $y$.\n\n$$\nL(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y})\n$$\n\nLogistic regressions are always trained numerically.\n\nIf you decide to pass categorical\nfeatures: `annotating_roles_categorical` to the\n[`LogisticRegression`][getml.predictors.LogisticRegression], it will be trained\nusing the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm.\nOtherwise, it will be trained using adaptive moments (Adam). BFGS\nis more accurate, but less scalable than Adam.\n\nArgs:\n    learning_rate (float, optional):\n        The learning rate used for the Adaptive Moments algorithm\n        (only relevant when categorical features are\n        included). Range: (0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization parameter. Range: [0, $\\infty$]\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    learning_rate: float = 0.9\n    reg_lambda: float = 1e-10\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Examples:\n            ```python\n            l = getml.predictors.LogisticRegression()\n            l.learning_rate = 20\n            l.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.LogisticRegression.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> <p>Examples:</p> <pre><code>l = getml.predictors.LogisticRegression()\nl.learning_rate = 20\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Examples:\n        ```python\n        l = getml.predictors.LogisticRegression()\n        l.learning_rate = 20\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.ScaleGBMClassifier","title":"<code>ScaleGBMClassifier</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting classifier that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>early_stopping_rounds</code> <code>int</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <code>10</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>goss_a</code> <code>float</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <code>1.0</code> <code>goss_b</code> <code>float</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>5843</code> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>@dataclass(repr=False)\nclass ScaleGBMClassifier(_Predictor):\n    \"\"\"Standard gradient boosting classifier that fully supports memory mapping\n   and can be used for datasets that do not fit into memory.\n\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that ScaleGBM grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    early_stopping_rounds (int, optional):\n        The number of early_stopping_rounds for which we see\n        no improvement on the validation set until we stop\n        the training process.\n\n        Range: (0, $\\infty$]\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    goss_a (float, optional):\n        Share of the samples with the largest residuals\n        taken for each tree.\n\n        If `goss_a` is set to 1, then gradients one-sided\n        sampling is effectively turned off.\n\n        Range: [0, 1]\n\n    goss_b (float, optional):\n        Share of the samples that are not in the `goss_a`\n        percentile of largest residuals randomly sampled\n        for each tree.\n\n        The sum of `goss_a` and `goss_b` cannot exceed\n        1.\n\n        Range: [0, 1]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    seed (int, optional):\n        Seed used for random sampling and other random\n        factors.\n\n        Range: [0, $\\infty$]\n\n    \"\"\"\n\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    goss_a: float = 1.0\n    goss_b: float = 0.0\n    learning_rate: float = 0.1\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    n_jobs: int = 1\n    objective: str = \"binary:logistic\"\n    reg_lambda: float = 1.0\n    seed: int = 5843\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.ScaleGBMClassifier.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.ScaleGBMRegressor","title":"<code>ScaleGBMRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting regressor that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>The regressor implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>early_stopping_rounds</code> <code>int</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <code>10</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>goss_a</code> <code>float</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <code>1.0</code> <code>goss_b</code> <code>float</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>5843</code> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>@dataclass(repr=False)\nclass ScaleGBMRegressor(_Predictor):\n    \"\"\"Standard gradient boosting regressor that fully supports memory mapping\n   and can be used for datasets that do not fit into memory.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nThe regressor implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that ScaleGBM grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    early_stopping_rounds (int, optional):\n        The number of early_stopping_rounds for which we see\n        no improvement on the validation set until we stop\n        the training process.\n\n        Range: (0, $\\infty$]\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    goss_a (float, optional):\n        Share of the samples with the largest residuals\n        taken for each tree.\n\n        If `goss_a` is set to 1, then gradients one-sided\n        sampling is effectively turned off.\n\n        Range: [0, 1]\n\n    goss_b (float, optional):\n        Share of the samples that are not in the `goss_a`\n        percentile of largest residuals randomly sampled\n        for each tree.\n\n        The sum of `goss_a` and `goss_b` cannot exceed\n        1.\n\n        Range: [0, 1]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    seed (int, optional):\n        Seed used for random sampling and other random\n        factors.\n\n        Range: [0, $\\infty$]\n    \"\"\"\n\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    goss_a: float = 1.0\n    goss_b: float = 0.0\n    learning_rate: float = 0.1\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    n_jobs: int = 1\n    objective: str = \"reg:squarederror\"\n    reg_lambda: float = 1.0\n    seed: int = 5843\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.ScaleGBMRegressor.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.XGBoostClassifier","title":"<code>XGBoostClassifier</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting classifier based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>booster</code> <code>string</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on 'dart'.</li> </ul> <code>'gbtree'</code> <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>external_memory</code> <code>bool</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_delta_step</code> <code>float</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <code>0.0</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>normalize_type</code> <code>string</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p>'tree': a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p>'forest': a new tree has the same weight as a the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>'tree'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>objective</code> <code>string</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:logistic</code></li> <li><code>binary:logistic</code></li> <li><code>binary:logitraw</code></li> </ul> <code>'binary:logistic'</code> <code>one_drop</code> <code>bool</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>False</code> <code>rate_drop</code> <code>float</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>0.0</code> <code>reg_alpha</code> <code>float</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>sample_type</code> <code>string</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>'uniform'</code> <code>silent</code> <code>bool</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <code>True</code> <code>skip_drop</code> <code>float</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>subsample</code> <code>float</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>@dataclass(repr=False)\nclass XGBoostClassifier(_Predictor):\n    \"\"\"Gradient boosting classifier based on\n[xgboost ](https://xgboost.readthedocs.io/en/latest/).\n\nXGBoost is an implementation of the gradient tree boosting algorithm that\nis widely recognized for its efficiency and predictive accuracy.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    booster (string, optional):\n        Which base classifier to use.\n\n        Possible values:\n\n        * `gbtree`: normal gradient boosted decision trees\n        * `gblinear`: uses a linear model instead of decision trees\n        * 'dart': adds dropout to the standard gradient boosting algorithm.\n          Please also refer to the remarks on *rate_drop* for further\n          explanation on 'dart'.\n\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that XGBoost grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    external_memory (bool, optional):\n        When the in_memory flag of the engine is set to False,\n        XGBoost can use the external memory functionality.\n        This reduces the memory consumption,\n        but can also affect the quality of the predictions.\n        External memory is deactivated by default and it\n        is recommended to only use external memory\n        for feature selection.\n        When the in_memory flag of the engine is set to True,\n        (the default value), XGBoost will never use\n        external memory.\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_delta_step (float, optional):\n        The maximum delta step allowed for the weight estimation\n        of each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$)\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n\n    normalize_type (string, optional):\n        This determines how to normalize trees during 'dart'.\n\n        Possible values:\n\n        * 'tree': a new tree has the same weight as a single\n          dropped tree.\n\n        * 'forest': a new tree has the same weight as a the sum of\n          all dropped trees.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    objective (string, optional):\n        Specify the learning task and the corresponding\n        learning objective.\n\n        Possible values:\n\n        * `reg:logistic`\n        * `binary:logistic`\n        * `binary:logitraw`\n\n    one_drop (bool, optional):\n        If set to True, then at least one tree will always be\n        dropped out. Setting this hyperparameter to *true* reduces\n        the likelihood of overfitting.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    rate_drop (float, optional):\n        Dropout rate for trees - determines the probability\n        that a tree will be dropped out. Dropout is an\n        algorithm that enjoys considerable popularity in\n        the deep learning community. It means that every node can\n        be randomly removed during training.\n\n        This approach\n        can also be applied to gradient boosting, where it\n        means that every tree can be randomly removed with\n        a certain probability. Said probability is determined\n        by *rate_drop*. Dropout for gradient boosting is\n        referred to as the 'dart' algorithm.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    reg_alpha (float, optional):\n        L1 regularization on the weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    sample_type (string, optional):\n        Possible values:\n\n        * `uniform`: every tree is equally likely to be dropped\n          out\n\n        * `weighted`: the dropout probability will be proportional\n          to a tree's weight\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    silent (bool, optional):\n        In silent mode, XGBoost will not print out information on\n        the training progress.\n\n    skip_drop (float, optional):\n        Probability of skipping the dropout during a given\n        iteration. Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n        Range: [0, 1]\n\n    subsample (float, optional):\n        Subsample ratio from the training set. This means\n        that for every tree a subselection of *samples*\n        from the training set will be included into training.\n        Please note that this samples *without* replacement -\n        the common approach for random forests is to sample\n        *with* replace.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    \"\"\"\n\n    booster: str = \"gbtree\"\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    learning_rate: float = 0.1\n    max_delta_step: float = 0.0\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    external_memory: bool = False\n    normalize_type: str = \"tree\"\n    num_parallel_tree: int = 1\n    n_jobs: int = 1\n    objective: str = \"binary:logistic\"\n    one_drop: bool = False\n    rate_drop: float = 0.0\n    reg_alpha: float = 0.0\n    reg_lambda: float = 1.0\n    sample_type: str = \"uniform\"\n    silent: bool = True\n    skip_drop: float = 0.0\n    subsample: float = 1.0\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Example:\n            ```python\n            x = getml.predictors.XGBoostClassifier()\n            x.gamma = 200\n            x.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_xgboost_parameters(params)\n\n        # ------------------------------------------------------------\n\n        if params[\"objective\"] not in [\n            \"reg:logistic\",\n            \"binary:logistic\",\n            \"binary:logitraw\",\n        ]:\n            raise ValueError(\n                \"\"\"'objective' supported in XGBoostClassifier\n                                 are 'reg:logistic', 'binary:logistic',\n                                 and 'binary:logitraw'\"\"\"\n            )\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.XGBoostClassifier.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Example <pre><code>x = getml.predictors.XGBoostClassifier()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostClassifier()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\n        \"reg:logistic\",\n        \"binary:logistic\",\n        \"binary:logitraw\",\n    ]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostClassifier\n                             are 'reg:logistic', 'binary:logistic',\n                             and 'binary:logitraw'\"\"\"\n        )\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.XGBoostRegressor","title":"<code>XGBoostRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting regressor based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>booster</code> <code>string</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on <code>dart</code>.</li> </ul> <code>'gbtree'</code> <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>external_memory</code> <code>bool</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_delta_step</code> <code>float</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <code>0.0</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>normalize_type</code> <code>string</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p><code>tree</code>: a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p><code>forest</code>: a new tree has the same weight as the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>'tree'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>objective</code> <code>string</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> <li><code>reg:tweedie</code></li> </ul> <code>'reg:squarederror'</code> <code>one_drop</code> <code>bool</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>False</code> <code>rate_drop</code> <code>float</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>0.0</code> <code>reg_alpha</code> <code>float</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>sample_type</code> <code>string</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>'uniform'</code> <code>silent</code> <code>bool</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <code>True</code> <code>skip_drop</code> <code>float</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>subsample</code> <code>float</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>@dataclass(repr=False)\nclass XGBoostRegressor(_Predictor):\n    \"\"\"Gradient boosting regressor based on [xgboost ](https://xgboost.readthedocs.io/en/latest/).\n\nXGBoost is an implementation of the gradient tree boosting algorithm that\nis widely recognized for its efficiency and predictive accuracy.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    booster (string, optional):\n        Which base classifier to use.\n\n        Possible values:\n\n        * `gbtree`: normal gradient boosted decision trees\n        * `gblinear`: uses a linear model instead of decision trees\n        * 'dart': adds dropout to the standard gradient boosting algorithm.\n          Please also refer to the remarks on *rate_drop* for further\n          explanation on `dart`.\n\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that XGBoost grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    external_memory (bool, optional):\n        When the in_memory flag of the engine is set to False,\n        XGBoost can use the external memory functionality.\n        This reduces the memory consumption,\n        but can also affect the quality of the predictions.\n        External memory is deactivated by default and it\n        is recommended to only use external memory\n        for feature selection.\n        When the in_memory flag of the engine is set to True,\n        (the default value), XGBoost will never use\n        external memory.\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_delta_step (float, optional):\n        The maximum delta step allowed for the weight estimation\n        of each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$)\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    normalize_type (string, optional):\n        This determines how to normalize trees during 'dart'.\n\n        Possible values:\n\n        * `tree`: a new tree has the same weight as a single\n          dropped tree.\n\n        * `forest`: a new tree has the same weight as the sum of\n          all dropped trees.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    objective (string, optional):\n        Specify the learning task and the corresponding\n        learning objective.\n\n        Possible values:\n\n        * `reg:squarederror`\n        * `reg:tweedie`\n\n    one_drop (bool, optional):\n        If set to True, then at least one tree will always be\n        dropped out. Setting this hyperparameter to *true* reduces\n        the likelihood of overfitting.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    rate_drop (float, optional):\n        Dropout rate for trees - determines the probability\n        that a tree will be dropped out. Dropout is an\n        algorithm that enjoys considerable popularity in\n        the deep learning community. It means that every node can\n        be randomly removed during training.\n\n        This approach\n        can also be applied to gradient boosting, where it\n        means that every tree can be randomly removed with\n        a certain probability. Said probability is determined\n        by *rate_drop*. Dropout for gradient boosting is\n        referred to as the 'dart' algorithm.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    reg_alpha (float, optional):\n        L1 regularization on the weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    sample_type (string, optional):\n        Possible values:\n\n        * `uniform`: every tree is equally likely to be dropped\n          out\n\n        * `weighted`: the dropout probability will be proportional\n          to a tree's weight\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    silent (bool, optional):\n        In silent mode, XGBoost will not print out information on\n        the training progress.\n\n    skip_drop (float, optional):\n        Probability of skipping the dropout during a given\n        iteration. Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n        Range: [0, 1]\n\n    subsample (float, optional):\n        Subsample ratio from the training set. This means\n        that for every tree a subselection of *samples*\n        from the training set will be included into training.\n        Please note that this samples *without* replacement -\n        the common approach for random forests is to sample\n        *with* replace.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n    \"\"\"\n\n    booster: str = \"gbtree\"\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    external_memory: bool = False\n    gamma: float = 0.0\n    learning_rate: float = 0.1\n    max_delta_step: float = 0.0\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    normalize_type: str = \"tree\"\n    num_parallel_tree: int = 1\n    n_jobs: int = 1\n    objective: str = \"reg:squarederror\"\n    one_drop: bool = False\n    rate_drop: float = 0.0\n    reg_alpha: float = 0.0\n    reg_lambda: float = 1.0\n    sample_type: str = \"uniform\"\n    silent: bool = True\n    skip_drop: float = 0.0\n    subsample: float = 1.0\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Example:\n            ```python\n            x = getml.predictors.XGBoostRegressor()\n            x.gamma = 200\n            x.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_xgboost_parameters(params)\n\n        # ------------------------------------------------------------\n\n        if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n            raise ValueError(\n                \"\"\"'objective' supported in XGBoostRegressor\n                                 are 'reg:squarederror', 'reg:tweedie',\n                                 and 'reg:linear'\"\"\"\n            )\n</code></pre>"},{"location":"reference/predictors/__init__/#getml.predictors.XGBoostRegressor.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Example <pre><code>x = getml.predictors.XGBoostRegressor()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostRegressor()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostRegressor\n                             are 'reg:squarederror', 'reg:tweedie',\n                             and 'reg:linear'\"\"\"\n        )\n</code></pre>"},{"location":"reference/predictors/linear_regression/","title":"Linear regression","text":"<p>A simple linear regression model for predicting regression problems.</p>"},{"location":"reference/predictors/linear_regression/#getml.predictors.linear_regression.LinearRegression","title":"<code>LinearRegression</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for regression problems.</p> <p>Learns a simple linear relationship using ordinary least squares (OLS) regression:</p> \\[ \\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ... \\] <p>The weights are optimized by minimizing the squared loss of the predictions \\(\\hat{y}\\) w.r.t. the target \\(y\\).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2 \\] <p>Linear regressions can be trained arithmetically or numerically. Training arithmetically is more accurate, but suffers worse scalability.</p> <p>If you decide to pass categorical features to the <code>LinearRegression</code>, it will be trained numerically. Otherwise, it will be trained arithmetically.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate used for training numerically (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <code>0.9</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <code>1e-10</code> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>@dataclass(repr=False)\nclass LinearRegression(_Predictor):\n    \"\"\"\nSimple predictor for regression problems.\n\nLearns a simple linear relationship using ordinary least squares (OLS)\nregression:\n\n$$\n\\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ...\n$$\n\nThe weights are optimized by minimizing the squared loss of the\npredictions $\\hat{y}$ w.r.t. the [target][annotating-data-target] $y$.\n\n$$\nL(y,\\hat{y}) = \\\\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2\n$$\n\nLinear regressions can be trained arithmetically or numerically.\nTraining arithmetically is more accurate, but suffers worse\nscalability.\n\nIf you decide to pass [categorical features][annotating-data-categorical] to the\n[`LinearRegression`][getml.predictors.LinearRegression], it will be trained\nnumerically. Otherwise, it will be trained arithmetically.\n\nArgs:\n    learning_rate (float, optional):\n        The learning rate used for training numerically (only\n        relevant when categorical features are included). Range:\n        (0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization parameter. Range: [0, $\\infty$]\n\n\n\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    learning_rate: float = 0.9\n    reg_lambda: float = 1e-10\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If nothing is passed,\n                the default parameters will be validated.\n\n        Example:\n            ```python\n            l = getml.predictors.LinearRegression()\n            l.learning_rate = 8.1\n            l.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/linear_regression/#getml.predictors.linear_regression.LinearRegression.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If nothing is passed, the default parameters will be validated.</p> <code>None</code> Example <pre><code>l = getml.predictors.LinearRegression()\nl.learning_rate = 8.1\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If nothing is passed,\n            the default parameters will be validated.\n\n    Example:\n        ```python\n        l = getml.predictors.LinearRegression()\n        l.learning_rate = 8.1\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/logistic_regression/","title":"Logistic regression","text":"<p>A simple logistic regression model for predicting classification problems.</p>"},{"location":"reference/predictors/logistic_regression/#getml.predictors.logistic_regression.LogisticRegression","title":"<code>LogisticRegression</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for classification problems.</p> <p>Learns a simple linear relationship using the sigmoid function:</p> \\[ \\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...) \\] <p>\\(\\sigma\\) denotes the sigmoid function:</p> \\[ \\sigma(z) = \\frac{1}{1 + exp(-z)} \\] <p>The weights are optimized by minimizing the cross entropy loss of the predictions \\(\\hat{y}\\) w.r.t. the targets \\(y\\).</p> \\[ L(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y}) \\] <p>Logistic regressions are always trained numerically.</p> <p>If you decide to pass categorical features: <code>annotating_roles_categorical</code> to the <code>LogisticRegression</code>, it will be trained using the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm. Otherwise, it will be trained using adaptive moments (Adam). BFGS is more accurate, but less scalable than Adam.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate used for the Adaptive Moments algorithm (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <code>0.9</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <code>1e-10</code> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>@dataclass(repr=False)\nclass LogisticRegression(_Predictor):\n    \"\"\"Simple predictor for classification problems.\n\nLearns a simple linear relationship using the sigmoid function:\n\n$$\n\\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...)\n$$\n\n$\\sigma$ denotes the sigmoid function:\n\n$$\n\\sigma(z) = \\\\frac{1}{1 + exp(-z)}\n$$\n\nThe weights are optimized by minimizing the cross entropy loss of\nthe predictions $\\hat{y}$ w.r.t. the [targets][annotating-data-target] $y$.\n\n$$\nL(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y})\n$$\n\nLogistic regressions are always trained numerically.\n\nIf you decide to pass categorical\nfeatures: `annotating_roles_categorical` to the\n[`LogisticRegression`][getml.predictors.LogisticRegression], it will be trained\nusing the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm.\nOtherwise, it will be trained using adaptive moments (Adam). BFGS\nis more accurate, but less scalable than Adam.\n\nArgs:\n    learning_rate (float, optional):\n        The learning rate used for the Adaptive Moments algorithm\n        (only relevant when categorical features are\n        included). Range: (0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization parameter. Range: [0, $\\infty$]\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    learning_rate: float = 0.9\n    reg_lambda: float = 1e-10\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Examples:\n            ```python\n            l = getml.predictors.LogisticRegression()\n            l.learning_rate = 20\n            l.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/logistic_regression/#getml.predictors.logistic_regression.LogisticRegression.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> <p>Examples:</p> <pre><code>l = getml.predictors.LogisticRegression()\nl.learning_rate = 20\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Examples:\n        ```python\n        l = getml.predictors.LogisticRegression()\n        l.learning_rate = 20\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/predictor/","title":"Predictor","text":"<p>Base class. Should not ever be directly initialized!</p>"},{"location":"reference/predictors/scale_gbm_classifier/","title":"Scale gbm classifier","text":"<p>A gradient boosting model for predicting classification problems.</p>"},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.scale_gbm_classifier.ScaleGBMClassifier","title":"<code>ScaleGBMClassifier</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting classifier that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>early_stopping_rounds</code> <code>int</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <code>10</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>goss_a</code> <code>float</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <code>1.0</code> <code>goss_b</code> <code>float</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>5843</code> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>@dataclass(repr=False)\nclass ScaleGBMClassifier(_Predictor):\n    \"\"\"Standard gradient boosting classifier that fully supports memory mapping\n   and can be used for datasets that do not fit into memory.\n\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that ScaleGBM grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    early_stopping_rounds (int, optional):\n        The number of early_stopping_rounds for which we see\n        no improvement on the validation set until we stop\n        the training process.\n\n        Range: (0, $\\infty$]\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    goss_a (float, optional):\n        Share of the samples with the largest residuals\n        taken for each tree.\n\n        If `goss_a` is set to 1, then gradients one-sided\n        sampling is effectively turned off.\n\n        Range: [0, 1]\n\n    goss_b (float, optional):\n        Share of the samples that are not in the `goss_a`\n        percentile of largest residuals randomly sampled\n        for each tree.\n\n        The sum of `goss_a` and `goss_b` cannot exceed\n        1.\n\n        Range: [0, 1]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    seed (int, optional):\n        Seed used for random sampling and other random\n        factors.\n\n        Range: [0, $\\infty$]\n\n    \"\"\"\n\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    goss_a: float = 1.0\n    goss_b: float = 0.0\n    learning_rate: float = 0.1\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    n_jobs: int = 1\n    objective: str = \"binary:logistic\"\n    reg_lambda: float = 1.0\n    seed: int = 5843\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.scale_gbm_classifier.ScaleGBMClassifier.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_regressor/","title":"Scale gbm regressor","text":"<p>A gradient boosting model for predicting regression problems.</p>"},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.scale_gbm_regressor.ScaleGBMRegressor","title":"<code>ScaleGBMRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting regressor that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>The regressor implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>early_stopping_rounds</code> <code>int</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <code>10</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>goss_a</code> <code>float</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <code>1.0</code> <code>goss_b</code> <code>float</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>seed</code> <code>int</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>5843</code> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>@dataclass(repr=False)\nclass ScaleGBMRegressor(_Predictor):\n    \"\"\"Standard gradient boosting regressor that fully supports memory mapping\n   and can be used for datasets that do not fit into memory.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nThe regressor implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that ScaleGBM grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    early_stopping_rounds (int, optional):\n        The number of early_stopping_rounds for which we see\n        no improvement on the validation set until we stop\n        the training process.\n\n        Range: (0, $\\infty$]\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    goss_a (float, optional):\n        Share of the samples with the largest residuals\n        taken for each tree.\n\n        If `goss_a` is set to 1, then gradients one-sided\n        sampling is effectively turned off.\n\n        Range: [0, 1]\n\n    goss_b (float, optional):\n        Share of the samples that are not in the `goss_a`\n        percentile of largest residuals randomly sampled\n        for each tree.\n\n        The sum of `goss_a` and `goss_b` cannot exceed\n        1.\n\n        Range: [0, 1]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    seed (int, optional):\n        Seed used for random sampling and other random\n        factors.\n\n        Range: [0, $\\infty$]\n    \"\"\"\n\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    goss_a: float = 1.0\n    goss_b: float = 0.0\n    learning_rate: float = 0.1\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    n_jobs: int = 1\n    objective: str = \"reg:squarederror\"\n    reg_lambda: float = 1.0\n    seed: int = 5843\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        unsupported_params = [\n            k for k in params if k not in type(self)._supported_params\n        ]\n\n        if unsupported_params:\n            raise KeyError(\n                \"The following instance variables are not supported \"\n                + f\"in {self.type}: {unsupported_params}\"\n            )\n\n        _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.scale_gbm_regressor.ScaleGBMRegressor.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/xgboost_classifier/","title":"Xgboost classifier","text":"<p>A gradient boosting model for predicting classification problems.</p>"},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.xgboost_classifier.XGBoostClassifier","title":"<code>XGBoostClassifier</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting classifier based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>booster</code> <code>string</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on 'dart'.</li> </ul> <code>'gbtree'</code> <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>external_memory</code> <code>bool</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_delta_step</code> <code>float</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <code>0.0</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>normalize_type</code> <code>string</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p>'tree': a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p>'forest': a new tree has the same weight as a the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>'tree'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>objective</code> <code>string</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:logistic</code></li> <li><code>binary:logistic</code></li> <li><code>binary:logitraw</code></li> </ul> <code>'binary:logistic'</code> <code>one_drop</code> <code>bool</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>False</code> <code>rate_drop</code> <code>float</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>0.0</code> <code>reg_alpha</code> <code>float</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>sample_type</code> <code>string</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>'uniform'</code> <code>silent</code> <code>bool</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <code>True</code> <code>skip_drop</code> <code>float</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>subsample</code> <code>float</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>@dataclass(repr=False)\nclass XGBoostClassifier(_Predictor):\n    \"\"\"Gradient boosting classifier based on\n[xgboost ](https://xgboost.readthedocs.io/en/latest/).\n\nXGBoost is an implementation of the gradient tree boosting algorithm that\nis widely recognized for its efficiency and predictive accuracy.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    booster (string, optional):\n        Which base classifier to use.\n\n        Possible values:\n\n        * `gbtree`: normal gradient boosted decision trees\n        * `gblinear`: uses a linear model instead of decision trees\n        * 'dart': adds dropout to the standard gradient boosting algorithm.\n          Please also refer to the remarks on *rate_drop* for further\n          explanation on 'dart'.\n\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that XGBoost grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    external_memory (bool, optional):\n        When the in_memory flag of the engine is set to False,\n        XGBoost can use the external memory functionality.\n        This reduces the memory consumption,\n        but can also affect the quality of the predictions.\n        External memory is deactivated by default and it\n        is recommended to only use external memory\n        for feature selection.\n        When the in_memory flag of the engine is set to True,\n        (the default value), XGBoost will never use\n        external memory.\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_delta_step (float, optional):\n        The maximum delta step allowed for the weight estimation\n        of each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$)\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n\n    normalize_type (string, optional):\n        This determines how to normalize trees during 'dart'.\n\n        Possible values:\n\n        * 'tree': a new tree has the same weight as a single\n          dropped tree.\n\n        * 'forest': a new tree has the same weight as a the sum of\n          all dropped trees.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    objective (string, optional):\n        Specify the learning task and the corresponding\n        learning objective.\n\n        Possible values:\n\n        * `reg:logistic`\n        * `binary:logistic`\n        * `binary:logitraw`\n\n    one_drop (bool, optional):\n        If set to True, then at least one tree will always be\n        dropped out. Setting this hyperparameter to *true* reduces\n        the likelihood of overfitting.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    rate_drop (float, optional):\n        Dropout rate for trees - determines the probability\n        that a tree will be dropped out. Dropout is an\n        algorithm that enjoys considerable popularity in\n        the deep learning community. It means that every node can\n        be randomly removed during training.\n\n        This approach\n        can also be applied to gradient boosting, where it\n        means that every tree can be randomly removed with\n        a certain probability. Said probability is determined\n        by *rate_drop*. Dropout for gradient boosting is\n        referred to as the 'dart' algorithm.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    reg_alpha (float, optional):\n        L1 regularization on the weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    sample_type (string, optional):\n        Possible values:\n\n        * `uniform`: every tree is equally likely to be dropped\n          out\n\n        * `weighted`: the dropout probability will be proportional\n          to a tree's weight\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    silent (bool, optional):\n        In silent mode, XGBoost will not print out information on\n        the training progress.\n\n    skip_drop (float, optional):\n        Probability of skipping the dropout during a given\n        iteration. Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n        Range: [0, 1]\n\n    subsample (float, optional):\n        Subsample ratio from the training set. This means\n        that for every tree a subselection of *samples*\n        from the training set will be included into training.\n        Please note that this samples *without* replacement -\n        the common approach for random forests is to sample\n        *with* replace.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    \"\"\"\n\n    booster: str = \"gbtree\"\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    gamma: float = 0.0\n    learning_rate: float = 0.1\n    max_delta_step: float = 0.0\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    external_memory: bool = False\n    normalize_type: str = \"tree\"\n    num_parallel_tree: int = 1\n    n_jobs: int = 1\n    objective: str = \"binary:logistic\"\n    one_drop: bool = False\n    rate_drop: float = 0.0\n    reg_alpha: float = 0.0\n    reg_lambda: float = 1.0\n    sample_type: str = \"uniform\"\n    silent: bool = True\n    skip_drop: float = 0.0\n    subsample: float = 1.0\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Example:\n            ```python\n            x = getml.predictors.XGBoostClassifier()\n            x.gamma = 200\n            x.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_xgboost_parameters(params)\n\n        # ------------------------------------------------------------\n\n        if params[\"objective\"] not in [\n            \"reg:logistic\",\n            \"binary:logistic\",\n            \"binary:logitraw\",\n        ]:\n            raise ValueError(\n                \"\"\"'objective' supported in XGBoostClassifier\n                                 are 'reg:logistic', 'binary:logistic',\n                                 and 'binary:logitraw'\"\"\"\n            )\n</code></pre>"},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.xgboost_classifier.XGBoostClassifier.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Example <pre><code>x = getml.predictors.XGBoostClassifier()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostClassifier()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\n        \"reg:logistic\",\n        \"binary:logistic\",\n        \"binary:logitraw\",\n    ]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostClassifier\n                             are 'reg:logistic', 'binary:logistic',\n                             and 'binary:logitraw'\"\"\"\n        )\n</code></pre>"},{"location":"reference/predictors/xgboost_regressor/","title":"Xgboost regressor","text":"<p>A gradient boosting model for predicting regression problems.</p>"},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.xgboost_regressor.XGBoostRegressor","title":"<code>XGBoostRegressor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting regressor based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> <p>Parameters:</p> Name Type Description Default <code>booster</code> <code>string</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on <code>dart</code>.</li> </ul> <code>'gbtree'</code> <code>colsample_bylevel</code> <code>float</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>colsample_bytree</code> <code>float</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> <code>external_memory</code> <code>bool</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <code>False</code> <code>gamma</code> <code>float</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <code>0.1</code> <code>max_delta_step</code> <code>float</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <code>0.0</code> <code>max_depth</code> <code>int</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>3</code> <code>min_child_weights</code> <code>float</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>n_estimators</code> <code>int</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <code>100</code> <code>normalize_type</code> <code>string</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p><code>tree</code>: a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p><code>forest</code>: a new tree has the same weight as the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>'tree'</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1</code> <code>objective</code> <code>string</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> <li><code>reg:tweedie</code></li> </ul> <code>'reg:squarederror'</code> <code>one_drop</code> <code>bool</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>False</code> <code>rate_drop</code> <code>float</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <code>0.0</code> <code>reg_alpha</code> <code>float</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>0.0</code> <code>reg_lambda</code> <code>float</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <code>1.0</code> <code>sample_type</code> <code>string</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <code>'uniform'</code> <code>silent</code> <code>bool</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <code>True</code> <code>skip_drop</code> <code>float</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p>Range: [0, 1]</p> <code>0.0</code> <code>subsample</code> <code>float</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <code>1.0</code> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>@dataclass(repr=False)\nclass XGBoostRegressor(_Predictor):\n    \"\"\"Gradient boosting regressor based on [xgboost ](https://xgboost.readthedocs.io/en/latest/).\n\nXGBoost is an implementation of the gradient tree boosting algorithm that\nis widely recognized for its efficiency and predictive accuracy.\n\nGradient tree boosting trains an ensemble of decision trees by training\neach tree to predict the *prediction error of all previous trees* in the\nensemble:\n\n$$\n\\min_{\\\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\\\nabla f_{t,i}; y_i),\n$$\n\nwhere $\\\\nabla f_{t,i}$ is the prediction generated by the\nnewest decision tree for sample $i$ and $f_{t-1,i}$ is\nthe prediction generated by all previous trees, $L(...)$ is\nthe loss function used and $y_i$ is the [target][annotating-data-target] we are trying to predict.\n\nXGBoost implements this general approach by adding two specific components:\n\n1. The loss function $L(...)$ is approximated using a Taylor series.\n\n2. The leaves of the decision tree $\\\\nabla f_{t,i}$ contain weights\n   that can be regularized.\n\nThese weights are calculated as follows:\n\n$$\nw_l = -\\\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda},\n$$\n\nwhere $g_i$ and $h_i$ are the first and second order derivative\nof $L(...)$ w.r.t. $f_{t-1,i}$, $w_l$ denotes the weight\non leaf $l$ and $i \\in l$ denotes all samples on that leaf.\n\n$\\lambda$ is the regularization parameter `reg_lambda`.\nThis hyperparameter can be set by the users or the hyperparameter\noptimization algorithm to avoid overfitting.\n\nArgs:\n    booster (string, optional):\n        Which base classifier to use.\n\n        Possible values:\n\n        * `gbtree`: normal gradient boosted decision trees\n        * `gblinear`: uses a linear model instead of decision trees\n        * 'dart': adds dropout to the standard gradient boosting algorithm.\n          Please also refer to the remarks on *rate_drop* for further\n          explanation on `dart`.\n\n    colsample_bylevel (float, optional):\n        Subsample ratio for the columns used, for each level\n        inside a tree.\n\n        Note that XGBoost grows its trees level-by-level, not\n        node-by-node.\n        At each level, a subselection of the features will be randomly\n        picked and the best\n        feature for each split will be chosen. This hyperparameter\n        determines the share of features randomly picked at each level.\n        When set to 1, then now such sampling takes place.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    colsample_bytree (float, optional):\n        Subsample ratio for the columns used, for each tree.\n        This means that for each tree, a subselection\n        of the features will be randomly chosen. This hyperparameter\n        determines the share of features randomly picked for each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n\n    external_memory (bool, optional):\n        When the in_memory flag of the engine is set to False,\n        XGBoost can use the external memory functionality.\n        This reduces the memory consumption,\n        but can also affect the quality of the predictions.\n        External memory is deactivated by default and it\n        is recommended to only use external memory\n        for feature selection.\n        When the in_memory flag of the engine is set to True,\n        (the default value), XGBoost will never use\n        external memory.\n\n    gamma (float, optional):\n        Minimum loss reduction required for any update\n        to the tree. This means that every potential update\n        will first be evaluated for its improvement to the loss\n        function. If the improvement exceeds gamma,\n        the update will be accepted.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    learning_rate (float, optional):\n        Learning rate for the gradient boosting algorithm.\n        When a new tree $\\\\nabla f_{t,i}$ is trained,\n        it will be added to the existing trees\n        $f_{t-1,i}$. Before doing so, it will be\n        multiplied by the *learning_rate*.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, 1]\n\n    max_delta_step (float, optional):\n        The maximum delta step allowed for the weight estimation\n        of each tree.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$)\n\n    max_depth (int, optional):\n        Maximum allowed depth of the trees.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    min_child_weights (float, optional):\n        Minimum sum of weights needed in each child node for a\n        split. The idea here is that any leaf should have\n        a minimum number of samples in order to avoid overfitting.\n        This very common form of regularizing decision trees is\n        slightly\n        modified to refer to weights instead of number of samples,\n        but the basic idea is the same.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    n_estimators (int, optional):\n        Number of estimators (trees).\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [10, $\\infty$]\n\n    normalize_type (string, optional):\n        This determines how to normalize trees during 'dart'.\n\n        Possible values:\n\n        * `tree`: a new tree has the same weight as a single\n          dropped tree.\n\n        * `forest`: a new tree has the same weight as the sum of\n          all dropped trees.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    n_jobs (int, optional):\n        Number of parallel threads. When set to zero, then\n        the optimal number of threads will be inferred automatically.\n\n        Range: [0, $\\infty$]\n\n    objective (string, optional):\n        Specify the learning task and the corresponding\n        learning objective.\n\n        Possible values:\n\n        * `reg:squarederror`\n        * `reg:tweedie`\n\n    one_drop (bool, optional):\n        If set to True, then at least one tree will always be\n        dropped out. Setting this hyperparameter to *true* reduces\n        the likelihood of overfitting.\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    rate_drop (float, optional):\n        Dropout rate for trees - determines the probability\n        that a tree will be dropped out. Dropout is an\n        algorithm that enjoys considerable popularity in\n        the deep learning community. It means that every node can\n        be randomly removed during training.\n\n        This approach\n        can also be applied to gradient boosting, where it\n        means that every tree can be randomly removed with\n        a certain probability. Said probability is determined\n        by *rate_drop*. Dropout for gradient boosting is\n        referred to as the 'dart' algorithm.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n    reg_alpha (float, optional):\n        L1 regularization on the weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    reg_lambda (float, optional):\n        L2 regularization on the weights. Please refer to\n        the introductory remarks to understand how this\n        hyperparameter influences your weights.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: [0, $\\infty$]\n\n    sample_type (string, optional):\n        Possible values:\n\n        * `uniform`: every tree is equally likely to be dropped\n          out\n\n        * `weighted`: the dropout probability will be proportional\n          to a tree's weight\n\n        Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        Will be ignored if `booster` is not set to 'dart'.\n\n    silent (bool, optional):\n        In silent mode, XGBoost will not print out information on\n        the training progress.\n\n    skip_drop (float, optional):\n        Probability of skipping the dropout during a given\n        iteration. Please also refer to the remarks on\n        *rate_drop* for further explanation.\n\n        *Increasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Will be ignored if `booster` is not set to `dart`.\n\n        Range: [0, 1]\n\n    subsample (float, optional):\n        Subsample ratio from the training set. This means\n        that for every tree a subselection of *samples*\n        from the training set will be included into training.\n        Please note that this samples *without* replacement -\n        the common approach for random forests is to sample\n        *with* replace.\n\n        *Decreasing* this hyperparameter reduces the\n        likelihood of overfitting.\n\n        Range: (0, 1]\n    \"\"\"\n\n    booster: str = \"gbtree\"\n    colsample_bylevel: float = 1.0\n    colsample_bytree: float = 1.0\n    early_stopping_rounds: int = 10\n    external_memory: bool = False\n    gamma: float = 0.0\n    learning_rate: float = 0.1\n    max_delta_step: float = 0.0\n    max_depth: int = 3\n    min_child_weights: float = 1.0\n    n_estimators: int = 100\n    normalize_type: str = \"tree\"\n    num_parallel_tree: int = 1\n    n_jobs: int = 1\n    objective: str = \"reg:squarederror\"\n    one_drop: bool = False\n    rate_drop: float = 0.0\n    reg_alpha: float = 0.0\n    reg_lambda: float = 1.0\n    sample_type: str = \"uniform\"\n    silent: bool = True\n    skip_drop: float = 0.0\n    subsample: float = 1.0\n\n    # ----------------------------------------------------------------\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional): A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n\n        Example:\n            ```python\n            x = getml.predictors.XGBoostRegressor()\n            x.gamma = 200\n            x.validate()\n            ```\n\n        Note:\n            This method is called at end of the \\_\\_init\\_\\_ constructor\n            and every time before the predictor - or a class holding\n            it as an instance variable - is sent to the getML engine.\n        \"\"\"\n\n        # ------------------------------------------------------------\n\n        if params is None:\n            params = self.__dict__\n        else:\n            params = {**self.__dict__, **params}\n\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be None or a dictionary!\")\n\n        _validate_xgboost_parameters(params)\n\n        # ------------------------------------------------------------\n\n        if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n            raise ValueError(\n                \"\"\"'objective' supported in XGBoostRegressor\n                                 are 'reg:squarederror', 'reg:tweedie',\n                                 and 'reg:linear'\"\"\"\n            )\n</code></pre>"},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.xgboost_regressor.XGBoostRegressor.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Example <pre><code>x = getml.predictors.XGBoostRegressor()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostRegressor()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostRegressor\n                             are 'reg:squarederror', 'reg:tweedie',\n                             and 'reg:linear'\"\"\"\n        )\n</code></pre>"},{"location":"reference/preprocessors/__init__/","title":"init","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.CategoryTrimmer","title":"<code>CategoryTrimmer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>Reduces the cardinality of high-cardinality categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>max_num_categories</code> <code>int</code> <p>The maximum cardinality allowed. If the cardinality is higher than that only the most frequent categories will be kept, all others will be trimmed.</p> <code>999</code> <code>min_freq</code> <code>int</code> <p>The minimum frequency required for a category to be included.</p> <code>30</code> Example <pre><code>category_trimmer = getml.preprocessors.CategoryTrimmer()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[category_trimmer],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/category_trimmer.py</code> <pre><code>@dataclass(repr=False)\nclass CategoryTrimmer(_Preprocessor):\n    \"\"\"\n    Reduces the cardinality of high-cardinality categorical columns.\n\n    Args:\n        max_num_categories (int, optional):\n            The maximum cardinality allowed. If the cardinality is\n            higher than that only the most frequent categories will\n            be kept, all others will be trimmed.\n\n        min_freq (int, optional):\n            The minimum frequency required for a category to be\n            included.\n\n    Example:\n        ```python\n        category_trimmer = getml.preprocessors.CategoryTrimmer()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[category_trimmer],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    max_num_categories: int = 999\n    min_freq: int = 30\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"max_num_categories\"], int):\n            raise TypeError(\"'max_num_categories' must be an int.\")\n\n        if not isinstance(params[\"min_freq\"], int):\n            raise TypeError(\"'min_freq' must be an int.\")\n\n        if params[\"max_num_categories\"] &lt; 0:\n            raise ValueError(\"'max_num_categories' cannot be negative.\")\n\n        if params[\"min_freq\"] &lt; 0:\n            raise ValueError(\"'min_freq' cannot be negative.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.CategoryTrimmer.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/category_trimmer.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"max_num_categories\"], int):\n        raise TypeError(\"'max_num_categories' must be an int.\")\n\n    if not isinstance(params[\"min_freq\"], int):\n        raise TypeError(\"'min_freq' must be an int.\")\n\n    if params[\"max_num_categories\"] &lt; 0:\n        raise ValueError(\"'max_num_categories' cannot be negative.\")\n\n    if params[\"min_freq\"] &lt; 0:\n        raise ValueError(\"'min_freq' cannot be negative.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.EmailDomain","title":"<code>EmailDomain</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The EmailDomain preprocessor extracts the domain from e-mail addresses.</p> <p>For instance, if the e-mail address is 'some.guy@domain.com', the preprocessor will automatically extract '@domain.com'.</p> <p>The preprocessor will be applied to all <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.email</code> or <code>only.email</code>.</p> <p>It is recommended that you assign <code>only.email</code>, because it is unlikely that the e-mail address itself is interesting.</p> Example <pre><code>my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\ndomain = getml.preprocessors.EmailDomain()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[domain],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/email_domain.py</code> <pre><code>@dataclass(repr=False)\nclass EmailDomain(_Preprocessor):\n    \"\"\"\n    The EmailDomain preprocessor extracts the domain from e-mail addresses.\n\n    For instance, if the e-mail address is 'some.guy@domain.com',\n    the preprocessor will automatically extract '@domain.com'.\n\n    The preprocessor will be applied to all [`text`][getml.data.roles.text]\n    columns that were assigned one of the [`subroles`][getml.data.subroles]\n    [`include.email`][getml.data.subroles.include.email] or\n    [`only.email`][getml.data.subroles.only.email].\n\n    It is recommended that you assign [`only.email`][getml.data.subroles.only.email],\n    because it is unlikely that the e-mail address itself is interesting.\n\n    Example:\n        ```python\n        my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\n        domain = getml.preprocessors.EmailDomain()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[domain],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.EmailDomain.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/email_domain.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Imputation","title":"<code>Imputation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Imputation preprocessor replaces all NULL values in numerical columns with the mean of the remaining columns.</p> <p>Optionally, it can additionally add a dummy column that signifies whether the original value was imputed.</p> <p>Parameters:</p> Name Type Description Default <code>add_dummies</code> <code>bool</code> <p>Whether you want to add dummy variables that signify whether the original value was imputed..</p> <code>False</code> Example <pre><code>imputation = getml.preprocessors.Imputation()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[imputation],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/imputation.py</code> <pre><code>@dataclass(repr=False)\nclass Imputation(_Preprocessor):\n    \"\"\"\n    The Imputation preprocessor replaces all NULL values in\n    numerical columns with the mean of the remaining\n    columns.\n\n    Optionally, it can additionally add a dummy column\n    that signifies whether the original value was imputed.\n\n    Args:\n        add_dummies (bool):\n            Whether you want to add dummy variables\n            that signify whether the original value was imputed..\n\n    Example:\n        ```python\n        imputation = getml.preprocessors.Imputation()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[imputation],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    add_dummies: bool = False\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"add_dummies\"], bool):\n            raise TypeError(\"'add_dummies' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Imputation.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/imputation.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"add_dummies\"], bool):\n        raise TypeError(\"'add_dummies' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Mapping","title":"<code>Mapping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>A mapping preprocessor maps categorical values, discrete values and individual words in a text field to numerical values. These numerical values are retrieved by aggregating targets in the relational neighbourhood.</p> <p>You are particularly encouraged to use the mapping preprocessor in combination with <code>FastProp</code>.</p> <p>Refer to the User guide for more information.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation</code> <code>List[[`aggregations`][getml.feature_learning.aggregations]]</code> <p>The aggregation function to use over the targets.</p> <p>Must be from <code>aggregations</code>.</p> <code>lambda: Default()</code> <code>min_freq</code> <code>int</code> <p>The minimum number of targets required for a value to be included in the mapping. Range: [0, \\(\\infty\\)]</p> <code>30</code> <code>multithreading</code> <code>bool</code> <p>Whether you want to apply multithreading.</p> <code>True</code> Example <pre><code>mapping = getml.preprocessors.Mapping()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[mapping],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/preprocessors/mapping.py</code> <pre><code>@dataclass(repr=False)\nclass Mapping(_Preprocessor):\n    \"\"\"\n    A mapping preprocessor maps categorical values, discrete values and individual\n    words in a text field to numerical values. These numerical values are retrieved\n    by aggregating targets in the relational neighbourhood.\n\n    You are particularly encouraged to use the mapping preprocessor in combination with\n    [`FastProp`][getml.feature_learning.FastProp].\n\n    Refer to the [User guide][preprocessing-mappings] for more information.\n\n    Args:\n        aggregation (List[[`aggregations`][getml.feature_learning.aggregations]], optional):\n            The aggregation function to use over the targets.\n\n            Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n        min_freq (int, optional):\n            The minimum number of targets required for a value to be included in\n            the mapping. Range: [0, $\\infty$]\n\n        multithreading (bool, optional):\n            Whether you want to apply multithreading.\n\n    Example:\n        ```python\n        mapping = getml.preprocessors.Mapping()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[mapping],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    agg_sets: ClassVar[_Aggregations] = mapping_aggregations\n\n    aggregation: List[str] = field(default_factory=lambda: mapping_aggregations.Default)\n    min_freq: int = 30\n    multithreading: bool = True\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not all([agg in mapping_aggregations.All for agg in params[\"aggregation\"]]):\n            raise ValueError(\n                \"'aggregation' must be from Mapping.agg_sets.All, \"\n                + \"meaning from the following set: \"\n                + str(mapping_aggregations.All)\n                + \".\"\n            )\n\n        if not isinstance(params[\"min_freq\"], int):\n            raise TypeError(\"'min_freq' must be an int.\")\n\n        if params[\"min_freq\"] &lt; 0:\n            raise TypeError(\"'min_freq' cannot be negative.\")\n\n        if not isinstance(params[\"multithreading\"], bool):\n            raise TypeError(\"'multithreading' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Mapping.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/mapping.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not all([agg in mapping_aggregations.All for agg in params[\"aggregation\"]]):\n        raise ValueError(\n            \"'aggregation' must be from Mapping.agg_sets.All, \"\n            + \"meaning from the following set: \"\n            + str(mapping_aggregations.All)\n            + \".\"\n        )\n\n    if not isinstance(params[\"min_freq\"], int):\n        raise TypeError(\"'min_freq' must be an int.\")\n\n    if params[\"min_freq\"] &lt; 0:\n        raise TypeError(\"'min_freq' cannot be negative.\")\n\n    if not isinstance(params[\"multithreading\"], bool):\n        raise TypeError(\"'multithreading' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Seasonal","title":"<code>Seasonal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Seasonal preprocessor extracts seasonal data from time stamps.</p> <p>The preprocessor automatically iterates through all time stamps in any data frame and extracts seasonal parameters.</p> <p>These include:</p> <ul> <li>year</li> <li>month</li> <li>weekday</li> <li>hour</li> <li>minute</li> </ul> <p>The algorithm also evaluates the potential usefulness of any extracted seasonal parameter. Parameters that are unlikely to be useful are not included.</p> <p>Parameters:</p> Name Type Description Default <code>disable_year</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the year from time stamps.</p> <code>False</code> <code>disable_month</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the month from time stamps.</p> <code>False</code> <code>disable_weekday</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the weekday from time stamps.</p> <code>False</code> <code>disable_hour</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the hour from time stamps.</p> <code>False</code> <code>disable_minute</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the minute from time stamps.</p> <code>False</code> Example <pre><code>seasonal = getml.preprocessors.Seasonal()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[seasonal],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/seasonal.py</code> <pre><code>@dataclass(repr=False)\nclass Seasonal(_Preprocessor):\n    \"\"\"\n    The Seasonal preprocessor extracts seasonal data from time stamps.\n\n    The preprocessor automatically iterates through\n    all time stamps in any data frame and extracts\n    seasonal parameters.\n\n    These include:\n\n    - year\n    - month\n    - weekday\n    - hour\n    - minute\n\n    The algorithm also evaluates the potential\n    usefulness of any extracted seasonal parameter.\n    Parameters that are unlikely to be useful are\n    not included.\n\n    Args:\n        disable_year (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the year from time stamps.\n\n        disable_month (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the month from time stamps.\n\n        disable_weekday (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the weekday from time stamps.\n\n        disable_hour (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the hour from time stamps.\n\n        disable_minute (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the minute from time stamps.\n\n    Example:\n        ```python\n        seasonal = getml.preprocessors.Seasonal()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[seasonal],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    disable_year: bool = False\n    disable_month: bool = False\n    disable_weekday: bool = False\n    disable_hour: bool = False\n    disable_minute: bool = False\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Seasonal.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/seasonal.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Substring","title":"<code>Substring</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Substring preprocessor extracts substrings from categorical columns and unused string columns.</p> <p>The preprocessor will be applied to all <code>categorical</code> and <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.substring</code> or <code>only.substring</code>.</p> <p>To further limit the scope of a substring preprocessor, you can also assign a unit.</p> <p>Parameters:</p> Name Type Description Default <code>begin</code> <code>int</code> <p>Index of the beginning of the substring (starting from 0).</p> required <code>length</code> <code>int</code> <p>The length of the substring.</p> required <code>unit</code> <code>str</code> <p>The unit of all columns to which the preprocessor should be applied. These columns must also have the subrole substring.</p> <p>If it is left empty, then the preprocessor will be applied to all columns with the subrole <code>include.substring</code> or <code>only.substring</code>.</p> <code>''</code> Example <pre><code>my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\nmy_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\nmy_df.set_unit(\"col2\", \"substr14\")\n\n# Will be applied to col1 and col2\nsubstr13 = getml.preprocessors.Substring(0, 3)\n\n# Will only be applied to col2\nsubstr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[substr13],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/substring.py</code> <pre><code>@dataclass(repr=False)\nclass Substring(_Preprocessor):\n    \"\"\"\n    The Substring preprocessor extracts substrings from\n    categorical columns and unused string columns.\n\n    The preprocessor will be applied to all\n    [`categorical`][getml.data.roles.categorical] and [`text`][getml.data.roles.text]\n    columns that were assigned one of the [`subroles`][getml.data.subroles]\n    [`include.substring`][getml.data.subroles.include.substring] or\n    [`only.substring`][getml.data.subroles.only.substring].\n\n    To further limit the scope of a substring preprocessor,\n    you can also assign a *unit*.\n\n    Args:\n        begin (int):\n            Index of the beginning of the substring (starting from 0).\n\n        length (int):\n            The length of the substring.\n\n        unit (str, optional):\n            The unit of all columns to which the preprocessor\n            should be applied. These columns must also have the subrole\n            substring.\n\n            If it is left empty, then the preprocessor\n            will be applied to all columns with the subrole\n            [`include.substring`][getml.data.subroles.include.substring] or\n            [`only.substring`][getml.data.subroles.only.substring].\n\n    Example:\n        ```python\n        my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\n        my_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\n        my_df.set_unit(\"col2\", \"substr14\")\n\n        # Will be applied to col1 and col2\n        substr13 = getml.preprocessors.Substring(0, 3)\n\n        # Will only be applied to col2\n        substr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[substr13],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    begin: int\n    length: int\n    unit: str = \"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"begin\"], int):\n            raise TypeError(\"'begin' must be an integer!\")\n\n        if not isinstance(params[\"length\"], int):\n            raise TypeError(\"'length' must be an integer!\")\n\n        if not isinstance(params[\"unit\"], str):\n            raise TypeError(\"'unit' must be a string!\")\n\n        if params[\"begin\"] &lt; 0:\n            raise ValueError(\"'begin' must be &gt;= 0!\")\n\n        if params[\"length\"] &lt;= 0:\n            raise ValueError(\"'length' must be &gt; 0!\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.Substring.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/substring.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"begin\"], int):\n        raise TypeError(\"'begin' must be an integer!\")\n\n    if not isinstance(params[\"length\"], int):\n        raise TypeError(\"'length' must be an integer!\")\n\n    if not isinstance(params[\"unit\"], str):\n        raise TypeError(\"'unit' must be a string!\")\n\n    if params[\"begin\"] &lt; 0:\n        raise ValueError(\"'begin' must be &gt;= 0!\")\n\n    if params[\"length\"] &lt;= 0:\n        raise ValueError(\"'length' must be &gt; 0!\")\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.TextFieldSplitter","title":"<code>TextFieldSplitter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>A TextFieldSplitter splits columns with role <code>text</code> into relational bag-of-words representations to allow the feature learners to learn patterns based on the prescence of certain words within the text fields.</p> <p>Text fields will be split on a whitespace or any of the following characters:</p> <p><pre><code>; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n</code></pre> Refer to the User Guide for more information.</p> Example <pre><code>text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[text_field_splitter],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/text_field_splitter.py</code> <pre><code>@dataclass(repr=False)\nclass TextFieldSplitter(_Preprocessor):\n    r\"\"\"\n    A TextFieldSplitter splits columns with role [`text`][getml.data.roles.text]\n    into relational bag-of-words representations to allow the\n    feature learners to learn patterns based on\n    the prescence of certain words within the text fields.\n\n    Text fields will be split on a whitespace or any of the\n    following characters:\n\n    ```python\n    ; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n    ```\n    Refer to the [User Guide][preprocessing-free-form-text] for more information.\n\n    Example:\n        ```python\n        text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[text_field_splitter],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/__init__/#getml.preprocessors.TextFieldSplitter.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/text_field_splitter.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/category_trimmer/","title":"Category trimmer","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/category_trimmer/#getml.preprocessors.category_trimmer.CategoryTrimmer","title":"<code>CategoryTrimmer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>Reduces the cardinality of high-cardinality categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>max_num_categories</code> <code>int</code> <p>The maximum cardinality allowed. If the cardinality is higher than that only the most frequent categories will be kept, all others will be trimmed.</p> <code>999</code> <code>min_freq</code> <code>int</code> <p>The minimum frequency required for a category to be included.</p> <code>30</code> Example <pre><code>category_trimmer = getml.preprocessors.CategoryTrimmer()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[category_trimmer],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/category_trimmer.py</code> <pre><code>@dataclass(repr=False)\nclass CategoryTrimmer(_Preprocessor):\n    \"\"\"\n    Reduces the cardinality of high-cardinality categorical columns.\n\n    Args:\n        max_num_categories (int, optional):\n            The maximum cardinality allowed. If the cardinality is\n            higher than that only the most frequent categories will\n            be kept, all others will be trimmed.\n\n        min_freq (int, optional):\n            The minimum frequency required for a category to be\n            included.\n\n    Example:\n        ```python\n        category_trimmer = getml.preprocessors.CategoryTrimmer()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[category_trimmer],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    max_num_categories: int = 999\n    min_freq: int = 30\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"max_num_categories\"], int):\n            raise TypeError(\"'max_num_categories' must be an int.\")\n\n        if not isinstance(params[\"min_freq\"], int):\n            raise TypeError(\"'min_freq' must be an int.\")\n\n        if params[\"max_num_categories\"] &lt; 0:\n            raise ValueError(\"'max_num_categories' cannot be negative.\")\n\n        if params[\"min_freq\"] &lt; 0:\n            raise ValueError(\"'min_freq' cannot be negative.\")\n</code></pre>"},{"location":"reference/preprocessors/category_trimmer/#getml.preprocessors.category_trimmer.CategoryTrimmer.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/category_trimmer.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"max_num_categories\"], int):\n        raise TypeError(\"'max_num_categories' must be an int.\")\n\n    if not isinstance(params[\"min_freq\"], int):\n        raise TypeError(\"'min_freq' must be an int.\")\n\n    if params[\"max_num_categories\"] &lt; 0:\n        raise ValueError(\"'max_num_categories' cannot be negative.\")\n\n    if params[\"min_freq\"] &lt; 0:\n        raise ValueError(\"'min_freq' cannot be negative.\")\n</code></pre>"},{"location":"reference/preprocessors/email_domain/","title":"Email domain","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/email_domain/#getml.preprocessors.email_domain.EmailDomain","title":"<code>EmailDomain</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The EmailDomain preprocessor extracts the domain from e-mail addresses.</p> <p>For instance, if the e-mail address is 'some.guy@domain.com', the preprocessor will automatically extract '@domain.com'.</p> <p>The preprocessor will be applied to all <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.email</code> or <code>only.email</code>.</p> <p>It is recommended that you assign <code>only.email</code>, because it is unlikely that the e-mail address itself is interesting.</p> Example <pre><code>my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\ndomain = getml.preprocessors.EmailDomain()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[domain],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/email_domain.py</code> <pre><code>@dataclass(repr=False)\nclass EmailDomain(_Preprocessor):\n    \"\"\"\n    The EmailDomain preprocessor extracts the domain from e-mail addresses.\n\n    For instance, if the e-mail address is 'some.guy@domain.com',\n    the preprocessor will automatically extract '@domain.com'.\n\n    The preprocessor will be applied to all [`text`][getml.data.roles.text]\n    columns that were assigned one of the [`subroles`][getml.data.subroles]\n    [`include.email`][getml.data.subroles.include.email] or\n    [`only.email`][getml.data.subroles.only.email].\n\n    It is recommended that you assign [`only.email`][getml.data.subroles.only.email],\n    because it is unlikely that the e-mail address itself is interesting.\n\n    Example:\n        ```python\n        my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\n        domain = getml.preprocessors.EmailDomain()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[domain],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/email_domain/#getml.preprocessors.email_domain.EmailDomain.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/email_domain.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/imputation/","title":"Imputation","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/imputation/#getml.preprocessors.imputation.Imputation","title":"<code>Imputation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Imputation preprocessor replaces all NULL values in numerical columns with the mean of the remaining columns.</p> <p>Optionally, it can additionally add a dummy column that signifies whether the original value was imputed.</p> <p>Parameters:</p> Name Type Description Default <code>add_dummies</code> <code>bool</code> <p>Whether you want to add dummy variables that signify whether the original value was imputed..</p> <code>False</code> Example <pre><code>imputation = getml.preprocessors.Imputation()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[imputation],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/imputation.py</code> <pre><code>@dataclass(repr=False)\nclass Imputation(_Preprocessor):\n    \"\"\"\n    The Imputation preprocessor replaces all NULL values in\n    numerical columns with the mean of the remaining\n    columns.\n\n    Optionally, it can additionally add a dummy column\n    that signifies whether the original value was imputed.\n\n    Args:\n        add_dummies (bool):\n            Whether you want to add dummy variables\n            that signify whether the original value was imputed..\n\n    Example:\n        ```python\n        imputation = getml.preprocessors.Imputation()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[imputation],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    add_dummies: bool = False\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"add_dummies\"], bool):\n            raise TypeError(\"'add_dummies' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/imputation/#getml.preprocessors.imputation.Imputation.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/imputation.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"add_dummies\"], bool):\n        raise TypeError(\"'add_dummies' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/mapping/","title":"Mapping","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/mapping/#getml.preprocessors.mapping.Mapping","title":"<code>Mapping</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>A mapping preprocessor maps categorical values, discrete values and individual words in a text field to numerical values. These numerical values are retrieved by aggregating targets in the relational neighbourhood.</p> <p>You are particularly encouraged to use the mapping preprocessor in combination with <code>FastProp</code>.</p> <p>Refer to the User guide for more information.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation</code> <code>List[[`aggregations`][getml.feature_learning.aggregations]]</code> <p>The aggregation function to use over the targets.</p> <p>Must be from <code>aggregations</code>.</p> <code>lambda: Default()</code> <code>min_freq</code> <code>int</code> <p>The minimum number of targets required for a value to be included in the mapping. Range: [0, \\(\\infty\\)]</p> <code>30</code> <code>multithreading</code> <code>bool</code> <p>Whether you want to apply multithreading.</p> <code>True</code> Example <pre><code>mapping = getml.preprocessors.Mapping()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[mapping],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/preprocessors/mapping.py</code> <pre><code>@dataclass(repr=False)\nclass Mapping(_Preprocessor):\n    \"\"\"\n    A mapping preprocessor maps categorical values, discrete values and individual\n    words in a text field to numerical values. These numerical values are retrieved\n    by aggregating targets in the relational neighbourhood.\n\n    You are particularly encouraged to use the mapping preprocessor in combination with\n    [`FastProp`][getml.feature_learning.FastProp].\n\n    Refer to the [User guide][preprocessing-mappings] for more information.\n\n    Args:\n        aggregation (List[[`aggregations`][getml.feature_learning.aggregations]], optional):\n            The aggregation function to use over the targets.\n\n            Must be from [`aggregations`][getml.feature_learning.aggregations].\n\n        min_freq (int, optional):\n            The minimum number of targets required for a value to be included in\n            the mapping. Range: [0, $\\infty$]\n\n        multithreading (bool, optional):\n            Whether you want to apply multithreading.\n\n    Example:\n        ```python\n        mapping = getml.preprocessors.Mapping()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[mapping],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    agg_sets: ClassVar[_Aggregations] = mapping_aggregations\n\n    aggregation: List[str] = field(default_factory=lambda: mapping_aggregations.Default)\n    min_freq: int = 30\n    multithreading: bool = True\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not all([agg in mapping_aggregations.All for agg in params[\"aggregation\"]]):\n            raise ValueError(\n                \"'aggregation' must be from Mapping.agg_sets.All, \"\n                + \"meaning from the following set: \"\n                + str(mapping_aggregations.All)\n                + \".\"\n            )\n\n        if not isinstance(params[\"min_freq\"], int):\n            raise TypeError(\"'min_freq' must be an int.\")\n\n        if params[\"min_freq\"] &lt; 0:\n            raise TypeError(\"'min_freq' cannot be negative.\")\n\n        if not isinstance(params[\"multithreading\"], bool):\n            raise TypeError(\"'multithreading' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/mapping/#getml.preprocessors.mapping.Mapping.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/mapping.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not all([agg in mapping_aggregations.All for agg in params[\"aggregation\"]]):\n        raise ValueError(\n            \"'aggregation' must be from Mapping.agg_sets.All, \"\n            + \"meaning from the following set: \"\n            + str(mapping_aggregations.All)\n            + \".\"\n        )\n\n    if not isinstance(params[\"min_freq\"], int):\n        raise TypeError(\"'min_freq' must be an int.\")\n\n    if params[\"min_freq\"] &lt; 0:\n        raise TypeError(\"'min_freq' cannot be negative.\")\n\n    if not isinstance(params[\"multithreading\"], bool):\n        raise TypeError(\"'multithreading' must be a bool.\")\n</code></pre>"},{"location":"reference/preprocessors/preprocessor/","title":"Preprocessor","text":"<p>Base class - not meant for the end user.</p>"},{"location":"reference/preprocessors/seasonal/","title":"Seasonal","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/seasonal/#getml.preprocessors.seasonal.Seasonal","title":"<code>Seasonal</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Seasonal preprocessor extracts seasonal data from time stamps.</p> <p>The preprocessor automatically iterates through all time stamps in any data frame and extracts seasonal parameters.</p> <p>These include:</p> <ul> <li>year</li> <li>month</li> <li>weekday</li> <li>hour</li> <li>minute</li> </ul> <p>The algorithm also evaluates the potential usefulness of any extracted seasonal parameter. Parameters that are unlikely to be useful are not included.</p> <p>Parameters:</p> Name Type Description Default <code>disable_year</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the year from time stamps.</p> <code>False</code> <code>disable_month</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the month from time stamps.</p> <code>False</code> <code>disable_weekday</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the weekday from time stamps.</p> <code>False</code> <code>disable_hour</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the hour from time stamps.</p> <code>False</code> <code>disable_minute</code> <code>bool</code> <p>Prevents the Seasonal preprocessor from extracting the minute from time stamps.</p> <code>False</code> Example <pre><code>seasonal = getml.preprocessors.Seasonal()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[seasonal],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/seasonal.py</code> <pre><code>@dataclass(repr=False)\nclass Seasonal(_Preprocessor):\n    \"\"\"\n    The Seasonal preprocessor extracts seasonal data from time stamps.\n\n    The preprocessor automatically iterates through\n    all time stamps in any data frame and extracts\n    seasonal parameters.\n\n    These include:\n\n    - year\n    - month\n    - weekday\n    - hour\n    - minute\n\n    The algorithm also evaluates the potential\n    usefulness of any extracted seasonal parameter.\n    Parameters that are unlikely to be useful are\n    not included.\n\n    Args:\n        disable_year (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the year from time stamps.\n\n        disable_month (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the month from time stamps.\n\n        disable_weekday (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the weekday from time stamps.\n\n        disable_hour (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the hour from time stamps.\n\n        disable_minute (bool, optional):\n            Prevents the Seasonal preprocessor from\n            extracting the minute from time stamps.\n\n    Example:\n        ```python\n        seasonal = getml.preprocessors.Seasonal()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[seasonal],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    disable_year: bool = False\n    disable_month: bool = False\n    disable_weekday: bool = False\n    disable_hour: bool = False\n    disable_minute: bool = False\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/seasonal/#getml.preprocessors.seasonal.Seasonal.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/seasonal.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/substring/","title":"Substring","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/substring/#getml.preprocessors.substring.Substring","title":"<code>Substring</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>The Substring preprocessor extracts substrings from categorical columns and unused string columns.</p> <p>The preprocessor will be applied to all <code>categorical</code> and <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.substring</code> or <code>only.substring</code>.</p> <p>To further limit the scope of a substring preprocessor, you can also assign a unit.</p> <p>Parameters:</p> Name Type Description Default <code>begin</code> <code>int</code> <p>Index of the beginning of the substring (starting from 0).</p> required <code>length</code> <code>int</code> <p>The length of the substring.</p> required <code>unit</code> <code>str</code> <p>The unit of all columns to which the preprocessor should be applied. These columns must also have the subrole substring.</p> <p>If it is left empty, then the preprocessor will be applied to all columns with the subrole <code>include.substring</code> or <code>only.substring</code>.</p> <code>''</code> Example <pre><code>my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\nmy_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\nmy_df.set_unit(\"col2\", \"substr14\")\n\n# Will be applied to col1 and col2\nsubstr13 = getml.preprocessors.Substring(0, 3)\n\n# Will only be applied to col2\nsubstr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[substr13],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/substring.py</code> <pre><code>@dataclass(repr=False)\nclass Substring(_Preprocessor):\n    \"\"\"\n    The Substring preprocessor extracts substrings from\n    categorical columns and unused string columns.\n\n    The preprocessor will be applied to all\n    [`categorical`][getml.data.roles.categorical] and [`text`][getml.data.roles.text]\n    columns that were assigned one of the [`subroles`][getml.data.subroles]\n    [`include.substring`][getml.data.subroles.include.substring] or\n    [`only.substring`][getml.data.subroles.only.substring].\n\n    To further limit the scope of a substring preprocessor,\n    you can also assign a *unit*.\n\n    Args:\n        begin (int):\n            Index of the beginning of the substring (starting from 0).\n\n        length (int):\n            The length of the substring.\n\n        unit (str, optional):\n            The unit of all columns to which the preprocessor\n            should be applied. These columns must also have the subrole\n            substring.\n\n            If it is left empty, then the preprocessor\n            will be applied to all columns with the subrole\n            [`include.substring`][getml.data.subroles.include.substring] or\n            [`only.substring`][getml.data.subroles.only.substring].\n\n    Example:\n        ```python\n        my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\n        my_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\n        my_df.set_unit(\"col2\", \"substr14\")\n\n        # Will be applied to col1 and col2\n        substr13 = getml.preprocessors.Substring(0, 3)\n\n        # Will only be applied to col2\n        substr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[substr13],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    begin: int\n    length: int\n    unit: str = \"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        params = _validate(self, params)\n\n        if not isinstance(params[\"begin\"], int):\n            raise TypeError(\"'begin' must be an integer!\")\n\n        if not isinstance(params[\"length\"], int):\n            raise TypeError(\"'length' must be an integer!\")\n\n        if not isinstance(params[\"unit\"], str):\n            raise TypeError(\"'unit' must be a string!\")\n\n        if params[\"begin\"] &lt; 0:\n            raise ValueError(\"'begin' must be &gt;= 0!\")\n\n        if params[\"length\"] &lt;= 0:\n            raise ValueError(\"'length' must be &gt; 0!\")\n</code></pre>"},{"location":"reference/preprocessors/substring/#getml.preprocessors.substring.Substring.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/substring.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    params = _validate(self, params)\n\n    if not isinstance(params[\"begin\"], int):\n        raise TypeError(\"'begin' must be an integer!\")\n\n    if not isinstance(params[\"length\"], int):\n        raise TypeError(\"'length' must be an integer!\")\n\n    if not isinstance(params[\"unit\"], str):\n        raise TypeError(\"'unit' must be a string!\")\n\n    if params[\"begin\"] &lt; 0:\n        raise ValueError(\"'begin' must be &gt;= 0!\")\n\n    if params[\"length\"] &lt;= 0:\n        raise ValueError(\"'length' must be &gt; 0!\")\n</code></pre>"},{"location":"reference/preprocessors/text_field_splitter/","title":"Text field splitter","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/text_field_splitter/#getml.preprocessors.text_field_splitter.TextFieldSplitter","title":"<code>TextFieldSplitter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_Preprocessor</code></p> <p>A TextFieldSplitter splits columns with role <code>text</code> into relational bag-of-words representations to allow the feature learners to learn patterns based on the prescence of certain words within the text fields.</p> <p>Text fields will be split on a whitespace or any of the following characters:</p> <p><pre><code>; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n</code></pre> Refer to the User Guide for more information.</p> Example <pre><code>text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[text_field_splitter],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Source code in <code>getml/preprocessors/text_field_splitter.py</code> <pre><code>@dataclass(repr=False)\nclass TextFieldSplitter(_Preprocessor):\n    r\"\"\"\n    A TextFieldSplitter splits columns with role [`text`][getml.data.roles.text]\n    into relational bag-of-words representations to allow the\n    feature learners to learn patterns based on\n    the prescence of certain words within the text fields.\n\n    Text fields will be split on a whitespace or any of the\n    following characters:\n\n    ```python\n    ; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n    ```\n    Refer to the [User Guide][preprocessing-free-form-text] for more information.\n\n    Example:\n        ```python\n        text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\n        pipe = getml.Pipeline(\n            population=population_placeholder,\n            peripheral=[order_placeholder, trans_placeholder],\n            preprocessors=[text_field_splitter],\n            feature_learners=[feature_learner_1, feature_learner_2],\n            feature_selectors=feature_selector,\n            predictors=predictor,\n            share_selected_features=0.5\n        )\n        ```\n    \"\"\"\n\n    def validate(self, params=None):\n        \"\"\"Checks both the types and the values of all instance\n        variables and raises an exception if something is off.\n\n        Args:\n            params (dict, optional):\n                A dictionary containing\n                the parameters to validate. If not is passed,\n                the own parameters will be validated.\n        \"\"\"\n        _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/text_field_splitter/#getml.preprocessors.text_field_splitter.TextFieldSplitter.validate","title":"<code>validate(params=None)</code>","text":"<p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <code>None</code> Source code in <code>getml/preprocessors/text_field_splitter.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional):\n            A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n    _validate(self, params)\n</code></pre>"},{"location":"reference/preprocessors/validate/","title":"Validate","text":"<p>Reduces boilerplate code for the validation.</p>"},{"location":"reference/project/__init__/","title":"init","text":"<p>This module helps you handle your current project.</p>"},{"location":"reference/project/__init__/#getml.project.DataFrames","title":"<code>DataFrames</code>","text":"<p>Container which holds all data frames associated with the running project that are currently stored in memory. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>class DataFrames:\n    \"\"\"\n    Container which holds all data frames associated with the running\n    project that are currently stored in memory. The container supports\n    slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self._in_memory = list_data_frames()[\"in_memory\"]\n        self._on_disk = list_data_frames()[\"on_disk\"]\n\n        if data is None:\n            self.data = [load_data_frame(name) for name in self._in_memory]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            dfs_subset = self.data[key]\n            return DataFrames(data=dfs_subset)\n        if isinstance(key, str):\n            if key in self.in_memory:\n                return [df for df in self.data if df.name == key][0]\n            if key in self.on_disk:\n                raise AttributeError(f\"DataFrame {key} not loaded from disk.\")\n            raise AttributeError(f\"No DataFrame with name: {key}\")\n        raise TypeError(\n            f\"DataFrames can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.in_memory) == 0:\n            output = \"No data frames in memory.\"\n        else:\n            output = self._format()._render_string()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"\\n\\nOn disk:\\n\"\n            output += \"\\n\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.in_memory) == 0:\n            output = \"&lt;p&gt;No data frames in memory.&lt;/p&gt;\"\n        else:\n            output = self._format()._render_html()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"&lt;p&gt;On disk:&lt;/p&gt;\"\n            output += \"&lt;br&gt;\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n\n        headers = [[\"name\", \"rows\", \"columns\", \"memory usage\"]]\n\n        rows = [[df.name, df.nrows(), df.ncols(), df.memory_usage] for df in self.data]\n\n        formatted = _Formatter(headers, rows)\n\n        formatted[4].cell_template = \"{:{width}.2f} MB\"\n\n        return formatted\n\n    # ----------------------------------------------------------------\n\n    def delete(self):\n        \"\"\"\n        Deletes all data frames in the current project.\n\n        Args:\n            mem_only (bool):\n                If called with the `mem_only` option set to True, the data\n                frames will be kept on disk (in the project folder).\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).delete()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def in_memory(self):\n        \"\"\"\n        Returns the names of all data frames currently in memory.\n        \"\"\"\n        return self._in_memory\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the data frames container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered data frames.\n\n        Example:\n            ```python\n            big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n            ```\n        \"\"\"\n\n        dfs_filtered = [df for df in self.data if conditional(df)]\n        return DataFrames(data=dfs_filtered)\n\n    # ----------------------------------------------------------------\n\n    def load(self):\n        \"\"\"\n        Loads all data frames stored in the project folder to memory.\n        \"\"\"\n\n        for df in self.on_disk:\n            if df not in self.in_memory:\n                self.data.append(load_data_frame(df))\n\n    # ----------------------------------------------------------------\n\n    @property\n    def on_disk(self):\n        \"\"\"\n        Returns the names of all data frames stored in the project folder.\n        \"\"\"\n        return self._on_disk\n\n    # ----------------------------------------------------------------\n\n    def retrieve(self):\n        \"\"\"\n        Retrieve a dict of all data frames in memory.\n        \"\"\"\n\n        return {df.name: df for df in self.data}\n\n    # ----------------------------------------------------------------\n\n    def save(self):\n        \"\"\"\n        Saves all data frames currently in memory to disk.\n        \"\"\"\n\n        for df in self.data:\n            df.save()\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the data frames container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted data frames.\n\n        Example:\n            ```python\n            by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n            ```\n        \"\"\"\n\n        dfs_sorted = sorted(self.data, key=key, reverse=descending)\n        return DataFrames(data=dfs_sorted)\n\n    # ----------------------------------------------------------------\n\n    def unload(self):\n        \"\"\"\n        Unloads all data frames in the current project from memory.\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.in_memory","title":"<code>in_memory</code>  <code>property</code>","text":"<p>Returns the names of all data frames currently in memory.</p>"},{"location":"reference/project/__init__/#getml.project.DataFrames.on_disk","title":"<code>on_disk</code>  <code>property</code>","text":"<p>Returns the names of all data frames stored in the project folder.</p>"},{"location":"reference/project/__init__/#getml.project.DataFrames.delete","title":"<code>delete()</code>","text":"<p>Deletes all data frames in the current project.</p> <p>Parameters:</p> Name Type Description Default <code>mem_only</code> <code>bool</code> <p>If called with the <code>mem_only</code> option set to True, the data frames will be kept on disk (in the project folder).</p> required Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Deletes all data frames in the current project.\n\n    Args:\n        mem_only (bool):\n            If called with the `mem_only` option set to True, the data\n            frames will be kept on disk (in the project folder).\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered data frames.</p> Example <pre><code>big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the data frames container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered data frames.\n\n    Example:\n        ```python\n        big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n        ```\n    \"\"\"\n\n    dfs_filtered = [df for df in self.data if conditional(df)]\n    return DataFrames(data=dfs_filtered)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.load","title":"<code>load()</code>","text":"<p>Loads all data frames stored in the project folder to memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads all data frames stored in the project folder to memory.\n    \"\"\"\n\n    for df in self.on_disk:\n        if df not in self.in_memory:\n            self.data.append(load_data_frame(df))\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.retrieve","title":"<code>retrieve()</code>","text":"<p>Retrieve a dict of all data frames in memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def retrieve(self):\n    \"\"\"\n    Retrieve a dict of all data frames in memory.\n    \"\"\"\n\n    return {df.name: df for df in self.data}\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.save","title":"<code>save()</code>","text":"<p>Saves all data frames currently in memory to disk.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves all data frames currently in memory to disk.\n    \"\"\"\n\n    for df in self.data:\n        df.save()\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted data frames.</p> Example <pre><code>by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the data frames container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted data frames.\n\n    Example:\n        ```python\n        by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n        ```\n    \"\"\"\n\n    dfs_sorted = sorted(self.data, key=key, reverse=descending)\n    return DataFrames(data=dfs_sorted)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.DataFrames.unload","title":"<code>unload()</code>","text":"<p>Unloads all data frames in the current project from memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads all data frames in the current project from memory.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Hyperopts","title":"<code>Hyperopts</code>","text":"<p>Container which holds all hyperopts associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>class Hyperopts:\n    \"\"\"\n    Container which holds all hyperopts associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_hyperopts()\n\n        if data is None:\n            self.data = [load_hyperopt(id) for id in self.ids]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            hyperopts_subset = self.data[key]\n            return Hyperopts(data=hyperopts_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [hyperopt for hyperopt in self.data if hyperopt.name == key][0]\n            raise AttributeError(f\"No Hyperopt with id: {key}\")\n        raise TypeError(\n            f\"Hyperopts can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(list_hyperopts()) == 0:\n            return \"No hyperopt in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(list_hyperopts()) == 0:\n            return \"&lt;p&gt;No hyperopt in memory.&lt;/p&gt;\"\n\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        headers = [[\"id\", \"type\", \"best pipeline\"]]\n\n        rows = [\n            [self.ids[index], hyperopt.type, hyperopt.best_pipeline.name]\n            for index, hyperopt in enumerate(self.data)\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the hyperopts container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered hyperopts.\n\n        Example:\n            ```python\n            gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n            ```\n        \"\"\"\n        hyperopts_filtered = [\n            hyperopt for hyperopt in self.data if conditional(hyperopt)\n        ]\n        return Hyperopts(data=hyperopts_filtered)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the hyperopts container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted hyperopts.\n\n        Example:\n            ```python\n            by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n            ```\n        \"\"\"\n        hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n        return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Hyperopts.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered hyperopts.</p> Example <pre><code>gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the hyperopts container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered hyperopts.\n\n    Example:\n        ```python\n        gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n        ```\n    \"\"\"\n    hyperopts_filtered = [\n        hyperopt for hyperopt in self.data if conditional(hyperopt)\n    ]\n    return Hyperopts(data=hyperopts_filtered)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Hyperopts.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted hyperopts.</p> Example <pre><code>by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the hyperopts container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted hyperopts.\n\n    Example:\n        ```python\n        by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n        ```\n    \"\"\"\n    hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n    return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Pipelines","title":"<code>Pipelines</code>","text":"<p>Container which holds all pipelines associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Example <p>Show the first 10 pipelines belonging to the current project: <pre><code>getml.project.pipelines[:10]\n</code></pre> You can use nested list comprehensions to retrieve a scoring history for your project: <pre><code>import matplotlib.pyplot as plt\n\nhyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                      for score in pipe.scores[\"data_test\"]\n                      if \"hyperopt\" in pipe.tags]\n\nfig, ax = plt.subplots()\nax.bar(*zip(*hyperopt_scores))\n</code></pre></p> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>class Pipelines:\n    \"\"\"\n    Container which holds all pipelines associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n\n    Example:\n        Show the first 10 pipelines belonging to the current project:\n        ```python\n        getml.project.pipelines[:10]\n        ```\n        You can use nested list comprehensions to retrieve a scoring history\n        for your project:\n        ```python\n        import matplotlib.pyplot as plt\n\n        hyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                              for score in pipe.scores[\"data_test\"]\n                              if \"hyperopt\" in pipe.tags]\n\n        fig, ax = plt.subplots()\n        ax.bar(*zip(*hyperopt_scores))\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_pipelines()\n\n        if data is None:\n            self.data = _refresh_all()\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            pipelines_subset = self.data[key]\n            return Pipelines(data=pipelines_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [pipeline for pipeline in self.data if pipeline.id == key][0]\n            raise AttributeError(f\"No Pipeline with id: {key}\")\n        raise TypeError(\n            f\"Pipelines can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.ids) == 0:\n            return \"No pipelines in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.ids) == 0:\n            return \"&lt;p&gt;No pipelines in memory.&lt;/p&gt;\"\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_regresion_pipelines(self):\n        return any(pipe.is_regression for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_classification_pipelines(self):\n        return any(pipe.is_classification for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        scores: List[Any] = []\n        scores_headers: List[Any] = []\n\n        if self._contains_classification_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(accuracy, []),\n                    pipeline._scores.get(auc, []),\n                    pipeline._scores.get(cross_entropy, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([accuracy, auc, cross_entropy])\n\n        if self._contains_regresion_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(mae, []),\n                    pipeline._scores.get(rmse, []),\n                    pipeline._scores.get(rsquared, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([mae, rmse, rsquared])\n\n        if (\n            self._contains_classification_pipelines\n            and self._contains_regresion_pipelines\n        ):\n            scores = [\n                [*classf, *reg]\n                for classf, reg in zip(\n                    scores[: len(self.data)], scores[len(self.data) :]\n                )\n            ]\n\n        sets_used = [pipeline._scores.get(\"set_used\", \"\") for pipeline in self.data]\n\n        targets = [pipeline.targets for pipeline in self.data]\n\n        feature_learners = [\n            [feature_learner.type for feature_learner in pipeline.feature_learners]\n            for pipeline in self.data\n        ]\n\n        tags = [pipeline.tags for pipeline in self.data]\n\n        headers = [\n            [\n                \"id\",\n                \"tags\",\n                \"feature learners\",\n                \"targets\",\n                *scores_headers,\n                \"set used\",\n            ]\n        ]\n\n        rows = [\n            [\n                pipeline.id,\n                tags[index],\n                feature_learners[index],\n                targets[index],\n                *scores[index],\n                sets_used[index],\n            ]\n            for index, pipeline in enumerate(self.data)\n        ]\n\n        # ------------------------------------------------------------\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the pipelines container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n                A container of sorted pipelines.\n\n        Example:\n            ```python\n            by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n            by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n            ```\n        \"\"\"\n        pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n        return Pipelines(data=pipelines_sorted)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the pipelines container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered pipelines.\n\n        Example:\n            ```python\n            pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n            accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n            ```\n        \"\"\"\n        pipelines_filtered = [\n            pipeline for pipeline in self.data if conditional(pipeline)\n        ]\n\n        return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Pipelines.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered pipelines.</p> Example <pre><code>pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\naccurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the pipelines container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered pipelines.\n\n    Example:\n        ```python\n        pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n        accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n        ```\n    \"\"\"\n    pipelines_filtered = [\n        pipeline for pipeline in self.data if conditional(pipeline)\n    ]\n\n    return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/__init__/#getml.project.Pipelines.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> <p>Returns:</p> Type Description <p>A container of sorted pipelines.</p> Example <pre><code>by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\nby_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the pipelines container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted pipelines.\n\n    Example:\n        ```python\n        by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n        by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n        ```\n    \"\"\"\n    pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n    return Pipelines(data=pipelines_sorted)\n</code></pre>"},{"location":"reference/project/attrs/","title":"Attrs","text":"<p>Handles access to project-related information.</p>"},{"location":"reference/project/attrs/#getml.project.attrs.delete","title":"<code>delete()</code>","text":"<p>Deletes the currently connected project. All related pipelines, data frames and hyperopts will be irretrievably deleted.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef delete():\n    \"\"\"\n    Deletes the currently connected project. All related pipelines,\n    data frames and hyperopts will be irretrievably deleted.\n    \"\"\"\n    comm._delete_project(_name())\n</code></pre>"},{"location":"reference/project/attrs/#getml.project.attrs.load","title":"<code>load(bundle, name=None)</code>","text":"<p>Loads a project from a bundle and connects to it.</p> <p>Parameters:</p> Name Type Description Default <code>bundle</code> <code>str</code> <p>The <code>.getml</code> bundle file to load.</p> required <code>name</code> <code>str</code> <p>A name for the project contained in the bundle. If None, the name will be extracted from the bundle.</p> <code>None</code> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef load(bundle, name=None):\n    \"\"\"\n    Loads a project from a bundle and connects to it.\n\n    Args:\n        bundle (str): The `.getml` bundle file to load.\n\n        name (str): A name for the project contained in the bundle.\n          If None, the name will be extracted from the bundle.\n    \"\"\"\n    return comm._load_project(bundle, name)\n</code></pre>"},{"location":"reference/project/attrs/#getml.project.attrs.restart","title":"<code>restart()</code>","text":"<p>Suspends and then relaunches the currently connected project. This will kill all jobs currently running on that process.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef restart():\n    \"\"\"\n    Suspends and then relaunches the currently connected project.\n    This will kill all jobs currently running on that process.\n    \"\"\"\n    comm._set_project(_name(), restart=True)\n</code></pre>"},{"location":"reference/project/attrs/#getml.project.attrs.save","title":"<code>save(filename=None, target_dir=None, replace=True)</code>","text":"<p>Saves the currently connected project to disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The name of the <code>.getml</code> bundle file</p> <code>None</code> <code>target_dir</code> <code>str</code> <p>the directory to save the bundle to. If None, the current working directory is used.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>Whether to replace an existing bundle.</p> <code>True</code> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef save(filename=None, target_dir=None, replace=True):\n    \"\"\"\n    Saves the currently connected project to disk.\n\n    Args:\n        filename (str): The name of the `.getml` bundle file\n\n        target_dir (str): the directory to save the bundle to.\n          If None, the current working directory is used.\n\n        replace (bool): Whether to replace an existing bundle.\n    \"\"\"\n    return comm._save_project(_name(), filename, target_dir, replace)\n</code></pre>"},{"location":"reference/project/attrs/#getml.project.attrs.suspend","title":"<code>suspend()</code>","text":"<p>Suspends the currently connected project.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef suspend():\n    \"\"\"\n    Suspends the currently connected project.\n    \"\"\"\n    return comm._suspend_project(_name())\n</code></pre>"},{"location":"reference/project/attrs/#getml.project.attrs.switch","title":"<code>switch(name)</code>","text":"<p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the engine, a new one will be created. See the User guide for more information.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the new project.</p> required Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef switch(name):\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the engine, a new one will\n    be created. See the [User guide][project-management] for more\n    information.\n\n    Args:\n        name (str): Name of the new project.\n    \"\"\"\n    comm._set_project(name)\n</code></pre>"},{"location":"reference/project/containers/__init__/","title":"init","text":"<p>Containers for representing data of the current project.</p>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames","title":"<code>DataFrames</code>","text":"<p>Container which holds all data frames associated with the running project that are currently stored in memory. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>class DataFrames:\n    \"\"\"\n    Container which holds all data frames associated with the running\n    project that are currently stored in memory. The container supports\n    slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self._in_memory = list_data_frames()[\"in_memory\"]\n        self._on_disk = list_data_frames()[\"on_disk\"]\n\n        if data is None:\n            self.data = [load_data_frame(name) for name in self._in_memory]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            dfs_subset = self.data[key]\n            return DataFrames(data=dfs_subset)\n        if isinstance(key, str):\n            if key in self.in_memory:\n                return [df for df in self.data if df.name == key][0]\n            if key in self.on_disk:\n                raise AttributeError(f\"DataFrame {key} not loaded from disk.\")\n            raise AttributeError(f\"No DataFrame with name: {key}\")\n        raise TypeError(\n            f\"DataFrames can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.in_memory) == 0:\n            output = \"No data frames in memory.\"\n        else:\n            output = self._format()._render_string()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"\\n\\nOn disk:\\n\"\n            output += \"\\n\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.in_memory) == 0:\n            output = \"&lt;p&gt;No data frames in memory.&lt;/p&gt;\"\n        else:\n            output = self._format()._render_html()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"&lt;p&gt;On disk:&lt;/p&gt;\"\n            output += \"&lt;br&gt;\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n\n        headers = [[\"name\", \"rows\", \"columns\", \"memory usage\"]]\n\n        rows = [[df.name, df.nrows(), df.ncols(), df.memory_usage] for df in self.data]\n\n        formatted = _Formatter(headers, rows)\n\n        formatted[4].cell_template = \"{:{width}.2f} MB\"\n\n        return formatted\n\n    # ----------------------------------------------------------------\n\n    def delete(self):\n        \"\"\"\n        Deletes all data frames in the current project.\n\n        Args:\n            mem_only (bool):\n                If called with the `mem_only` option set to True, the data\n                frames will be kept on disk (in the project folder).\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).delete()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def in_memory(self):\n        \"\"\"\n        Returns the names of all data frames currently in memory.\n        \"\"\"\n        return self._in_memory\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the data frames container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered data frames.\n\n        Example:\n            ```python\n            big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n            ```\n        \"\"\"\n\n        dfs_filtered = [df for df in self.data if conditional(df)]\n        return DataFrames(data=dfs_filtered)\n\n    # ----------------------------------------------------------------\n\n    def load(self):\n        \"\"\"\n        Loads all data frames stored in the project folder to memory.\n        \"\"\"\n\n        for df in self.on_disk:\n            if df not in self.in_memory:\n                self.data.append(load_data_frame(df))\n\n    # ----------------------------------------------------------------\n\n    @property\n    def on_disk(self):\n        \"\"\"\n        Returns the names of all data frames stored in the project folder.\n        \"\"\"\n        return self._on_disk\n\n    # ----------------------------------------------------------------\n\n    def retrieve(self):\n        \"\"\"\n        Retrieve a dict of all data frames in memory.\n        \"\"\"\n\n        return {df.name: df for df in self.data}\n\n    # ----------------------------------------------------------------\n\n    def save(self):\n        \"\"\"\n        Saves all data frames currently in memory to disk.\n        \"\"\"\n\n        for df in self.data:\n            df.save()\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the data frames container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted data frames.\n\n        Example:\n            ```python\n            by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n            ```\n        \"\"\"\n\n        dfs_sorted = sorted(self.data, key=key, reverse=descending)\n        return DataFrames(data=dfs_sorted)\n\n    # ----------------------------------------------------------------\n\n    def unload(self):\n        \"\"\"\n        Unloads all data frames in the current project from memory.\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.in_memory","title":"<code>in_memory</code>  <code>property</code>","text":"<p>Returns the names of all data frames currently in memory.</p>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.on_disk","title":"<code>on_disk</code>  <code>property</code>","text":"<p>Returns the names of all data frames stored in the project folder.</p>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.delete","title":"<code>delete()</code>","text":"<p>Deletes all data frames in the current project.</p> <p>Parameters:</p> Name Type Description Default <code>mem_only</code> <code>bool</code> <p>If called with the <code>mem_only</code> option set to True, the data frames will be kept on disk (in the project folder).</p> required Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Deletes all data frames in the current project.\n\n    Args:\n        mem_only (bool):\n            If called with the `mem_only` option set to True, the data\n            frames will be kept on disk (in the project folder).\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered data frames.</p> Example <pre><code>big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the data frames container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered data frames.\n\n    Example:\n        ```python\n        big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n        ```\n    \"\"\"\n\n    dfs_filtered = [df for df in self.data if conditional(df)]\n    return DataFrames(data=dfs_filtered)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.load","title":"<code>load()</code>","text":"<p>Loads all data frames stored in the project folder to memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads all data frames stored in the project folder to memory.\n    \"\"\"\n\n    for df in self.on_disk:\n        if df not in self.in_memory:\n            self.data.append(load_data_frame(df))\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.retrieve","title":"<code>retrieve()</code>","text":"<p>Retrieve a dict of all data frames in memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def retrieve(self):\n    \"\"\"\n    Retrieve a dict of all data frames in memory.\n    \"\"\"\n\n    return {df.name: df for df in self.data}\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.save","title":"<code>save()</code>","text":"<p>Saves all data frames currently in memory to disk.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves all data frames currently in memory to disk.\n    \"\"\"\n\n    for df in self.data:\n        df.save()\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted data frames.</p> Example <pre><code>by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the data frames container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted data frames.\n\n    Example:\n        ```python\n        by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n        ```\n    \"\"\"\n\n    dfs_sorted = sorted(self.data, key=key, reverse=descending)\n    return DataFrames(data=dfs_sorted)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.DataFrames.unload","title":"<code>unload()</code>","text":"<p>Unloads all data frames in the current project from memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads all data frames in the current project from memory.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Hyperopts","title":"<code>Hyperopts</code>","text":"<p>Container which holds all hyperopts associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>class Hyperopts:\n    \"\"\"\n    Container which holds all hyperopts associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_hyperopts()\n\n        if data is None:\n            self.data = [load_hyperopt(id) for id in self.ids]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            hyperopts_subset = self.data[key]\n            return Hyperopts(data=hyperopts_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [hyperopt for hyperopt in self.data if hyperopt.name == key][0]\n            raise AttributeError(f\"No Hyperopt with id: {key}\")\n        raise TypeError(\n            f\"Hyperopts can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(list_hyperopts()) == 0:\n            return \"No hyperopt in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(list_hyperopts()) == 0:\n            return \"&lt;p&gt;No hyperopt in memory.&lt;/p&gt;\"\n\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        headers = [[\"id\", \"type\", \"best pipeline\"]]\n\n        rows = [\n            [self.ids[index], hyperopt.type, hyperopt.best_pipeline.name]\n            for index, hyperopt in enumerate(self.data)\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the hyperopts container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered hyperopts.\n\n        Example:\n            ```python\n            gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n            ```\n        \"\"\"\n        hyperopts_filtered = [\n            hyperopt for hyperopt in self.data if conditional(hyperopt)\n        ]\n        return Hyperopts(data=hyperopts_filtered)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the hyperopts container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted hyperopts.\n\n        Example:\n            ```python\n            by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n            ```\n        \"\"\"\n        hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n        return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Hyperopts.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered hyperopts.</p> Example <pre><code>gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the hyperopts container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered hyperopts.\n\n    Example:\n        ```python\n        gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n        ```\n    \"\"\"\n    hyperopts_filtered = [\n        hyperopt for hyperopt in self.data if conditional(hyperopt)\n    ]\n    return Hyperopts(data=hyperopts_filtered)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Hyperopts.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted hyperopts.</p> Example <pre><code>by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the hyperopts container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted hyperopts.\n\n    Example:\n        ```python\n        by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n        ```\n    \"\"\"\n    hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n    return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Pipelines","title":"<code>Pipelines</code>","text":"<p>Container which holds all pipelines associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Example <p>Show the first 10 pipelines belonging to the current project: <pre><code>getml.project.pipelines[:10]\n</code></pre> You can use nested list comprehensions to retrieve a scoring history for your project: <pre><code>import matplotlib.pyplot as plt\n\nhyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                      for score in pipe.scores[\"data_test\"]\n                      if \"hyperopt\" in pipe.tags]\n\nfig, ax = plt.subplots()\nax.bar(*zip(*hyperopt_scores))\n</code></pre></p> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>class Pipelines:\n    \"\"\"\n    Container which holds all pipelines associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n\n    Example:\n        Show the first 10 pipelines belonging to the current project:\n        ```python\n        getml.project.pipelines[:10]\n        ```\n        You can use nested list comprehensions to retrieve a scoring history\n        for your project:\n        ```python\n        import matplotlib.pyplot as plt\n\n        hyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                              for score in pipe.scores[\"data_test\"]\n                              if \"hyperopt\" in pipe.tags]\n\n        fig, ax = plt.subplots()\n        ax.bar(*zip(*hyperopt_scores))\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_pipelines()\n\n        if data is None:\n            self.data = _refresh_all()\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            pipelines_subset = self.data[key]\n            return Pipelines(data=pipelines_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [pipeline for pipeline in self.data if pipeline.id == key][0]\n            raise AttributeError(f\"No Pipeline with id: {key}\")\n        raise TypeError(\n            f\"Pipelines can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.ids) == 0:\n            return \"No pipelines in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.ids) == 0:\n            return \"&lt;p&gt;No pipelines in memory.&lt;/p&gt;\"\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_regresion_pipelines(self):\n        return any(pipe.is_regression for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_classification_pipelines(self):\n        return any(pipe.is_classification for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        scores: List[Any] = []\n        scores_headers: List[Any] = []\n\n        if self._contains_classification_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(accuracy, []),\n                    pipeline._scores.get(auc, []),\n                    pipeline._scores.get(cross_entropy, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([accuracy, auc, cross_entropy])\n\n        if self._contains_regresion_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(mae, []),\n                    pipeline._scores.get(rmse, []),\n                    pipeline._scores.get(rsquared, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([mae, rmse, rsquared])\n\n        if (\n            self._contains_classification_pipelines\n            and self._contains_regresion_pipelines\n        ):\n            scores = [\n                [*classf, *reg]\n                for classf, reg in zip(\n                    scores[: len(self.data)], scores[len(self.data) :]\n                )\n            ]\n\n        sets_used = [pipeline._scores.get(\"set_used\", \"\") for pipeline in self.data]\n\n        targets = [pipeline.targets for pipeline in self.data]\n\n        feature_learners = [\n            [feature_learner.type for feature_learner in pipeline.feature_learners]\n            for pipeline in self.data\n        ]\n\n        tags = [pipeline.tags for pipeline in self.data]\n\n        headers = [\n            [\n                \"id\",\n                \"tags\",\n                \"feature learners\",\n                \"targets\",\n                *scores_headers,\n                \"set used\",\n            ]\n        ]\n\n        rows = [\n            [\n                pipeline.id,\n                tags[index],\n                feature_learners[index],\n                targets[index],\n                *scores[index],\n                sets_used[index],\n            ]\n            for index, pipeline in enumerate(self.data)\n        ]\n\n        # ------------------------------------------------------------\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the pipelines container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n                A container of sorted pipelines.\n\n        Example:\n            ```python\n            by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n            by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n            ```\n        \"\"\"\n        pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n        return Pipelines(data=pipelines_sorted)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the pipelines container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered pipelines.\n\n        Example:\n            ```python\n            pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n            accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n            ```\n        \"\"\"\n        pipelines_filtered = [\n            pipeline for pipeline in self.data if conditional(pipeline)\n        ]\n\n        return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Pipelines.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered pipelines.</p> Example <pre><code>pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\naccurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the pipelines container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered pipelines.\n\n    Example:\n        ```python\n        pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n        accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n        ```\n    \"\"\"\n    pipelines_filtered = [\n        pipeline for pipeline in self.data if conditional(pipeline)\n    ]\n\n    return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/containers/__init__/#getml.project.containers.Pipelines.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> <p>Returns:</p> Type Description <p>A container of sorted pipelines.</p> Example <pre><code>by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\nby_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the pipelines container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted pipelines.\n\n    Example:\n        ```python\n        by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n        by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n        ```\n    \"\"\"\n    pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n    return Pipelines(data=pipelines_sorted)\n</code></pre>"},{"location":"reference/project/containers/data_frames/","title":"Data frames","text":"<p>Container for data frames in memory.</p>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames","title":"<code>DataFrames</code>","text":"<p>Container which holds all data frames associated with the running project that are currently stored in memory. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>class DataFrames:\n    \"\"\"\n    Container which holds all data frames associated with the running\n    project that are currently stored in memory. The container supports\n    slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self._in_memory = list_data_frames()[\"in_memory\"]\n        self._on_disk = list_data_frames()[\"on_disk\"]\n\n        if data is None:\n            self.data = [load_data_frame(name) for name in self._in_memory]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            dfs_subset = self.data[key]\n            return DataFrames(data=dfs_subset)\n        if isinstance(key, str):\n            if key in self.in_memory:\n                return [df for df in self.data if df.name == key][0]\n            if key in self.on_disk:\n                raise AttributeError(f\"DataFrame {key} not loaded from disk.\")\n            raise AttributeError(f\"No DataFrame with name: {key}\")\n        raise TypeError(\n            f\"DataFrames can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.in_memory) == 0:\n            output = \"No data frames in memory.\"\n        else:\n            output = self._format()._render_string()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"\\n\\nOn disk:\\n\"\n            output += \"\\n\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.in_memory) == 0:\n            output = \"&lt;p&gt;No data frames in memory.&lt;/p&gt;\"\n        else:\n            output = self._format()._render_html()\n\n        if len(self.on_disk) &gt; 0:\n            output += \"&lt;p&gt;On disk:&lt;/p&gt;\"\n            output += \"&lt;br&gt;\".join(self.on_disk)\n\n        return output\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n\n        headers = [[\"name\", \"rows\", \"columns\", \"memory usage\"]]\n\n        rows = [[df.name, df.nrows(), df.ncols(), df.memory_usage] for df in self.data]\n\n        formatted = _Formatter(headers, rows)\n\n        formatted[4].cell_template = \"{:{width}.2f} MB\"\n\n        return formatted\n\n    # ----------------------------------------------------------------\n\n    def delete(self):\n        \"\"\"\n        Deletes all data frames in the current project.\n\n        Args:\n            mem_only (bool):\n                If called with the `mem_only` option set to True, the data\n                frames will be kept on disk (in the project folder).\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).delete()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def in_memory(self):\n        \"\"\"\n        Returns the names of all data frames currently in memory.\n        \"\"\"\n        return self._in_memory\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the data frames container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered data frames.\n\n        Example:\n            ```python\n            big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n            ```\n        \"\"\"\n\n        dfs_filtered = [df for df in self.data if conditional(df)]\n        return DataFrames(data=dfs_filtered)\n\n    # ----------------------------------------------------------------\n\n    def load(self):\n        \"\"\"\n        Loads all data frames stored in the project folder to memory.\n        \"\"\"\n\n        for df in self.on_disk:\n            if df not in self.in_memory:\n                self.data.append(load_data_frame(df))\n\n    # ----------------------------------------------------------------\n\n    @property\n    def on_disk(self):\n        \"\"\"\n        Returns the names of all data frames stored in the project folder.\n        \"\"\"\n        return self._on_disk\n\n    # ----------------------------------------------------------------\n\n    def retrieve(self):\n        \"\"\"\n        Retrieve a dict of all data frames in memory.\n        \"\"\"\n\n        return {df.name: df for df in self.data}\n\n    # ----------------------------------------------------------------\n\n    def save(self):\n        \"\"\"\n        Saves all data frames currently in memory to disk.\n        \"\"\"\n\n        for df in self.data:\n            df.save()\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the data frames container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted data frames.\n\n        Example:\n            ```python\n            by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n            ```\n        \"\"\"\n\n        dfs_sorted = sorted(self.data, key=key, reverse=descending)\n        return DataFrames(data=dfs_sorted)\n\n    # ----------------------------------------------------------------\n\n    def unload(self):\n        \"\"\"\n        Unloads all data frames in the current project from memory.\n        \"\"\"\n\n        for name in self.on_disk:\n            DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.in_memory","title":"<code>in_memory</code>  <code>property</code>","text":"<p>Returns the names of all data frames currently in memory.</p>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.on_disk","title":"<code>on_disk</code>  <code>property</code>","text":"<p>Returns the names of all data frames stored in the project folder.</p>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.delete","title":"<code>delete()</code>","text":"<p>Deletes all data frames in the current project.</p> <p>Parameters:</p> Name Type Description Default <code>mem_only</code> <code>bool</code> <p>If called with the <code>mem_only</code> option set to True, the data frames will be kept on disk (in the project folder).</p> required Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Deletes all data frames in the current project.\n\n    Args:\n        mem_only (bool):\n            If called with the `mem_only` option set to True, the data\n            frames will be kept on disk (in the project folder).\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered data frames.</p> Example <pre><code>big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the data frames container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered data frames.\n\n    Example:\n        ```python\n        big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n        ```\n    \"\"\"\n\n    dfs_filtered = [df for df in self.data if conditional(df)]\n    return DataFrames(data=dfs_filtered)\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.load","title":"<code>load()</code>","text":"<p>Loads all data frames stored in the project folder to memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads all data frames stored in the project folder to memory.\n    \"\"\"\n\n    for df in self.on_disk:\n        if df not in self.in_memory:\n            self.data.append(load_data_frame(df))\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.retrieve","title":"<code>retrieve()</code>","text":"<p>Retrieve a dict of all data frames in memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def retrieve(self):\n    \"\"\"\n    Retrieve a dict of all data frames in memory.\n    \"\"\"\n\n    return {df.name: df for df in self.data}\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.save","title":"<code>save()</code>","text":"<p>Saves all data frames currently in memory to disk.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves all data frames currently in memory to disk.\n    \"\"\"\n\n    for df in self.data:\n        df.save()\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the data frames container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted data frames.</p> Example <pre><code>by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the data frames container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted data frames.\n\n    Example:\n        ```python\n        by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n        ```\n    \"\"\"\n\n    dfs_sorted = sorted(self.data, key=key, reverse=descending)\n    return DataFrames(data=dfs_sorted)\n</code></pre>"},{"location":"reference/project/containers/data_frames/#getml.project.containers.data_frames.DataFrames.unload","title":"<code>unload()</code>","text":"<p>Unloads all data frames in the current project from memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads all data frames in the current project from memory.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/containers/hyperopts/","title":"Hyperopts","text":"<p>Container which holds a project's hyperopts.</p>"},{"location":"reference/project/containers/hyperopts/#getml.project.containers.hyperopts.Hyperopts","title":"<code>Hyperopts</code>","text":"<p>Container which holds all hyperopts associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>class Hyperopts:\n    \"\"\"\n    Container which holds all hyperopts associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_hyperopts()\n\n        if data is None:\n            self.data = [load_hyperopt(id) for id in self.ids]\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            hyperopts_subset = self.data[key]\n            return Hyperopts(data=hyperopts_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [hyperopt for hyperopt in self.data if hyperopt.name == key][0]\n            raise AttributeError(f\"No Hyperopt with id: {key}\")\n        raise TypeError(\n            f\"Hyperopts can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(list_hyperopts()) == 0:\n            return \"No hyperopt in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(list_hyperopts()) == 0:\n            return \"&lt;p&gt;No hyperopt in memory.&lt;/p&gt;\"\n\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        headers = [[\"id\", \"type\", \"best pipeline\"]]\n\n        rows = [\n            [self.ids[index], hyperopt.type, hyperopt.best_pipeline.name]\n            for index, hyperopt in enumerate(self.data)\n        ]\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the hyperopts container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered hyperopts.\n\n        Example:\n            ```python\n            gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n            ```\n        \"\"\"\n        hyperopts_filtered = [\n            hyperopt for hyperopt in self.data if conditional(hyperopt)\n        ]\n        return Hyperopts(data=hyperopts_filtered)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the hyperopts container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Return:\n                A container of sorted hyperopts.\n\n        Example:\n            ```python\n            by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n            ```\n        \"\"\"\n        hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n        return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/containers/hyperopts/#getml.project.containers.hyperopts.Hyperopts.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered hyperopts.</p> Example <pre><code>gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the hyperopts container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered hyperopts.\n\n    Example:\n        ```python\n        gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n        ```\n    \"\"\"\n    hyperopts_filtered = [\n        hyperopt for hyperopt in self.data if conditional(hyperopt)\n    ]\n    return Hyperopts(data=hyperopts_filtered)\n</code></pre>"},{"location":"reference/project/containers/hyperopts/#getml.project.containers.hyperopts.Hyperopts.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the hyperopts container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> Return <p>A container of sorted hyperopts.</p> Example <pre><code>by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the hyperopts container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted hyperopts.\n\n    Example:\n        ```python\n        by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n        ```\n    \"\"\"\n    hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n    return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/containers/pipelines/","title":"Pipelines","text":"<p>Container which holds all of a project's pipelines.</p>"},{"location":"reference/project/containers/pipelines/#getml.project.containers.pipelines.Pipelines","title":"<code>Pipelines</code>","text":"<p>Container which holds all pipelines associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Example <p>Show the first 10 pipelines belonging to the current project: <pre><code>getml.project.pipelines[:10]\n</code></pre> You can use nested list comprehensions to retrieve a scoring history for your project: <pre><code>import matplotlib.pyplot as plt\n\nhyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                      for score in pipe.scores[\"data_test\"]\n                      if \"hyperopt\" in pipe.tags]\n\nfig, ax = plt.subplots()\nax.bar(*zip(*hyperopt_scores))\n</code></pre></p> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>class Pipelines:\n    \"\"\"\n    Container which holds all pipelines associated with the currently running\n    project. The container supports slicing and is sort- and filterable.\n\n    Example:\n        Show the first 10 pipelines belonging to the current project:\n        ```python\n        getml.project.pipelines[:10]\n        ```\n        You can use nested list comprehensions to retrieve a scoring history\n        for your project:\n        ```python\n        import matplotlib.pyplot as plt\n\n        hyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                              for score in pipe.scores[\"data_test\"]\n                              if \"hyperopt\" in pipe.tags]\n\n        fig, ax = plt.subplots()\n        ax.bar(*zip(*hyperopt_scores))\n        ```\n    \"\"\"\n\n    # ----------------------------------------------------------------\n\n    def __init__(self, data=None):\n        self.ids = list_pipelines()\n\n        if data is None:\n            self.data = _refresh_all()\n        else:\n            self.data = data\n\n    # ----------------------------------------------------------------\n\n    def __getitem__(self, key):\n        if isinstance(key, int):\n            return self.data[key]\n        if isinstance(key, slice):\n            pipelines_subset = self.data[key]\n            return Pipelines(data=pipelines_subset)\n        if isinstance(key, str):\n            if key in self.ids:\n                return [pipeline for pipeline in self.data if pipeline.id == key][0]\n            raise AttributeError(f\"No Pipeline with id: {key}\")\n        raise TypeError(\n            f\"Pipelines can only be indexed by: int, slices, or str, not {type(key).__name__}\"\n        )\n\n    # ----------------------------------------------------------------\n\n    def __len__(self):\n        return len(self.data)\n\n    # ----------------------------------------------------------------\n\n    def __repr__(self):\n        if len(self.ids) == 0:\n            return \"No pipelines in memory.\"\n\n        return self._format()._render_string()\n\n    # ----------------------------------------------------------------\n\n    def _repr_html_(self):\n        if len(self.ids) == 0:\n            return \"&lt;p&gt;No pipelines in memory.&lt;/p&gt;\"\n        return self._format()._render_html()\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_regresion_pipelines(self):\n        return any(pipe.is_regression for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    @property\n    def _contains_classification_pipelines(self):\n        return any(pipe.is_classification for pipe in self.data)\n\n    # ----------------------------------------------------------------\n\n    def _format(self):\n        scores: List[Any] = []\n        scores_headers: List[Any] = []\n\n        if self._contains_classification_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(accuracy, []),\n                    pipeline._scores.get(auc, []),\n                    pipeline._scores.get(cross_entropy, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([accuracy, auc, cross_entropy])\n\n        if self._contains_regresion_pipelines:\n            scores.extend(\n                [\n                    pipeline._scores.get(mae, []),\n                    pipeline._scores.get(rmse, []),\n                    pipeline._scores.get(rsquared, []),\n                ]\n                for pipeline in self.data\n            )\n\n            scores_headers.extend([mae, rmse, rsquared])\n\n        if (\n            self._contains_classification_pipelines\n            and self._contains_regresion_pipelines\n        ):\n            scores = [\n                [*classf, *reg]\n                for classf, reg in zip(\n                    scores[: len(self.data)], scores[len(self.data) :]\n                )\n            ]\n\n        sets_used = [pipeline._scores.get(\"set_used\", \"\") for pipeline in self.data]\n\n        targets = [pipeline.targets for pipeline in self.data]\n\n        feature_learners = [\n            [feature_learner.type for feature_learner in pipeline.feature_learners]\n            for pipeline in self.data\n        ]\n\n        tags = [pipeline.tags for pipeline in self.data]\n\n        headers = [\n            [\n                \"id\",\n                \"tags\",\n                \"feature learners\",\n                \"targets\",\n                *scores_headers,\n                \"set used\",\n            ]\n        ]\n\n        rows = [\n            [\n                pipeline.id,\n                tags[index],\n                feature_learners[index],\n                targets[index],\n                *scores[index],\n                sets_used[index],\n            ]\n            for index, pipeline in enumerate(self.data)\n        ]\n\n        # ------------------------------------------------------------\n\n        return _Formatter(headers, rows)\n\n    # ----------------------------------------------------------------\n\n    def sort(self, key, descending=False):\n        \"\"\"\n        Sorts the pipelines container.\n\n        Args:\n            key (callable, optional):\n                A callable that evaluates to a sort key for a given item.\n\n            descending (bool, optional):\n                Whether to sort in descending order.\n\n        Returns:\n                A container of sorted pipelines.\n\n        Example:\n            ```python\n            by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n            by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n            ```\n        \"\"\"\n        pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n        return Pipelines(data=pipelines_sorted)\n\n    # ----------------------------------------------------------------\n\n    def filter(self, conditional):\n        \"\"\"\n        Filters the pipelines container.\n\n        Args:\n            conditional (callable):\n                A callable that evaluates to a boolean for a given item.\n\n        Returns:\n                A container of filtered pipelines.\n\n        Example:\n            ```python\n            pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n            accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n            ```\n        \"\"\"\n        pipelines_filtered = [\n            pipeline for pipeline in self.data if conditional(pipeline)\n        ]\n\n        return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/containers/pipelines/#getml.project.containers.pipelines.Pipelines.filter","title":"<code>filter(conditional)</code>","text":"<p>Filters the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>conditional</code> <code>callable</code> <p>A callable that evaluates to a boolean for a given item.</p> required <p>Returns:</p> Type Description <p>A container of filtered pipelines.</p> Example <pre><code>pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\naccurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def filter(self, conditional):\n    \"\"\"\n    Filters the pipelines container.\n\n    Args:\n        conditional (callable):\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered pipelines.\n\n    Example:\n        ```python\n        pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n        accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n        ```\n    \"\"\"\n    pipelines_filtered = [\n        pipeline for pipeline in self.data if conditional(pipeline)\n    ]\n\n    return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/project/containers/pipelines/#getml.project.containers.pipelines.Pipelines.sort","title":"<code>sort(key, descending=False)</code>","text":"<p>Sorts the pipelines container.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>callable</code> <p>A callable that evaluates to a sort key for a given item.</p> required <code>descending</code> <code>bool</code> <p>Whether to sort in descending order.</p> <code>False</code> <p>Returns:</p> Type Description <p>A container of sorted pipelines.</p> Example <pre><code>by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\nby_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def sort(self, key, descending=False):\n    \"\"\"\n    Sorts the pipelines container.\n\n    Args:\n        key (callable, optional):\n            A callable that evaluates to a sort key for a given item.\n\n        descending (bool, optional):\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted pipelines.\n\n    Example:\n        ```python\n        by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n        by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n        ```\n    \"\"\"\n    pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n    return Pipelines(data=pipelines_sorted)\n</code></pre>"},{"location":"reference/sqlite3/__init__/","title":"init","text":"<p>This module contains wrappers around sqlite3 and related utility functions, which enable you to productionize pipelines using only sqlite3 and Python, fully based on open-source code.</p> <p>This requires SQLite version 3.33.0 or above. To check the sqlite3 version of your Python distribution, do the following:</p> <pre><code>import sqlite3\nsqlite3.sqlite_version\n</code></pre> Example <p>For our example we will assume that you want to productionize the CORA project.</p> <p>First, we want to transpile the features into SQL code, like this:</p> <p><pre><code># Set targets to False, if you want an inference pipeline.\npipe1.features.to_sql(targets=True).save(\"cora\")\n</code></pre> This transpiles the features learned by pipe1 into a set of SQLite3 scripts ready to be executed. These scripts are contained in a folder called \"cora\".</p> <p>We also assume that you have the three tables needed for the CORA project in the form of pandas.DataFrames (other data sources are possible).</p> <p>We want to create a new sqlite3 connection and then read in the data: <pre><code>conn = getml.sqlite3.connect(\"cora.db\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"cites\", data_frame=cites, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"content\", data_frame=content, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"paper\", data_frame=paper, if_exists=\"replace\")\n</code></pre> Now we can execute the scripts we have just created: <pre><code>conn = getml.sqlite3.execute(conn, \"cora\")\n</code></pre> The transpiled pipeline will always create a table called \"FEATURES\", which contain the features. Here is how we retrieve them: <pre><code>features = getml.sqlite3.to_pandas(conn, \"FEATURES\")\n</code></pre> Now you have created your features in a pandas DataFrame ready to be inserted into your favorite machine learning library.</p> <p>To build stable data science pipelines, it is often a good idea to ensure type safety by hard-coding your table schema. You can use the sniff... methods to do that: <pre><code>getml.sqlite3.sniff_pandas(\"cites\", cites)\n</code></pre> This will generate SQLite3 code that creates the \"cites\" table. You can hard-code that into your pipeline. This will ensure that the data always have the correct types, avoiding awkward problems in the future.</p>"},{"location":"reference/sqlite3/connect/","title":"Connect","text":"<p>Contains a wrapper around connect.</p>"},{"location":"reference/sqlite3/connect/#getml.sqlite3.connect.connect","title":"<code>connect(database)</code>","text":"<p>Generates a new sqlite3 connection.</p> <p>This connection contains all customized aggregations and transformation functions needed to execute the SQL pipeline generated by getML. Other than that it behaves just like a normal sqlite3 connection from the Python standard library.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>Filename of the database. Use ':memory:' to create an in-memory database.</p> required Source code in <code>getml/sqlite3/connect.py</code> <pre><code>def connect(database: str):\n    \"\"\"\n    Generates a new sqlite3 connection.\n\n    This connection contains all customized aggregations\n    and transformation functions needed to execute the\n    SQL pipeline generated by getML. Other than that\n    it behaves just like a normal sqlite3 connection from\n    the Python standard library.\n\n    Args:\n        database (str):\n            Filename of the database. Use ':memory:' to\n            create an in-memory database.\n    \"\"\"\n\n    if not isinstance(database, str):\n        raise TypeError(\"'database' must be of type str\")\n\n    if sqlite3.sqlite_version &lt; \"3.33.0\":\n        raise ValueError(\n            \"getML requires SQLite version 3.33.0 or above. Found version \"\n            + sqlite3.sqlite_version\n            + \". Please upgrade Python and/or the Python sqlite3 package.\"\n        )\n\n    conn = sqlite3.connect(database)\n\n    conn.create_function(\"contains\", 2, _contains)\n    conn.create_function(\"email_domain\", 1, _email_domain)\n    conn.create_function(\"get_word\", 2, _get_word)\n    conn.create_function(\"num_words\", 1, _num_words)\n\n    conn.create_aggregate(\"COUNT_ABOVE_MEAN\", 1, _CountAboveMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_BELOW_MEAN\", 1, _CountBelowMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_DISTINCT_OVER_COUNT\", 1, _CountDistinctOverCount)  # type: ignore\n    conn.create_aggregate(\"EWMA_1S\", 2, _EWMA1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_1M\", 2, _EWMA1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_1H\", 2, _EWMA1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_1D\", 2, _EWMA1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_7D\", 2, _EWMA7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_30D\", 2, _EWMA30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_90D\", 2, _EWMA90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_365D\", 2, _EWMA365D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1S\", 2, _EWMATrend1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1M\", 2, _EWMATrend1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1H\", 2, _EWMATrend1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1D\", 2, _EWMATrend1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_7D\", 2, _EWMATrend7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_30D\", 2, _EWMATrend30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_90D\", 2, _EWMATrend90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_365D\", 2, _EWMATrend365D)  # type: ignore\n    conn.create_aggregate(\"FIRST\", 2, _First)  # type: ignore\n    conn.create_aggregate(\"KURTOSIS\", 1, _Kurtosis)\n    conn.create_aggregate(\"LAST\", 2, _Last)  # type: ignore\n    conn.create_aggregate(\"MEDIAN\", 1, _Median)\n    conn.create_aggregate(\"MODE\", 1, _Mode)\n    conn.create_aggregate(\"NUM_MAX\", 1, _NumMax)  # type: ignore\n    conn.create_aggregate(\"NUM_MIN\", 1, _NumMin)  # type: ignore\n    conn.create_aggregate(\"Q1\", 1, _Q1)  # type: ignore\n    conn.create_aggregate(\"Q5\", 1, _Q5)  # type: ignore\n    conn.create_aggregate(\"Q10\", 1, _Q10)  # type: ignore\n    conn.create_aggregate(\"Q25\", 1, _Q25)  # type: ignore\n    conn.create_aggregate(\"Q75\", 1, _Q75)  # type: ignore\n    conn.create_aggregate(\"Q90\", 1, _Q90)  # type: ignore\n    conn.create_aggregate(\"Q95\", 1, _Q95)  # type: ignore\n    conn.create_aggregate(\"Q99\", 1, _Q99)  # type: ignore\n    conn.create_aggregate(\"SKEW\", 1, _Skew)\n    conn.create_aggregate(\"STDDEV\", 1, _Stddev)\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MAXIMUM\", 2, _TimeSinceFirstMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MINIMUM\", 2, _TimeSinceFirstMinimum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MAXIMUM\", 2, _TimeSinceLastMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MINIMUM\", 2, _TimeSinceLastMinimum)  # type: ignore\n    conn.create_aggregate(\"TREND\", 2, _Trend)  # type: ignore\n    conn.create_aggregate(\"VAR\", 1, _Var)\n    conn.create_aggregate(\"VARIATION_COEFFICIENT\", 1, _VariationCoefficient)\n\n    return conn\n</code></pre>"},{"location":"reference/sqlite3/contains/","title":"Contains","text":"<p>Checks whether a textfield contains a certain keyword.</p>"},{"location":"reference/sqlite3/count_above_mean/","title":"Count above mean","text":"<p>COUNT_ABOVE_MEAN aggregation.</p>"},{"location":"reference/sqlite3/count_below_mean/","title":"Count below mean","text":"<p>COUNT_BELOW_MEAN aggregation.</p>"},{"location":"reference/sqlite3/count_distinct_over_count/","title":"Count distinct over count","text":"<p>COUNT_DISTINCT_OVER_COUNT aggregation.</p>"},{"location":"reference/sqlite3/email_domain/","title":"Email domain","text":"<p>Extracts the email domain.</p>"},{"location":"reference/sqlite3/ewma/","title":"Ewma","text":"<p>Contains the exponentially weighted moving average aggregations.</p>"},{"location":"reference/sqlite3/ewma_trend/","title":"Ewma trend","text":"<p>Contains the exponentially weighted moving average aggregations.</p>"},{"location":"reference/sqlite3/execute/","title":"Execute","text":"<p>Executes SQL scripts on SQLite3</p>"},{"location":"reference/sqlite3/execute/#getml.sqlite3.execute.execute","title":"<code>execute(conn, fname)</code>","text":"<p>Executes an SQL script or several SQL scripts on SQLite3.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Connection</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>fname</code> <code>str</code> <p>The names of the SQL script or a folder containing SQL scripts. If you decide to pass a folder, the SQL scripts must have the ending '.sql'.</p> required Source code in <code>getml/sqlite3/execute.py</code> <pre><code>def execute(conn: sqlite3.Connection, fname: str):\n    \"\"\"\n    Executes an SQL script or several SQL scripts on SQLite3.\n\n    Args:\n        conn (sqlite3.Connection):\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        fname (str):\n            The names of the SQL script or a folder containing SQL scripts.\n            If you decide to pass a folder, the SQL scripts must have the ending '.sql'.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    # ------------------------------------------------------------\n\n    # Store temporary object in-memory.\n    conn.execute(\"PRAGMA temp_store=2;\")\n\n    if os.path.isdir(fname):\n        scripts = _retrieve_scripts(fname, \".sql\")\n        for script in scripts:\n            execute(conn, script)\n        return\n\n    _log(\"Executing \" + fname + \"...\")\n\n    with open(fname, \"rt\", encoding=\"utf-8\") as sqlfile:\n        queries = sqlfile.read().split(\";\")\n\n    for query in queries:\n        conn.execute(query + \";\")\n\n    conn.commit()\n</code></pre>"},{"location":"reference/sqlite3/first/","title":"First","text":"<p>FIRST aggregation.</p>"},{"location":"reference/sqlite3/get_word/","title":"Get word","text":"<p>Returns the word indicated by i from the textfield. Note that this begins with 1, following the SQLite convention.</p>"},{"location":"reference/sqlite3/helpers/","title":"Helpers","text":"<p>Contains simple helper functions for the sqlite3 module</p>"},{"location":"reference/sqlite3/kurtosis/","title":"Kurtosis","text":"<p>KURTOSIS aggregation.</p>"},{"location":"reference/sqlite3/last/","title":"Last","text":"<p>LAST aggregation.</p>"},{"location":"reference/sqlite3/median/","title":"Median","text":"<p>MEDIAN aggregation.</p>"},{"location":"reference/sqlite3/mode/","title":"Mode","text":"<p>MODE aggregation.</p>"},{"location":"reference/sqlite3/num_max/","title":"Num max","text":"<p>NUM_MAX aggregation.</p>"},{"location":"reference/sqlite3/num_min/","title":"Num min","text":"<p>NUM_MIN aggregation.</p>"},{"location":"reference/sqlite3/num_words/","title":"Num words","text":"<p>Counts the number of words in a textfield.</p>"},{"location":"reference/sqlite3/quantiles/","title":"Quantiles","text":"<p>Contains the quantile aggregations.</p>"},{"location":"reference/sqlite3/read_csv/","title":"Read csv","text":"<p>Contains utility functions for reading CSV files into sqlite3.</p>"},{"location":"reference/sqlite3/read_csv/#getml.sqlite3.read_csv.read_csv","title":"<code>read_csv(conn, fnames, table_name, header=True, if_exists='append', quotechar='\"', sep=',', skip=0, colnames=None)</code>","text":"<p>Reads a list of CSV files and writes them into an sqlite3 table.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Connection</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>fnames</code> <code>str or List[str]</code> <p>The names of the CSV files.</p> required <code>fnames</code> <code>str or List[str]</code> <p>The names of the CSV files.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to write to.</p> required <code>header</code> <code>bool</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <code>True</code> <code>quotechar</code> <code>str</code> <p>The string escape character.</p> <code>'\"'</code> <code>if_exists</code> <code>str</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values to the existing table.</li> </ul> <code>'append'</code> <code>sep</code> <code>str</code> <p>The field separator.</p> <code>','</code> <code>skip</code> <code>int</code> <p>The number of lines to skip (before a possible header)</p> <code>0</code> <code>colnames</code> <code>List[str] or None</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <code>None</code> Source code in <code>getml/sqlite3/read_csv.py</code> <pre><code>def read_csv(\n    conn,\n    fnames,\n    table_name,\n    header=True,\n    if_exists=\"append\",\n    quotechar='\"',\n    sep=\",\",\n    skip=0,\n    colnames=None,\n):\n    \"\"\"\n    Reads a list of CSV files and writes them into an sqlite3 table.\n\n    Args:\n        conn (sqlite3.Connection):\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        fnames (str or List[str]):\n            The names of the CSV files.\n\n        fnames (str or List[str]):\n            The names of the CSV files.\n\n        table_name (str):\n            The name of the table to write to.\n\n        header (bool):\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        quotechar (str):\n            The string escape character.\n\n        if_exists (str):\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values to the existing table.\n\n        sep (str):\n            The field separator.\n\n        skip (int):\n            The number of lines to skip (before a possible header)\n\n        colnames (List[str] or None, optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    schema = sniff_csv(\n        fnames=fnames,\n        table_name=table_name,\n        header=header,\n        quotechar=quotechar,\n        sep=sep,\n        skip=skip,\n        colnames=colnames,\n    )\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    for fname in fnames:\n        _log(\"Loading '\" + fname + \"' into '\" + table_name + \"'...\")\n        data = _read_csv_file(fname, sep, quotechar, header and not colnames, skip)\n        read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/read_list/","title":"Read list","text":"<p>Reads data into an sqlite3 table.</p>"},{"location":"reference/sqlite3/read_list/#getml.sqlite3.read_list.read_list","title":"<code>read_list(conn, table_name, data)</code>","text":"<p>Reads data into an sqlite3 table.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to write to.</p> required <code>data</code> <code>List[List[Object]]</code> <p>The data to insert into the table. Every list represents one row to be read into the table.</p> required Source code in <code>getml/sqlite3/read_list.py</code> <pre><code>def read_list(conn, table_name, data):\n    \"\"\"\n    Reads data into an sqlite3 table.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        table_name (str):\n            The name of the table to write to.\n\n        data (List[List[Object]]):\n            The data to insert into the table.\n            Every list represents one row to be read into the table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(data, list):\n        raise TypeError(\"'data' must be a list of lists\")\n\n    # ------------------------------------------------------------\n\n    ncols = _get_num_columns(conn, table_name)\n    old_length = len(data)\n    data = [line for line in data if len(line) == ncols]\n    placeholders = \"(\" + \",\".join([\"?\"] * ncols) + \")\"\n    query = 'INSERT INTO \"' + table_name + '\" VALUES ' + placeholders\n    conn.executemany(query, data)\n    conn.commit()\n    _log(\n        \"Read \"\n        + str(len(data))\n        + \" lines. \"\n        + str(old_length - len(data))\n        + \" invalid lines.\"\n    )\n</code></pre>"},{"location":"reference/sqlite3/read_pandas/","title":"Read pandas","text":"<p>Contains utility functions for reading a pandas DataFrame into sqlite3.</p>"},{"location":"reference/sqlite3/read_pandas/#getml.sqlite3.read_pandas.read_pandas","title":"<code>read_pandas(conn, table_name, data_frame, if_exists='append')</code>","text":"<p>Loads a pandas.DataFrame into SQLite3.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>table_name</code> <code>str</code> <p>The name of the table to write to.</p> required <code>data_frame</code> <code>DataFrame</code> <p>The pandas.DataFrame to read into the table. The column names must match the column names of the target table in the SQLite3 database, but their order is not important.</p> required <code>if_exists</code> <code>str</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values into the existing table.</li> </ul> <code>'append'</code> Source code in <code>getml/sqlite3/read_pandas.py</code> <pre><code>def read_pandas(conn, table_name, data_frame, if_exists=\"append\"):\n    \"\"\"\n    Loads a pandas.DataFrame into SQLite3.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        table_name (str):\n            The name of the table to write to.\n\n        data_frame (pandas.DataFrame):\n            The pandas.DataFrame to read\n            into the table. The column names must match the column\n            names of the target table in the SQLite3 database, but\n            their order is not important.\n\n        if_exists (str):\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values into the existing table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    # ------------------------------------------------------------\n\n    _log(\"Loading pandas.DataFrame into '\" + table_name + \"'...\")\n\n    schema = sniff_pandas(table_name, data_frame)\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    colnames = _get_colnames(conn, table_name)\n    data = data_frame[colnames].values.tolist()\n    data = [\n        [\n            field\n            if isinstance(field, (numbers.Number, str)) or field is None\n            else str(field)\n            for field in row\n        ]\n        for row in data\n    ]\n    read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/skew/","title":"Skew","text":"<p>SKEW aggregation.</p>"},{"location":"reference/sqlite3/sniff_csv/","title":"Sniff csv","text":"<p>Contains utility functions for sniffing sqlite data types from CSV files.</p>"},{"location":"reference/sqlite3/sniff_csv/#getml.sqlite3.sniff_csv.sniff_csv","title":"<code>sniff_csv(fnames, table_name, header=True, num_lines_sniffed=1000, quotechar='\"', sep=',', skip=0, colnames=None)</code>","text":"<p>Sniffs a list of csv files.</p> <p>Parameters:</p> Name Type Description Default <code>fnames</code> <code>List[str]</code> <p>The list of CSV file names to be read.</p> required <code>table_name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>header</code> <code>bool</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <code>True</code> <code>num_lines_sniffed</code> <code>int</code> <p>Number of lines analyzed by the sniffer.</p> <code>1000</code> <code>quotechar</code> <code>str</code> <p>The character used to wrap strings.</p> <code>'\"'</code> <code>sep</code> <code>str</code> <p>The separator used for separating fields.</p> <code>','</code> <code>skip</code> <code>int</code> <p>Number of lines to skip at the beginning of each file.</p> <code>0</code> <code>colnames</code> <code>List[str]</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_csv.py</code> <pre><code>def sniff_csv(\n    fnames,\n    table_name,\n    header=True,\n    num_lines_sniffed=1000,\n    quotechar='\"',\n    sep=\",\",\n    skip=0,\n    colnames=None,\n):\n    \"\"\"\n    Sniffs a list of csv files.\n\n    Args:\n        fnames (List[str]):\n            The list of CSV file names to be read.\n\n        table_name (str):\n            Name of the table in which the data is to be inserted.\n\n        header (bool):\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        num_lines_sniffed (int, optional):\n            Number of lines analyzed by the sniffer.\n\n        quotechar (str, optional):\n            The character used to wrap strings.\n\n        sep (str, optional):\n            The separator used for separating fields.\n\n        skip (int, optional):\n            Number of lines to skip at the beginning of each\n            file.\n\n        colnames (List[str], optional):\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n\n    Returns:\n        str:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(num_lines_sniffed, int):\n        raise TypeError(\"'num_lines_sniffed' must be a int\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    header_lines = 0 if header and not colnames else None\n\n    def read(fname):\n        return pd.read_csv(\n            fname,\n            nrows=num_lines_sniffed,\n            header=header_lines,\n            sep=sep,\n            quotechar=quotechar,\n            skiprows=skip,\n            names=colnames,\n        )\n\n    data_frames = [read(fname) for fname in fnames]\n\n    merged = pd.concat(data_frames, join=\"inner\")\n\n    return sniff_pandas(table_name, merged)\n</code></pre>"},{"location":"reference/sqlite3/sniff_pandas/","title":"Sniff pandas","text":"<p>Contains utility functions for siffing sqlite data types from pandas DataFrames.</p>"},{"location":"reference/sqlite3/sniff_pandas/#getml.sqlite3.sniff_pandas.sniff_pandas","title":"<code>sniff_pandas(table_name, data_frame)</code>","text":"<p>Sniffs a pandas data frame.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table in which the data is to be inserted.</p> required <code>data_frame</code> <code>DataFrame</code> <p>The pandas.DataFrame to read into the table.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_pandas.py</code> <pre><code>def sniff_pandas(table_name: str, data_frame: str) -&gt; str:\n    \"\"\"\n    Sniffs a pandas data frame.\n\n    Args:\n        table_name (str):\n            Name of the table in which the data is to be inserted.\n\n        data_frame (pandas.DataFrame):\n            The pandas.DataFrame to read into the table.\n\n    Returns:\n        str:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    # ------------------------------------------------------------\n\n    colnames = data_frame.columns\n    coltypes = data_frame.dtypes\n\n    sql_types: Dict[str, List[str]] = {\"INTEGER\": [], \"REAL\": [], \"TEXT\": []}\n\n    for cname, ctype in zip(colnames, coltypes):\n        if _is_int_type(ctype):\n            sql_types[\"INTEGER\"].append(cname)\n            continue\n        if _is_numerical_type(ctype):\n            sql_types[\"REAL\"].append(cname)\n        else:\n            sql_types[\"TEXT\"].append(cname)\n\n    return _generate_schema(table_name, sql_types)\n</code></pre>"},{"location":"reference/sqlite3/split_text_field/","title":"Split text field","text":"<p>Splits a text field into its individual components. This can not be called directly from SQLite3. It is a helper function.</p>"},{"location":"reference/sqlite3/stddev/","title":"Stddev","text":"<p>STDDEV aggregation.</p>"},{"location":"reference/sqlite3/time_since_first_maximum/","title":"Time since first maximum","text":"<p>TIME_SINCE_FIRST_MAXIMUM aggregation.</p>"},{"location":"reference/sqlite3/time_since_first_minimum/","title":"Time since first minimum","text":"<p>TIME_SINCE_FIRST_MINIMUM aggregation.</p>"},{"location":"reference/sqlite3/time_since_last_maximum/","title":"Time since last maximum","text":"<p>TIME_SINCE_LAST_MAXIMUM aggregation.</p>"},{"location":"reference/sqlite3/time_since_last_minimum/","title":"Time since last minimum","text":"<p>TIME_SINCE_LAST_MINIMUM aggregation.</p>"},{"location":"reference/sqlite3/to_list/","title":"To list","text":"<p>Returns a table as a list of lists.</p>"},{"location":"reference/sqlite3/to_list/#getml.sqlite3.to_list.to_list","title":"<code>to_list(conn, query)</code>","text":"<p>Transforms a query or table into a list of lists. Returns a tuple which contains the column names and the actual data.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>query</code> <code>str</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> required Source code in <code>getml/sqlite3/to_list.py</code> <pre><code>def to_list(conn, query):\n    \"\"\"\n    Transforms a query or table into a list of lists. Returns\n    a tuple which contains the column names and the actual data.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        query (str):\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    query = _handle_query(query)\n    cursor = conn.execute(query)\n    colnames = [description[0] for description in cursor.description]\n    data = cursor.fetchall()\n    return colnames, data\n</code></pre>"},{"location":"reference/sqlite3/to_pandas/","title":"To pandas","text":"<p>Returns a table as a pandas.DataFrame.</p>"},{"location":"reference/sqlite3/to_pandas/#getml.sqlite3.to_pandas.to_pandas","title":"<code>to_pandas(conn, query)</code>","text":"<p>Returns a table as a pandas.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> required <code>query</code> <code>str</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> required Source code in <code>getml/sqlite3/to_pandas.py</code> <pre><code>def to_pandas(conn, query):\n    \"\"\"\n    Returns a table as a pandas.DataFrame.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect].\n\n        query (str):\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    colnames, data = to_list(conn, query)\n    data_frame = pd.DataFrame(data)\n    data_frame.columns = colnames\n    return data_frame\n</code></pre>"},{"location":"reference/sqlite3/trend/","title":"Trend","text":"<p>Extracts linear trends over time.</p>"},{"location":"reference/sqlite3/var/","title":"Var","text":"<p>VAR aggregation.</p>"},{"location":"reference/sqlite3/variation_coefficient/","title":"Variation coefficient","text":"<p>VARIATION_COEFFICIENT aggregation.</p>"},{"location":"reference/utilities/__init__/","title":"init","text":""},{"location":"reference/utilities/formatting/__init__/","title":"init","text":""},{"location":"reference/utilities/formatting/cell_formatter/","title":"Cell formatter","text":""},{"location":"reference/utilities/formatting/cell_formatter/#getml.utilities.formatting.cell_formatter.CellFormatter","title":"<code>CellFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Custom formatter for cells in output columns. Supports all python-native format specs  https://docs.python.org/3/library/string.html#formatspec plus the following custom  format specs:</p> <ul> <li> <p><code>&lt;w_idth&gt;.&lt;p_recision&gt;fd</code> (float): like <code>&lt;w&gt;.&lt;p&gt;f</code>, but the values of a column are   decimal point aligned</p> </li> <li> <p><code>\\&lt;w_idth&gt;.&lt;p_recision&gt;d</code> (str): formats strings (holdings numbers) in a column on   the decimal point by taking into account the precision</p> </li> </ul> Source code in <code>getml/utilities/formatting/cell_formatter.py</code> <pre><code>class CellFormatter(string.Formatter):\n    \"\"\"\n    Custom formatter for cells in output columns. Supports all python-native format specs\n    [https://docs.python.org/3/library/string.html#formatspec](https://docs.python.org/3/library/string.html#formatspec) plus the following custom\n    format specs:\n\n   - `&lt;w_idth&gt;.&lt;p_recision&gt;fd` (float): like `&lt;w&gt;.&lt;p&gt;f`, but the values of a column are\n     decimal point aligned\n\n   - `\\&lt;w_idth&gt;.&lt;p_recision&gt;d` (str): formats strings (holdings numbers) in a column on\n     the decimal point by taking into account the precision\n    \"\"\"\n\n    integer_overhang = 1\n\n    # ------------------------------------------------------------\n\n    def format_field(self, value, format_spec):\n\n        if format_spec.endswith(\"fd\"):\n            return self._format_float_decimal_point(value, format_spec)\n\n        if format_spec.endswith(\"d\"):\n            return self._format_string_decimal_point(value, format_spec)\n\n        return super().format_field(value, format_spec)\n\n    # ------------------------------------------------------------\n\n    def _format_float_decimal_point(self, value, format_spec):\n        width = int(format_spec.split(\".\")[0].strip())\n        precision = int(format_spec.split(\".\")[1][0])\n        padding = precision - self.integer_overhang\n\n        if value.is_integer():\n            formatted = f\"{value:{width-padding}.{self.integer_overhang}f}\"\n            # fix misalignment due to missing decimal point\n            if self.integer_overhang == 0:\n                formatted = formatted[1:]\n            return f\"{formatted:{width}}\"\n\n        formatted = super().format_field(value, format_spec[:-1])\n        stripped = formatted.rstrip(\"0\")\n        return f\"{stripped:{width}}\"\n\n    # ------------------------------------------------------------\n\n    def _format_string_decimal_point(self, value, format_spec):\n        width = int(format_spec.split(\".\")[0].strip())\n        precision = int(format_spec.split(\".\")[1][0])\n\n        if value.strip(\"-\").isdigit() or value == \"nan\":\n            formatted = f\"{value:&gt;{width-precision-1}}\"\n            return f\"{formatted:{width}}\"\n\n        if \".\" in value:\n            digits = len(value.split(\".\")[1])\n        else:\n            digits = 0\n        formatted = f\"{value:&gt;{width-precision+digits}}\"\n        return f\"{formatted:{width}}\"\n</code></pre>"},{"location":"reference/utilities/formatting/column_formatter/","title":"Column formatter","text":"<p>Displays a column</p>"},{"location":"reference/utilities/formatting/data_frame_formatter/","title":"Data frame formatter","text":"<p>Holds the DataFrameFormatter.</p>"},{"location":"reference/utilities/formatting/data_frame_formatter/#getml.utilities.formatting.data_frame_formatter.UNITS","title":"<code>UNITS = ['D', 'h', 'm', 's', 'ms']</code>  <code>module-attribute</code>","text":"<p>Numpy datetime units.</p>"},{"location":"reference/utilities/formatting/data_frame_formatter/#getml.utilities.formatting.data_frame_formatter.UNITS_FORMATS","title":"<code>UNITS_FORMATS = dict(D='{:%Y-%m-%d}', h='{:%Y-%m-%d %H}', m='{:%Y-%m-%d %H:%M}', s='{:%Y-%m-%d %H:%M:%S}', ms='{:%Y-%m-%d %H:%M:%S.%f}')</code>  <code>module-attribute</code>","text":"<p>A mapping of numpy datetime units to strftime format specifiers.</p>"},{"location":"reference/utilities/formatting/ellipsis/","title":"Ellipsis","text":"<p>A class with a nice repr for suppressing parameter values.</p>"},{"location":"reference/utilities/formatting/formatter/","title":"Formatter","text":"<p>Holds Formatter and FormatColumn classes which are used for formatting tabular output within the getML python API.</p>"},{"location":"reference/utilities/formatting/formatter/#getml.utilities.formatting.formatter.BASE_PRECISION","title":"<code>BASE_PRECISION = 4</code>  <code>module-attribute</code>","text":"<p>The baseline (minimum) number of significant digits when formatting floats.</p>"},{"location":"reference/utilities/formatting/helpers/","title":"Helpers","text":"<p>Collection of helper functions that are not intended to be used by the end-user.</p>"},{"location":"reference/utilities/formatting/signature_formatter/","title":"Signature formatter","text":"<p>Holds the SignatureFormatter class that allows for the proper formatting of function and class signatures.</p>"},{"location":"reference/utilities/formatting/view_formatter/","title":"View formatter","text":"<p>Displays a View.</p>"},{"location":"reference/utilities/templates/__init__/","title":"init","text":"<p>Manages jinja2 templates.</p>"},{"location":"user_guide/annotating_data/annotating_data/","title":"Annotating data","text":""},{"location":"user_guide/annotating_data/annotating_data/#annotating-data_1","title":"Annotating data","text":"<p>After you have imported your data into the getML engine, there is one more step to undertake before you can start learning features: You need to assign a role to each column. Why is that?</p> <p>First, the general structure of the individual data frames is needed to construct the relational data model. This is done by assigning the roles join key and time stamp. The former defines the columns that are used to join different data frames, the latter ensures that only rows in a reasonable time frame are taken into account (otherwise there might be data leaks).</p> <p>Second, you need to tell the feature learning algorithm how to interpret the individual columns for it to construct sophisticated features. That is why we need the roles numerical, categorical, and target. You can also assign units to each column in a Data Frame.</p> <p>This chapter contains detailed information on the individual roles and units.</p>"},{"location":"user_guide/annotating_data/annotating_data/#in-short","title":"In short","text":"<p>When building the data model, you should keep the following things in mind:</p> <ul> <li>Every <code>DataFrame</code> in a data model needs to have at least one column (<code>columns</code>) with the role join key.</li> <li>The role time stamp has to be used to prevent data leaks (refer to data model time series for details).</li> </ul> <p>When learning features, please keep the following things in mind:</p> <ul> <li>Only <code>columns</code> with roles of categorical, numerical, and time stamp will be used by the feature learning algorithm for aggregations or conditions, unless you explicitly tell it to aggregate target columns as well (refer to <code>allow_lagged_target</code> in <code>join()</code>).</li> <li>Columns are only compared with each other if they have the same unit.</li> <li>If you want to make sure that a column is only used for comparison, you can set <code>comparison_only</code> (refer to annotating units). Time stamps are automatically set to <code>comparison_only</code>.</li> </ul> <p></p>"},{"location":"user_guide/annotating_data/annotating_data/#roles","title":"Roles","text":"<p>Roles determine if and how <code>columns</code> are handled during the construction of the data model and how they are interpreted by the feature learning algorithm. The following roles are available in getML:</p> Role Class Included in FL algorithm <code>categorical</code> <code>StringColumn</code> yes <code>numerical</code> <code>FloatColumn</code> yes <code>text</code> <code>StringColumn</code> yes <code>time_stamp</code> <code>FloatColumn</code> yes <code>join_key</code> <code>StringColumn</code> no <code>target</code> <code>FloatColumn</code> not by default <code>unused_float</code> <code>FloatColumn</code> no <code>unused_string</code> <code>StringColumn</code> no <p>When constructing a <code>DataFrame</code> via the class methods <code>from_csv</code>, <code>from_pandas</code>, <code>from_db</code>, and <code>from_json</code>, all <code>columns</code> will have either the role unused float or unused string . Unused columns will be ignored by the feature learning and machine learning (ML) algorithms.</p> <pre><code>import pandas as pd\ndata_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    weight=[12.14, 12.6, 11.92],\n    animal_id=[123, 512, 671],\n    date=[\"2019-05-02\", \"2019-02-28\", \"2018-12-24\"]\n)\npandas_df = pd.DataFrame(data=data_df)\ngetml_df = getml.data.DataFrame.from_pandas(pandas_df, name='animal elections')\n\ngetml_df\n# Output:\n# | votes        | weight       | animal_id    | animal        | date          |\n# | unused float | unused float | unused float | unused string | unused string |\n# ------------------------------------------------------------------------------\n# | 12341        | 12.14        | 123          | hawk          | 2019-05-02    |\n# | 5127         | 12.6         | 512          | parrot        | 2019-02-28    |\n# | 65311        | 11.92        | 671          | goose         | 2018-12-24    |\n</code></pre> <p>To make use of the imported data, you have to tell getML how you intend to use each column by assigning a role (<code>roles</code>). This is done by using the <code>set_role</code> method of the <code>DataFrame</code>. Each column must have exactly one role. If you wish to use a column in two different roles, you have to add it twice and assign each copy a different role.</p> <p><pre><code>getml_df.set_role(['animal_id'], getml.data.roles.join_key)\ngetml_df.set_role(['animal'], getml.data.roles.categorical)\ngetml_df.set_role(['votes', 'weight'], getml.data.roles.numerical)\ngetml_df.set_role(['date'], getml.data.roles.time_stamp)    \ngetml_df\n# Output:\n# | date                        | animal_id | animal      | votes     | weight    |\n# | time stamp                  | join key  | categorical | numerical | numerical |\n# ---------------------------------------------------------------------------------\n# | 2019-05-02T00:00:00.000000Z | 123       | hawk        | 12341     | 12.14     |\n# | 2019-02-28T00:00:00.000000Z | 512       | parrot      | 5127      | 12.6      |\n# | 2018-12-24T00:00:00.000000Z | 671       | goose       | 65311     | 11.92     |\n</code></pre> When assigning new roles to existing columns, you might notice that some of these calls are completed in an instance while others might take a considerable amount of time. What's happening here? A column's role also determines its type.  When you set a new role, an implicit type conversion might take place.</p>"},{"location":"user_guide/annotating_data/annotating_data/#a-note-on-reproducibility-and-efficiency","title":"A note on reproducibility and efficiency","text":"<p>When building a stable pipeline you want to deploy in a productive environment, the flexible default behavior of the import interface might be more of an obstacle. For instance, CSV files are not type-safe. A column that was interpreted as a float column for one set of files might be interpreted as a string column for a different set of files. This obviously has implications for the stability of your pipeline. Therefore, it might be a good idea to hard-code column roles.</p> <p>In the getML Python API, you can bypass the default deduction of the role of each column by providing a dictionary mapping each column name to a role in the import interface.</p> <p><pre><code>roles = {\"categorical\": [\"colname1\", \"colname2\"], \"target\": [\"colname3\"]}\ndf_expd = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles,\n    ignore=True\n)\n</code></pre> If the <code>ignore</code> argument is set to <code>True</code>, any columns missing in the dictionary won't be imported at all.</p> <p>If you feel that writing the roles by hand is too tedious, you can use <code>dry</code>: If you call the import interface while setting the <code>dry</code> argument to <code>True</code>, no data is read. Instead, the default roles of all columns will be returned as a dictionary. You can store, alter, and hard-code this dictionary into your stable pipeline.</p> <pre><code>roles = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True                                     \n)\n</code></pre> <p>Even if your data source is type safe, setting roles is still a good idea because it is also more efficient. Using <code>set_role()</code> creates a deep copy of the original column and might perform an implicit type conversion. If you already know where you want your data to end up, it might be a good idea to set roles in advance. </p>"},{"location":"user_guide/annotating_data/annotating_data/#join-key","title":"Join key","text":"<p>Join keys are required to establish a relation between two <code>DataFrame</code> objects. Please refer to the data models for details.</p> <p>The content of this column is allowed to contain NULL values. NULL values won't be matched to anything, not even to NULL values in other join keys.</p> <p><code>columns</code> of this role will not be aggregated by the feature learning algorithm or used for conditions. </p>"},{"location":"user_guide/annotating_data/annotating_data/#time-stamp","title":"Time stamp","text":"<p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps:</p> <p><pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre> This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a  <code>StringColumn</code>,  you need to specify the format of this string. You can do so by using  the <code>time_formats</code> argument of <code>set_role()</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign </li> </ul> <p>If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL. <pre><code>data_df = dict(\ndate1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\ndate2=['1971-01-01', '1971-01-02', '1971-01-03'],\ndate3=['1|1|71', '1|2|71', '1|3|71'],\n)\ndf = getml.data.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n# | date1                       | date2                       | date3                       |\n# | time stamp                  | time stamp                  | time stamp                  |\n# -------------------------------------------------------------------------------------------\n# | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n# | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n# | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre></p> <p>Note</p> <p>getML time stamps are actually floats expressing the number of seconds since  UNIX time (1970-01-01T00:00:00).</p> <p></p>"},{"location":"user_guide/annotating_data/annotating_data/#target","title":"Target","text":"<p>The associated columns contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>allow_lagged_target</code> in <code>join()</code>). However, they are such an important part of the analysis that the population table is required to contain at least one of them (refer to data model tables).</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>. </p>"},{"location":"user_guide/annotating_data/annotating_data/#numerical","title":"Numerical","text":"<p>This role tells the getML engine to include the associated <code>FloatColumn</code> during the feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch. </p>"},{"location":"user_guide/annotating_data/annotating_data/#categorical","title":"Categorical","text":"<p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integers instead of strings. </p>"},{"location":"user_guide/annotating_data/annotating_data/#text","title":"Text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. For more information on getML's handling of text fields, refer to the Preprocessing section </p>"},{"location":"user_guide/annotating_data/annotating_data/#unused_float","title":"Unused_float","text":"<p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/annotating_data/annotating_data/#unused_string","title":"Unused_string","text":"<p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/annotating_data/annotating_data/#units","title":"Units","text":"<p>By default, all columns of role categorical or numerical will only be compared to fixed values.</p> <p><pre><code>...\nWHERE numerical_column &gt; some_value \nOR categorical_column == 'some string'\n...\n</code></pre> If you want the feature learning algorithms to compare these columns with each other (like in the snippet below),  you have to explicitly set a unit. </p> <p><pre><code>...\nWHERE numerical_column1 - numerical_column2 &gt; some_value\nOR categorical_column1 != categorical_column2\n...\n</code></pre> Using <code>set_unit()</code> you can set the unit of a column to an arbitrary, non-empty string. If it matches the string of another column, both of them will be compared by the getML engine. Please note that a column can not have more than one unit.</p> <p>There are occasions where only a pairwise comparison of columns but not a comparison with fixed values is useful. To cope with this problem, you can set the <code>comparison_only</code> flag in <code>set_unit()</code>.</p> <p>Note</p> <p>Note that time stamps are used for comparison only by default. The feature  learning algorithm will not compare them to a fixed date, because it is very unlikely that such a feature would perform well out-of-sample.</p>"},{"location":"user_guide/data_model/data_model/","title":"Data model","text":""},{"location":"user_guide/data_model/data_model/#data-model_1","title":"Data model","text":"<p>Defining the data model is a crucial step before training one of getML's <code>Pipeline</code>s. You typically deal with this step after having imported your data and specified the roles of each column.</p> <p>When working with getML, the raw data usually comes in the form of relational data. That means the information relevant for a prediction is spread over several tables. The data model is the definition of the relationships between all of them.</p> <p>Most relational machine learning problems can be represented in the form of a star schema, in which case you can use the <code>StarSchema</code> abstraction. If your data set is a time series, you can use the <code>TimeSeries</code> abstraction. </p>"},{"location":"user_guide/data_model/data_model/#tables","title":"Tables","text":"<p>When defining the data model, we distinguish between a population table and one or more peripheral tables. In the context of this tutorial, we will use the term \"table\" as a catch-all for <code>DataFrame</code>s and <code>View</code>s. </p>"},{"location":"user_guide/data_model/data_model/#the-population-table","title":"The population table","text":"<p>The population table is the main table of the analysis. It defines the statistical population of your machine learning problem and contains the targetvariable(s), which we want to predict. Furthermore, the table usually also contains one or more columns with the role join_key. These are keys used to establish a relationship \u2013 also called joins \u2013 with one or more peripheral tables.</p> <p>The example below contains the population table of a customer churn analysis. The target variable is <code>churn</code> \u2013 whether a person stops using the services and products of a company. It also contains the information whether or not a given customer has churned after a certain reference date. The join key <code>customer_id</code> is used to establish relations with a peripheral table. Additionally, the date the customer joined the company is contained in the column <code>date_joined</code>, which we have assigned the role time_stamp.</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#peripheral-tables","title":"Peripheral tables","text":"<p>Peripheral tables contain additional information relevant for the prediction of the target variable in the population table. Each of them is related to the latter (or another peripheral table, refer to the snowflake schema) via a join_key.</p> <p>The images below represent two peripheral tables that could be used for our customer churn analysis. One table represents complaints a customer made with a certain agent, and the other represents the transactions the customer made using their account.</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#placeholders","title":"Placeholders","text":"<p>In getML, <code>Placeholder</code>s are used to construct the <code>DataModel</code>. They are abstract representations of <code>DataFrame</code>s or <code>View</code>s and the relationships among each other, but do not contain any data themselves.</p> <p>The idea behind the placeholder concept is that they allow constructing an abstract data model without any reference to an actual data set. This data model serves as input for the <code>Pipeline</code>. Later on, the <code>feature_learning</code> algorithms can be trained and applied on any data set that follows this data model.</p> <p>More information on how to construct placeholders and build a data model can be found in the API documentation for <code>Placeholder</code> and <code>DataModel</code>. </p>"},{"location":"user_guide/data_model/data_model/#joins","title":"Joins","text":"<p>Joins are used to establish relationships between placeholders. To join two placeholders, the data frames used to derive them should both have at least one join_key. The joining itself is done using the <code>join()</code> method.</p> <p>All columns corresponding to time stamps have to be given the role join_key, and one of them in both the population and peripheral table is usually passed to the <code>join()</code> method. This approach ensures that no information from the future is considered during training by including only those rows of the peripheral table in the join operation for which the time stamp of the corresponding row in the population table is either the same or more recent.</p> <p></p>"},{"location":"user_guide/data_model/data_model/#data-schemata","title":"Data schemata","text":"<p>After creating placeholders for all data frames in an analysis, we are ready to create the actual data schema. A data schema is a certain way of assembling population and peripheral tables.</p>"},{"location":"user_guide/data_model/data_model/#the-star-schema","title":"The star schema","text":"<p>The <code>StarSchema</code> is the simplest way of establishing relations between the population and the peripheral tables, sufficient for the majority of data science projects.</p> <p>In the star schema, the population table is surrounded by any number of peripheral tables, all joined via a certain join key. However, no joins between peripheral tables are allowed.</p> <p>Because this is a very popular schema in many machine learning problems on relational data, getML contains a special class for these sorts of problems: <code>StarSchema</code>.</p> <p>The population table and two peripheral tables introduced in Tables can be arranged in a star schema like this:</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#the-snowflake-schema","title":"The snowflake schema","text":"<p>In some cases, the star schema is not enough to represent the complexity of a data set. This is where the snowflake schema comes in: In a snowflake schema, peripheral tables can have peripheral tables of their own.</p> <p>Assume that in the customer churn analysis shown earlier, there is an additional table containing information about the calls a certain agent made in customer service. It can be joined to the <code>COMPLAINTS</code> table using the key <code>agent_id</code>.</p> <p></p> <p>To model snowflake schemata, you need to use the <code>DataModel</code> and <code>Container</code> classes.</p> <p></p>"},{"location":"user_guide/data_model/data_model/#time-series","title":"Time series","text":"<p>Time series can be handled by a self-join. In addition, some extra parameters and considerations are required when building features based on time stamps. </p>"},{"location":"user_guide/data_model/data_model/#self-joining-a-single-table","title":"Self-joining a single table","text":"<p>If you are dealing with a classical (multivariate) time series and all your data is contained in a single table, all the concepts covered so far still apply. You just have to perform a so-called self-join by providing your table as both the population and peripheral table and join them.</p> <p>The process works as follows: Whenever a row in the population table - a single measurement - is taken, it will be combined with all the content of the peripheral table - the same time series - for which the time stamps are smaller than the one in the line we picked.</p> <p>You can also use the <code>TimeSeries</code> abstraction, which abstracts away the self-join. In this case, you do not have to think about self-joins too much.</p>"},{"location":"user_guide/data_model/data_model/#horizon-and-memory","title":"Horizon and Memory","text":"<p>Crucial concepts of time series analysis are horizon and memory. In the context of getML's time series analysis, horizon is defined as a point forecast. That means the prediction of the target variable at the point as far in the future as defined by the horizon.   </p> <p>Memory, on the other hand is the time duration into the past, that is considered when making a prediction. The memory is used to define the time window of data entering the model between the past and now. The horizon defines the point in the future that the predictions is being made for.   </p>"},{"location":"user_guide/data_model/data_model/#time_stamps-and-on","title":"<code>time_stamps</code> and <code>on</code>","text":"<p>Two parameters in the time series signature determine how the self join is carried out. The <code>time_stamps</code> parameter defines what column is the underlying time dimension to which memory and horizon are applied to. The chosen column must also be of role time_stamp.  </p> <p><code>on</code> simply provides an extra handle to control, what subset of the data is part of any given time series. For example if you have a time series of sales data, you might want to only consider the sales data of a certain product category. In this case you would specify the <code>on</code> parameter to be the column containing the product category.</p> <p>Tip</p> <p>If you assign a column to the <code>on</code> parameter, then this column will not enter the model as a predictor. If you have reason to believe that this column is relevant to the model (i.e. the actual product category), duplicate that column in advance and assign the duplicate to the <code>on</code> parameter. (see class method <code>add()</code>)</p>"},{"location":"user_guide/data_model/data_model/#lagged-target-and-horizon","title":"Lagged Target and horizon","text":"<p>Another useful parameter in time series analysis is <code>lagged_target</code>. This boolean controls whether the target variable is used as a predictor. Including the target variable as a predictor can be useful in time series analysis, when at time of prediction, the target variable up until and including now is known. In turn, this means lagged target variables are only permissible if the target variable is predicted for some when in the future. That is, the horizon must be assigned a positive value.</p>"},{"location":"user_guide/data_model/data_model/#features-based-on-time-stamps","title":"Features based on time stamps","text":"<p>The getML engine is able to automatically generate features based on aggregations over time windows. Both the length of the time window and the aggregation itself will be determined by the feature learning algorithm. The only requirement is to provide the temporal resolution your time series is sampled with in the <code>delta_t</code> parameter in any feature learning algorithm.</p>"},{"location":"user_guide/deployment/deployment/","title":"Deployment","text":"<p>The results of the feature learning and the prediction can be retrieved in different ways and formats.</p> <p>Transpiling pipelines</p> <p>Using <code>SQLCode.save()</code>, you can transpile Pipelines to SQL code, which can be used without any proprietary components.</p> <p>Returning Python objects</p> <p>Using the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods of a trained <code>Pipeline</code>, you can access both the features and the predictions as <code>numpy.ndarray</code> via the Python API.</p> <p>Writing into a database</p> <p>You can also write both features and prediction results back into a new table of the connected database by providing the <code>table_name</code> argument in the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods. Please refer to the unified import interface for information on how to connect to a database.</p> <p>Responding to a HTTP POST request</p> <p>The getML suite contains HTTP endpoints to post new data via a JSON string and retrieve either the resulting features or the predictions.</p>"},{"location":"user_guide/deployment/deployment/#batch-prediction","title":"Batch prediction","text":"<p>Batch prediction pipelines are the most common way of productionizing machine learning pipelines on relational data. These pipelines are usually set to run regularly (once a month, once a week, once a day...) to create a batch of predictions on the newest data. They are typically inserted into a Docker container and scheduled using tools like Jenkins and/or Airflow.</p> <p>If you are looking for a pure Python, 100% open-source way to productionize getML's <code>Pipeline</code>s, you can transpile all the features into sqlite3 code. sqlite3 is part of the Python standard library, and you can use getML's 100% open source and pure Python <code>sqlite3</code> which provides some useful extra functionality not included in Python's standard library.</p>"},{"location":"user_guide/deployment/deployment/#http-endpoints","title":"HTTP Endpoints","text":"<p>As soon as you have trained a pipeline, whitelisted it for external access using its <code>deploy</code> method, and configured the getML monitor for remote access, you can transform new data into features or make predictions on them using these endpoints:</p> <ul> <li>Transform endpoint: <code>http://localhost:1709/transform/PIPELINE_NAME</code></li> <li>Predict endpoint: <code>http://localhost:1709/predict/PIPELINE_NAME</code></li> </ul> <p>To each of them, you must send a POST request containing the new data as a JSON string in a specific request format.</p> <p>Note</p> <p>For testing and developing purposes, you can also use the HTTP port of the monitor to query the endpoints. Note that this is only possible within the same host. The corresponding syntax is  http://localhost:1709/predict/PIPELINE_NAME</p> <p></p>"},{"location":"user_guide/deployment/deployment/#request-format","title":"Request Format","text":"<p>In all POST requests to the endpoints, a JSON string with the following syntax has to be provided in the body:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_1\": [],\n    \"column_2\": []\n  },{\n    \"column_1\": [],\n    \"column_2\": []\n  }],\n  \"population\": {\n    \"column_1\": [],\n    \"column_2\": []\n  }\n}\n</code></pre> <p>It has to have exactly two keys in the top level called <code>population</code> and <code>peripheral</code>. These will contain the new input data.</p> <p>The order of the columns is irrelevant. They will be matched according to their names. However, the order of the individual peripheral tables is very important and has to exactly match the order the corresponding <code>Placeholder</code> have been provided in the constructor of <code>pipeline</code>.</p> <p>In our example above, we could post a JSON string like this:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [0.1, 0.2, 0.3, 0.4, 0.8]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/deployment/deployment/#time-stamp-formats-in-requests","title":"Time stamp formats in requests","text":"<p>You might have noticed that the time stamps in the example above have been passed as numerical values and not as their string representations shown in the beginning. Both ways are supported by the getML monitor. But if you choose to pass the string representation, you also have to specify the particular format in order for the getML engine to interpret your data properly.</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 00:15:00\", \"2010-01-01 08:00:00\", \"2010-01-01 09:30:00\", \"2010-01-01 13:00:00\", \"2010-01-01 23:35:00\"]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 12:30:00\", \"2010-01-01 23:30:00\"]\n  },\n  \"timeFormats\": [\"%Y-%m-%d %H:%M:%S\"]\n}\n</code></pre> <p>All special characters available for specifying the format of the time stamps are listed and described in e.g. <code>read_csv()</code>.</p>"},{"location":"user_guide/deployment/deployment/#using-an-existing-dataframe","title":"Using an existing <code>DataFrame</code>","text":"<p>You can also use a <code>DataFrame</code> that already  exists on the getML engine:</p> <pre><code>{\n  \"peripheral\": [{\n    \"df\": \"peripheral_table\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/deployment/deployment/#using-data-from-a-database","title":"Using data from a database","text":"<p>You can also read the data from the connected database (see unified import interface)  by passing an arbitrary query to the <code>query</code> key:</p> <pre><code>{\n  \"peripheral\": [{\n    \"query\": \"SELECT * FROM PERIPHERAL WHERE join_key = '0';\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre> <p></p>"},{"location":"user_guide/deployment/deployment/#transform-endpoint","title":"Transform Endpoint","text":"<p>The transform endpoint returns the generated features.</p> <p>http://localhost:1709/transform/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility.</p> <p><pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/transform/PIPELINE_NAME\n</code></pre> </p>"},{"location":"user_guide/deployment/deployment/#predict-endpoint","title":"Predict Endpoint","text":"<p>When using getML as an end-to-end data science pipeline, you can use the predict endpoint to upload new, unseen data and receive the resulting predictions as a response via HTTP.</p> <p>http://localhost:1709/predict/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility.</p> <pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/predict/PIPELINE_NAME\n</code></pre>"},{"location":"user_guide/feature_engineering/feature_engineering/","title":"Feature engineering","text":""},{"location":"user_guide/feature_engineering/feature_engineering/#feature-engineering_1","title":"Feature engineering","text":"<p>The deep learning revolution has enabled automated feature engineering for images and sound data. Yet, for relational data and classical time series analysis, feature engineering is still done by hand or using very simple brute force methods. Our mission is to change that.</p> <p>The automation of feature engineering on relational data and time series is at the heart of the getML software suite. There are other libraries that implement feature engineering tools on top of frameworks like <code>data.tables</code> in R, <code>pandas</code> in Python, or <code>Apache Spark</code>. In essence, they all use a brute force approach: Generate a large number of features, then use some sort of feature selection routine to pick a small subselection of them.</p> <p>getML has chosen another path: Our highly efficient feature learning algorithms produce features that are far more advanced than what manual feature engineering could achieve or what could be accomplished using simple brute force approaches.</p>"},{"location":"user_guide/feature_engineering/feature_engineering/#definition","title":"Definition","text":"<p>Feature engineering is the process of constructing variables, so-called features, from a dataset. These features are used as the input for machine learning algorithms. In most real-world datasets, the raw data is spread over multiple tables and the task is to bring these tables together and construct features based on their relationships. These features are stored in a flat feature table. In other words, feature engineering is the operation of merging and aggregating a relational data model into a flat (feature) table. From an academic point of view, most machine learning algorithms used nowadays can be classified as propositional learners. The process of creating flat attribute-value representations from relational data through simple rules or aggregation functions therefore is called propositionalization.</p> <p>Usually, feature engineering is done manually, by using brute force approaches or domain knowledge. This process is sometimes also referred to as data wrangling. In any case it is a tedious, time-consuming, and error-prone process. Manual feature engineering is often done by writing scripts in languages like Python, R, or SQL.</p> <p>Note</p> <p>Unfortunately, the term feature engineering is ambiguous. More often than not, feature engineering is meant to describe numerical transformations or encoding techniques on a single table. The definition used above assumes that the raw data comes in relational form, which is true for almost all real-world data sets.</p>"},{"location":"user_guide/feature_engineering/feature_engineering/#feature-learning-vs-propositionalization","title":"Feature learning vs. propositionalization","text":"<p>We follow academia and classify techniques that use simple, merely unconditional transformations (like aggregations) to construct flat (attribute-value) representations as propositionalization approaches, while we classify algorithms which directly learn from relational data structures as feature learning. Here, we pick up a term coined in the deep learning context, where complex relationships are equally learned from raw input data.</p> <p>getML provides a framework capable of automatically extracting useful and meaningful features from a relational data model by finding the best merge and aggregate operations. In fact, the relationships between the target and the original data is learned through one of getML's feature learning algorithms.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#design-principles","title":"Design principles","text":"<p>The general procedure for feature learning on relational data and time series using getML looks like this:</p> <p>The only required input is a relational data schema. In particular, there needs to be some sort of target variable(s), which shall be predicted. For time series, the schema would typically be a self-join. In addition to this general information on the data schema, the intended usage of the variables has to be provided by setting the roles of the corresponding columns. How to setup a data scheme is described in data model.</p> <p>Features are often of the type (illustrated in pseudo SQL-like syntax): <pre><code>COUNT the number of `transactions` within the last X `days`\n</code></pre> where \\(X\\) is some sort of fixed numerical value. getML's algorithms do identify appropriate values for \\(X\\) automatically and there is no need for you to provide them by hand.</p> <p>Features can also take the form of:</p> <p><pre><code>COUNT the number of `transactions` for which the `transaction type` is \u2018type_1\u2019 OR \u2018type_2\u2019 OR \u2019type_3\u2019 OR \u2026\n</code></pre> getML's algorithms also find appropriate conditions based on categorical data without any input from the user.</p> <p>The feature learning algorithms can handle combinations of conditions too. So, features of the form:</p> <p><pre><code>SOME_AGGREGATION( over some column ) WHERE ( condition_1 AND\ncondition_2 ) OR ( condition_3 AND condition_4 ) OR condition_5\n</code></pre> will be engineered automatically as well. Again, no input from the user is required.</p> <p>To increase transparency relating to the created features, they can be expressed in SQL code. Even though automatically generated features will always be less intuitive than hand-crafted ones and could be quite complex, we want the user to get an understanding of what is going on. </p>"},{"location":"user_guide/feature_engineering/feature_engineering/#algorithms","title":"Algorithms","text":"<p>getML contains four powerful feature learning algorithms: <code>FastProp</code>, <code>Multirel</code>, <code>Relboost</code> and <code>RelMT</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#fastprop","title":"FastProp","text":"<p><code>FastProp</code> is getML's take on propositionalization. It is a fast and efficient implementation utilizing aggregations-based operations, which transform a relational data structure to a flat table. FastProp allows for the really fast generation of a substantial number of features based on simple (unconditional) aggregations.</p> <p>A typical FastProp feature looks like this:</p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT MAX( t2.column ) AS feature_1,\n      t1.rowid AS \"rownum\"\nFROM \"population\" t1\nLEFT JOIN \"peripheral\" t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nORDER BY t2.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> You may notice that such a feature looks pretty similar to the Multirel feature below. And indeed, FastProp shares some of its aggregations with Multirel. FastProp features, however, are usually much simpler because they lack the complex conditions learned by getML's other algorithms (the <code>WHERE</code> statement in the SQL representation). FastProp is an excellent choice in an exploration phase of a data science project and delivers decent results out of the box in many cases. It is recommended that you combine FastProp with mappings.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#multirel","title":"Multirel","text":"<p>Simply speaking, <code>Multirel</code> is a more efficient variation of Multi-relational Decision Tree Learning (MRDTL). The core idea is to minimize redundancies in the original algorithm by incremental updates. We then combined our improved version of MRDTL with ensemble learning methods.</p> <p>MRDTL is a strain of academic literature that was particularly popular in the early 2000s. It is based on a greedy, tree-like approach:</p> <ul> <li>Define some sort of objective function that evaluates the quality of your feature as it relates to the target variable(s).</li> <li>Pick an aggregation and some column to be aggregated.</li> <li>Try out different conditions. Keep the one that generates the greatest improvement of your objective. Repeat until no improvement can be found or some sort of stopping criterion is reached.</li> </ul> <p>The reason this approach has never really taken off outside of academia is that an efficient implementation is far from trivial. Most papers on MRDTL implement the algorithm on top of an existing relational database system, like MySQL.</p> <p>The main problem with trying to implement something like this on top of an existing database is that it requires many redundant operations. Consider a feature like:</p> <p><pre><code>COUNT the number of `transactions` in the last X `days`\n</code></pre> As we iterate through different values for the threshold \\(X\\), we are forced to repeat the same operations on the same data over and over again. Tasks like this bring traditional database engines to their knees.</p> <p>The core idea of getML's Multirel algorithm is to minimize redundancies through <code>incremental updates</code>. To allow for incremental updates and maximal efficiency, we developed a database engine from scratch in C++. When we evaluate a feature like:</p> <pre><code>COUNT the number of `transactions` in the last 90 `days`\n</code></pre> <p>and</p> <pre><code>COUNT the number of `transactions` in the last 91 `days`\n</code></pre> <p>very little changes in between. Multirel only recalculates what has changed and keeps everything else untouched. Therefore, it needs two ingredients that can be incrementally updated: An objective function and the actual aggregation(s).</p> <p>Our first ingredient is an objective function that must be suited for incremental updates. When we move from 90 to 91 days, presumably only very few lines in the population table actually change. We do not need to recalculate the entire table. In practice, most commonly used objective functions are fine and this is not much of a limitation. However, there are some, like rank correlation, that cannot be used.</p> <p>The second ingredient, the aggregations, must allow for incremental updates too. This part is a bit harder, so let us elaborate: Let\u2019s say we have a match between the population table that contains our targets and another table (or a self-join). This match happens to be between the two thresholds 90 and 91 days. As we move from 90 to 91 days, we have to update our aggregation for that match. For maximum efficiency, this needs also to be done incrementally. That means we do not want to recalculate the entire aggregation for all matches that it aggregates - instead just for the one match in between the two thresholds.</p> <p>We want to also support the <code>AND</code> and <code>OR</code> combinations of conditions. Therefore, it is possible that a match was not included in the aggregation before, but becomes part of it as we move the threshold. It is also possible that the match was included in the aggregation, but now it isn\u2019t anymore.</p> <p>For an aggregation like <code>Count</code>, incremental updates are straight-forward. If the match was not included, but now it is, then increment by 1. If was included, but it isn\u2019t anymore, then decrement by 1.</p> <p>Things are more tricky for aggregations like <code>Max</code>, <code>Median</code>, or <code>CountDistinct</code>. For instance, whereas incrementing <code>Max</code> is easy, decrementing it is hard. If the match used to be included and is in fact the maximum value, we now have to find the next biggest match. And we have to find it quickly - ideally iterating through a set of thresholds should take linear time in the number of matches. To make it even more complicated, some cross-joins might result in a lot of matches, so any data structures that have non-trivial memory overhead are a no-go.</p> <p>Everything so far has shed light on how we train one feature. But in practice, we want more than one. So, how do we do that? Since we are using a tree-based algorithm anyway, we are able to harness the power of ensemble learning algorithms that have been shown to work very well with non-relational decision trees, namely bagging and gradient boosting.</p> <p>With bagging, we just sample randomly from our population table. We train a feature on that sample and then pick a different random sample to train the next feature.</p> <p>With gradient boosting, we calculate the pseudo-residuals of our previously trained features. We then train features that predict these pseudo-residuals. This procedure guarantees that new features are targeted and compensate the weaknesses of older ones.</p> <p>Transpiled to SQL, a typical feature generated by Multirel looks like this:</p> <p></p> <pre><code>CREATE TABLE FEATURE_1 AS\nSELECT COUNT( * ) AS feature_1,\n       t1.join_key,\n       t1.time_stamp\nFROM (\n     SELECT * ,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> <p>Further information can be found in the API documentation for <code>Multirel</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#relboost","title":"Relboost","text":"<p><code>Relboost</code> is a generalization of the gradient boosting algorithm. More specifically, it generalizes the xgboost implementation to relational learning.</p> <p>The main difference between Relboost and Multirel is that Multirel aggregates columns, whereas Relboost aggregates learnable weights.</p> <p>Relboost addresses a problem with Multirel that is related to computational complexity theory: In Multirel, every column can be aggregated and/or used for generating a condition. That means that the number of possible features is \\(\\mathcal{O}(n^2)\\) in the number of columns in the original tables. As a result, having twice as many columns will lead to a search space that is four times as large (in reality, it is a bit more complicated than that, but the basic point is true).</p> <p>Any computer scientist or applied mathematician will tell you that \\(\\mathcal{O}(n^2)\\) is a problem. If you have tables with many columns, it might turn out to be a problem. Of course, this issue is not specific to Multirel: It is a very fundamental problem that you would also have, if you were to write your features by hand or use brute force.</p> <p>Relboost offers a way out of this dilemma: Because Relboost aggregates learnable weights and columns will only be used for conditions, but not for aggregation. So, now the search space is \\(\\mathcal{O}(n)\\) in the number of columns in the original tables - much better.</p> <p>This might seem very theoretical, but it has considerable implications: From our experience with real-world data in various projects, we know that Relboost usually outperforms Multirel in terms of predictive accuracy and training time.</p> <p>However, these advantages come at a price: First, the features generated by Relboost are less intuitive. They are further away from what you might write by hand, even though they can still be expressed as SQL code. Second, it is more difficult to apply Relboost to multiple targets, because Relboost has to learn separate rules and weights for each target.</p> <p>Expressed as SQL code, a typical feature generated by Relboost looks like this:</p> <p></p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n     WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN 0.0\n     WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN 1.0\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> Further information can be found in the API documentation for <code>Relboost</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#relmt","title":"RelMT","text":"<p><code>RelMT</code> is a generalization of linear model trees to relational data. Linear model trees are decision trees with a linear model at each leaf, resulting in a hybrid model that combines the strengths of linear models (like interpretability or the ability to capture linear relationships) with those of tree-based algorithms (like good performance or the ability to capture nonlinear relationships).</p> <p>RelMT features are particularly well-suited for time-series applications because time series often carry autoregressive structures, which can be approximated well by linear models. Think that this month's revenue can usually be modeled particularly well as a (linear) function of last month's revenue and so on. Purely tree-based models often struggle to learn such relationships because they have to fit a piecewise-constant model by predicting the average of all observations associated with each leaf. Thus, it can require a vast amount of splits to approximate a linear relationship.</p> <p>Here is a typical RelMT feature:</p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n    WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * -122.121 + COALESCE( t2.column - 3301.156, 0.0 ) * -0.003 \n    WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * 3.654 + COALESCE( t2.column - 3301.156, 0.0 ) * -1.824 + -8.720\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> RelMT features share some characteristics with Relboost features: Compare the example feature to the Relboost feature above. Both algorithms generate splits based on a combination of conditions (the <code>WHEN</code> part of the <code>CASE WHEN</code> statement above). But while Relboost learns weights for its leaves (the <code>THEN</code> part of the <code>CASE WHEN</code> statement), RelMT learns a linear model, allowing for linear combinations between columns from the population table and columns of a certain peripheral table.</p>"},{"location":"user_guide/getml_suite/engine/","title":"Engine","text":""},{"location":"user_guide/getml_suite/engine/#the-getml-engine","title":"The getML engine","text":"<p>The getML engine is a standalone program written in C++ that does the actual work of feature engineering and prediction.</p> <p></p>"},{"location":"user_guide/getml_suite/engine/#starting-the-engine","title":"Starting the engine","text":"<p>The engine can be started using the dedicated launcher icon or by using the getML command line interface (CLI). For more information, check out the installation instructions for your operating system.</p>"},{"location":"user_guide/getml_suite/engine/#shutting-down-the-engine","title":"Shutting down the engine","text":"<p>There are several ways to shut down the getML engine:</p> <ul> <li>Click the '\u23fb Shutdown' tab in the sidebar of the monitor</li> <li>Press <code>Ctrl-C</code> (if started via the command line)</li> <li>Run the getML command-line interface (CLI) (see installation) using the <code>-stop</code> option</li> </ul>"},{"location":"user_guide/getml_suite/engine/#logging","title":"Logging","text":"<p>The engine keeps a log about what it is currently doing.</p> <p>The easiest way to view the log is to click the '&lt;&gt; Log' tab in the sidebar of the getML monitor. The engine will also output its log to the command line when it is started using the command-line interface.</p>"},{"location":"user_guide/getml_suite/getml_suite/","title":"The getML suite","text":"<p>The getML software consists of three fundamental components:</p> <ul> <li>Engine</li> <li>Monitor</li> <li>Python API</li> </ul> <p>The getML engine is written in C++ and is the heart of the getML suite. It holds all data, is responsible for the feature engineering and the machine learning (ML) part, and does all the heavy lifting.</p> <p>You can control the engine using the getML Python API. The API provides handlers to the objects in the engine and all functionalities necessary to do an end-to-end data science project.</p> <p>To help you explore the various data sets and ML models built during your analysis, we provide the getML monitor. The monitor is written in Go. In addition to visualization, it lets you handle the login and the account management for the getML suite.</p> <p>To get started with the getML, head over to the installation instructions.</p>"},{"location":"user_guide/getml_suite/monitor/","title":"Monitor","text":""},{"location":"user_guide/getml_suite/monitor/#the-getml-monitor","title":"The getML monitor","text":"<p>The getML monitor contains information on the data imported into the engine as well as the trained pipelines and their performance. It is written in Go and compiled into a binary that is separate from the getML engine.</p>"},{"location":"user_guide/getml_suite/monitor/#accessing-the-monitor","title":"Accessing the monitor","text":"<p>The monitor is always started on the same machine as the engine. The engine and the monitor use sockets to communicate. The monitor opens an HTTP port - 1709 by default - for you to access it via your favorite internet browser. Entering the following address into the navigation bar will point your browser to the monitor:</p> <p>http://localhost:1709</p> <p>The HTTP port can only be accessed from within the host the getML suite is running on.</p> <p>The main purpose of the monitor is to help you with your data science project by providing visual feedback.</p> <p>Tip</p> <p>If you experience any issues opening the monitor, try any of these steps:</p> <ul> <li>Manually shutdown the engine and restart it: <code>getml.engine.shutdown()</code> and <code>getml.engine.launch()</code></li> <li>Kill the associated background process in the terminal and restart the engine</li> <li>Close all tabs and windows in which the monitor ran previously and try again</li> </ul>"},{"location":"user_guide/getml_suite/python_api/","title":"Python api","text":""},{"location":"user_guide/getml_suite/python_api/#the-getml-python-api","title":"The getML Python API","text":"<p>The getML Python API is shipped along with the matching version of the getML engine and monitor in the file you can download from getml.com (see Installation).</p> <p>The most important thing you have to keep in mind when working with the Python API is this:</p> <p>\u00a0\u00a0\u00a0\u00a0All classes in the Python API are just handles to objects living in the getML engine.</p> <p>In addition, two basic requirements need to be fulfilled to successfully use the API:</p> <ol> <li>You need a running getML engine (on the same host as your Python session) (see starting the engine)</li> <li>You need to set a project in the getML engine using <code>getml.engine.set_project()</code>.</li> </ol> <p><pre><code>import getml\ngetml.engine.set_project('test')\n</code></pre> This section provides some general information about the API and how it interacts with the engine. For an in-depth read about its individual classes, check out the Python API documentation.</p>"},{"location":"user_guide/getml_suite/python_api/#connecting-to-the-getml-engine","title":"Connecting to the getML engine","text":"<p>The getML Python API automatically connects to the engine with every command you execute. It establishes a socket connection to the engine through a port inferred by the time you connect to a project (or create a new project).</p>"},{"location":"user_guide/getml_suite/python_api/#session-management","title":"Session management","text":"<p>You can set the current project (see Managing projects) using <code>set_project()</code>. If no project matches the supplied name, a new one will be created. To get a list of all available projects in your engine, you can use <code>list_projects()</code> and to remove an entire project, you can use <code>delete_project()</code>. </p>"},{"location":"user_guide/getml_suite/python_api/#lifecycles-and-synchronization-between-engine-and-api","title":"Lifecycles and synchronization between engine and API","text":"<p>The most important objects are the following:</p> <ul> <li>Data frames (<code>DataFrame</code>), which act as a container for all your data.</li> <li>Pipelines (<code>Pipeline</code>), which hold the trained states of the algorithms. </li> </ul>"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-dataframe","title":"Lifecycle of a <code>DataFrame</code>","text":"<p>You can create a <code>DataFrame</code> by calling one of the class methods: <code>from_csv()</code>, <code>from_db()</code>, <code>from_json()</code>, or <code>from_pandas()</code>. These create a data frame object in the getML engine, import the provided data, and return a handler to the object as a <code>DataFrame</code> in the Python API (see Importing data).</p> <p>When you apply any method, like <code>add()</code>, the changes will be automatically reflected in both the engine and Python. Under the hood, the Python API sends a command to create a new column to the getML engine. The moment the engine is done, it informs the Python API and the latter triggers the <code>refresh()</code> method to update the Python handler.</p> <p>Data frames are never saved automatically and never loaded automatically. All unsaved changes to a <code>DataFrame</code> will be lost when restarting the engine. To save a <code>DataFrame</code>, use <code>save()</code>. You can also use batch operations like <code>save_all()</code> and <code>load_all()</code> to save or load all data frames associated with the current project. The <code>DataFrames</code> container for the current project in memory can be accessed through <code>getml.project.data_frames</code>.</p> <p>To access the container, holding all of a project's data frames:</p> <p><pre><code>getml.project.data_frames\n</code></pre> To save all data frames in memory to the project folder:</p> <pre><code>getml.project.data_frames.save_all()\n</code></pre> <p>You can subset the container to access single <code>DataFrame</code> instances. You can then call all available methods on those instances. For example, to store a single data frame to disk: <pre><code>getml.project.data_frames[0].save()\n</code></pre> If a <code>DataFrame</code> called NAME_OF_THE_DATA_FRAME is already available in memory, <code>load_data_frame()</code> will return a handle to that data frame. If no such <code>DataFrame</code> is held in memory, the function will try to load the data frame from disk and then return a handle. If that is unsuccessful, an exception is thrown.</p> <p>If you want to force the API to load the version stored on disk over the one held in memory, you can use the <code>load()</code> method as follows:</p> <pre><code>df = getml.data.DataFrame(NAME_OF_THE_DATA_FRAME).load()\n</code></pre>"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-pipeline","title":"Lifecycle of a <code>Pipeline</code>","text":"<p>The lifecycle of a <code>Pipeline</code> is straightforward since the getML engine automatically saves all changes made to a pipeline and automatically loads all pipelines contained in a project.</p> <p>Using the constructors, the individual pipelines are created within the Python API, where they are represented as a set of hyperparameters. The actual weights of the machine learning algorithms are only stored in the getML engine and never transferred to the Python API.</p> <p>When applying any method, like <code>fit()</code>, the changes will be automatically reflected in both the engine and the Python API.</p> <p>When using <code>set_project()</code> to load an existing project, all pipelines contained in that project will be automatically loaded into memory. You can get an overview of all pipelines associated with the current project by accessing the <code>Pipelines</code> container, accessible through <code>getml.project.pipelines</code>.</p> <p>In order to create a corresponding handle in the Python API, you can use <code>load()</code>: <pre><code>pipe = getml.pipeline.load(NAME_OF_THE_PIPELINE)\n</code></pre> The function <code>list_pipelines()</code> lists all available pipelines within a project.</p>"},{"location":"user_guide/hyperopt/hyperopt/","title":"Hyperopt","text":""},{"location":"user_guide/hyperopt/hyperopt/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>In the sections on feature engineering and predicting, we learned how to train both the feature learning algorithm and the machine learning algorithm used for prediction in the getML engine. However, there are lots of parameters involved. <code>Multirel</code>, <code>Relboost</code>, <code>RelMT</code>, <code>FastProp</code>, <code>LinearRegression</code>, <code>LogisticRegression</code>, <code>XGBoostClassifier</code>, and <code>XGBoostRegressor</code> all have their own settings. That is why you might want to use hyperparameter optimization.</p> <p>The most relevant parameters of these classes can be chosen to constitute individual dimensions of a parameter space. For each parameter, a lower and upper bound has to be provided and the hyperparameter optimization will search the space within these bounds. This will be done iteratively by drawing a specific parameter combination, overwriting the corresponding parameters in a base pipeline, and then fitting and scoring it. The algorithm used to draw from the parameter space is represented by the different classes of <code>hyperopt</code>.</p> <p>While <code>RandomSearch</code> and <code>LatinHypercubeSearch</code> are purely statistical approaches, <code>GaussianHyperparameterSearch</code> uses prior knowledge obtained from evaluations of previous parameter combinations to select the next one.</p>"},{"location":"user_guide/hyperopt/hyperopt/#tuning-routines","title":"Tuning routines","text":"<p>The easiest way to conduct a hyperparameter optimization in getML are the tuning routines <code>tune_feature_learners()</code> and <code>tune_predictors()</code>. They roughly work as follows:</p> <ul> <li> <p>They begin with a base pipeline, in which the default parameters for the feature learner or the predictor are used.</p> </li> <li> <p>They then proceed by optimizing 2 or 3 parameters at a time using a <code>GaussianHyperparameterSearch</code>. If the best pipeline outperforms the base pipeline, the best pipeline becomes the new base pipeline.</p> </li> <li> <p>Taking the base pipeline from the previous steps, the tuning routine then optimizes the next 2 or 3 hyperparameters. If the best pipeline from that hyperparameter optimization outperforms the current base pipeline, that pipeline becomes the new base pipeline.</p> </li> <li> <p>These steps are then repeated for more hyperparameters.</p> </li> </ul> <p>The following tables list the tuning recipes and hyperparameter subspaces for each step:</p>"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-predictors","title":"Tuning recipes for predictors","text":"Predictor Stage Hyperparameter Subspace <code>LinearRegression</code>; <code>LogisticRegression</code> 1 (base parameters) reg_lambda [1E-11, 100] learning_rate [0.5, 0.99] <code>XGBoostClassifier</code>; <code>XGBoostRegressor</code> 1 (base parameters) learning_rate [0.05, 0.3] 2 (tree parameters) max_depth [1, 15] min_child_weights [1, 6] gamma [0, 5] 3 (sampling parameters) colsample_bytree [0.75, 0.9] subsample [0.75, 0.9] 4 (regularization parameters) reg_alpha [0, 5] reg_lambda [0, 10]"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-feature-learners","title":"Tuning recipes for feature learners","text":"Feature Learner Stage Hyperparameter Subspace <code>FastProp</code> 1 (base parameters) num_features [50, 500] n_most_frequent [0, 20] <code>Multirel</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>Relboost</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>RelMT</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_depth [1, 8] min_num_samples [1, 500] 3 (regularization parameters) reg_lambda [0, 0.0001] <p>The advantage of the tuning routines is that they provide a convenient out-of-the-box experience for hyperparameter tuning. For most use cases, it is sufficient to tune the XGBoost predictor.</p> <p>More advanced users can rely on the more low-level hyperparameter optimization routines.</p>"},{"location":"user_guide/hyperopt/hyperopt/#random-search","title":"Random search","text":"<p>A <code>RandomSearch</code> draws random hyperparameter sets from the hyperparameter space.</p>"},{"location":"user_guide/hyperopt/hyperopt/#latin-hypercube-search","title":"Latin hypercube search","text":"<p>A <code>LatinHypercubeSearch</code> draws almost random hyperparameter sets from the hyperparameter space, but ensures that they are sufficiently different from each other.</p>"},{"location":"user_guide/hyperopt/hyperopt/#gaussian-hyperparameter-search","title":"Gaussian hyperparameter search","text":"<p>A <code>GaussianHyperparameterSearch</code> works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90% of all iterations. During that burn-in phase, the hyperparameter space is sampled more or less at random, using either a random search or a latin hypercube search. You can control this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a Gaussian process on the hyperparameters with the score we want to maximize or minimize as the predicted variable. Note that the Gaussian process has hyperparameters itself, which are also optimized. You can control this phase using <code>gaussian_kernel</code>, <code>gaussian_optimization_algorithm</code>, <code>gaussian_optimization_burn_in_algorithm</code>, and <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information (EI). The EI is a measure of how much additional information it might get from evaluating a particular point in the hyperparameter space. The expected information is to be maximized. The point in the hyperparameter space with the maximum expected information is the next point that is actually evaluated (meaning a new pipeline with these hyperparameters is trained). You can control this phase using <code>optimization_algorithm</code>, <code>optimization_burn_ins</code>, and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space, it starts evaluating hyperparameter combinations that are particularly interesting.</p> </li> </ul>"},{"location":"user_guide/importing_data/csv_interface/","title":"Csv interface","text":""},{"location":"user_guide/importing_data/csv_interface/#csv-interface","title":"CSV interface","text":"<p>The fastest way to import data into the getML engine is to read it directly from CSV files.</p>"},{"location":"user_guide/importing_data/csv_interface/#import-from-csv","title":"Import from CSV","text":"<p>Using the <code>from_csv()</code> class method, you can create a new <code>DataFrame</code> based on a table stored in the provided file(s). The <code>read_csv()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/csv_interface/#export-to-csv","title":"Export to CSV","text":"<p>In addition to reading data from a CSV file, you can also write an existing <code>DataFrame</code> back into one using <code>to_csv()</code>.</p>"},{"location":"user_guide/importing_data/greenplum_interface/","title":"Greenplum interface","text":""},{"location":"user_guide/importing_data/greenplum_interface/#greenplum-interface","title":"Greenplum interface","text":"<p>Greenplum is an open source database system maintained by Pivotal Software, Inc. It can be connected to the getML engine using the function <code>connect_greenplum()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.</p>"},{"location":"user_guide/importing_data/greenplum_interface/#import-from-greenplum","title":"Import from Greenplum","text":"<p>By selecting an existing table of your database in the <code>from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/greenplum_interface/#export-to-greenplum","title":"Export to Greenplum","text":"<p>You can also write your results back into the Greenplum database. By providing a name for the destination table in <code>Pipeline.transform()</code>, the features generated from your raw data will be written back. Passing it into <code>Pipeline.predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/importing_data/","title":"Importing data","text":""},{"location":"user_guide/importing_data/importing_data/#importing-data_1","title":"Importing data","text":"<p>Before being able to analyze and process your data using the getML software, you have to import it into the engine. At the end of this step, you will have your data in data frame objects in the getML engine and will be ready to annotate them.</p> <p>Note</p> <p>If you have imported your data into the engine before and want to restore it, refer to Lifecycle of DataFrame</p> <p></p>"},{"location":"user_guide/importing_data/importing_data/#unified-import-interface","title":"Unified import interface","text":"<p>The getML Python API provides a unified import interface requiring similar arguments and resulting in the same output format, regardless of the data source.</p> <p>You can use one of the dedicated <code>from_csv()</code>, <code>from_pandas()</code>, <code>from_db()</code>, and <code>from_json()</code> class methods to construct a data frame object in the getML engine, fill it with the provided data, and retrieve a <code>DataFrame</code> handle in the Python API. </p> <p>If you already have a data frame object in place, you can use the <code>read_csv()</code>, <code>read_pandas()</code>, <code>read_db()</code>, or <code>read_json()</code> methods of the corresponding <code>DataFrame</code> handle to either replace its content with new data or append to it.</p> <p>All those functions also have their counterparts for exporting called <code>to_csv()</code>, <code>to_pandas()</code>, <code>to_db()</code>, and <code>to_json()</code>.</p> <p>The particularities of the individual formats will be covered in the following sections:</p> <ul> <li>CSV interface</li> <li>Pandas interface</li> <li>JSON interface</li> <li>SQLite3 interface</li> <li>MySQL interface</li> <li>MariaDB interface</li> <li>PostgreSQL interface</li> <li>Greenplum interface</li> <li>ODBC interface</li> </ul>"},{"location":"user_guide/importing_data/importing_data/#data-frames","title":"Data Frames","text":"<p>The resulting <code>DataFrame</code> instance in the Python API represents a handle to the corresponding data frame object in the getML engine. The mapping between the two is done based on the name of the object, which has to be unique. Similarly, the names of  the <code>columns</code> are required to be unique within the data frame they are associated with.</p>"},{"location":"user_guide/importing_data/importing_data/#handling-of-null-values","title":"Handling of NULL values","text":"<p>Unfortunately, data sources often  contain missing or corrupt data - also called NULL values. getML is able to work with missing values except for the target variable, which must not contain any NULL values (because having NULL targets does not make any sense). Please refer to the section on  join keys for details about their handling during the construction of the data model.</p> <p>During import, a NULL value is automatically inserted at all occurrences of the strings \"nan\", \"None\", \"NA\", or an empty string as well as at all occurrences of <code>None</code> and <code>NaN</code>.</p>"},{"location":"user_guide/importing_data/json_interface/","title":"Json interface","text":""},{"location":"user_guide/importing_data/json_interface/#json-interface","title":"JSON interface","text":"<p>The another convenient but slow way to import data into the getML engine via its Python API.</p>"},{"location":"user_guide/importing_data/json_interface/#import-from-json","title":"Import from JSON","text":"<p>Using the <code>from_json()</code> class method, you can create a new <code>DataFrame</code> based on a JSON string. The <code>read_json()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/json_interface/#export-to-json","title":"Export to JSON","text":"<p>In addition to reading data from a JSON string, you can also convert an existing <code>DataFrame</code> into one using <code>to_json()</code>.</p>"},{"location":"user_guide/importing_data/mariadb_interface/","title":"Mariadb interface","text":""},{"location":"user_guide/importing_data/mariadb_interface/#mariadb-interface","title":"MariaDB interface","text":"<p>MariaDB is a popular open source fork of MySQL. It can be connected to the getML engine using the function <code>connect_mariadb()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your username and password ready, and you can reach it from your command line.</p> <p>If you are unsure which port or socket your MariaDB is running on, you can start the <code>mysql</code> command line interface </p> <p><pre><code>$ mysql\n</code></pre> and use the following queries to get the required insights.</p> <pre><code>MariaDB [(none)]&gt; SELECT @@port;\n\nMariaDB [(none)]&gt; SELECT @@socket;\n</code></pre>"},{"location":"user_guide/importing_data/mariadb_interface/#import-from-mariadb","title":"Import from MariaDB","text":"<p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/mariadb_interface/#export-to-mariadb","title":"Export to MariaDB","text":"<p>You can also write your results back into the MariaDB database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/mysql_interface/","title":"Mysql interface","text":""},{"location":"user_guide/importing_data/mysql_interface/#mysql-interface","title":"MySQL interface","text":"<p>MySQL is one of the most popular databases in use today. It can be connected to the getML engine using the function <code>connect_mysql()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.</p> <p>If you are unsure which port or socket your MySQL is running on, you can start the <code>mysql</code> command line interface <pre><code>$ mysql\n</code></pre> and use the following queries to get the required insights.</p> <pre><code>&gt; SELECT @@port;\n\n&gt; SELECT @@socket;\n</code></pre>"},{"location":"user_guide/importing_data/mysql_interface/#import-from-mysql","title":"Import from MySQL","text":"<p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/mysql_interface/#export-to-mysql","title":"Export to MySQL","text":"<p>You can also write your results back into the MySQL database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/odbc_interface/","title":"Odbc interface","text":""},{"location":"user_guide/importing_data/odbc_interface/#odbc-interface","title":"ODBC interface","text":"<p>ODBC (Open Database Connectivity) is an API specification for connecting software programming language to a database, developed by Microsoft.</p> <p>In a nutshell, it works like this:</p> <ul> <li>Any database of relevance has an ODBC driver that translates calls from the ODBC API into a format the database can understand, returning the query results in a format understood by the ODBC API.</li> <li>To connect getML or other software to a database using ODBC, you first need to install the ODBC driver provided by your database vendor.</li> <li>In theory, ODBC drivers should translate queries from the SQL 99 standard into the SQL dialect, but this is often ignored in practice. Also, not all ODBC drivers support all ODBC calls.</li> </ul> <p>At getML, native APIs are preferred for connecting to relational databases. ODBC is used when native APIs are not feasible due to licensing or other restrictions, especially for connecting to proprietary databases like Oracle, Microsoft SQL Server, or IBM DB2.</p> <p>ODBC is pre-installed on modern Windows operating systems, while Linux and macOS can use open-source implementations like unixODBC and iODBC, with getML using unixODBC.</p>"},{"location":"user_guide/importing_data/odbc_interface/#an-example-microsoft-sql-server","title":"An example: Microsoft SQL Server","text":"<p>To connect to Microsoft SQL Server using ODBC:</p> <ol> <li>If you do not have a Microsoft SQL Server instance, you can download a trial or development version.</li> <li>Download the ODBC driver for SQL Server.</li> <li>Configure the ODBC driver. Many drivers provide customized scripts for this, so manual configuration might not be necessary.</li> </ol> <p>For Linux and macOS, create a <code>.odbc.ini</code> file in your home directory with the following contents:</p> <p><pre><code>[ANY-NAME-YOU-WANT]\nDriver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.5.so.2.1\nServer = 123.45.67.890\nPort = 1433\nUser = YOUR-USERNAME\nPassword = YOUR-PASSWORD\nDatabase = YOUR-DATABASE\nLanguage = us_english\nNeedODBCTypesOnly = 1\n</code></pre> On Docker, you can make appropriate changes to the Dockerfile and then rerun <code>./setup.sh</code> or <code>bash setup.sh</code>.</p> <p>You will need to set the following parameters:</p> <ul> <li>The first line is the server name or data source name. You can use this name to tell getML that this is the server you want to connect to.</li> <li>The Driver is the location of the ODBC driver you have just downloaded. The location or file name might be different on your system.</li> <li>The Server is the IP address of the server. If the server is on the same machine as getML, just write \"localhost\".</li> <li>The Port is the port on which to connect the server. The default port for SQL Server is 1433.</li> <li>User and Password are the user name and password that allow access to the server.</li> <li>The Database is the database inside the server you want to connect to.</li> </ul> <p>You can now connect getML to the database:</p> <pre><code>getml.database.connect_odbc(\n    server_name=\"ANY-NAME-YOU-WANT\",\n    user=\"YOUR-USERNAME\",\n    password=\"YOUR-PASSWORD\",\n    escape_chars=\"[]\")\n</code></pre>"},{"location":"user_guide/importing_data/odbc_interface/#important-always-pass-escape_chars","title":"Important: Always pass escape_chars","text":"<p>Earlier we mentioned that ODBC drivers are supposed to translate standard SQL queries into the specific SQL dialects, but this requirement is often ignored.</p> <p>A typical issue is escape characters, needed when the names of your schemas, tables, or columns are SQL keywords, like the loans dataset containing a table named ORDER.</p> <p>To avoid this problem, you can envelop the schema, table, and column names in escape characters.  <pre><code>SELECT \"some_column\" FROM \"SOME_SCHEMA\".\"SOME_TABLE\";\n</code></pre> getML always uses escape characters for its automatically generated queries.</p> <p>The SQL standard requires that the quotation mark (\") be used as the escape character. However, many SQL dialects do not follow this requirement, e.g., SQL Server uses \"[]\":</p> <p><pre><code>SELECT [some_column] FROM [SOME_SCHEMA].[SOME_TABLE];\n</code></pre> MySQL and MariaDB work like this: <pre><code>SELECT `some_column` FROM `SOME_SCHEMA`.`SOME_TABLE`;\n</code></pre> To avoid frustration, determine your server's escape characters and explicitly pass them to <code>connect_odbc()</code>.</p>"},{"location":"user_guide/importing_data/odbc_interface/#import-data-using-odbc","title":"Import data using ODBC","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/odbc_interface/#export-data-using-odbc","title":"Export data using ODBC","text":"<p>You can also write your results back into the PostgreSQL database. When you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/pandas_interface/","title":"Pandas interface","text":""},{"location":"user_guide/importing_data/pandas_interface/#pandas-interface","title":"Pandas interface","text":"<p>Pandas is one of the key packages used in most data science projects done in Python. The associated import interface is one of the slowest, but you can harness the good data exploration and manipulation capabilities of this Python package.</p>"},{"location":"user_guide/importing_data/pandas_interface/#import-from-pandas","title":"Import from Pandas","text":"<p>Using the <code>DataFrame.from_pandas()</code> class method, you can create a new <code>DataFrame</code> based on the provided <code>pandas.DataFrame</code>. The <code>read_pandas()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/pandas_interface/#export-to-pandas","title":"Export to Pandas","text":"<p>In addition to reading data from a <code>pandas.DataFrame</code>, you can also write an existing <code>DataFrame</code> back into a <code>pandas.DataFrame</code> using <code>DataFrame.to_pandas()</code>. Due to the way data is stored within the getML engine, the dtypes of the original <code>pandas.DataFrame</code> cannot be restored properly and there might be inconsistencies in the order of microseconds being introduced into timestamps.</p>"},{"location":"user_guide/importing_data/postgres_interface/","title":"Postgres interface","text":""},{"location":"user_guide/importing_data/postgres_interface/#postgresql-interface","title":"PostgreSQL interface","text":"<p>PostgreSQL is a powerful and well-established open source database system. It can be connected to the getML engine using the function <code>connect_postgres()</code>. Make sure your database is running, you have the corresponding hostname, port, user name, and password ready, and you can reach it from your command line.</p>"},{"location":"user_guide/importing_data/postgres_interface/#import-from-postgresql","title":"Import from PostgreSQL","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/postgres_interface/#export-to-postgresql","title":"Export to PostgreSQL","text":"<p>You can also write your results back into the PostgreSQL database. If you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/","title":"Sqlite3 interface","text":""},{"location":"user_guide/importing_data/sqlite3_interface/#sqlite3-interface","title":"SQLite3 interface","text":"<p>SQLite3 is a popular in-memory database. It is faster than classical relational databases like PostgreSQL but less stable under massive parallel access. It requires all contained datasets to be loaded into memory, which might use up too much RAM, especially for large datasets.</p> <p>As with all other databases in the unified import interface of the getML Python API, you first need to connect to it using <code>connect_sqlite3()</code>.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/#import-from-sqlite3","title":"Import from SQLite3","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/#export-to-sqlite3","title":"Export to SQLite3","text":"<p>You can also write your results back into the SQLite3 database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/predicting/predicting/","title":"Predicting","text":""},{"location":"user_guide/predicting/predicting/#predicting_1","title":"Predicting","text":"<p>Now that you know how to engineer a flat table of features, you are ready to make predictions of the target variable(s).</p>"},{"location":"user_guide/predicting/predicting/#using-getml","title":"Using getML","text":"<p>getML comes with four built-in machine learning predictors:</p> <ul> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> </ul> <p>Using one of them in your analysis is very simple. Just pass one as the <code>predictor</code> argument to either <code>Pipeline</code> on initialization. As a list, more than one predictor can be passed to the pipeline.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor,\n)\n</code></pre> <p>When you call <code>fit()</code> on a pipeline, the entire pipeline will be trained.</p> <p>Note</p> <p>The time estimation for training a pipeline is a rough estimate. Occasionally, the training time can be significantly longer than the estimate. But the pipeline never silently crashes. Given enough time, computations always finish.</p> <p>Note that <code>Pipeline</code> comes with dependency tracking. That means it can figure out on its own what has changed and what needs to be trained again.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    population=population_placeholder,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor \n)\n\npipe.fit(...)\n\npipe.predictors[0].n_estimators = 50\n\n# Only the predictor has changed,\n# so only the predictor will be refitted.\npipe.fit(...)\n</code></pre> <p>To score the performance of your prediction on a test data set, the getML models come with a <code>score()</code> method. The available metrics are documented in <code>scores</code>.</p> <p>To use a trained model, including both the trained features and the predictor, to make predictions on new, unseen data, call the <code>predict()</code> method of your model.</p>"},{"location":"user_guide/predicting/predicting/#using-external-software","title":"Using external software","text":"<p>In our experience, the most relevant contribution to making accurate predictions are the generated features. Before trying to tweak your analysis by using sophisticated prediction algorithms and tuning their hyperparameters, we recommend tuning the hyperparameters of your <code>Multirel</code> or <code>Relboost</code> instead. You can do so either by hand or using getML's automated hyperparameter optimization.</p> <p>If you wish to use external predictors, you can transform new data, which is compliant with your relational data model, to a flat feature table using the <code>transform()</code> method of your pipeline.</p>"},{"location":"user_guide/preprocessing/preprocessing/","title":"Preprocessing","text":""},{"location":"user_guide/preprocessing/preprocessing/#preprocessing_1","title":"Preprocessing","text":"<p>As preprocessing, we categorize operations on data frames that are not directly related to the relational data model. While feature learning and propositionalization deal with relational data structures and result in a single-table representation thereof, we categorize all operations that work on single tables as preprocessing. This includes numerical transformations, encoding techniques, or alternative representations.</p> <p>getML's preprocessors allow you to extract domains from email addresses (<code>EmailDomain</code>), impute missing values (<code>Imputation</code>), map categorical columns to a continuous representation (<code>Mapping</code>), extract seasonal components from time stamps (<code>Seasonal</code>), extract sub strings from string-based columns (<code>Substring</code>) and split up <code>text</code> columns (<code>TextFieldSplitter</code>). Preprocessing operations in getML are very efficient and happen really fast. In fact, most of the time you won't even notice the presence of a preprocessor in your pipeline. getML's preprocessors operate on an abstract level without polluting your original data, are evaluated lazily and their set-up requires minimal effort.</p> <p>Here is a small example that shows the <code>Seasonal</code> preprocessor in action. <pre><code>import getml\n\ngetml.project.switch(\"seasonal\")\n\ntraffic = getml.datasets.load_interstate94()\n\n# traffic explicitly holds seasonal components (hour, day, month, ...)\n# extracted from column ds; we copy traffic and delete all those components\ntraffic2 = traffic.drop([\"hour\", \"weekday\", \"day\", \"month\", \"year\"])\n\nstart_test = getml.data.time.datetime(2018, 3, 14)\n\nsplit = getml.data.split.time(\n    population=traffic,\n    test=start_test,\n    time_stamp=\"ds\",\n)\n\ntime_series1 = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\ntime_series2 = getml.data.TimeSeries(\n    population=traffic2,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_function.SquareLoss)\n\npipe1 = getml.pipeline.Pipeline(\n    data_model=time_series1.data_model,\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\npipe2 = getml.pipeline.Pipeline(\n    data_model=time_series2.data_model,\n    preprocessors=[getml.preprocessors.Seasonal()],\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\n# pipe1 includes no preprocessor but receives the data frame with the components\npipe1.fit(time_series1.train)\n\n# pipe2 includes the preprocessor; receives data w/o components\npipe2.fit(time_series2.train)\n\nmonth_based1 = pipe1.features.filter(lambda feat: \"month\" in feat.sql)\nmonth_based2 = pipe2.features.filter(\n    lambda feat: \"COUNT( DISTINCT t2.\\\"strftime('%m'\" in feat.sql\n)\n\nprint(month_based1[1].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_10\";\n# \n# CREATE TABLE \"FEATURE_1_10\" AS\n# SELECT COUNT( t2.\"month\"  ) - COUNT( DISTINCT t2.\"month\" ) AS \"feature_1_10\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n\nprint(month_based2[0].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_5\";\n# \n# CREATE TABLE \"FEATURE_1_5\" AS\n# SELECT COUNT( t2.\"strftime('%m', ds )\"  ) - COUNT( DISTINCT t2.\"strftime('%m', ds )\" ) AS \"feature_1_5\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n</code></pre></p> <p>If you compare both of the features above, you will notice they are exactly the same: <code>COUNT - COUNT(DISTINCT)</code> on the month component conditional on the time-based restrictions introduced through memory and horizon.</p> <p>Pipelines can include more than one preprocessor.</p> <p>While most of getML's preprocessors are straightforward, two of them deserve a more detailed introduction: <code>Mapping</code> and <code>TextFieldSplitter</code>. </p>"},{"location":"user_guide/preprocessing/preprocessing/#mappings","title":"Mappings","text":"<p><code>Mapping</code> s are an alternative representation for categorical columns, text columns, and (quasi-categorical) discrete-numerical columns. Each discrete value (category) of a categorical column is mapped to a continuous spectrum by calculating the average target value for the subset of all rows belonging to the respective category. For columns from peripheral tables, the average target values are propagated back by traversing the relational structure.</p> <p>Mappings are a simple and interpretable alternative representation for categorical data. By introducing a continuous representation, mappings allow getML's feature learning algorithms to apply arbitrary aggregations to categorical columns. Further, mappings enable huge gains in efficiency when learning patterns from categorical data. You can control the extent mappings are utilized by specifying the minimum number of matching rows required for categories that constitutes a mapping through the <code>min_freq</code> parameter.</p> <p>Here is an example mapping from the CORA notebook: <pre><code> DROP TABLE IF EXISTS \"CATEGORICAL_MAPPING_1_1_1\";\n CREATE TABLE \"CATEGORICAL_MAPPING_1_1_1\"(key TEXT NOT NULL PRIMARY KEY, value NUMERIC);\n INSERT INTO \"CATEGORICAL_MAPPING_1_1_1\"(key, value)\n VALUES('Case_Based', 0.7109826589595376),\n       ('Rule_Learning', 0.07368421052631578),\n       ('Reinforcement_Learning', 0.0576923076923077),\n       ('Theory', 0.0547945205479452),\n       ('Genetic_Algorithms', 0.03157894736842105),\n       ('Neural_Networks', 0.02088772845953003),\n       ('Probabilistic_Methods', 0.01293103448275862);\n</code></pre> Inspecting the actual values, it's highly likely, that this mapping stems from a feature learned by a sub learner targeting the label \"Case_Based\". In addition to the trivial case, we can see that the next closed neighboring category is the \"Rule_Learning\" category, to which 7.3 % of the papers citing the target papers are categorized. </p>"},{"location":"user_guide/preprocessing/preprocessing/#handling-of-free-form-text","title":"Handling of free form text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. Learning from <code>text</code> columns works as follows: First, for each of the <code>text</code> columns, a vocabulary is built by taking into account the feature learner's text mining specific hyperparameter <code>vocab_size</code>. If a text field contains words that belong to the vocabulary, getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either can be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. Opting for the second approach is as easy as adding the <code>TextFieldSplitter</code> to the list of <code>preprocessors</code> on your <code>Pipeline</code>. The resulting bag of words can be viewed as another one-to-many relationship within our data model where each row holding a text field is related to n peripheral rows (n is the number of words in the text field). Consider the following example, where the text field is split into a relational bag of words.</p>"},{"location":"user_guide/preprocessing/preprocessing/#one-row-of-a-table-with-a-text-field","title":"One row of a table with a text field","text":"rownum text field 52 The quick brown fox jumps over the lazy dog"},{"location":"user_guide/preprocessing/preprocessing/#the-implicit-peripheral-table-that-results-from-splitting","title":"The (implicit) peripheral table that results from splitting","text":"rownum words 52 the 52 quick 52 brown 52 fox 52 jumps 52 over 52 the 52 lazy 52 dog <p>As text fields now present another relation, getML's feature learning algorithms are able to learn structural logic from text fields' contents by applying aggregations over the resulting bag of words itself (<code>COUNT WHERE words IN ('quick', 'jumps')</code>). Further, by utilizing mappings, any aggregation applicable to a (mapped) categorical column can be applied to bag-of-words mappings as well.</p> <p>Note that the splitting of text fields can be computationally expensive. If performance suffers too much, you may resort to the default behavior by removing the <code>TextFieldSplitter</code> from your <code>Pipeline</code>.</p>"},{"location":"user_guide/project_management/project_management/","title":"Project management","text":""},{"location":"user_guide/project_management/project_management/#managing-projects","title":"Managing projects","text":"<p>When working with getML, all data is bundled into projects. getML's projects are managed through the <code>getml.project</code> module.</p>"},{"location":"user_guide/project_management/project_management/#the-relationship-between-projects-and-engine-processes","title":"The relationship between projects and engine processes","text":"<p>Each project is tied to a specific instance of the getML engine running as a global process (independent from your python session). In this way, it is possible to share one getML instance with multiple users to work on different projects. When switching projects through <code>getml.project.switch()</code>, the python API spawns a new process and establishes a connection to this process, while the currently loaded project remains in memory and its process is delegated to the background (until you explicitly <code>suspend()</code> the project). You can also work on multiple projects simultaneously from different python sessions. This comes in particularly handy if you use Jupyter Lab to open multiple notebooks and manage multiple python kernels simultaneously.</p> <p>To load an existing project or create a new one, you can do so from the 'Projects' view in the monitor or use the API (<code>getml.engine.set_project()</code>).</p> <p>If you want to shut down the engine process associated with the current project, you can call <code>getml.project.suspend()</code>. When you suspend the project, the memory of the engine is flushed and all unsaved changes to the data frames are lost (see lifecycles and synchronization between engine and API for details). All pipelines of the new project are automatically loaded into memory. You can retrieve all of your project's pipelines through <code>getml.project.pipelines</code>.</p> <p>Projects can be deleted by clicking the trash can icon in the 'Projects' tab of the getML monitor or by calling <code>getml.engine.delete_project()</code> (to delete a project by name) or <code>getml.project.delete()</code> (to suspend and delete the project currently loaded).</p>"},{"location":"user_guide/project_management/project_management/#managing-data-using-projects","title":"Managing data using projects","text":"<p>Every project has its own folder in <code>~/.getML/getml-VERSION/projects</code> (for Linux and macOS) in which all of its data and pipelines are stored. On Windows, the projects folder is in the same location as <code>getML.exe</code>. These folders can be easily shared between different instances of getML; even between different operating systems. However, individual pipelines or data frames cannot be simply copied to another project folder \u2013 they are tied to the project. Projects can be bundled and exported/imported.</p>"},{"location":"user_guide/project_management/project_management/#using-the-project-module-to-manage-your-project","title":"Using the project module to manage your project","text":"<p>The <code>getml.project</code> module is the entry point to your projects. From here, you can: query project-specific data (<code>getml.project.pipelines</code>, <code>getml.project.data_frames</code>, <code>getml.project.hyperopts</code>), manage the state of the current project (<code>getml.project.delete()</code>, <code>getml.project.restart()</code>, <code>getml.project.switch()</code>, <code>getml.project.suspend()</code>), and import projects from or export projects as a <code>.getml</code> bundle to disk (<code>getml.project.load()</code>, <code>getml.project.save()</code>).</p>"}]}