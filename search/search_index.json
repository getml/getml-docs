{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"getML Documentation","text":"<p>Welcome to the getML technical documentation. This document is written for data scientists who want to use the getML software suite for their projects. For general information about getML visit getml.com. For a collection of demo notebooks, visit getml-demo. You can also contact us for any questions or inquiries.</p> <p>Note</p> <p>Some components of getML have been open sourced as part of getML community edition.  You may have a look at  community vs enterprise edition table to see the highlights of both the editions. </p>"},{"location":"#getml-in-one-minute","title":"getML in one minute","text":"<p>getML is an innovative tool for the end-to-end automation of data science projects. It covers everything from convenient data loading procedures  to the deployment of trained models. </p> <p>Most notably, getML includes advanced algorithms for automated feature engineering (feature learning) on relational data and time series. Feature engineering on relational data is defined as the creation of a  flat table by merging and aggregating data. It is sometimes also referred to as data wrangling. Feature engineering is necessary if your data is distributed over more than one data table.</p> <p>Automated feature engineering</p> <ul> <li>Saves up to 90% of the time spent on a data science project</li> <li>Increases the prediction accuracy over manual feature engineering </li> </ul> <p>Andrew Ng, Professor at Stanford University and Co-founder of Google Brain described manual feature engineering as follows:</p> <p>Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.</p> <p>The main purpose of getML is to automate this \"difficult, time-consuming\" process as much as possible.</p> <p>getML comes with a high-performance engine written in C++ and an intuitive Python API. Completing a data science project with getML consists of seven simple steps.</p> <pre><code>import getml\n\ngetml.engine.launch()\ngetml.engine.set_project('one_minute_to_getml')\n</code></pre> <ol> <li>Load the data into the engine</li> </ol> <p><pre><code>population = getml.data.DataFrame.from_csv('data_population.csv',\n            name='population_table')\nperipheral = getml.data.DataFrame.from_csv('data_peripheral.csv',\n            name='peripheral_table')\n</code></pre> 2. Annotate the data</p> <p><pre><code>population.set_role('target', getml.data.role.target)\npopulation.set_role('join_key', getml.data.role.join_key)\n...\n</code></pre> 3. Define the data model</p> <p><pre><code>dm = getml.data.DataModel(population.to_placeholder(\"POPULATION\"))\ndm.add(peripheral.to_placeholder(\"PERIPHERAL\"))\ndm.POPULATION.join(\n   dm.PERIPHERAL,\n   on=\"join_key\",\n)\n</code></pre> 4. Train the feature learning algorithm and the predictor</p> <p><pre><code>pipe = getml.pipeline.Pipeline(\n    data_model=dm,\n    feature_learners=getml.feature_learning.FastProp()\n    predictors=getml.predictors.LinearRegression()\n)\n\npipe.fit(\n    population=population,\n    peripheral=[peripheral]\n)\n</code></pre> 5. Evaluate</p> <p><pre><code>pipe.score(\n    population=population_unseen,\n    peripheral=[peripheral_unseen]\n)\n</code></pre> 6. Predict   </p> <p><pre><code>pipe.predict(\n    population=population_unseen,\n    peripheral=[peripheral_unseen]\n)\n</code></pre> 7. Deploy</p> <p><pre><code># Allow the pipeline to respond to HTTP requests\npipe.deploy(True)\n</code></pre> Check out the rest of this documentation to find out how getML achieves top performance on real-world data science projects with many tables and complex data schemes.</p>"},{"location":"#how-to-use-this-guide","title":"How to use this guide","text":"<p>If you want to get started with getML right away, we recommend to follow the installation instructions and then go through the getting started guide. </p> <p>If you are looking for more detailed information, other sections of this documentation are more suitable. There are three major parts: </p> <p>Tutorials</p> <p>The tutorials section contains examples of how to use getML in    real-world projects. All tutorials are based on public data sets    so that   you can follow along. If you are looking for an intuitive access to   getML, the tutorials section is the right place to go. Also, the   code examples are explicitly intended to be used as a template for   your own projects.  </p> <p>User guide</p> <p>The user guide explains all conceptional details behind getML in   depth. It can serve as a reference guide for experienced users but it's also   suitable for first day users who want to get a deeper understanding   of how getML works. Each chapter in the   user guide represents one step of a typical data science project.</p> <p>API documentation</p> <p>The API documentation covers everything related to the Python   interface to the getML engine. Each module comes with a dedicated   section that contains concrete code examples.</p> <p>You can also check out our other resources</p> <p>getML homepage</p>"},{"location":"home/","title":"Home","text":"<ul> <li>Installation</li> <li>Getting Started</li> <li>Support</li> </ul>"},{"location":"home/how_to_use_this_guide/","title":"How to use this guide","text":"<p>If you want to get started with getML right away, we recommend to follow the installation instructions and then go through the getting started guide. </p> <p>If you are looking for more detailed information, other sections of this documentation are more suitable. There are three major parts: </p>"},{"location":"home/how_to_use_this_guide/#user-guide","title":"User guide","text":"<p>The user guide explains all conceptional details behind getML in depth. It can serve as a reference guide for experienced users but it's also suitable for first day users who want to get a deeper understanding of how getML works. Each chapter in the user guide represents one step of a typical data science project.</p>"},{"location":"home/how_to_use_this_guide/#api-documentation","title":"API documentation","text":"<p>The API documentation covers everything related to the Python interface to the getML engine. Each module comes with a dedicated section that contains concrete code examples.</p> <p>You can also check out our other resources</p> <ul> <li>Tutorials</li> <li>getML homepage</li> <li>Blog articles and case studies</li> </ul>"},{"location":"home/getting_started/getting_started/","title":"Getting Started","text":""},{"location":"home/getting_started/getting_started/#get-started-with-getml","title":"Get started with getML","text":"<p>In this example, you will learn about the basic concepts of getML. You will tackle a simple problem using the Python API in order to gain a technical understanding of the benefits of getML. More specifically, you will learn how to do the following:</p> <ol> <li>Start a new project</li> <li>Define a data model</li> <li>Building a pipeline</li> <li>Working with a pipeline</li> </ol> <p>The guide is applicable to both the enterprise and the community editions of getML. The highlights of the two are mentioned under community vs enterprise edition section below.</p> <p>You have not installed getML on your machine yet? Before you get started, head over  to the installation instructions: for the enterprise edition here or  for the community edition here.</p>"},{"location":"home/getting_started/getting_started/#introduction","title":"Introduction","text":"<p>Automated machine learning (AutoML) has attracted a great deal of attention in recent years. The goal is to simplify the application of traditional machine learning methods to real-world business problems by automating key steps of a data science project, such as feature extraction, model selection, and hyperparameter optimization. With AutoML, data scientists are able to develop and compare dozens of models, gain insights, generate predictions, and solve more business problems in less time.</p> <p>While it is often claimed that AutoML covers the complete workflow of a data science project - from the raw data set to the deployable machine learning models - current solutions have one major drawback: They cannot handle real world business data. This data typically comes in the form relational data. The relevant information is scattered over a multitude of tables that are related via so-called join keys. In order to start an AutoML pipeline, a flat feature table has to be created from the raw relational data by hand. This step is called feature engineering and is a tedious and error-prone process that accounts for up to 90% of the time in a data science project.</p> <p></p> <p>getML adds automated feature engineering on relational data and time series to AutoML. The getML algorithms, Multirel and Relboost, find the right aggregations and subconditions needed to construct meaningful features from the raw relational data. This is done by performing a sophisticated, gradient-boosting-based heuristic. In doing so, getML brings the vision of end-to-end automation of machine learning within reach for the first time. Note that getML also includes automated model deployment via a HTTP endpoint or database connectors. This topic is covered in other material.</p> <p>All functionality of getML is implemented in the so-called getML engine. It is written in C++ to achieve the highest performance and efficiency possible and is responsible for all the heavy lifting. The getML Python API acts as a bridge to communicate with engine. In addition, the getML monitor (available in enterprise edition) provides a Go-based graphical user interface to ease working with getML and significantly accelerate your workflow.</p> <p>In this article, we start with a brief glimpse of different toolsets offered by getML community and enterprise editions. Later on, you will learn the basic steps and commands to tackle your data science projects using the Python API. For illustration purpose we will also touch how an example data set like the one used here would have been dealt with using classical data science tools. In contrast, we will show how the most tedious part of a data science project - merging and aggregating a relation data set - is automated using getML. At the end of this tutorial you are ready to tackle your own use cases with getML or dive deeper into our software using a variety of follow-up material.</p>"},{"location":"home/getting_started/getting_started/#community-vs-enterprise-edition","title":"Community vs enterprise edition","text":"<p>Before you start the tutorial, here are the highlights of the open-source getML community edition and full-featured getML enterprise edition:</p> Community edition Enterprise edition License Elastic Licence v2 Proprietary Platform Linux &amp; Docker Also macOS Preprocessors EmailDomain, Imputation, Seasonal, Substring, TextFieldSplitter Also Mapping Feature learners FastProp Also Multirel, Relboost, RelMT Predictors LinearRegression, LogisticRegression, XGBoost Same Productionization Transpilation to human-readable SQL Also transpilation to SQLite, Spark SQL, SAP HANA, BigQuery, TSQL. Built-in HTTP Endpoints. Hyperparameter optimization Not supported RandomSearch, LatinHypercube, GaussianOptimization, Customized tuning routines Database connectors SQLite, MySQL, MariaDB, PostgreSQL Also Greenplum, ODBC, SAP HANA, BigQuery Other data sources CSV, Parquet, Pandas, Arrow, Pyspark, JSON Also S3 Other functionalities Memory mapping Also web frontend"},{"location":"home/getting_started/getting_started/#starting-a-new-project","title":"Starting a new project","text":"<p>After you\u2019ve successfully installed getML (enterprise or community), you can begin by executing the following in a jupyter-notebook:</p> <p><pre><code>import getml\nprint(f\"getML API version: {getml.__version__}\\n\")\ngetml.engine.launch()\n    Launched the getML engine. The log output will be stored in\n    /home/xxxxx/.getML/logs/xxxxxxxxxxxxxx.log.\n</code></pre> This will import the getML Python API, launch the engine, and the monitor.</p> <p>Alternatively, you can also launch the getML engine and the monitor as follows:</p> <ul> <li>On Mac, execute  <code>getml-cli</code> inside a terminal or double-click the application icon.  </li> <li>On Windows/docker, execute <code>run.sh</code> in Git Bash.  </li> <li>On Linux, execute <code>getML</code> inside a terminal.</li> </ul> <p>Now, inside Python, execute <code>import getml</code> to import the API.</p> <p>The getML Monitor, available in the enterprise edition, is the frontend to the engine. It should open automatically by launching the engine. In case it does not, visit http://localhost:1709/ to open it. From now on, the entire analysis is run from Python. We will cover the getML monitor in a later tutorial, but feel free to check what is going on while following this guide.</p> <p>The entry-point for your project is the <code>getml.project</code> module. From here, you can start projects and control running projects. Further, you have access to all project-specific entities, and you can export a project as a <code>.getml</code> bundle to disk or load a <code>.getml</code> bundle from disk. To see the running projects, you can execute:</p> <pre><code>getml.project\nCannot reach the getML engine. Please make sure you have set a project.\nTo set: `getml.engine.set_project`\nAvailable projects:\n</code></pre> <p>This message tells us that we have no running engine instance because we have not set a project. So, we follow the advice and create a new project. All datasets and models belonging to a project will be stored in <code>~/.getML/projects</code>.</p> <p><pre><code>getml.engine.set_project(\"getting_started\")\nConnected to project 'getting_started'\nhttp://localhost:1709/#/listprojects/getting_started/\n</code></pre> Now, when you check the current projects:</p> <pre><code>getml.project\nCurrent project:\ngetting_started\n</code></pre>"},{"location":"home/getting_started/getting_started/#data-set","title":"Data Set","text":"<p>The data set used in this tutorial consists of 2 tables. The so-called population table represents the entities we want to make a prediction about in the analysis. The peripheral table contains additional information and is related to the population table via a join key. Such a data set could appear, for example, in a customer churn analysis where each row in the population table represents a customer and each row in the peripheral table represents a transaction. It could also be part of a predictive maintenance campaign where each row in the population table corresponds to a particular machine in a production line and each row in the peripheral table to a measurement from a certain sensor.</p> <p>In this guide, however, we do not assume any particular use case. After all, getML is applicable to a wide range of problems from different domains. Use cases from specific fields are covered in other articles.</p> <pre><code>population_table, peripheral_table = getml.datasets.make_numerical(\n     n_rows_population=500,\n     n_rows_peripheral=100000,\n     random_state=1709\n)\n\ngetml.project.data_frames\n    name                        rows     columns   memory usage\n0   numerical_peripheral_1709   100000         3           2.00 MB\n1   numerical_population_1709      500         4           0.01 MB\n\npopulation_table\nName   time_stamp                    join_key   targets   column_01\nRole   time_stamp                    join_key    target   numerical\nUnits   time stamp, comparison only                                 \n    0   1970-01-01 00:00:00.470834           0       101     -0.6295\n    1   1970-01-01 00:00:00.899782           1        88     -0.9622\n    2   1970-01-01 00:00:00.085734           2        17      0.7326\n    3   1970-01-01 00:00:00.365223           3        74     -0.4627\n    4   1970-01-01 00:00:00.442957           4        96     -0.8374\n        ...                                ...       ...     ...    \n  495   1970-01-01 00:00:00.945288         495        93      0.4998\n  496   1970-01-01 00:00:00.518100         496       101     -0.4657\n  497   1970-01-01 00:00:00.312872         497        59      0.9932\n  498   1970-01-01 00:00:00.973845         498        92      0.1197\n  499   1970-01-01 00:00:00.688690         499       101     -0.1274\n\n\n  500 rows x 4 columns\n  memory usage: 0.01 MB\n  name: numerical_population_1709\n  type: getml.data.DataFrame\n  url: http://localhost:1709/#/getdataframe/getting_started/numerical_population_1709/\n</code></pre> <p>The population table contains 4 columns. The column called <code>column_01</code> contains a random numerical value. The next column, <code>targets</code>, is the one we want to predict in the analysis. To this end, we also need to use the information from the peripheral table.</p> <p>The relationship between the population and peripheral table is established using the <code>join_key</code> and <code>time_stamp</code> columns: Join keys are used to connect one or more rows from one table with one or more rows from the other table. Time stamps are used to limit these joins by enforcing causality and thus ensuring that no data from the future is used during the training.</p> <p>In the peripheral table, <code>columns_01</code> also contains a random numerical value. The population table and the peripheral table have a one-to-many relationship via <code>join_key</code>. This means that one row in the population table is associated with many rows in the peripheral table. In order to use the information from the peripheral table, we need to merge the many rows corresponding to one entry in the population table into so-called features. This is done using certain aggregations.</p> <p></p> <p>For example, such an aggregation could be the sum of all values in <code>column_01</code>. We could also apply a subcondition, like taking only values into account that fall into a certain time range with respect to the entry in the population table. In SQL code such a feature would look like this:</p> <pre><code>SELECT COUNT( * )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n  ( t1.time_stamp - t2.time_stamp &lt;= TIME_WINDOW )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n    t1.time_stamp;\n</code></pre> <p>Unfortunately, neither the right aggregation nor the right subconditions are clear a priori. The feature that allows us to predict the target best could very well be e.g.\u00a0the average of all values in <code>column_01</code> that fall below a certain threshold, or something completely different. If you were to tackle this problem with classical machine learning tools, you would have to write many SQL features by hand and find the best ones in a trial-and-error-like fashion. At best, you could apply some domain knowledge that guides you towards the right direction. This approach, however, bears two major disadvantages that prevent you from finding the best-performing features.</p> <ol> <li>You might not have sufficient domain knowledge.</li> <li>You might not have sufficient resources for such a time-consuming, tedious, and error-prone process.</li> </ol> <p>This is where getML comes in. It finds the correct features for you - automatically. You do not need to manually merge and aggregate tables in order to get started with a data science project. In addition, getML uses the derived features in a classical AutoML setting to easily make predictions with established and well-performing algorithms. This means getML provides an end-to-end solution starting from the relational data to a trained ML-model. How this is done via the getML Python API is demonstrated in the following.</p>"},{"location":"home/getting_started/getting_started/#defining-the-data-model","title":"Defining the data model","text":"<p>Most machine learning problems on relational data can be expressed as a simple star schema. This example is no exception, so we will use the predefined <code>StarSchema</code> class.</p> <pre><code>split = getml.data.split.random(train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_table, alias=\"population\", split=split)\n\nstar_schema.join(peripheral_table,\n                 alias=\"peripheral\",\n                 on=\"join_key\",\n                 time_stamps=\"time_stamp\",\n)\n</code></pre>"},{"location":"home/getting_started/getting_started/#building-a-pipeline","title":"Building a pipeline","text":"<p>Now we can define the feature learner.  Additionally, you can alter some hyperparameters like the number of features you want to train or the list of aggregations to select from when building features.</p> <p><pre><code>fastprop = getml.feature_learning.FastProp(\n     num_features=10,\n     aggregation=[\n         getml.feature_learning.aggregations.Count,\n         getml.feature_learning.aggregations.Sum\n     ],\n)\n</code></pre> getML bundles the sequential operations of a data science project (preprocessing, feature engineering, and predicting) into <code>Pipeline</code> objects. In addition to the <code>Placeholders</code> representing the <code>DataFrames</code> you also have to provide a feature learner (from <code>getml.feature_learning</code>) and a predictor (from <code>getml.predictors</code>).</p> <pre><code>pipe = getml.pipeline.Pipeline(\n     data_model=star_schema.data_model,\n     feature_learners=[fastprop],\n     predictors=[getml.predictors.LinearRegression()],\n)\n</code></pre> <p>We have chosen a narrow search field in aggregation space by only letting FastProp use <code>Count</code> and <code>Sum</code>. For the sake of demonstration, we use a simple <code>LinearRegression</code> and construct only 10 different features. In real world projects you would construct at least ten times this number and get results significantly better than what we will achieve here.</p>"},{"location":"home/getting_started/getting_started/#working-with-a-pipeline","title":"Working with a pipeline","text":"<p>Now, that we have defined a <code>Pipeline</code>, we can let getML do the heavy lifting of your typical data science project. With a well-defined <code>Pipeline</code>, you can, i.a.:</p> <ul> <li><code>fit()</code> the pipeline, to learn the logic behind your features (also referred to as training);</li> <li><code>score()</code> the pipeline to evaluate its performance on unseen data;</li> <li><code>transform()</code> the pipeline and materialize the learned logic into concrete (numerical) features;</li> <li><code>predict()</code> the <code>target</code>s for unseen data;</li> <li><code>deploy()</code> the pipeline to an http endpoint.</li> </ul>"},{"location":"home/getting_started/getting_started/#training","title":"Training","text":"<p>When fitting the model, we pass the handlers to the actual data residing in the getML engine \u2013 the <code>DataFrame</code>s.</p> <pre><code>pipe.fit(star_schema.train)\nChecking data model...\n\nStaging...\n[========================================] 100%\n\nChecking...\n[========================================] 100%\n\n\nOK.\n\nStaging...\n[========================================] 100%\n\nFastProp: Training 5 features...\n[========================================] 100%\n\nFastProp: Building features...\n[========================================] 100%\n\nLinearRegression: Training as predictor...\n[========================================] 100%\n\nTrained pipeline.\nTime taken: 0h:0m:0.049154\n\nPipeline(data_model='population',\n        feature_learners=['FastProp'],\n        feature_selectors=[],\n        include_categorical=False,\n        loss_function='SquareLoss',\n        peripheral=['peripheral'],\n        predictors=['LinearRegression'],\n        preprocessors=[],\n        share_selected_features=0.5,\n        tags=['container-s0mKB6'])\n\nurl: http://localhost:1709/#/getpipeline/getting_started/MXzNDT/0/\n</code></pre> <p>That\u2019s it. The features learned by <code>FastProp</code> as well as the <code>LinearRegression</code> are now trained on our data set.</p>"},{"location":"home/getting_started/getting_started/#scoring","title":"Scoring","text":"<p>We can also score our algorithms on the test set.</p> <pre><code>pipe.score(star_schema.test)\n\nStaging...\n[========================================] 100%\n\nPreprocessing...\n[========================================] 100%\n\nFastProp: Building features...\n[========================================] 100%\n\n    date time             set used    target         mae      rmse   rsquared\n0   2022-09-02 10:14:12   train       targets     3.3721    4.1891     0.9853\n1   2022-09-02 10:14:12   test        targets     3.7548    4.7093     0.981\n</code></pre> <p>Our model is able to predict the target variable in the newly generated data set pretty accurately. Though, the enterprise feature learner <code>Multirel</code> performs even better here with R<sup>2</sup> of 0.9995 and MAE and RMSE of 0.07079 and 0.1638 respectively.</p>"},{"location":"home/getting_started/getting_started/#making-predictions","title":"Making predictions","text":"<p>Let\u2019s simulate the arrival of unseen data and generate another population table. Since the data model is already stored in the pipeline, we do not need to recreate it and can just use a <code>Container</code> instead of a <code>StarSchema</code>.</p> <pre><code>population_table_unseen, peripheral_table_unseen = getml.datasets.make_numerical(\n    n_rows_population=200,\n    n_rows_peripheral=8000,\n    random_state=1711,\n)\n\ncontainer_unseen = getml.data.Container(population_table_unseen)\n\ncontainer_unseen.add(peripheral=peripheral_table_unseen)\n\nyhat = pipe.predict(container_unseen.full)\n\nStaging...\n[========================================] 100%\n\nPreprocessing...\n[========================================] 100%\n\nFastProp: Building features...\n[========================================] 100%\n\nprint(yhat[:10])\n[[ 4.16876676]\n [17.32933   ]\n [26.62467516]\n [-5.30655759]\n [27.4984785 ]\n [21.48631811]\n [18.16896219]\n [ 5.2784719 ]\n [20.5992354 ]\n [26.20538556]]\n</code></pre>"},{"location":"home/getting_started/getting_started/#extracting-features","title":"Extracting features","text":"<p>Of course, you can also transform a specific data set into the corresponding features in order to insert them into another machine learning algorithm.</p> <pre><code>features = pipe.transform(container_unseen.full)\n\nprint(features)\n[[-7.14232213e-01  2.39745475e-01  2.62855261e-01  1.28462060e-02\n   5.00000000e+00 -3.18568319e-01]\n [-1.17601634e-01  3.42472663e+00  3.61423201e+00  3.24305583e-02\n   1.40000000e+01  3.94656676e-01]\n [-2.48645436e+00  1.27495266e+01  1.33228011e+01  1.99520872e-02\n   3.60000000e+01  1.24700392e-01]\n ...\n [ 9.55124379e-01  9.16437833e-01  9.40897830e-01  2.73040074e-02\n   8.00000000e+00 -7.49963688e-01]\n [-3.56023429e+00  3.37346772e+00  2.11562428e+00  2.53698895e-02\n   1.50000000e+01 -7.27880243e-01]\n [ 2.72804029e-02  2.87302783e-02  5.36035230e-02  2.77103542e-02\n   2.00000000e+00 -3.53700424e-01]]\n</code></pre> <p>If you want to see a SQL transpilation of a feature's logic, you can do so by clicking on the feature in the monitor (enterprise edition only) or by inspecting the sql attribute on a feature. A <code>Pipeline</code>'s features are held by the <code>Features</code> container. For example, to inspect the SQL code of one of the features: <pre><code>pipe.features[1].sql\n</code></pre></p> <p>That should return something like this:</p> <pre><code>DROP TABLE IF EXISTS \"FEATURE_1_5\";\n\nCREATE TABLE \"FEATURE_1_5\" AS\nSELECT COUNT( * ) AS \"feature_1_5\",\n     t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"PERIPHERAL__STAGING_TABLE_2\" t2\nON t1.\"join_key\" = t2.\"join_key\"\nWHERE t2.\"time_stamp\" &lt;= t1.\"time_stamp\"\nGROUP BY t1.rowid;\n</code></pre> <p>This very much resembles the ad hoc definition we tried in the beginning. The correct aggregation to use on this data set is <code>Count</code>. getML extracted this definition completely autonomously.</p>"},{"location":"home/getting_started/getting_started/#next-steps","title":"Next steps","text":"<p>This guide has shown you the very basics of getML. Starting with raw data, you have completed a full project including feature engineering and linear regression using an automated end-to-end pipeline. The most tedious part of this process - finding the right aggregations and subconditions to construct a feature table from the relational data model - was also included in this pipeline.</p> <p>But there\u2019s more! Related articles show application  of getML on real world data sets.</p> <p>Also, don\u2019t hesitate to contact us with your feedback.</p>"},{"location":"home/installation/","title":"Index","text":""},{"location":"home/installation/#installation_1","title":"Installation","text":"<p>getML is a software suite for automated feature engineering on relational data and time series. It enables you to complete your data science projects in a fraction of their usual time and with better results. getML comes with an easy-to-use Python API that allows you to get started quickly and to start working on your own data after only a short learning curve.</p> <p>You can download and install getML on multiple platforms. Please refer to one of the following sections:</p> <ul> <li>macOS</li> <li>Windows/Docker</li> <li>Linux</li> </ul> <p>If you want to install and use getML on a remote machine, follow the instructions in:</p> <ul> <li>Remote Access</li> </ul>"},{"location":"home/installation/linux/","title":"Linux","text":""},{"location":"home/installation/linux/#install-getml-on-linux","title":"Install getML on Linux","text":"<p>This installation guide explains all necessary steps to install getML on Linux. To download the getML suite, go to </p> <p>https://www.getml.com/download</p> <p>and click the download button. This will download a tarball containing everything you need to use getML: The getML engine, the getML monitor, and the Python API.</p>"},{"location":"home/installation/linux/#system-requirements","title":"System requirements","text":"<p>Your Linux should meet at least the following requirements to successfully install getML:</p> <ul> <li> <p>GLIBC 2.17 or above (check by using <code>ldd --version</code>)</p> </li> <li> <p>If you are using Fedora 30, you need libxcrypt-compat. Install using <code>yum install libxcrypt-compat</code>.</p> </li> <li> <p>Python 3.7 or above must be installed on your machine. Furthermore, <code>numpy</code> and <code>pandas</code> are required dependencies for the getML Python API.</p> </li> </ul>"},{"location":"home/installation/linux/#install-and-run-the-getml-engine-and-monitor","title":"Install and run the getML engine and monitor","text":"<p>The getML engine is the C++ backend of getML. It comes with a graphical user interface - the getML monitor - that runs in your browser. To install these components, do the following:</p> <ol> <li> <p>Extract the tarball using     <code>tar -xzvf getml-VERSION-linux.tar.gz</code>.</p> </li> <li> <p><code>cd</code> into the resulting folder. Then, type <code>./getML</code>.    This will create a hidden folder called <code>.getML</code> in your home directory. If your computer has a desktop environment, you will also have a getML icon in your Applications menu. </p> </li> <li> <p>(optional) You can now install getML command-line interface (getML CLI). See below for further instructions. </p> </li> <li> <p>Open a browser and visit http://localhost:1709/ (if launching getML did not point you there automatically). </p> </li> </ol>"},{"location":"home/installation/linux/#install-the-getml-python-api","title":"Install the getML Python API","text":"<p>The Python API is a convenient way to interact with and to control the getML engine. There are two options to install the getML Python API:</p>"},{"location":"home/installation/linux/#from-pypi","title":"From PyPI","text":"<p>In a terminal, execute the following command to install the remote version from the Python Package Index:</p> <pre><code>pip install getml\n</code></pre>"},{"location":"home/installation/linux/#install-the-getml-cli","title":"Install the getML CLI","text":"<p>getML comes with a command-line interface (CLI) that lets you configure the most  important parameters on startup. The CLI is a standalone Go-binary located in the downloaded bundle.</p> <p>Note</p> <p>Before you can use the CLI, you have to follow steps 1 and 2 of the installation  instructions above.</p> <p>After deflating the tarball, you should find the <code>getML</code> binary in the resulting folder.</p> <p>After you have started getML for the first time, you can move the <code>getML</code> binary anywhere you want. We recommend moving the <code>getML</code> binary to a location included in the <code>PATH</code> environment variable, such as <code>~/.local/bin</code>. You can inspect the content of the aforementioned variable in a shell using:</p> <pre><code>echo $PATH\n</code></pre> <p>and check if it can be properly found by executing:</p> <pre><code>which getML\n</code></pre> <p>If this returns the location you moved the binary to, you are ready to go.</p> <p>For further help on how to use the CLI, just use <code>getML -h</code> or <code>getML -help</code>.</p>"},{"location":"home/installation/linux/#uninstall-getml","title":"Uninstall getML","text":"<p>To uninstall getML from your computer:</p> <ol> <li>Remove the folder <code>.getML</code> from your home directory. To do so, open a terminal and enter the following command:</li> </ol> <pre><code>rm -r $HOME/.getML\n</code></pre> <ol> <li>Delete <code>getML</code> binary from wherever you have put it (if you have decided to install the getML CLI).</li> </ol>"},{"location":"home/installation/linux/#where-to-go-next","title":"Where to go next","text":"<p>The Getting started guide provides an overview of the functionality of getML and a basic example of how to use the Python API. In order to get help or provide feedback, please contact our support.</p>"},{"location":"home/installation/mac/","title":"macOS","text":""},{"location":"home/installation/mac/#install-getml-on-macos","title":"Install getML on macOS","text":"<p>This installation guide explains all necessary steps to install getML on macOS. To download the getML suite, go to </p> <p>https://www.getml.com/download</p> <p>and click the download button. This will download a tarball containing everything you need to use getML: The getML engine, the getML monitor, and the Python API.</p>"},{"location":"home/installation/mac/#system-requirements","title":"System requirements","text":"<p>Your Mac should meet at least the following requirements to successfully install getML:</p> <ul> <li>macOS must be version 10.14 or newer. You can check your macOS version by running the following command in a terminal: <code>sw_vers</code></li> <li>Python 3.7 or above must be installed on your machine. Furthermore, <code>numpy</code> and <code>pandas</code> are required dependencies for the getML Python API.</li> </ul>"},{"location":"home/installation/mac/#install-and-run-the-getml-engine-and-monitor","title":"Install and run the getML engine and monitor","text":"<p>The getML engine is the C++ backend of getML. It comes with a graphical user interface - the getML monitor - that runs in your browser. To install these components:</p> <ol> <li> <p>Double-click the <code>dmg</code>-file to open the installer, then drag the getML Icon into the Applications folder.    </p> </li> <li> <p>Double-click getML.app in the Applications folder to start getML. A security feature in macOS will ask you to confirm that you want to open getML when launching it for the first time. After confirmation, the getML icon in your status bar indicates that the getML engine is running.    </p> </li> <li> <p>Open a browser and visit http://localhost:1709/ (if opening the getML.app did not point you there automatically). </p> </li> </ol>"},{"location":"home/installation/mac/#install-the-getml-python-api","title":"Install the getML Python API","text":"<p>The Python API is a convenient way to interact with and to control the getML engine. There are two options to install the getML Python API:</p>"},{"location":"home/installation/mac/#from-pypi","title":"From PyPI","text":"<p>In a terminal execute the following command to install the remote version from the Python Package Index:</p> <pre><code>pip install getml\n</code></pre> <p>To make sure that the Python API was installed properly, you can use</p> <pre><code>python -c 'import getml'\n</code></pre>"},{"location":"home/installation/mac/#install-the-getml-cli","title":"Install the getML-CLI","text":"<p>getML comes with a command-line interface (CLI) that lets you configure the most  important parameters on startup. The CLI is a standalone Go-binary. The CLI is optional. If you are fine with launching getML from your Launchpad, you can stop reading.</p> <p>Before you can use the CLI, you have to have launched getML from your Launchpad at least once. This is because when you launch getML for the first time, it creates a hidden folder in your home directory (called <code>.getML</code>) into which it copies the binaries and all necessary resources. Said hidden folder is then accessed by the CLI. </p> <p>After launching getML for the first time, you can find the CLI in <code>$HOME/.getML/getml-VERSION</code>.</p> <pre><code>./getml-cli\n</code></pre> <p>You can move <code>getml-cli</code> anywhere you want. We recommend moving the <code>getml-cli</code> to a location included in the <code>PATH</code> environment variable, such as <code>/usr/local/bin</code>. You can inspect the content of the aforementioned variable in a shell using:</p> <pre><code>echo $PATH\n</code></pre> <p>and check if it can be properly found by executing</p> <pre><code>which getml-cli\n</code></pre> <p>If you see the location you moved the binary to as output, you are ready to go.</p> <p>For further help on how to use the CLI, just use <code>getml-cli -h</code> or <code>getml-cli -help</code>.</p>"},{"location":"home/installation/mac/#uninstall-getml","title":"Uninstall getML","text":"<p>To uninstall getML from your Mac:</p> <ol> <li>Drag <code>getml.app</code> from your applications folder into the trash.</li> <li>Remove the folder <code>.getML</code> from your home directory. To do so, open your Finder and go to your home directory (<code>Cmd + Shift + H</code>). Hit <code>Cmd + Shift + .</code> to show the hidden files. Identify <code>.getML</code> and move it to the trash.</li> <li>Delete <code>getml-cli</code> from wherever you have put it (if you have decided to install <code>getml-cli</code>).</li> </ol>"},{"location":"home/installation/mac/#where-to-go-next","title":"Where to go next","text":"<p>The Getting started guide provides an overview of the functionality of getML and a basic example of how to use the Python API. In order to get help or provide feedback, please contact our support.</p>"},{"location":"home/installation/remote_access/","title":"Remote access","text":""},{"location":"home/installation/remote_access/#remote-access_1","title":"Remote Access","text":"<p>This guide helps you set up getML on a remote server and access it from your local machine.</p>"},{"location":"home/installation/remote_access/#running-the-getml-suite-remotely","title":"Running the getML Suite Remotely","text":"<p>Note that this section is more of a how-to guide for SSH and bash than something that is specific to getML.</p>"},{"location":"home/installation/remote_access/#prerequisites","title":"Prerequisites","text":"<p>To run the getML software on a remote server, you should ensure the following:</p> <ol> <li>You know the IP of the server and can log into it using a USER account and corresponding password.</li> <li>Linux is running on the server.</li> <li>The server has a working internet connection (required to authenticate your user account) and is accessible via SSH.</li> </ol>"},{"location":"home/installation/remote_access/#remote-installation","title":"Remote Installation","text":"<p>If all conditions are met, download the Linux version of the getML suite from getml.com and copy it to the remote server. <pre><code>scp getml-VERSION-linux.tar.gz USER@IP:\n</code></pre> This will copy the entire bundle into the home folder of your USER on the remote host. Then you need to log onto the server. <pre><code>ssh USER@IP\n</code></pre> Follow the installation instructions to install getML on the remote host.</p> <p></p>"},{"location":"home/installation/remote_access/#starting-engine-and-monitor","title":"Starting Engine and Monitor","text":"<p>Start getML using the command-line interface. It is a good idea to <code>disown</code> or <code>nohup</code> the process, so that it keeps running when you close the SSH terminal or if the connection breaks down temporarily.</p> <p><pre><code>./getML &gt; run.log &amp;\ndisown\n</code></pre> or <pre><code>nohup ./getML &amp;\n</code></pre></p> <p>Both methods will pipe the log of the engine into a file - either run.log or nohup.out.</p>"},{"location":"home/installation/remote_access/#login","title":"Login","text":"<p>Now the getML engine and monitor are running. To view the monitor, use port forwarding via SSH. <pre><code>ssh -L 2222:localhost:1709 USER@IP\n</code></pre> This collects all traffic on port 1709 of the remote host\u2014the HTTP port of the getML monitor\u2014and binds it to port 2222 of your local computer. By entering localhost:2222 into the navigation bar of your web browser, you can log into the remote instance. Note that this connection is only available as long as the SSH session started with the previous command is still active and running.</p>"},{"location":"home/installation/remote_access/#running-analyses-using-the-python-api","title":"Running Analyses Using the Python API","text":"<p>When you start a Python script, you should also <code>disown</code> or <code>nohup</code> it, as explained in the previous section.</p> <p>If you want to know whether the Python process is still running, use <code>ps -aux</code>. <pre><code>ps -aux | grep python\n</code></pre> It lists all running processes and filters only those containing the letters 'python'. If your scripts appear in the listings, they are still running.</p> <p>Running an interactive session using <code>IPython</code> is also possible but should not be done directly (since you will lose all progress the moment you get disconnected). Instead, we recommend using third-party helper programs, like GNU screen or tmux.</p> <p>Note</p> <p>It is usually NOT a good idea to forward the port of the getML engine to your local computer and then run the Python API locally. If you decide to do so anyway, make sure to always use absolute paths for data loading.</p>"},{"location":"home/installation/remote_access/#retrieving-results","title":"Retrieving Results","text":"<p>Once your analysis is done, all results are located in the corresponding project folder. You can access them directly on the server or copy them to your local machine. <pre><code>scp USER@IP:~/.getML/getml-&lt;version&gt;/projects/* ~/.getML/getml-&lt;version&gt;/projects\n</code></pre></p>"},{"location":"home/installation/remote_access/#stopping-engine-and-monitor","title":"Stopping Engine and Monitor","text":"<p>If you want to shutdown getML, you can use the appropriate command. <pre><code>./getML -stop\n</code></pre></p>"},{"location":"home/installation/remote_access/#accessing-the-getml-monitor-via-the-internet","title":"Accessing the getML Monitor Via the Internet","text":"<p>Up to now you only have used the HTTP port of the monitor and required no encryption. Isn't this insecure?</p> <p>Not at all. The getML monitor is implemented in such a way the HTTP port can only be accessed from a browser located at the same machine the monitor is running on. No one else will have access to it. In the scenario discussed in the previous section all communication with the remote host had been encrypted using the strong SSH protocol and all queries of the getML suite to authenticate your login were encrypted too.</p> <p>But allowing access to the monitor over the internet is not a bad idea  in principle. It allows you to omit the port forwarding step and grants other entities permission to view the results of your analysis in e.g. your company's intranet. This is where the HTTPS port opened by the monitor comes in.</p>"},{"location":"home/installation/remote_access/#what-is-accessible-and-what-is-not","title":"What is Accessible and What is Not?","text":"<p>Only the getML monitor is accessible via the HTTPS port. There is no way to connect to the getML engine via the internet (the engine will reject any command sent remotely).</p> <p>After having started the engine and monitor on your server, connect to the latter by entering <code>https://host-ip:1710</code> into the navigation bar of your web browser. Every user still needs to log into the getML monitor using a valid getML account and needs to be whitelisted in order to have access to the monitor.</p>"},{"location":"home/installation/remote_access/#creating-and-using-tls-certificates","title":"Creating and Using TLS Certificates","text":"<p>The encryption via HTTPS requires a valid TLS certificate. The TLS certificate is created when you start getML for the first time. You can discard the current certificate and generate a new one in the configuration tab of the getML monitor. When doing so, you can choose whether the certificate should be self-signed or not. This is because HTTPS encryption is based on the so-called web of trust. Every certificate has to be checked and validated by a Certificate Authority (CA). If your browser knows and trusts the CA, it will display a closed lock in the left part of its navigation bar. If not, it will warn you and not establish the connection right away. But since a certificate must include the exact hostname including the subdomain it is used for, almost every certificate for every getML monitor will look different and they all have to be validated by a CA somehow. This is neither cheap nor feasible. That's why the monitor can act as a CA itself.</p> <p>When accessing the getML monitor via HTTPS (even locally on https://localhost:1710), your browser will be alarmed, refuse to access the page at first, and tell you it doesn't know the CA. You have to allow an exception manually. Since every monitor will be a different CA, there is no loss in security either.</p>"},{"location":"home/installation/remote_access/#adding-an-exception-in-browsers","title":"Adding an Exception in Browsers","text":"<p>In Firefox, you first have to click on 'Advanced',</p> <p></p> <p>followed by 'Accept the Risk and Continue'. </p> <p></p> <p>In Chrome, you first have to click on 'Advanced',</p> <p></p> <p>followed by 'Proceed to localhost (unsafe)'.</p> <p></p>"},{"location":"home/installation/remote_access/#opening-the-https-port","title":"Opening the HTTPS Port","text":"<p>Telling the getML monitor to serve its web frontend via HTTPS on a specific port usually does not make it accessible from the outside yet. Your computer or the server does not allow arbitrary programs to open connections to the outside world. You need to add the corresponding port number to a whitelist in your system's configuration. Since there are far too many combinations of systems and applications used as firewalls, we won't cover them here. If you have questions or need help concerning this step, please feel free to contact us.</p>"},{"location":"home/installation/windows/","title":"Windows/Docker","text":""},{"location":"home/installation/windows/#install-getml-on-macoswindowsdocker","title":"Install getML on macOS/Windows/Docker","text":"<p>getML for Docker is the recommended way of running getML on Windows and macOS. However, it can also be used on Linux for a more \"out-of-the-box\" experience.</p> <p>This installation guide explains all necessary steps to install getML on Docker. To download the getML suite, go to </p> <p>https://www.getml.com/download</p> <p>and click the download button. This will download a ZIP archive containing everything you need to use getML for Docker.</p>"},{"location":"home/installation/windows/#system-requirements","title":"System Requirements","text":"<p>Your system should meet the following requirements to successfully install getML for Docker:</p> <ul> <li>Docker. If you are on Linux, make sure that you can run docker without root rights/sudo.</li> <li>Git Bash. Only for Windows users. (On Linux and   macOS, it is pre-installed)</li> <li>OpenSSL. This should be pre-installed on most systems as well.</li> </ul>"},{"location":"home/installation/windows/#setup-and-run-getml-for-docker","title":"Setup and Run getML for Docker","text":"<ol> <li>Make sure that Docker is running (more precisely, the Docker daemon).</li> <li>Unzip <code>getml-X.X.X-docker.zip</code>, where <code>X.X.X</code> is a placeholder for the version number.</li> <li> <p>Execute <code>setup.sh</code>. This will run the Dockerfile and set up your Docker image. It will also create a Docker volume called 'getml'. On Windows, you can just click on <code>setup.sh</code>.</p> <p>On macOS and Linux, do the following:</p> <pre><code>cd getml-X.X.X-docker\nbash setup.sh # or ./setup.sh\n</code></pre> <p>(Please make sure that you actually <code>cd</code> into that directory, otherwise <code>setup.sh</code> will not find the Dockerfile.)</p> </li> <li> <p>Execute <code>run.sh</code>. This will run the Docker image.</p> </li> </ol> <p>On Windows, you can just click on <code>run.sh</code>.</p> <p>On macOS and Linux, do the following:</p> <pre><code>cd getml-X.X.X-docker\nbash run.sh # or ./run.sh\n</code></pre>"},{"location":"home/installation/windows/#uninstall-getml","title":"Uninstall getML","text":"<p>To uninstall getML for Docker, execute <code>uninstall.sh</code>. On Windows, you can just click on <code>uninstall.sh</code>.</p> <p>On macOS and Linux, do the following:</p> <pre><code>cd getml-X.X.X-docker\nbash uninstall.sh # or ./uninstall.sh\n</code></pre>"},{"location":"home/installation/windows/#where-to-go-next","title":"Where to Go Next","text":"<p>The Getting started guide provides an overview of the functionality of getML and a basic example of how to use the Python API. In order to get help or provide feedback, please contact our support.</p>"},{"location":"home/support/support/","title":"Support","text":""},{"location":"home/support/support/#support_1","title":"Support","text":"<p>If you are looking for help, you came to the right place. Contact us for general inquiries or write an e-mail to support@getml.com.</p>"},{"location":"integration/about/about/","title":"About","text":""},{"location":"integration/about/about/#about_1","title":"About","text":"<p>Copyright (c) 2020 by The SQLNet Company GmbH. All rights reserved.</p> <p>getML includes POCO (https://pocoproject.org/). Thanks and keep up the good work!</p> <p>getML includes zlib (https://www.zlib.net/). Thanks and keep up the good work!</p> <p>getML includes SELinux (https://github.com/SELinuxProject/selinux/tree/master/libselinux). Thanks and keep up the good work!</p> <p>getML includes xgboost (https://github.com/dmlc/xgboost). Copyright (c) 2019 by Contributors. Licensed under the Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0).</p> <p>getML includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit (https://www.openssl.org). Copyright (c) 1998-2019 The OpenSSL Project. All rights reserved. 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. All advertising materials mentioning features or use of this software must display the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit. (http://www.openssl.org/)\" 4. The names \"OpenSSL Toolkit\" and \"OpenSSL Project\" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact openssl-core@openssl.org. 5. Products derived from this software may not be called \"OpenSSL\" nor may \"OpenSSL\" appear in their names without prior written permission of the OpenSSL Project. 6. Redistributions of any form whatsoever must retain the following acknowledgment: \"This product includes software developed by the OpenSSL Project for use in the OpenSSL Toolkit (http://www.openssl.org/)\" THIS SOFTWARE IS PROVIDED BY THE OpenSSL PROJECT ``AS IS'' AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE OpenSSL PROJECT OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes yyjson (https://github.com/ibireme/yyjson). Copyright (c) 2020 YaoYuan ibireme@gmail.com Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>getML includes libpq (https://github.com/postgres/postgres). Portions Copyright \u00a9 1996-2019, The PostgreSQL Global Development Group. Portions Copyright \u00a9 1994, The Regents of the University of California. IN NO EVENT SHALL THE UNIVERSITY OF CALIFORNIA BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF THE UNIVERSITY OF CALIFORNIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THE UNIVERSITY OF CALIFORNIA SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS, AND THE UNIVERSITY OF CALIFORNIA HAS NO OBLIGATIONS TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.</p> <p>getML includes The MariaDB C Connector (https://github.com/mariadb-corporation/mariadb-connector-c). Copyright \u00a9 2019, The MariaDB Team, licensed under the LGPL (https://www.gnu.org/licenses/lgpl-3.0.html).</p> <p>getML include libiconv (https://www.gnu.org/software/libiconv/). Copyright \u00a9 2019, Bruno Haible, licensed under the LGPL (https://www.gnu.org/licenses/lgpl-3.0.html).</p> <p>getML include libcharset (https://www.haible.de/bruno/packages-libcharset.html). Copyright \u00a9 2019, Bruno Haible, licensed under the LGPL (https://www.gnu.org/licenses/lgpl-3.0.html).</p> <p>getML includes Eigen (https://eigen.tuxfamily.org/dox/), including only the source code covered by the MPL2 (https://www.mozilla.org/en-US/MPL/2.0/). The source code for Eigen can be downloaded from bitbucket (https://bitbucket.org/eigen/eigen/src/default/).</p> <p>getML includes NSS (https://hg.mozilla.org/projects/nss). Copyright (c) 2020, The Mozilla Foundation. Licensed under the MPL2 (https://www.mozilla.org/en-US/MPL/2.0/). The source code for NSS can be downloaded from mercurial (https://hg.mozilla.org/projects/nss).</p> <p>getML includes NSPR (https://hg.mozilla.org/projects/nspr). Copyright (c) 2020, The Mozilla Foundation. Licensed under the MPL2 (https://www.mozilla.org/en-US/MPL/2.0/). The source code for NSPR can be downloaded from mercurial (https://hg.mozilla.org/projects/nspr).</p> <p>getML includes keyutils (https://github.com/auristor/keyutils). Copyright (c) 2020, the authors. Licensed under the LGPL (https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html).</p> <p>getML includes Cyrus SASL (https://github.com/cyrusimap/cyrus-sasl). Copyright (c) 1998-2003 Carnegie Mellon University. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. The name \"Carnegie Mellon University\" must not be used to endorse or promote products derived from this software without prior written permission. For permission or any other legal details, please contact Office of Technology Transfer Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213-3890 (412) 268-4387, fax: (412) 268-7395 tech-transfer@andrew.cmu.edu 4. Redistributions of any form whatsoever must retain the following acknowledgment: \"This product includes software developed by Computing Services at Carnegie Mellon University (http://www.cmu.edu/computing/).\" CARNEGIE MELLON UNIVERSITY DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.</p> <p>getML includes PCRE (http://www.pcre.org/). Copyright (c) 1997-2020 University of Cambridge. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notices, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notices, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the University of Cambridge nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes Kerberos 5 (https://github.com/krb5/krb5). Copyright (C) 1985-2020 by the Massachusetts Institute of Technology. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Downloading of this software may constitute an export of cryptographic software from the United States of America that is subject to the United States Export Administration Regulations (EAR), 15 CFR 730-774. Additional laws or regulations may apply. It is the responsibility of the person or entity contemplating export to comply with all applicable export laws and regulations, including obtaining any required license from the U.S. government. The U.S. government prohibits export of encryption source code to certain countries and individuals, including, but not limited to, the countries of Cuba, Iran, North Korea, Sudan, Syria, and residents and nationals of those countries. Documentation components of this software distribution are licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. (https://creativecommons.org/licenses/by-sa/3.0/) Individual source code files are copyright MIT, Cygnus Support, Novell, OpenVision Technologies, Oracle, Red Hat, Sun Microsystems, FundsXpress, and others. Project Athena, Athena, Athena MUSE, Discuss, Hesiod, Kerberos, Moira, and Zephyr are trademarks of the Massachusetts Institute of Technology (MIT). No commercial use of these trademarks may be made without prior written permission of MIT. \"Commercial use\" means use of a name in a product or other for-profit manner. It does NOT prevent a commercial firm from referring to the MIT trademarks in order to convey information (although in doing so, recognition of their trademark status should be given). ====================================================================== The following copyright and permission notice applies to the OpenVision Kerberos Administration system located in \"kadmin/create\", \"kadmin/dbutil\", \"kadmin/passwd\", \"kadmin/server\", \"lib/kadm5\", and portions of \"lib/rpc\": Copyright, OpenVision Technologies, Inc., 1993-1996, All Rights Reserved WARNING: Retrieving the OpenVision Kerberos Administration system source code, as described below, indicates your acceptance of the following terms. If you do not agree to the following terms, do not retrieve the OpenVision Kerberos administration system. You may freely use and distribute the Source Code and Object Code compiled from it, with or without modification, but this Source Code is provided to you \"AS IS\" EXCLUSIVE OF ANY WARRANTY, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, OR ANY OTHER WARRANTY, WHETHER EXPRESS OR IMPLIED. IN NO EVENT WILL OPENVISION HAVE ANY LIABILITY FOR ANY LOST PROFITS, LOSS OF DATA OR COSTS OF PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, OR FOR ANY SPECIAL, INDIRECT, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THIS AGREEMENT, INCLUDING, WITHOUT LIMITATION, THOSE RESULTING FROM THE USE OF THE SOURCE CODE, OR THE FAILURE OF THE SOURCE CODE TO PERFORM, OR FOR ANY OTHER REASON. OpenVision retains all copyrights in the donated Source Code. OpenVision also retains copyright to derivative works of the Source Code, whether created by OpenVision or by a third party. The OpenVision copyright notice must be preserved if derivative works are made based on the donated Source Code. OpenVision Technologies, Inc. has donated this Kerberos Administration system to MIT for inclusion in the standard Kerberos 5 distribution. This donation underscores our commitment to continuing Kerberos technology development and our gratitude for the valuable work which has been performed by MIT and the Kerberos community. ====================================================================== Portions contributed by Matt Crawford \"crawdad@fnal.gov\" were work performed at Fermi National Accelerator Laboratory, which is operated by Universities Research Association, Inc., under contract DE-AC02-76CHO3000 with the U.S. Department of Energy. ====================================================================== Portions of \"src/lib/crypto\" have the following copyright: Copyright (C) 1998 by the FundsXpress, INC. All rights reserved. Export of this software from the United States of America may require a specific license from the United States Government. It is the responsibility of any person or organization contemplating export to obtain such a license before exporting. WITHIN THAT CONSTRAINT, permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of FundsXpress. not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. FundsXpress makes no representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. THIS SOFTWARE IS PROVIDED \"AS IS\" AND WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE. ====================================================================== The implementation of the AES encryption algorithm in \"src/lib/crypto/builtin/aes\" has the following copyright: Copyright (C) 2001, Dr Brian Gladman \"brg@gladman.uk.net\", Worcester, UK. All rights reserved. LICENSE TERMS The free distribution and use of this software in both source and binary form is allowed (with or without changes) provided that: 1. distributions of this source code include the above copyright notice, this list of conditions and the following disclaimer; 2. distributions in binary form include the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other associated materials; 3. the copyright holder's name is not used to endorse products built using this software without specific written permission. DISCLAIMER This software is provided 'as is' with no explcit or implied warranties in respect of any properties, including, but not limited to, correctness and fitness for purpose. ====================================================================== Portions contributed by Red Hat, including the pre-authentication plug-in framework and the NSS crypto implementation, contain the following copyright: Copyright (C) 2006 Red Hat, Inc. Portions copyright (C) 2006 Massachusetts Institute of Technology All Rights Reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Red Hat, Inc., nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The bundled verto source code is subject to the following license: Copyright 2011 Red Hat, Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ====================================================================== The MS-KKDCP client implementation has the following copyright: Copyright 2013,2014 Red Hat, Inc. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The implementations of GSSAPI mechglue in GSSAPI-SPNEGO in \"src/lib/gssapi\", including the following files: lib/gssapi/generic/gssapi_err_generic.et lib/gssapi/mechglue/g_accept_sec_context.c lib/gssapi/mechglue/g_acquire_cred.c lib/gssapi/mechglue/g_canon_name.c lib/gssapi/mechglue/g_compare_name.c lib/gssapi/mechglue/g_context_time.c lib/gssapi/mechglue/g_delete_sec_context.c lib/gssapi/mechglue/g_dsp_name.c lib/gssapi/mechglue/g_dsp_status.c lib/gssapi/mechglue/g_dup_name.c lib/gssapi/mechglue/g_exp_sec_context.c lib/gssapi/mechglue/g_export_name.c lib/gssapi/mechglue/g_glue.c lib/gssapi/mechglue/g_imp_name.c lib/gssapi/mechglue/g_imp_sec_context.c lib/gssapi/mechglue/g_init_sec_context.c lib/gssapi/mechglue/g_initialize.c lib/gssapi/mechglue/g_inquire_context.c lib/gssapi/mechglue/g_inquire_cred.c lib/gssapi/mechglue/g_inquire_names.c lib/gssapi/mechglue/g_process_context.c lib/gssapi/mechglue/g_rel_buffer.c lib/gssapi/mechglue/g_rel_cred.c lib/gssapi/mechglue/g_rel_name.c lib/gssapi/mechglue/g_rel_oid_set.c lib/gssapi/mechglue/g_seal.c lib/gssapi/mechglue/g_sign.c lib/gssapi/mechglue/g_store_cred.c lib/gssapi/mechglue/g_unseal.c lib/gssapi/mechglue/g_userok.c lib/gssapi/mechglue/g_utils.c lib/gssapi/mechglue/g_verify.c lib/gssapi/mechglue/gssd_pname_to_uid.c lib/gssapi/mechglue/mglueP.h lib/gssapi/mechglue/oid_ops.c lib/gssapi/spnego/gssapiP_spnego.h lib/gssapi/spnego/spnego_mech.c and the initial implementation of incremental propagation, including the following new or changed files: include/iprop_hdr.h kadmin/server/ipropd_svc.c lib/kdb/iprop.x lib/kdb/kdb_convert.c lib/kdb/kdb_log.c lib/kdb/kdb_log.h lib/krb5/error_tables/kdb5_err.et kprop/kpropd_rpc.c kprop/kproplog.c are subject to the following license: Copyright (C) 2004 Sun Microsystems, Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ====================================================================== Kerberos V5 includes documentation and software developed at the University of California at Berkeley, which includes this copyright notice: Copyright (C) 1983 Regents of the University of California. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Portions contributed by Novell, Inc., including the LDAP database backend, are subject to the following license: Copyright (C) 2004-2005, Novell, Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * The copyright holder's name is not used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Portions funded by Sandia National Laboratory and developed by the University of Michigan's Center for Information Technology Integration, including the PKINIT implementation, are subject to the following license: COPYRIGHT (C) 2006-2007 THE REGENTS OF THE UNIVERSITY OF MICHIGAN ALL RIGHTS RESERVED Permission is granted to use, copy, create derivative works and redistribute this software and such derivative works for any purpose, so long as the name of The University of Michigan is not used in any advertising or publicity pertaining to the use of distribution of this software without specific, written prior authorization. If the above copyright notice or any other identification of the University of Michigan is included in any copy of any portion of this software, then the disclaimer below must also be included. THIS SOFTWARE IS PROVIDED AS IS, WITHOUT REPRESENTATION FROM THE UNIVERSITY OF MICHIGAN AS TO ITS FITNESS FOR ANY PURPOSE, AND WITHOUT WARRANTY BY THE UNIVERSITY OF MICHIGAN OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE REGENTS OF THE UNIVERSITY OF MICHIGAN SHALL NOT BE LIABLE FOR ANY DAMAGES, INCLUDING SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, WITH RESPECT TO ANY CLAIM ARISING OUT OF OR IN CONNECTION WITH THE USE OF THE SOFTWARE, EVEN IF IT HAS BEEN OR IS HEREAFTER ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. ====================================================================== The pkcs11.h file included in the PKINIT code has the following license: Copyright 2006 g10 Code GmbH Copyright 2006 Andreas Jellinghaus This file is free software; as a special exception the author gives unlimited permission to copy and/or distribute it, with or without modifications, as long as this notice is preserved. This file is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY, to the extent permitted by law; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ====================================================================== Portions contributed by Apple Inc. are subject to the following license: Copyright 2004-2008 Apple Inc. All Rights Reserved. Export of this software from the United States of America may require a specific license from the United States Government. It is the responsibility of any person or organization contemplating export to obtain such a license before exporting. WITHIN THAT CONSTRAINT, permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Apple Inc. not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. Apple Inc. makes no representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. THIS SOFTWARE IS PROVIDED \"AS IS\" AND WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE. ====================================================================== The implementations of UTF-8 string handling in src/util/support and src/lib/krb5/unicode are subject to the following copyright and permission notice: The OpenLDAP Public License Version 2.8, 17 August 2003 Redistribution and use of this software and associated documentation (\"Software\"), with or without modification, are permitted provided that the following conditions are met: 1. Redistributions in source form must retain copyright statements and notices, 2. Redistributions in binary form must reproduce applicable copyright statements and notices, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution, and 3. Redistributions must contain a verbatim copy of this document. The OpenLDAP Foundation may revise this license from time to time. Each revision is distinguished by a version number. You may use this Software under terms of this license revision or under the terms of any subsequent revision of the license. THIS SOFTWARE IS PROVIDED BY THE OPENLDAP FOUNDATION AND ITS CONTRIBUTORS \"AS IS\" AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE OPENLDAP FOUNDATION, ITS CONTRIBUTORS, OR THE AUTHOR(S) OR OWNER(S) OF THE SOFTWARE BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. The names of the authors and copyright holders must not be used in advertising or otherwise to promote the sale, use or other dealing in this Software without specific, written prior permission. Title to copyright in this Software shall at all times remain with copyright holders. OpenLDAP is a registered trademark of the OpenLDAP Foundation. Copyright 1999-2003 The OpenLDAP Foundation, Redwood City, California, USA. All Rights Reserved. Permission to copy and distribute verbatim copies of this document is granted. ====================================================================== Marked test programs in src/lib/krb5/krb have the following copyright: Copyright (C) 2006 Kungliga Tekniska H\u00f6gskola (Royal Institute of Technology, Stockholm, Sweden). All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of KTH nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY KTH AND ITS CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL KTH OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The KCM Mach RPC definition file used on macOS has the following copyright: Copyright (C) 2009 Kungliga Tekniska H\u00f6gskola (Royal Institute of Technology, Stockholm, Sweden). All rights reserved. Portions Copyright (C) 2009 Apple Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the Institute nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE INSTITUTE AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE INSTITUTE OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Portions of the RPC implementation in src/lib/rpc and src/include/gssrpc have the following copyright and permission notice: Copyright (C) 2010, Oracle America, Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the \"Oracle America, Inc.\" nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Copyright (C) 2006,2007,2009 NTT (Nippon Telegraph and Telephone Corporation). All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer as the first lines of this file unmodified. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY NTT \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NTT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Copyright 2000 by Carnegie Mellon University All Rights Reserved Permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Carnegie Mellon University not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. CARNEGIE MELLON UNIVERSITY DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. ====================================================================== Copyright (C) 2002 Naval Research Laboratory (NRL/CCS) Permission to use, copy, modify and distribute this software and its documentation is hereby granted, provided that both the copyright notice and this permission notice appear in all copies of the software, derivative works or modified versions, and any portions thereof. NRL ALLOWS FREE USE OF THIS SOFTWARE IN ITS \"AS IS\" CONDITION AND DISCLAIMS ANY LIABILITY OF ANY KIND FOR ANY DAMAGES WHATSOEVER RESULTING FROM THE USE OF THIS SOFTWARE. ====================================================================== Copyright (C) 1991, 1992, 1994 by Cygnus Support. Permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation. Cygnus Support makes no representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. ====================================================================== Copyright (C) 2006 Secure Endpoints Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ====================================================================== Portions of the implementation of the Fortuna-like PRNG are subject to the following notice: Copyright (C) 2005 Marko Kreen All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Copyright (C) 1994 by the University of Southern California EXPORT OF THIS SOFTWARE from the United States of America may require a specific license from the United States Government. It is the responsibility of any person or organization contemplating export to obtain such a license before exporting. WITHIN THAT CONSTRAINT, permission to copy, modify, and distribute this software and its documentation in source and binary forms is hereby granted, provided that any documentation or other materials related to such distribution or use acknowledge that the software was developed by the University of Southern California. DISCLAIMER OF WARRANTY. THIS SOFTWARE IS PROVIDED \"AS IS\". The University of Southern California MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. By way of example, but not limitation, the University of Southern California MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE. The University of Southern California shall not be held liable for any liability nor for any direct, indirect, or consequential damages with respect to any claim by the user or distributor of the ksu software. ====================================================================== Copyright (C) 1995 The President and Fellows of Harvard University This code is derived from software contributed to Harvard by Jeremy Rassen. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. All advertising materials mentioning features or use of this software must display the following acknowledgement: This product includes software developed by the University of California, Berkeley and its contributors. 4. Neither the name of the University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Copyright (C) 2008 by the Massachusetts Institute of Technology. Copyright 1995 by Richard P. Basch. All Rights Reserved. Copyright 1995 by Lehman Brothers, Inc. All Rights Reserved. Export of this software from the United States of America may require a specific license from the United States Government. It is the responsibility of any person or organization contemplating export to obtain such a license before exporting. WITHIN THAT CONSTRAINT, permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of Richard P. Basch, Lehman Brothers and M.I.T. not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. Richard P. Basch, Lehman Brothers and M.I.T. make no representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. ====================================================================== The following notice applies to \"src/lib/krb5/krb/strptime.c\" and \"src/include/k5-queue.h\". Copyright (C) 1997, 1998 The NetBSD Foundation, Inc. All rights reserved. This code was contributed to The NetBSD Foundation by Klaus Klein. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. All advertising materials mentioning features or use of this software must display the following acknowledgement: This product includes software developed by the NetBSD Foundation, Inc. and its contributors. 4. Neither the name of The NetBSD Foundation nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The following notice applies to Unicode library files in \"src/lib/krb5/unicode\": Copyright 1997, 1998, 1999 Computing Research Labs, New Mexico State University Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE COMPUTING RESEARCH LAB OR NEW MEXICO STATE UNIVERSITY BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ====================================================================== The following notice applies to \"src/util/support/strlcpy.c\": Copyright (C) 1998 Todd C. Miller \"Todd.Miller@courtesan.com\" Permission to use, copy, modify, and distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. ====================================================================== The following notice applies to \"src/util/profile/argv_parse.c\" and \"src/util/profile/argv_parse.h\": Copyright 1999 by Theodore Ts'o. Permission to use, copy, modify, and distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THEODORE TS'O (THE AUTHOR) DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. (Isn't it sick that the U.S. culture of lawsuit-happy lawyers requires this kind of disclaimer?) ====================================================================== The following notice applies to SWIG-generated code in \"src/util/profile/profile_tcl.c\": Copyright (C) 1999-2000, The University of Chicago This file may be freely redistributed without license or fee provided this copyright message remains intact. ====================================================================== The following notice applies to portiions of \"src/lib/rpc\" and \"src/include/gssrpc\": Copyright (C) 2000 The Regents of the University of Michigan. All rights reserved. Copyright (C) 2000 Dug Song \"dugsong@UMICH.EDU\". All rights reserved, all wrongs reversed. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the University nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== Implementations of the MD4 algorithm are subject to the following notice: Copyright (C) 1990, RSA Data Security, Inc. All rights reserved. License to copy and use this software is granted provided that it is identified as the \"RSA Data Security, Inc. MD4 Message Digest Algorithm\" in all material mentioning or referencing this software or this function. License is also granted to make and use derivative works provided that such works are identified as \"derived from the RSA Data Security, Inc. MD4 Message Digest Algorithm\" in all material mentioning or referencing the derived work. RSA Data Security, Inc. makes no representations concerning either the merchantability of this software or the suitability of this software for any particular purpose. It is provided \"as is\" without express or implied warranty of any kind. These notices must be retained in any copies of any part of this documentation and/or software. ====================================================================== Implementations of the MD5 algorithm are subject to the following notice: Copyright (C) 1990, RSA Data Security, Inc. All rights reserved. License to copy and use this software is granted provided that it is identified as the \"RSA Data Security, Inc. MD5 Message- Digest Algorithm\" in all material mentioning or referencing this software or this function. License is also granted to make and use derivative works provided that such works are identified as \"derived from the RSA Data Security, Inc. MD5 Message-Digest Algorithm\" in all material mentioning or referencing the derived work. RSA Data Security, Inc. makes no representations concerning either the merchantability of this software or the suitability of this software for any particular purpose. It is provided \"as is\" without express or implied warranty of any kind. These notices must be retained in any copies of any part of this documentation and/or software. ====================================================================== The following notice applies to \"src/lib/crypto/crypto_tests/t_mddriver.c\": Copyright (C) 1990-2, RSA Data Security, Inc. Created 1990. All rights reserved. RSA Data Security, Inc. makes no representations concerning either the merchantability of this software or the suitability of this software for any particular purpose. It is provided \"as is\" without express or implied warranty of any kind. These notices must be retained in any copies of any part of this documentation and/or software. ====================================================================== Portions of \"src/lib/krb5\" are subject to the following notice: Copyright (C) 1994 CyberSAFE Corporation. Copyright 1990,1991,2007,2008 by the Massachusetts Institute of Technology. All Rights Reserved. Export of this software from the United States of America may require a specific license from the United States Government. It is the responsibility of any person or organization contemplating export to obtain such a license before exporting. WITHIN THAT CONSTRAINT, permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of M.I.T. not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. Furthermore if you modify this software you must label your software as modified software and not distribute it in such a fashion that it might be confused with the original M.I.T. software. Neither M.I.T., the Open Computing Security Group, nor CyberSAFE Corporation make any representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. ====================================================================== Portions contributed by PADL Software are subject to the following license: Copyright (c) 2011, PADL Software Pty Ltd. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of PADL Software nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY PADL SOFTWARE AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL PADL SOFTWARE OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The bundled libev source code is subject to the following license: All files in libev are Copyright (C)2007,2008,2009 Marc Alexander Lehmann. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Alternatively, the contents of this package may be used under the terms of the GNU General Public License (\"GPL\") version 2 or any later version, in which case the provisions of the GPL are applicable instead of the above. If you wish to allow the use of your version of this package only under the terms of the GPL and not to allow others to use your version of this file under the BSD license, indicate your decision by deleting the provisions above and replace them with the notice and other provisions required by the GPL in this and the other files of this package. If you do not delete the provisions above, a recipient may use your version of this file under either the BSD or the GPL. ====================================================================== Files copied from the Intel AESNI Sample Library are subject to the following license: Copyright (C) 2010, Intel Corporation All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Intel Corporation nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The following notice applies to \"src/ccapi/common/win/OldCC/autolock.hxx\": Copyright (C) 1998 by Danilo Almeida. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ====================================================================== The following notice applies to portions of \"src/plugins/preauth/spake/edwards25519.c\" and \"src/plugins/preauth/spake/edwards25519_tables.h\": The MIT License (MIT) Copyright (c) 2015-2016 the fiat-crypto authors (see the AUTHORS file). Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ====================================================================== The following notice applies to portions of \"src/plugins/preauth/spake/edwards25519.c\": Copyright (c) 2015-2016, Google Inc. Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted, provided that the above copyright notice and this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.</p> <p>getML includes unixODBC (https://github.com/lurcher/unixODBC). Copyright \u00a9 2020, the authors (https://github.com/lurcher/unixODBC/blob/master/AUTHORS), licensed under the LGPL (https://www.gnu.org/licenses/lgpl-3.0.html).</p> <p>getML includes com_err from e2fsprogs (https://github.com/tytso/e2fsprogs). Copyright (c) 2020, the authors. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>getML include Go (https://github.com/golang/go). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes crypto (https://github.com/golang/crypto). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes exp (https://github.com/golang/exp). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes net (https://github.com/golang/net). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes sys (https://github.com/golang/sys). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes tools (https://github.com/golang/tools). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes gonum (https://github.com/gonum/gonum). Copyright \u00a92013 The Gonum Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the gonum project nor the names of its authors and contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes gopsutil (https://github.com/shirou/gopsutil). Copyright (c) 2014, WAKAYAMA Shirou All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the gopsutil authors nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes go-ole (https://github.com/go-ole/go-ole). Copyright \u00a9 2013-2017 Yasuhiro Matsumoto, Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>getML includes wmi (https://github.com/StackExchange/wmi). Copyright (c) 2013 Stack Exchange Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>getML includes analytics-go (https://github.com/segmentio/analytics-go). Copyright (c) 2016 Segment, Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>getML includes the AWS SDK for Go (https://github.com/aws/aws-sdk-go). Copyright (c) 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved. Copyright 2014-2015 Stripe, Inc. Licensed under the Apache License, Version 2.0.</p> <p>getML includes JMESpath for Go (https://github.com/jmespath/go-jmespath). Copyright (c) 2015 James Saryerwinnie Licensed under the Apache License, Version 2.0. </p> <p>getML includes go-hdb (https://github.com/SAP/go-hdb), copyright 2014-2021 SAP SE or an SAP affiliate company and go-hdb contributors. Licensed under the Apache License, Version 2.0.</p> <p>getML includes bigquery (https://pkg.go.dev/cloud.google.com/go/bigquery), copyright Copyright 2015 Google LLC. Licensed under the Apache License, Version 2.0.</p> <p>getML includes civil (https://pkg.go.dev/cloud.google.com/go/civil), copyright Copyright 2015 Google LLC. Licensed under the Apache License, Version 2.0.</p> <p>getML includes iam (https://pkg.go.dev/cloud.google.com/go/iam), copyright Copyright 2015 Google LLC. Licensed under the Apache License, Version 2.0.</p> <p>getML includes internal (https://pkg.go.dev/cloud.google.com/go/internal), copyright Copyright 2015 Google LLC. Licensed under the Apache License, Version 2.0.</p> <p>getML includes googleapi (google.golang.org/api/googleapi), copyright Copyright 2015 Google LLC. Licensed under the Apache License, Version 2.0.</p> <p>getML includes iterator (https://pkg.go.dev/google.golang.org/api/iterator), copyright (c) 2011 Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes bigquery/v2 (https://pkg.go.dev/google.golang.org/api/bigquery/v2), copyright (c) 2011 Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes option (https://pkg.go.dev/google.golang.org/api/option), copyright (c) 2011 Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes gax-go (https://pkg.go.dev/github.com/googleapis/gax-go/v2), copyright 2016, Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes crypto (https://github.com/golang/crypto). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes text (https://github.com/golang/text). Copyright (c) 2009 The Go Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>getML includes range-v3 (https://github.com/ericniebler/range-v3), subject to the following licenses: ======================================================== Boost Software License - Version 1.0 - August 17th, 2003 ======================================================== Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ============================================================================== libc++ License ============================================================================== The libc++ library is dual licensed under both the University of Illinois \"BSD-Like\" license and the MIT license. As a user of this code you may choose to use it under either license. As a contributor, you agree to allow your code to be used under both. Full text of the relevant licenses is included below. ============================================================================== University of Illinois/NCSA Open Source License Copyright (c) 2009-2014 by the contributors listed in CREDITS.TXT http://llvm.org/svn/llvm-project/libcxx/trunk/CREDITS.TXT All rights reserved. Developed by: LLVM Team University of Illinois at Urbana-Champaign http://llvm.org Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution. * Neither the names of the LLVM Team, University of Illinois at Urbana-Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE. ============================================================================== Copyright (c) 2009-2014 by the contributors listed in CREDITS.TXT http://llvm.org/svn/llvm-project/libcxx/trunk/CREDITS.TXT Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ============================================================================== Stepanov and McJones, \"Elements of Programming\" license ============================================================================== // Copyright (c) 2009 Alexander Stepanov and Paul McJones // // Permission to use, copy, modify, distribute and sell this software // and its documentation for any purpose is hereby granted without // fee, provided that the above copyright notice appear in all copies // and that both that copyright notice and this permission notice // appear in supporting documentation. The authors make no // representations about the suitability of this software for any // purpose. It is provided \"as is\" without express or implied // warranty. // // Algorithms from // Elements of Programming // by Alexander Stepanov and Paul McJones // Addison-Wesley Professional, 2009 ============================================================================== SGI C++ Standard Template Library license ============================================================================== // Copyright (c) 1994 // Hewlett-Packard Company // // Permission to use, copy, modify, distribute and sell this software // and its documentation for any purpose is hereby granted without fee, // provided that the above copyright notice appear in all copies and // that both that copyright notice and this permission notice appear // in supporting documentation. Hewlett-Packard Company makes no // representations about the suitability of this software for any // purpose. It is provided \"as is\" without express or implied warranty. // // Copyright (c) 1996 // Silicon Graphics Computer Systems, Inc. // // Permission to use, copy, modify, distribute and sell this software // and its documentation for any purpose is hereby granted without fee, // provided that the above copyright notice appear in all copies and // that both that copyright notice and this permission notice appear // in supporting documentation. Silicon Graphics makes no // representations about the suitability of this software for any // purpose. It is provided \"as is\" without express or implied warranty.</p> <p>getML includes Apache Arrow (https://github.com/apache/arrow) Licensed under the Apache License, Version 2.0, with the following 3rd party dependencies: 3rdparty dependency gRPC is statically linked in certain binary distributions, like the python wheels. gRPC has the following license: Copyright 2014 gRPC authors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -------------------------------------------------------------------------------- 3rdparty dependency Apache Thrift is statically linked in certain binary distributions, like the python wheels. Apache Thrift has the following license: Apache Thrift Copyright (C) 2006 - 2019, The Apache Software Foundation This product includes software developed at The Apache Software Foundation (http://www.apache.org/). Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -------------------------------------------------------------------------------- 3rdparty dependency Apache ORC is statically linked in certain binary distributions, like the python wheels. Apache ORC has the following license: Apache ORC Copyright 2013-2019 The Apache Software Foundation This product includes software developed by The Apache Software Foundation (http://www.apache.org/). This product includes software developed by Hewlett-Packard: (c) Copyright [2014-2015] Hewlett-Packard Development Company, L.P Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. -------------------------------------------------------------------------------- 3rdparty dependency zstd is statically linked in certain binary distributions, like the python wheels. ZSTD has the following license: BSD License For Zstandard software Copyright (c) 2016-present, Facebook, Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- 3rdparty dependency lz4 is statically linked in certain binary distributions, like the python wheels. lz4 has the following license: LZ4 Library Copyright (c) 2011-2016, Yann Collet All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- 3rdparty dependency Brotli is statically linked in certain binary distributions, like the python wheels. Brotli has the following license: Copyright (c) 2009, 2010, 2013-2016 by the Brotli Authors. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. -------------------------------------------------------------------------------- 3rdparty dependency rapidjson is statically linked in certain binary distributions, like the python wheels. rapidjson and its dependencies have the following licenses: Tencent is pleased to support the open source community by making RapidJSON available. Copyright (C) 2015 THL A29 Limited, a Tencent company, and Milo Yip. All rights reserved. If you have downloaded a copy of the RapidJSON binary from Tencent, please note that the RapidJSON binary is licensed under the MIT License. If you have downloaded a copy of the RapidJSON source code from Tencent, please note that RapidJSON source code is licensed under the MIT License, except for the third-party components listed below which are subject to different license terms. Your integration of RapidJSON into your own projects may require compliance with the MIT License, as well as the other licenses applicable to the third-party components included within RapidJSON. To avoid the problematic JSON license in your own projects, it's sufficient to exclude the bin/jsonchecker/ directory, as it's the only code under the JSON license. A copy of the MIT License is included in this file. Other dependencies and licenses: Open Source Software Licensed Under the BSD License: -------------------------------------------------------------------- The msinttypes r29 Copyright (c) 2006-2013 Alexander Chemeris All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS <code>AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Open Source Software Licensed Under the JSON License: -------------------------------------------------------------------- json.org Copyright (c) 2002 JSON.org All Rights Reserved. JSON_checker Copyright (c) 2002 JSON.org All Rights Reserved. Terms of the JSON License: --------------------------------------------------- Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. The Software shall be used for Good, not Evil. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Terms of the MIT License: -------------------------------------------------------------------- Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. -------------------------------------------------------------------------------- 3rdparty dependency snappy is statically linked in certain binary distributions, like the python wheels. snappy has the following license: Copyright 2011, Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. === Some of the benchmark data in testdata/ is licensed differently: - fireworks.jpeg is Copyright 2013 Steinar H. Gunderson, and is licensed under the Creative Commons Attribution 3.0 license (CC-BY-3.0). See https://creativecommons.org/licenses/by/3.0/ for more information. - kppkn.gtb is taken from the Gaviota chess tablebase set, and is licensed under the MIT License. See https://sites.google.com/site/gaviotachessengine/Home/endgame-tablebases-1 for more information. - paper-100k.pdf is an excerpt (bytes 92160 to 194560) from the paper \u201cCombinatorial Modeling of Chromatin Features Quantitatively Predicts DNA Replication Timing in _Drosophila_\u201d by Federico Comoglio and Renato Paro, which is licensed under the CC-BY license. See http://www.ploscompbiol.org/static/license for more ifnormation. - alice29.txt, asyoulik.txt, plrabn12.txt and lcet10.txt are from Project Gutenberg. The first three have expired copyrights and are in the public domain; the latter does not have expired copyright, but is still in the public domain according to the license information (http://www.gutenberg.org/ebooks/53). -------------------------------------------------------------------------------- 3rdparty dependency gflags is statically linked in certain binary distributions, like the python wheels. gflags has the following license: Copyright (c) 2006, Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- 3rdparty dependency glog is statically linked in certain binary distributions, like the python wheels. glog has the following license: Copyright (c) 2008, Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. A function gettimeofday in utilities.cc is based on http://www.google.com/codesearch/p?hl=en#dR3YEbitojA/COPYING&amp;q=GetSystemTimeAsFileTime%20license:bsd The license of this code is: Copyright (c) 2003-2008, Jouni Malinen and contributors All Rights Reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name(s) of the above-listed copyright holder(s) nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- 3rdparty dependency re2 is statically linked in certain binary distributions, like the python wheels. re2 has the following license: Copyright (c) 2009 The RE2 Authors. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- 3rdparty dependency c-ares is statically linked in certain binary distributions, like the python wheels. c-ares has the following license: # c-ares license Copyright (c) 2007 - 2018, Daniel Stenberg with many contributors, see AUTHORS file. Copyright 1998 by the Massachusetts Institute of Technology. Permission to use, copy, modify, and distribute this software and its documentation for any purpose and without fee is hereby granted, provided that the above copyright notice appear in all copies and that both that copyright notice and this permission notice appear in supporting documentation, and that the name of M.I.T. not be used in advertising or publicity pertaining to distribution of the software without specific, written prior permission. M.I.T. makes no representations about the suitability of this software for any purpose. It is provided \"as is\" without express or implied warranty. -------------------------------------------------------------------------------- 3rdparty dependency zlib is redistributed as a dynamically linked shared library in certain binary distributions, like the python wheels. In the future this will likely change to static linkage. zlib has the following license: zlib.h -- interface of the 'zlib' general purpose compression library version 1.2.11, January 15th, 2017 Copyright (C) 1995-2017 Jean-loup Gailly and Mark Adler This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software. Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions: 1. The origin of this software must not be misrepresented; you must not claim that you wrote the original software. If you use this software in a product, an acknowledgment in the product documentation would be appreciated but is not required. 2. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software. 3. This notice may not be removed or altered from any source distribution. Jean-loup Gailly Mark Adler jloup@gzip.org madler@alumni.caltech.edu -------------------------------------------------------------------------------- 3rdparty dependency openssl is redistributed as a dynamically linked shared library in certain binary distributions, like the python wheels. openssl preceding version 3 has the following license: LICENSE ISSUES ============== The OpenSSL toolkit stays under a double license, i.e. both the conditions of the OpenSSL License and the original SSLeay license apply to the toolkit. See below for the actual license texts. OpenSSL License --------------- /* ==================================================================== * Copyright (c) 1998-2019 The OpenSSL Project. All rights reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * 1. Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * 2. Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in * the documentation and/or other materials provided with the * distribution. * * 3. All advertising materials mentioning features or use of this * software must display the following acknowledgment: * \"This product includes software developed by the OpenSSL Project * for use in the OpenSSL Toolkit. (http://www.openssl.org/)\" * * 4. The names \"OpenSSL Toolkit\" and \"OpenSSL Project\" must not be used to * endorse or promote products derived from this software without * prior written permission. For written permission, please contact * openssl-core@openssl.org. * * 5. Products derived from this software may not be called \"OpenSSL\" * nor may \"OpenSSL\" appear in their names without prior written * permission of the OpenSSL Project. * * 6. Redistributions of any form whatsoever must retain the following * acknowledgment: * \"This product includes software developed by the OpenSSL Project * for use in the OpenSSL Toolkit (http://www.openssl.org/)\" * * THIS SOFTWARE IS PROVIDED BY THE OpenSSL PROJECT</code>AS IS'' AND ANY * EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE OpenSSL PROJECT OR * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED * OF THE POSSIBILITY OF SUCH DAMAGE. * ==================================================================== * * This product includes cryptographic software written by Eric Young * (eay@cryptsoft.com). This product includes software written by Tim * Hudson (tjh@cryptsoft.com). * / Original SSLeay License ----------------------- / Copyright (C) 1995-1998 Eric Young (eay@cryptsoft.com) * All rights reserved. * * This package is an SSL implementation written * by Eric Young (eay@cryptsoft.com). * The implementation was written so as to conform with Netscapes SSL. * * This library is free for commercial and non-commercial use as long as * the following conditions are aheared to. The following conditions * apply to all code found in this distribution, be it the RC4, RSA, * lhash, DES, etc., code; not just the SSL code. The SSL documentation * included with this distribution is covered by the same copyright terms * except that the holder is Tim Hudson (tjh@cryptsoft.com). * * Copyright remains Eric Young's, and as such any Copyright notices in * the code are not to be removed. * If this package is used in a product, Eric Young should be given attribution * as the author of the parts of the library used. * This can be in the form of a textual message at program startup or * in documentation (online or textual) provided with the package. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * 1. Redistributions of source code must retain the copyright * notice, this list of conditions and the following disclaimer. * 2. Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * 3. All advertising materials mentioning features or use of this software * must display the following acknowledgement: * \"This product includes cryptographic software written by * Eric Young (eay@cryptsoft.com)\" * The word 'cryptographic' can be left out if the rouines from the library * being used are not cryptographic related :-). * 4. If you include any Windows specific code (or a derivative thereof) from * the apps directory (application code) you must include an acknowledgement: * \"This product includes software written by Tim Hudson (tjh@cryptsoft.com)\" * * THIS SOFTWARE IS PROVIDED BY ERIC YOUNG ``AS IS'' AND * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF * SUCH DAMAGE. * * The licence and distribution terms for any publically available version or * derivative of this code cannot be changed. i.e. this code cannot simply be * copied and put under another distribution licence * [including the GNU Public Licence.] / -------------------------------------------------------------------------------- This project includes code from the rtools-backports project. * ci/scripts/PKGBUILD and ci/scripts/r_windows_build.sh are based on code from the rtools-backports project. Copyright: Copyright (c) 2013 - 2019, \u0410\u043b\u0435\u043a\u0441\u0435\u0439 and Jeroen Ooms. All rights reserved. Homepage: https://github.com/r-windows/rtools-backports License: 3-clause BSD -------------------------------------------------------------------------------- Some code from pandas has been adapted for the pyarrow codebase. pandas is available under the 3-clause BSD license, which follows: pandas license ============== Copyright (c) 2011-2012, Lambda Foundry, Inc. and PyData Development Team All rights reserved. Copyright (c) 2008-2011 AQR Capital Management, LLC All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. -------------------------------------------------------------------------------- Some bits from DyND, in particular aspects of the build system, have been adapted from libdynd and dynd-python under the terms of the BSD 2-clause license The BSD 2-Clause License Copyright (C) 2011-12, Dynamic NDArray Developers All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Dynamic NDArray Developers list: * Mark Wiebe * Continuum Analytics -------------------------------------------------------------------------------- Some source code from Ibis (https://github.com/cloudera/ibis) has been adapted for PyArrow. Ibis is released under the Apache License, Version 2.0. -------------------------------------------------------------------------------- This project includes code from the autobrew project. * r/tools/autobrew and dev/tasks/homebrew-formulae/autobrew/apache-arrow.rb are based on code from the autobrew project. Copyright (c) 2019, Jeroen Ooms License: MIT Homepage: https://github.com/jeroen/autobrew -------------------------------------------------------------------------------- dev/tasks/homebrew-formulae/apache-arrow.rb has the following license: BSD 2-Clause License Copyright (c) 2009-present, Homebrew contributors All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ---------------------------------------------------------------------- cpp/src/arrow/vendored/base64.cpp has the following license ZLIB License Copyright (C) 2004-2017 Ren\u00e9 Nyffenegger This source code is provided 'as-is', without any express or implied warranty. In no event will the author be held liable for any damages arising from the use of this software. Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions: 1. The origin of this source code must not be misrepresented; you must not claim that you wrote the original source code. If you use this source code in a product, an acknowledgment in the product documentation would be appreciated but is not required. 2. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original source code. 3. This notice may not be removed or altered from any source distribution. Ren\u00e9 Nyffenegger rene.nyffenegger@adp-gmbh.ch -------------------------------------------------------------------------------- The file cpp/src/arrow/vendored/optional.hpp has the following license Boost Software License - Version 1.0 - August 17th, 2003 Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. -------------------------------------------------------------------------------- The file cpp/src/arrow/vendored/musl/strptime.c has the following license Copyright \u00a9 2005-2020 Rich Felker, et al. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. -------------------------------------------------------------------------------- The file cpp/cmake_modules/BuildUtils.cmake contains code from https://gist.github.com/cristianadam/ef920342939a89fae3e8a85ca9459b49 which is made available under the MIT license Copyright (c) 2019 Cristian Adam Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. -------------------------------------------------------------------------------- The files in cpp/src/arrow/vendored/portable-snippets/ contain code from https://github.com/nemequ/portable-snippets and have the following copyright notice: Each source file contains a preamble explaining the license situation for that file, which takes priority over this file. With the exception of some code pulled in from other repositories (such as \u00b5nit, an MIT-licensed project which is used for testing), the code is public domain, released using the CC0 1.0 Universal dedication (). (*) https://creativecommons.org/publicdomain/zero/1.0/legalcode -------------------------------------------------------------------------------- The files in cpp/src/arrow/vendored/fast_float/ contain code from https://github.com/lemire/fast_float which is made available under the Apache License 2.0.</p>"},{"location":"integration/fastapi/fastapi/","title":"FastAPI","text":""},{"location":"integration/fastapi/fastapi/#provide-generic-prediction-endpoint-via-fastapi","title":"Provide generic prediction endpoint via FastAPI","text":"<p>A common way to communicate with resources is via REST-APIs. Under Python FastAPI is a well known web framework package to build web-APIs.</p> <p>The following shows an example how easy pipelines in a project can be made accessible via endpoints in FastAPI.</p> <p>It is assumed that you have some basic knowledge of FastAPI and the getML framework.</p> <p>Helpful resources to get started:</p> <p>FastAPI get started getML example notebooks getML documentation </p> <p>This integration example requires at least v1.4.0 of the getml package and at least Python 3.8.</p>"},{"location":"integration/fastapi/fastapi/#example-data","title":"Example Data","text":"<p>As an example project we first run the demo notebook \"Loan default prediction\" which creates a project named \"loans\" in the getML engine.</p>"},{"location":"integration/fastapi/fastapi/#code-explained","title":"Code Explained","text":"<p>First, import the necessary packages, create a FastAPI-App <code>app</code>. If the engine isn't running yet  (<code>getml.engine.is_engine_alive()</code>) launch the getML engine  (<code>getml.engine.launch()</code>). The <code>launch_browser=False</code> option prevents the browser to be opened  when the engine spins up. Further, direct the engine to load and set the previously created  <code>project</code> \"loans\". (<code>getml.engine.set_project()</code>)</p> <pre><code>from typing import Dict, List, Optional, Union\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom uvicorn import run\nfrom getml import engine, pipeline, Pipeline, DataFrame\n\napp: FastAPI = FastAPI()\n\nif not engine.is_alive():\n    engine.launch(launch_browser=False)\nengine.set_project(\"loans\")\n</code></pre> <p>Create the first GET endpoint which returns a list with all <code>pipeline</code>s present (<code>list_pipelines()</code>) in the project. The list itself will only contain the names of the pipelines and no additional metainformation. For sake of simplicity of the tutorial pagination is left out.</p> <pre><code>@app.get(\"/pipeline\")\nasync def get_pipeline() -&gt; List[str]:\n    return pipeline.list_pipelines()\n</code></pre> <p>The following is required to start the app with uvicorn. Run your Python code and test the endpoint via localhost:8080/pipeline.</p> <pre><code>if __name__ == \"__main__\":\n    run(app, host=\"localhost\", port=8080)\n</code></pre> <p>To expand the functionality, add another informative GET endpoint for a single pipeline. The <code>pipeline_id</code> can be retrieved from the previously created GET endpoint. The existence of the pipeline can be checked using <code>exists()</code>. After the existence validation the engine must be directed to load the pipeline identified with the provided <code>pipeline_id</code>. Information of interest could be the name of the population data frame and peripheral data frames, the applied preprocessors, used feature learners and selectors and target predictors. Those information can be retrieved from the member variable <code>metadata</code> of the pipeline (<code>pipeline_.metadata</code>) and the pipeline itself.  Again this endpoint can be tested by running your code and invoking the endpoint localhost:8080/pipeline/a1b2c3 assuming that the previously created pipeline has the id <code>a1b2c3</code>.</p> <pre><code>@app.get(\"/pipeline/{pipeline_id}\")\nasync def get_pipeline_pipeline_id(pipeline_id: str) -&gt; Dict[str, Union[str, List[str]]]:\n    if not pipeline.exists(pipeline_id):\n        raise HTTPException(status_code=404, detail=f'Pipeline {pipeline_id} not found.')\n\n    pipeline_ = pipeline.load(pipeline_id)\n\n    if pipeline_.metadata is None:\n        raise HTTPException(status_code=409,\n                            detail='The data schema is missing or pipeline is incomplete')\n\n    meta_data = pipeline_.metadata\n    metadata: Dict[str, Union[str, List[str]]] = {}\n    metadata[\"data_model\"] = meta_data.population.name\n    metadata[\"peripheral\"] = [_.name for _ in meta_data.peripheral]\n    metadata[\"preprocessors\"] = [_.type for _ in pipeline_.preprocessors]\n    metadata[\"feature_learners\"] = [_.type for _ in pipeline_.feature_learners]\n    metadata[\"feature_selectors\"] = [_.type for _ in pipeline_.feature_selectors]\n    metadata[\"predictors\"] = [_.type for _ in pipeline_.predictors]\n\n    return metadata\n</code></pre> <p>To create the prediction endpoint the data scheme for the request body needs to be created first. For a prediction the getML engine requires multiple data sets, the population data set <code>population</code> and any related peripheral data set <code>peripheral</code> based on the Data model of the pipeline. The peripheral data sets can be either a list or a dictionary where the order of the data sets in the list needs to match the order returned by <code>[_.name for _ in getml.pipeline.metadata.peripheral]</code>. This information can also be retrieved by calling the previously created GET endpoint.</p> <pre><code>class PredictionBody(BaseModel):\n    peripheral: Union[List[Dict[str, List]], Dict[str, Dict[str, List]]]\n    population: Dict[str, List]\n</code></pre> <p>Next up, implement the POST endpoint which accepts data to task the engine to make a prediction. Validate that the pipeline exist, load the pipeline (<code>load()</code>), and validate that the pipeline has been finalized.</p> <pre><code>@app.post(\"/pipeline/{pipeline_id}/predict\")\nasync def post_project_predict(pipeline_id: str, body: PredictionBody) -&gt; Optional[List]:\n    if not pipeline.exists(pipeline_id):\n        raise HTTPException(status_code=404,\n                            detail=f'Pipeline {pipeline_id} not found.')\n\n    pipeline_: Pipeline = pipeline.load(pipeline_id)\n\n    if pipeline_.metadata is None:\n        raise HTTPException(status_code=409,\n                            detail='The data schema is missing or pipeline is incomplete')\n</code></pre> <p>The request body should contain both the population and peripheral data. Check that the population in the request body contains any content. Create a data frame from the dictionary (<code>from_dict()</code>): the name of the data frame must not collide with an existing data frame in the pipeline, the roles of the population can be obtained from the pipeline, using <code>pipeline_.metadata.population.roles</code>.</p> <pre><code>if not body.population:\n    raise HTTPException(status_code=400, detail='Missing population data.')\n\npopulation_data_frame = DataFrame.from_dict(name='future',\n                                            roles=pipeline_.metadata.population.roles,\n                                            data=body.population)\n</code></pre> <p>The peripheral can be submitted in the request body both as list and dictionary. Check that in case the peripheral data sets are received as dictionaries that the names of all required peripheral data sets exist in the dictionary keys, and in case the peripheral data sets are received as a list check that the length of the list matches the number of peripheral data sets in the pipeline. After, create a list of data frames of the peripheral data. Again, ensure that the names of the created data frames do not collide with existing data frames and use the roles defined in the pipeline for the peripheral data sets (<code>pipeline_.metadata.peripheral[i].roles</code>).</p> <pre><code>peripheral_names = [_.name for _ in pipeline_.peripheral]\n\nif isinstance(body.peripheral, dict):\n    if set(peripheral_names) - set(body.peripheral.keys()):\n        raise HTTPException(\n            status_code=400,\n            detail=f'Missing peripheral data, expected {peripheral_names}')\n    periperal_raw_data = body.peripheral\nelse:\n    if len(peripheral_names) != len(body.peripheral):\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Expected {len(pipeline_.peripheral)} peripheral data frames.\")\n    periperal_raw_data = dict(zip(peripheral_names, body.peripheral))\n\nperipheral_data_frames = [\n    DataFrame.from_dict(name=name + '_predict',\n                        data=periperal_raw_data[name],\n                        roles=pipeline_.metadata.peripheral[i].roles)\n    for i, name in enumerate(peripheral_names)\n]\n</code></pre> <p>This leaves the actual call to the engine to make a prediction (<code>predict()</code>) using the previously created population data frame and peripheral data frames. The predicted target value is a numpy array and returned transformed to a list as request response. </p> <pre><code>prediction = pipeline_.predict(\n    population_table=population_data_frame,\n    peripheral_tables=peripheral_data_frames\n)\n\nif prediction:\n    return prediction.tolist()\n\nraise HTTPException(status_code=500, detail='GetML-Engine didn\\'t return a result.')\n</code></pre> <p>This endpoint can be called on localhost:8080/pipeline/a1b2c3/predict. where the body needs the form: </p> <pre><code>{\n    \"peripheral\": [{\n        \"column_1\": [2.4, 3.0, 1.2, 1.4, 2.2],\n        \"column_2\": [\"a\", \"b\", \"a\", \"b\", \"b\"]\n    }],\n    \"population\": {\n        \"column_1\": [0.2, 0.1],\n        \"column_2\": [\"a\", \"b\"],\n        \"time_stamp\": [\"2010-01-01 12:30:00\", \"2010-01-01 23:30:00\"]\n    }\n}\n</code></pre> <p>Example json data can be extracted from the notebook using the following code snippet at the end of the notebook used to create the Example Data.</p> <pre><code>from typing import Union, Any\nfrom datetime import datetime\nfrom json import dumps\n\n\ndef handle_timestamp(x: Union[Any, datetime]):\n    if isinstance(x, datetime):\n        return x.strftime(r'%Y-%m-%d %H:%M:%S')\n\n\npd_population_test = population_test.to_pandas()\naccount_id = pd_population_test.iloc[0][\"account_id\"]\npopulaton_dict = pd_population_test[pd_population_test[\"account_id\"] == account_id].to_dict()\npopulaton_json = dumps({k: list(v.values()) for k, v in populaton_dict.items()}, default=handle_timestamp)\npd_peripherals = {_.name: _.to_pandas() for _ in [order, trans, meta]}\nperipheral_dict = {k: v[v[\"account_id\"] == account_id].to_dict() for k, v in pd_peripherals.items()}\nperipheral_json = dumps(\n    {k: {vk: list(vv.values()) for vk, vv in v.items()} for k, v in peripheral_dict.items()},\n    default=handle_timestamp)\npopulaton_json\nperipheral_json\n</code></pre>"},{"location":"integration/fastapi/fastapi/#conclusion","title":"Conclusion","text":"<p>With only a few lines it is possible to create a web API to make project pipelines accessible and request target predictions for provided population and peripheral data.</p>"},{"location":"reference/","title":"Index","text":""},{"location":"reference/#python-api_1","title":"Python API","text":"<p>Welcome to the API documentation for Python. The Python API is a convenient, easy to use interface to the getML engine. General information about the interoperation of the different parts of getML can be found in the user guide.</p> <p>If you have never used the Python API, it is probably easiest to start with the getting started guide.</p> <p>toc to be added: https://docs.getml.com/latest/api_reference/index.html#python-api</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>communication </li> <li>constants </li> <li>cross_validation </li> <li>data <ul> <li>access </li> <li>columns <ul> <li>aggregation </li> <li>collect_footer_data </li> <li>column </li> <li>columns </li> <li>constants </li> <li>format </li> <li>from_value </li> <li>get_scalar </li> <li>last_change </li> <li>last_change_from_col </li> <li>length </li> <li>length_property </li> <li>make_iter </li> <li>parse </li> <li>random </li> <li>repr </li> <li>repr_html </li> <li>subroles </li> <li>to_arrow </li> <li>to_numpy </li> <li>unique </li> <li>unit </li> </ul> </li> <li>concat </li> <li>container </li> <li>data_frame </li> <li>data_model </li> <li>diagram </li> <li>helpers </li> <li>helpers2 </li> <li>load_container </li> <li>placeholder </li> <li>relationship </li> <li>roles </li> <li>roles_obj </li> <li>split <ul> <li>concat </li> <li>random </li> <li>time </li> </ul> </li> <li>staging </li> <li>star_schema </li> <li>subroles <ul> <li>exclude </li> <li>include </li> <li>only </li> </ul> </li> <li>subset </li> <li>time </li> <li>time_series </li> <li>view </li> </ul> </li> <li>database <ul> <li>connect_bigquery </li> <li>connect_greenplum </li> <li>connect_hana </li> <li>connect_mariadb </li> <li>connect_mysql </li> <li>connect_odbc </li> <li>connect_postgres </li> <li>connect_sqlite3 </li> <li>connection </li> <li>copy_table </li> <li>drop_table </li> <li>execute </li> <li>get </li> <li>get_colnames </li> <li>helpers </li> <li>list_connections </li> <li>list_tables </li> <li>read_csv </li> <li>read_s3 </li> <li>sniff_csv </li> <li>sniff_s3 </li> </ul> </li> <li>datasets <ul> <li>base </li> <li>samples_generator </li> </ul> </li> <li>engine <ul> <li>helpers </li> <li>launch </li> </ul> </li> <li>feature_learning <ul> <li>aggregations </li> <li>fastboost </li> <li>fastprop </li> <li>feature_learner </li> <li>loss_functions </li> <li>multirel </li> <li>relboost </li> <li>relmt </li> <li>validation </li> </ul> </li> <li>helpers </li> <li>hyperopt <ul> <li>burn_in </li> <li>helpers </li> <li>hyperopt </li> <li>kernels </li> <li>load_hyperopt </li> <li>optimization </li> <li>tuning </li> <li>validation </li> </ul> </li> <li>log </li> <li>pipeline <ul> <li>column </li> <li>columns </li> <li>dialect </li> <li>feature </li> <li>features </li> <li>helpers </li> <li>helpers2 </li> <li>issues </li> <li>metadata </li> <li>metrics </li> <li>pipeline </li> <li>plots </li> <li>score </li> <li>scores_container </li> <li>sql_code </li> <li>sql_string </li> <li>table </li> <li>tables </li> <li>tags </li> </ul> </li> <li>predictors <ul> <li>linear_regression </li> <li>logistic_regression </li> <li>predictor </li> <li>scale_gbm_classifier </li> <li>scale_gbm_regressor </li> <li>xgboost_classifier </li> <li>xgboost_regressor </li> </ul> </li> <li>preprocessors <ul> <li>category_trimmer </li> <li>email_domain </li> <li>imputation </li> <li>mapping </li> <li>preprocessor </li> <li>seasonal </li> <li>substring </li> <li>text_field_splitter </li> <li>validate </li> </ul> </li> <li>progress_bar </li> <li>project <ul> <li>attrs </li> <li>containers <ul> <li>data_frames </li> <li>hyperopts </li> <li>pipelines </li> </ul> </li> </ul> </li> <li>spark </li> <li>sqlite3 <ul> <li>connect </li> <li>contains </li> <li>count_above_mean </li> <li>count_below_mean </li> <li>count_distinct_over_count </li> <li>email_domain </li> <li>ewma </li> <li>ewma_trend </li> <li>execute </li> <li>first </li> <li>get_word </li> <li>helpers </li> <li>kurtosis </li> <li>last </li> <li>median </li> <li>mode </li> <li>num_max </li> <li>num_min </li> <li>num_words </li> <li>quantiles </li> <li>read_csv </li> <li>read_list </li> <li>read_pandas </li> <li>skew </li> <li>sniff_csv </li> <li>sniff_pandas </li> <li>split_text_field </li> <li>stddev </li> <li>time_since_first_maximum </li> <li>time_since_first_minimum </li> <li>time_since_last_maximum </li> <li>time_since_last_minimum </li> <li>to_list </li> <li>to_pandas </li> <li>trend </li> <li>var </li> <li>variation_coefficient </li> </ul> </li> <li>utilities <ul> <li>formatting <ul> <li>cell_formatter </li> <li>column_formatter </li> <li>data_frame_formatter </li> <li>ellipsis </li> <li>formatter </li> <li>helpers </li> <li>signature_formatter </li> <li>view_formatter </li> </ul> </li> <li>templates </li> </ul> </li> <li>version </li> </ul>"},{"location":"reference/cli/","title":"CLI","text":""},{"location":"reference/cli/#command-line-interface","title":"Command Line Interface","text":"<p>getML can be launched via the command line. The command line interface is called <code>getML</code> on Linux, <code>getml-cli</code> on macOS, and <code>getML.exe</code> on Windows.</p> <p>Refer to the installation section, for instructions on how to set up  the command line interface.</p> <p>Some parameters can be set via command line flags. If you do not explicitly set them, the values from your config.json are taken instead. The config.json is located  in <code>$HOME/.getML/getml-VERSION</code> on Linux and macOS. On Windows, it is located in the  same directory as <code>getML.exe</code>. The most elegant way to edit your config.json is via  the configuration view in the getML monitor:</p> <p>The help menu can also be displayed by passing the flag <code>-help</code> or <code>-h</code>. The default values displayed in the help menu are the values in the config.json (therefore, they are not hard-coded).</p> <pre><code>usage: ./getML &lt;command&gt; [&lt;args&gt;] or ./getML [&lt;args&gt;].\n</code></pre> <pre><code>Possible commands are:\n run        Runs getML. Type \"./getML -h\" or \"./getML run -h\" to display the arguments. \"run\" is executed by default.\n install    Installs getML.\n stop       Stops a running instance of getML. Type \"./getML stop -h\" to display the arguments.\n uninstall  Uninstalls getml-0.14-beta-macos.\n version    Prints the version (getml-0.14-beta-macos).\n</code></pre> <p><pre><code>Usage of run:\n  -allow-push-notifications\n        Whether you want to allow the getML monitor to send push notifications to your desktop. (default true)\n  -http-port int\n        The local port of the getML monitor. This port can only be accessed from your local computer. (default 1709)\n  -https-port int\n        The remote and encrypted port of the getML monitor. This port can be accessed remotely, but it is encrypted. (default 1710)\n  -install\n        Installs getml-0.14-beta-macos, even if it is already installed.\n  -launch-browser\n        Whether you want to automatically launch your browser. (default true)\n  -project-directory string\n        The directory in which to store all of your projects. (default \"~/.getML/getml-0.14-beta-macos/projects\")\n  -proxy-url string\n        The URL of any proxy server that that redirects to the getML monitor.\n  -tcp-port int\n        Local TCP port which serves as the communication point for the engine. This port can only be accessed from your local computer. (default 1711)\n</code></pre> <pre><code>Usage of stop:\n  -tcp-port int\n        The TCP port of the getML instance you would like to stop. (default 1711)\n</code></pre></p>"},{"location":"reference/version/","title":"Version","text":""},{"location":"reference/version/#getml.version","title":"getml.version","text":""},{"location":"reference/data/","title":"Index","text":""},{"location":"reference/data/#getml.data","title":"getml.data","text":"<p>Contains functionalities for importing, handling, and retrieving data from the getML engine.</p> <p>All data relevant for the getML suite has to be present in the getML engine. Its Python API itself does not store any of the data used for training or prediction. Instead, it provides a handler class for the data frame objects in the getML engine, the <code>DataFrame</code>. Either using this overall handler for the underlying data set or the individual <code>columns</code> it is composed of, one can both import and retrieve data from the engine as well as performing operations on them. In addition to the data frame objects, the engine also uses an abstract and lightweight version of the underlying data model, which is represented by the <code>Placeholder</code>.</p> <p>In general, working with data within the getML suite is organized in three different steps.</p> <ul> <li>Importing the data into the getML engine .</li> <li>Annotating the data by assigning   <code>roles</code> to the individual <code>columns</code></li> <li>Constructing the data model by deriving   <code>Placeholder</code> from the data and joining them to   represent the data schema.</li> </ul> Example <p>Creating a new data frame object in the getML engine and importing data is done by one of the class methods <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p>In this example we chose to directly load data from a public database in the internet. But, firstly, we have to connect the getML engine to the database (see MySQL interface in the user guide for further details).</p> <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"financial\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\",\n    time_formats=['%Y/%m/%d']\n)\n</code></pre> <p>Using the established connection, we can tell the engine to construct a new data frame object called <code>df_loan</code>, fill it with the data of <code>loan</code> table contained in the MySQL database, and return a <code>DataFrame</code> handler associated with it.</p> <p><pre><code>loan = getml.DataFrame.from_db('loan', 'df_loan')\n\nprint(loan)\n</code></pre> <pre><code>| loan_id      | account_id   | amount       | duration     | date          | payments      | status        |\n| unused float | unused float | unused float | unused float | unused string | unused string | unused string |\n-------------------------------------------------------------------------------------------------------------\n| 4959         | 2            | 80952        | 24           | 1994-01-05    | 3373.00       | A             |\n| 4961         | 19           | 30276        | 12           | 1996-04-29    | 2523.00       | B             |\n| 4962         | 25           | 30276        | 12           | 1997-12-08    | 2523.00       | A             |\n| 4967         | 37           | 318480       | 60           | 1998-10-14    | 5308.00       | D             |\n| 4968         | 38           | 110736       | 48           | 1998-04-19    | 2307.00       | C             |\n</code></pre> In order to construct the data model and for the feature learning algorithm to get the most out of your data, you have to assign roles to columns using the <code>set_role</code> method (see Annotating data for details).</p> <p>(For demonstration purposes, we assign <code>payments</code> the <code>target role</code>. In reality, you would want to forecast the defaulting behaviour, which is encoded in the <code>status</code> column. See the loans notebook.)</p> <p><pre><code>loan.set_role([\"duration\", \"amount\"], getml.data.roles.numerical)\nloan.set_role([\"loan_id\", \"account_id\"], getml.data.roles.join_key)\nloan.set_role(\"date\", getml.data.roles.time_stamp)\nloan.set_role([\"payments\"], getml.data.roles.target)\n\nprint(loan)\n</code></pre> <pre><code>| date                        | loan_id  | account_id | payments  | duration  | amount    | status        |\n| time stamp                  | join key | join key   | target    | numerical | numerical | unused string |\n-----------------------------------------------------------------------------------------------------------\n| 1994-01-05T00:00:00.000000Z | 4959     | 2          | 3373      | 24        | 80952     | A             |\n| 1996-04-29T00:00:00.000000Z | 4961     | 19         | 2523      | 12        | 30276     | B             |\n| 1997-12-08T00:00:00.000000Z | 4962     | 25         | 2523      | 12        | 30276     | A             |\n| 1998-10-14T00:00:00.000000Z | 4967     | 37         | 5308      | 60        | 318480    | D             |\n| 1998-04-19T00:00:00.000000Z | 4968     | 38         | 2307      | 48        | 110736    | C             |\n</code></pre> Finally, we are able to construct the data model by deriving <code>Placeholder</code> from each <code>DataFrame</code> and establishing relations between them using the <code>join</code> method.</p> <pre><code># But, first, we need a second data set to build a data model.\ntrans = getml.DataFrame.from_db(\n    'trans', 'df_trans',\n    roles = {getml.data.roles.numerical: [\"amount\", \"balance\"],\n             getml.data.roles.categorical: [\"type\", \"bank\", \"k_symbol\",\n                                            \"account\", \"operation\"],\n             getml.data.roles.join_key: [\"account_id\"],\n             getml.data.roles.time_stamp: [\"date\"]\n    }\n)\n\nph_loan = loan.to_placeholder()\nph_trans = trans.to_placeholder()\n\nph_loan.join(ph_trans, on=\"account_id\",\n            time_stamps=\"date\")\n</code></pre> <p>The data model contained in <code>ph_loan</code> can now be used to construct a <code>Pipeline</code>.</p>"},{"location":"reference/data/#getml.data.arange","title":"arange","text":"<pre><code>arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n)\n</code></pre> <p>Returns evenly spaced variables, within a given interval.</p> PARAMETER DESCRIPTION <code>start</code> <p>The beginning of the interval. Defaults to 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>stop</code> <p>The end of the interval.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The step taken. Defaults to 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start:\n            The beginning of the interval. Defaults to 0.\n\n        stop:\n            The end of the interval.\n\n        step:\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/#getml.data.rowid","title":"rowid","text":"<pre><code>rowid()\n</code></pre> <p>Get the row numbers of the table.</p> RETURNS DESCRIPTION <p><code>FloatColumnView</code>: (numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid():\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/#getml.data.list_data_frames","title":"list_data_frames","text":"<pre><code>list_data_frames() -&gt; Dict[str, List[str]]\n</code></pre> <p>Lists all available data frames of the project.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dict containing lists of strings representing the names of the data frames objects</p> <ul> <li>'in_memory'     held in memory (RAM).</li> <li>'on_disk'     stored on disk.</li> </ul> <p> TYPE: <code>Dict[str, List[str]]</code> </p> Example <pre><code>d, _ = getml.datasets.make_numerical()\ngetml.data.list_data_frames()\nd.save()\ngetml.data.list_data_frames()\n</code></pre> Source code in <code>getml/data/helpers.py</code> <pre><code>def list_data_frames() -&gt; Dict[str, List[str]]:\n    \"\"\"Lists all available data frames of the project.\n\n    Returns:\n        dict:\n            Dict containing lists of strings representing the names of\n            the data frames objects\n\n            - 'in_memory'\n                held in memory (RAM).\n            - 'on_disk'\n                stored on disk.\n\n    Example:\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        getml.data.list_data_frames()\n        d.save()\n        getml.data.list_data_frames()\n        ```\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_data_frames\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)\n</code></pre>"},{"location":"reference/data/#getml.data.delete","title":"delete","text":"<pre><code>delete(name: str)\n</code></pre> <p>If a data frame named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def delete(name: str):\n    \"\"\"\n    If a data frame named 'name' exists, it is deleted.\n\n    Args:\n        name:\n            Name of the data frame.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/data/#getml.data.exists","title":"exists","text":"<pre><code>exists(name: str)\n</code></pre> <p>Returns true if a data frame named 'name' exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def exists(name: str):\n    \"\"\"\n    Returns true if a data frame named 'name' exists.\n\n    Args:\n        name:\n            Name of the data frame.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_df = list_data_frames()\n\n    return name in (all_df[\"in_memory\"] + all_df[\"on_disk\"])\n</code></pre>"},{"location":"reference/data/#getml.data.load_data_frame","title":"load_data_frame","text":"<pre><code>load_data_frame(name: str) -&gt; DataFrame\n</code></pre> <p>Retrieves a <code>DataFrame</code> handler of data in the getML engine.</p> <p>A data frame object can be loaded regardless if it is held in memory or not. It only has to be present in the current project and thus listed in the output of <code>list_data_frames</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handle the underlying data frame in the getML engine.</p> <p>Examples:</p> <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\nd2 = getml.data.load_data_frame('test')\n</code></pre> Source code in <code>getml/data/helpers2.py</code> <pre><code>def load_data_frame(name: str) -&gt; DataFrame:\n    \"\"\"Retrieves a [`DataFrame`][getml.DataFrame] handler of data in the\n    getML engine.\n\n    A data frame object can be loaded regardless if it is held in\n    memory or not. It only has to be present in the current project\n    and thus listed in the output of\n    [`list_data_frames`][getml.data.list_data_frames].\n\n    Args:\n        name:\n            Name of the data frame.\n\n    Returns:\n            Handle the underlying data frame in the getML engine.\n\n    Examples:\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        d2 = getml.data.load_data_frame('test')\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    data_frames_available = list_data_frames()\n\n    if name in data_frames_available[\"in_memory\"]:\n        return DataFrame(name).refresh()\n\n    if name in data_frames_available[\"on_disk\"]:\n        return DataFrame(name).load()\n\n    raise ValueError(\n        \"No data frame holding the name '\" + name + \"' present on the getML engine.\"\n    )\n</code></pre>"},{"location":"reference/data/#getml.data.make_target_columns","title":"make_target_columns","text":"<pre><code>make_target_columns(\n    base: Union[DataFrame, View], colname: str\n) -&gt; View\n</code></pre> <p>Returns a view containing binary target columns.</p> <p>getML expects binary target columns for classification problems. This helper function allows you to split up a column into such binary target columns.</p> PARAMETER DESCRIPTION <code>base</code> <p>The original view or data frame. <code>base</code> will remain unaffected by this function, instead you will get a view with the appropriate changes.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>colname</code> <p>The column you would like to split. A column named <code>colname</code> should appear on <code>base</code>.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A view containing binary target columns.</p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def make_target_columns(base: Union[DataFrame, View], colname: str) -&gt; View:\n    \"\"\"\n    Returns a view containing binary target columns.\n\n    getML expects binary target columns for classification problems. This\n    helper function allows you to split up a column into such binary\n    target columns.\n\n    Args:\n        base:\n            The original view or data frame. `base` will remain unaffected\n            by this function, instead you will get a view with the appropriate\n            changes.\n\n        colname: The column you would like to split. A column named\n            `colname` should appear on `base`.\n\n    Returns:\n        A view containing binary target columns.\n    \"\"\"\n    if not isinstance(\n        base[colname], (FloatColumn, FloatColumnView, StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'\"\n            + colname\n            + \"' must be a FloatColumn, a FloatColumnView, \"\n            + \"a StringColumn or a StringColumnView.\"\n        )\n\n    unique_values = base[colname].unique()\n\n    if len(unique_values) &gt; 10:\n        logger.warning(\n            \"You are splitting the column into more than 10 target \"\n            + \"columns. This might take a long time to fit.\"\n        )\n\n    view = base\n\n    for label in unique_values:\n        col = (base[colname] == label).as_num()\n        name = colname + \"=\" + label\n        view = view.with_column(col=col, name=name, role=target)\n\n    return view.drop(colname)\n</code></pre>"},{"location":"reference/data/#getml.data.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(\n    *args: Union[\n        DataFrame, View, List[Union[DataFrame, View]]\n    ],\n    **kwargs: Union[\n        DataFrame, View, List[Union[DataFrame, View]]\n    ]\n) -&gt; List[Placeholder]\n</code></pre> <p>Factory function for extracting placeholders from a <code>DataFrame</code> or <code>View</code>.</p> PARAMETER DESCRIPTION <code>args</code> <p>The data frames or views you would like to convert to placeholders.</p> <p> TYPE: <code>Union[DataFrame, View, List[Union[DataFrame, View]]]</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The data frames or views you would like to convert to placeholders.</p> <p> TYPE: <code>Union[DataFrame, View, List[Union[DataFrame, View]]]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[Placeholder]</code> <p>A list of placeholders.</p> Example <p>Suppose we wanted to create a <code>DataModel</code>:</p> <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(meta.to_placeholder(\"meta\"))\ndm.add(order.to_placeholder(\"order\"))\ndm.add(trans.to_placeholder(\"trans\"))\n</code></pre> <p>But this is a bit repetitive. So instead, we can do the following: <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(getml.data.to_placeholder(\n    meta=meta, order=order, trans=trans))\n</code></pre></p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def to_placeholder(*args: Union[DataFrame, View, List[Union[DataFrame, View]]],\n                   **kwargs: Union[DataFrame, View, List[Union[DataFrame, View]]]) -&gt; List[Placeholder]:\n    \"\"\"\n    Factory function for extracting placeholders from a\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n\n    Args:\n        args:\n            The data frames or views you would like to convert to placeholders.\n\n        kwargs:\n            The data frames or views you would like to convert to placeholders.\n\n    Returns:\n        A list of placeholders.\n\n    Example:\n        Suppose we wanted to create a [`DataModel`][getml.data.DataModel]:\n\n\n\n            dm = getml.data.DataModel(\n                population_train.to_placeholder(\"population\")\n            )\n\n            # Add placeholders for the peripheral tables.\n            dm.add(meta.to_placeholder(\"meta\"))\n            dm.add(order.to_placeholder(\"order\"))\n            dm.add(trans.to_placeholder(\"trans\"))\n\n        But this is a bit repetitive. So instead, we can do\n        the following:\n        ```python\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        # Add placeholders for the peripheral tables.\n        dm.add(getml.data.to_placeholder(\n            meta=meta, order=order, trans=trans))\n        ```\n    \"\"\"\n\n    def to_ph_list(list_or_elem, key=None):\n        as_list = list_or_elem if isinstance(list_or_elem, list) else [list_or_elem]\n        return [elem.to_placeholder(key) for elem in as_list]\n\n    return [elem for item in args for elem in to_ph_list(item)] + [\n        elem for (k, v) in kwargs.items() for elem in to_ph_list(v, k)\n    ]\n</code></pre>"},{"location":"reference/data/#getml.data.load_container.load_container","title":"load_container","text":"<pre><code>load_container(container_id: str) -&gt; Container\n</code></pre> <p>Loads a container and all associated data frames from disk.</p> PARAMETER DESCRIPTION <code>container_id</code> <p>The id of the container you would like to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Container</code> <p>The container with the given id.</p> Source code in <code>getml/data/load_container.py</code> <pre><code>def load_container(container_id: str) -&gt; Container:\n    \"\"\"\n    Loads a container and all associated data frames from disk.\n\n    Args:\n        container_id:\n            The id of the container you would like to load.\n\n    Returns:\n        The container with the given id.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataContainer.load\"\n    cmd[\"name_\"] = container_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    cmd = json.loads(json_str)\n\n    population = _load_view(cmd[\"population_\"]) if \"population_\" in cmd else None\n\n    peripheral = {k: _load_view(v) for (k, v) in cmd[\"peripheral_\"].items()}\n\n    subsets = {k: _load_view(v) for (k, v) in cmd[\"subsets_\"].items()}\n\n    split = _parse(cmd[\"split_\"]) if \"split_\" in cmd else None\n\n    deep_copy = cmd[\"deep_copy_\"]\n    frozen_time = cmd[\"frozen_time_\"] if \"frozen_time_\" in cmd else None\n    last_change = cmd[\"last_change_\"]\n\n    container = Container(\n        population=population, peripheral=peripheral, deep_copy=deep_copy, **subsets\n    )\n\n    container._id = container_id\n    container._frozen_time = frozen_time\n    container._split = split\n    container._last_change = last_change\n\n    return container\n</code></pre>"},{"location":"reference/data/#getml.data.concat.concat","title":"concat","text":"<pre><code>concat(\n    name: str, data_frames: List[Union[DataFrame, View]]\n)\n</code></pre> <p>Creates a new data frame by concatenating a list of existing ones.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>data_frames</code> <p>The data frames to concatenate. Must be non-empty. However, it can contain only one data frame. Column names and roles must match. Columns will be appended by name, not order.</p> <p> TYPE: <code>List[Union[DataFrame, View]]</code> </p> <p>Examples:</p> <pre><code>new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n</code></pre> Source code in <code>getml/data/concat.py</code> <pre><code>def concat(name: str, data_frames: List[Union[DataFrame, View]]):\n    \"\"\"\n    Creates a new data frame by concatenating a list of existing ones.\n\n    Args:\n        name:\n            Name of the new column.\n\n        data_frames:\n            The data frames to concatenate.\n            Must be non-empty. However, it can contain only one data frame.\n            Column names and roles must match.\n            Columns will be appended by name, not order.\n\n    Examples:\n        ```python\n        new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    if not _is_non_empty_typed_list(data_frames, (View, DataFrame)):\n        raise TypeError(\n            \"'data_frames' must be a non-empty list of getml.data.Views \"\n            + \"or getml.DataFrames.\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [df._getml_deserialize() for df in data_frames]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/#getml.data.random.random","title":"random","text":"<pre><code>random(seed: int = 5849) -&gt; FloatColumnView\n</code></pre> <p>Create random column.</p> <p>The numbers will be uniformly distributed from 0.0 to 1.0. This can be used to randomly split a population table into a training and a test set</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>FloatColumn containing random numbers</p> Example <pre><code>population = getml.DataFrame('population')\npopulation.add(numpy.zeros(100), 'column_01')\n\nidx = random(seed=42)\npopulation_train = population[idx &gt; 0.7]\npopulation_test = population[idx &lt;= 0.7]\n</code></pre> Source code in <code>getml/data/columns/random.py</code> <pre><code>def random(seed: int = 5849) -&gt; FloatColumnView:\n    \"\"\"\n    Create random column.\n\n    The numbers will be uniformly distributed from 0.0 to 1.0. This can be\n    used to randomly split a population table into a training and a test\n    set\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n    Returns:\n            FloatColumn containing random numbers\n\n    Example:\n        ```python\n        population = getml.DataFrame('population')\n        population.add(numpy.zeros(100), 'column_01')\n\n        idx = random(seed=42)\n        population_train = population[idx &gt; 0.7]\n        population_test = population[idx &lt;= 0.7]\n        ```\n    \"\"\"\n\n    if not isinstance(seed, numbers.Real):\n        raise TypeError(\"'seed' must be a real number\")\n\n    col = FloatColumnView(operator=\"random\", operand1=None, operand2=None)\n    col.cmd[\"seed_\"] = seed\n    return col\n</code></pre>"},{"location":"reference/data/access/","title":"access","text":""},{"location":"reference/data/access/#getml.data.access","title":"getml.data.access","text":"<p>Manages the access to various data sources.</p>"},{"location":"reference/data/access/#getml.data.access.set_s3_access_key_id","title":"set_s3_access_key_id","text":"<pre><code>set_s3_access_key_id(value: str)\n</code></pre> <p>Sets the Access Key ID to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML engine, or you can set them from this module.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to which you want to set the Access Key ID.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_access_key_id(value: str):\n    \"\"\"Sets the Access Key ID to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML engine, or you can set them from\n    this module.\n\n    Args:\n        value:\n            The value to which you want to set the Access Key ID.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML engine.\n        Make sure the engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_access_key_id\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/data/access/#getml.data.access.set_s3_secret_access_key","title":"set_s3_secret_access_key","text":"<pre><code>set_s3_secret_access_key(value: str)\n</code></pre> <p>Sets the Secret Access Key to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML engine, or you can set them from this module.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to which you want to set the Secret Access Key.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_secret_access_key(value: str):\n    \"\"\"Sets the Secret Access Key to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML engine, or you can set them from\n    this module.\n\n    Args:\n        value:\n            The value to which you want to set the Secret Access Key.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML engine.\n        Make sure the engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_secret_access_key\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/data/columns/","title":"columns","text":""},{"location":"reference/data/columns/#getml.data.columns","title":"getml.data.columns","text":"<p>Handlers for 1-d arrays storing the data of an individual variable.</p> <p>Like the <code>DataFrame</code>, the <code>columns</code> do not contain any actual data themselves but are only handlers to objects within the getML engine. These containers store data of a single variable in a one-dimensional array of a uniform type.</p> <p>Columns are immutable and lazily evaluated.</p> <ul> <li> <p>Immutable means that there are no in-place   operation on the columns. Any change to the column   will return a new, changed column.</p> </li> <li> <p>Lazy evaluation means that operations won't be   executed until results are required. This is reflected   in the column views: Column views do not exist   until they are required.</p> </li> </ul> Example <p>This is what some column operations might look like:</p> <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\n# col2 is a column view.\n# The operation is not executed yet.\ncol2 = 2.0 - col1\n\n# This is when '2.0 - col1' is actually\n# executed.\nmy_df[\"column_02\"] = col2\nmy_df.set_role(\"column_02\", roles.numerical)\n\n# If you want to update column_01,\n# you can't do that in-place.\n# You need to replace it with a new column\ncol1 = col1 + col2\nmy_df[\"column_01\"] = col1\nmy_df.set_role(\"column_01\", roles.numerical)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView","title":"BooleanColumnView","text":"<pre><code>BooleanColumnView(\n    operator: str,\n    operand1: OptionalOperandType,\n    operand2: OptionalOperandType,\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Handle for a lazily evaluated boolean column view.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> <p>They can be used to take subselection of the data frame or to update other columns.</p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\nnames = my_df[\"names\"]\n\n# This is a virtual boolean column.\na_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n# Creates a view containing\n# only those entries, where \"names\" contains a or p.\nmy_view = my_df[a_or_p_in_names]\n\n# ----------------\n\n# Returns a new column, where all names\n# containing \"rick\" are replaced by \"Patrick\".\n# Again, columns are immutable - this returns an updated\n# version, but leaves the original column unchanged.\nnew_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\nmy_df[\"new_names\"] = new_names\n\n# ----------------\n\n# Boolean columns can also be used to\n# create binary target variables.\ntarget = (names == \"phil\")\n\nmy_df[\"target\"] = target\nmy_df.set_role(target, roles.target)\n\n# By the way, instead of using the\n# __setitem__ operator and .set_role(...)\n# you can just use .add(...).\nmy_df.add(target, \"target\", roles.target)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: OptionalOperandType,\n    operand2: OptionalOperandType,\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = BOOLEAN_COLUMN_VIEW\n\n    self.cmd[\"operator_\"] = operator\n\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView.is_false","title":"is_false","text":"<pre><code>is_false()\n</code></pre> <p>Whether an entry is False - effectively inverts the Boolean column.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def is_false(self):\n    \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n    return BooleanColumnView(\n        operator=\"not\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView.as_num","title":"as_num","text":"<pre><code>as_num()\n</code></pre> <p>Transforms the boolean column into a numerical column</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def as_num(self):\n    \"\"\"Transforms the boolean column into a numerical column\"\"\"\n    return FloatColumnView(\n        operator=\"boolean_as_num\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.FloatColumn","title":"FloatColumn","text":"<pre><code>FloatColumn(\n    name: str = \"\",\n    role: str = \"numerical\",\n    df_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>_Column</code></p> <p>Handle for numerical data in the engine.</p> <p>This is a handler for all numerical data in the getML engine, including time stamps.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Name of the categorical column.</p> <p> </p> <code>role</code> <p>Role that the column plays.</p> <p> </p> <code>df_name</code> <p><code>name</code> instance variable of the <code>DataFrame</code>  containing this column.</p> <p> </p> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\ncol2 = 2.0 - col1\n\nmy_df.add(col2, \"name\", roles.numerical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_float.\n\ncol3 = (col1 + 2.0*col2) / 3.0\n\nmy_df[\"column_03\"] = col3\nmy_df.set_role(\"column_03\", roles.numerical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(self, name: str = \"\", role: str = \"numerical\", df_name: str = \"\"):\n    super().__init__()\n\n    FloatColumn._num_columns += 1\n    if name == \"\":\n        name = FLOAT_COLUMN + \" \" + str(FloatColumn._num_columns)\n\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"operator_\"] = FLOAT_COLUMN\n\n    self.cmd[\"df_name_\"] = df_name\n\n    self.cmd[\"name_\"] = name\n\n    self.cmd[\"role_\"] = role\n\n    self.cmd[\"type_\"] = FLOAT_COLUMN\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.FloatColumnView","title":"FloatColumnView","text":"<pre><code>FloatColumnView(\n    operator: str,\n    operand1: Optional[\n        Union[float, int, datetime64, _Column, _View]\n    ],\n    operand2: Optional[\n        Union[float, int, datetime64, _Column, _View]\n    ],\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>FloatColumn</code>.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: Optional[Union[float, int, np.datetime64, _Column, _View]],\n    operand2: Optional[Union[float, int, np.datetime64, _Column, _View]],\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = FLOAT_COLUMN_VIEW\n\n    self.cmd[\"operator_\"] = operator\n\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.StringColumn","title":"StringColumn","text":"<pre><code>StringColumn(\n    name: str = \"\",\n    role: str = \"categorical\",\n    df_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>_Column</code></p> <p>Handle for categorical data that is kept in the getML engine</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Name of the categorical column.</p> <p> </p> <code>role</code> <p>Role that the column plays.</p> <p> </p> <code>df_name</code> <p><code>name</code> instance variable of the <code>DataFrame</code> containing this column.</p> <p> </p> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\ncol2 = col1.substr(4, 3)\n\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\ncol3 = \"user-\" + col1 + \"-\" + col2\n\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(self, name: str = \"\", role: str = \"categorical\", df_name: str = \"\"):\n    super().__init__()\n\n    StringColumn._num_columns += 1\n    if name == \"\":\n        name = STRING_COLUMN + \" \" + str(StringColumn._num_columns)\n\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"operator_\"] = STRING_COLUMN\n    self.cmd[\"df_name_\"] = df_name\n    self.cmd[\"name_\"] = name\n    self.cmd[\"role_\"] = role\n    self.cmd[\"type_\"] = STRING_COLUMN\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.StringColumnView","title":"StringColumnView","text":"<pre><code>StringColumnView(\n    operator: str,\n    operand1: Optional[Union[str, _Column, _View]],\n    operand2: Optional[Union[str, _Column, _View]],\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>StringColumn</code>.</p> <p>Columns views do not actually exist - they will be lazily evaluated when necessary.</p> <p>Examples: <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\n# col2 is a virtual column.\n# The substring operation is not\n# executed yet.\ncol2 = col1.substr(4, 3)\n\n# This is where the engine executes\n# the substring operation.\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\n# col3 is a virtual column.\n# The operation is not\n# executed yet.\ncol3 = \"user-\" + col1 + \"-\" + col2\n\n# This is where the operation is\n# is executed.\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre></p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: Optional[Union[str, _Column, _View]],\n    operand2: Optional[Union[str, _Column, _View]],\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = STRING_COLUMN_VIEW\n    self.cmd[\"operator_\"] = operator\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.arange","title":"arange","text":"<pre><code>arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n)\n</code></pre> <p>Returns evenly spaced variables, within a given interval.</p> PARAMETER DESCRIPTION <code>start</code> <p>The beginning of the interval. Defaults to 0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>stop</code> <p>The end of the interval.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The step taken. Defaults to 1.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: float = 0.0,\n    stop: Optional[float] = None,\n    step: float = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start:\n            The beginning of the interval. Defaults to 0.\n\n        stop:\n            The end of the interval.\n\n        step:\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.rowid","title":"rowid","text":"<pre><code>rowid()\n</code></pre> <p>Get the row numbers of the table.</p> RETURNS DESCRIPTION <p><code>FloatColumnView</code>: (numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid():\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n        [`FloatColumnView`][getml.data.columns.FloatColumnView]:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.collect_footer_data","title":"collect_footer_data","text":"<p>Collects the data necessary for displaying the column footer.</p>"},{"location":"reference/data/columns/#getml.data.columns.collect_footer_data.Footer","title":"Footer","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the data to be shown in the footer of the data frame or column.</p>"},{"location":"reference/data/columns/#getml.data.columns.aggregation","title":"aggregation","text":"<p>Lazily evaluated aggregation over a column.</p>"},{"location":"reference/data/columns/#getml.data.columns.aggregation.Aggregation","title":"Aggregation","text":"<pre><code>Aggregation(alias, col, agg_type)\n</code></pre> <p>Lazily evaluated aggregation over a column.</p> Example <pre><code>my_data_frame[\"my_column\"].avg()\n3.0\n</code></pre> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>def __init__(self, alias, col, agg_type):\n    self.cmd: Dict[str, Any] = {}\n    self.cmd[\"as_\"] = alias\n    self.cmd[\"col_\"] = col.cmd\n    self.cmd[\"type_\"] = agg_type\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.aggregation.Aggregation.get","title":"get","text":"<pre><code>get()\n</code></pre> <p>Receives the value of the aggregation over the column.</p> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>def get(self):\n    \"\"\"\n    Receives the value of the aggregation over the column.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"FloatColumn.aggregate\"\n\n    cmd[\"aggregation_\"] = self.cmd\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        mat = comm.recv_float_matrix(sock)\n\n    return mat.ravel()[0]\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.format","title":"format","text":"<p>Format the column</p>"},{"location":"reference/data/columns/#getml.data.columns.last_change","title":"last_change","text":"<p>Returns the last time a data frame has been changed.</p>"},{"location":"reference/data/columns/#getml.data.columns.last_change_from_col","title":"last_change_from_col","text":"<p>The last time any of the underlying data frames has been changed.</p>"},{"location":"reference/data/columns/#getml.data.columns.length","title":"length","text":"<p>Returns the length of the column</p>"},{"location":"reference/data/columns/#getml.data.columns.length_property","title":"length_property","text":"<p>The length of the column (number of rows in the data frame).</p>"},{"location":"reference/data/columns/#getml.data.columns.make_iter","title":"make_iter","text":"<p>Factory function for a function that can be used to iterate through a column.</p>"},{"location":"reference/data/columns/#getml.data.columns.parse","title":"parse","text":"<p>Parses the columns from a cmd</p>"},{"location":"reference/data/columns/#getml.data.columns.repr","title":"repr","text":"<p>ASCII representation of the column.</p>"},{"location":"reference/data/columns/#getml.data.columns.repr_html","title":"repr_html","text":"<p>HTML representation of the column.</p>"},{"location":"reference/data/columns/#getml.data.columns.subroles","title":"subroles","text":"<p>The subroles of this column.</p>"},{"location":"reference/data/columns/#getml.data.columns.to_arrow","title":"to_arrow","text":"<p>Transform column to a pyarrow.ChunkedArray</p>"},{"location":"reference/data/columns/#getml.data.columns.to_numpy","title":"to_numpy","text":"<p>Transform column to a numpy array.</p>"},{"location":"reference/data/columns/#getml.data.columns.unique","title":"unique","text":"<p>Transform column to numpy array containing unique values</p>"},{"location":"reference/data/columns/#getml.data.columns.unit","title":"unit","text":"<p>The unit of this column.</p>"},{"location":"reference/data/columns/#getml.data.columns.from_value.from_value","title":"from_value","text":"<pre><code>from_value(\n    val: Union[bool, str, int, float, datetime64]\n) -&gt; ReturnType\n</code></pre> <p>Creates an infinite column that contains the same value in all of its elements.</p> PARAMETER DESCRIPTION <code>val</code> <p>The value you want to insert into your column.</p> <p> TYPE: <code>Union[bool, str, int, float, datetime64]</code> </p> RETURNS DESCRIPTION <code>ReturnType</code> <p>The column view containing the value.</p> Source code in <code>getml/data/columns/from_value.py</code> <pre><code>def from_value(val: Union[bool, str, int, float, np.datetime64]) -&gt; ReturnType:\n    \"\"\"\n    Creates an infinite column that contains the same\n    value in all of its elements.\n\n    Args:\n        val:\n            The value you want to insert into your column.\n\n    Returns:\n        The column view containing the value.\n    \"\"\"\n    cmd = _value_to_cmd(val)\n\n    if isinstance(val, bool):\n        col: ReturnType = BooleanColumnView(\n            operator=\"const\",\n            operand1=None,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, str):\n        col = StringColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, (int, float, numbers.Number)):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, np.datetime64):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=np.datetime64(val, \"s\").astype(float),\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    raise TypeError(\"val must be bool, str or a number.\")\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.random.random","title":"random","text":"<pre><code>random(seed: int = 5849) -&gt; FloatColumnView\n</code></pre> <p>Create random column.</p> <p>The numbers will be uniformly distributed from 0.0 to 1.0. This can be used to randomly split a population table into a training and a test set</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>FloatColumn containing random numbers</p> Example <pre><code>population = getml.DataFrame('population')\npopulation.add(numpy.zeros(100), 'column_01')\n\nidx = random(seed=42)\npopulation_train = population[idx &gt; 0.7]\npopulation_test = population[idx &lt;= 0.7]\n</code></pre> Source code in <code>getml/data/columns/random.py</code> <pre><code>def random(seed: int = 5849) -&gt; FloatColumnView:\n    \"\"\"\n    Create random column.\n\n    The numbers will be uniformly distributed from 0.0 to 1.0. This can be\n    used to randomly split a population table into a training and a test\n    set\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n    Returns:\n            FloatColumn containing random numbers\n\n    Example:\n        ```python\n        population = getml.DataFrame('population')\n        population.add(numpy.zeros(100), 'column_01')\n\n        idx = random(seed=42)\n        population_train = population[idx &gt; 0.7]\n        population_test = population[idx &lt;= 0.7]\n        ```\n    \"\"\"\n\n    if not isinstance(seed, numbers.Real):\n        raise TypeError(\"'seed' must be a real number\")\n\n    col = FloatColumnView(operator=\"random\", operand1=None, operand2=None)\n    col.cmd[\"seed_\"] = seed\n    return col\n</code></pre>"},{"location":"reference/data/concat/","title":"Concat","text":""},{"location":"reference/data/concat/#getml.data.concat","title":"getml.data.concat","text":"<p>Creates a new data frame by concatenating a list of existing ones.</p>"},{"location":"reference/data/concat/#getml.data.concat.concat","title":"concat","text":"<pre><code>concat(\n    name: str, data_frames: List[Union[DataFrame, View]]\n)\n</code></pre> <p>Creates a new data frame by concatenating a list of existing ones.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>data_frames</code> <p>The data frames to concatenate. Must be non-empty. However, it can contain only one data frame. Column names and roles must match. Columns will be appended by name, not order.</p> <p> TYPE: <code>List[Union[DataFrame, View]]</code> </p> <p>Examples:</p> <pre><code>new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n</code></pre> Source code in <code>getml/data/concat.py</code> <pre><code>def concat(name: str, data_frames: List[Union[DataFrame, View]]):\n    \"\"\"\n    Creates a new data frame by concatenating a list of existing ones.\n\n    Args:\n        name:\n            Name of the new column.\n\n        data_frames:\n            The data frames to concatenate.\n            Must be non-empty. However, it can contain only one data frame.\n            Column names and roles must match.\n            Columns will be appended by name, not order.\n\n    Examples:\n        ```python\n        new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    if not _is_non_empty_typed_list(data_frames, (View, DataFrame)):\n        raise TypeError(\n            \"'data_frames' must be a non-empty list of getml.data.Views \"\n            + \"or getml.DataFrames.\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [df._getml_deserialize() for df in data_frames]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/container/","title":"Container","text":""},{"location":"reference/data/container/#getml.data.Container","title":"getml.data.Container","text":"<pre><code>Container(\n    population: Optional[Union[DataFrame, View]] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]]\n)\n</code></pre> <p>A container holds the actual data in the form of a <code>DataFrame</code> or a <code>View</code>.</p> <p>The purpose of a container is twofold:</p> <ul> <li> <p>Assigning concrete data to an abstract <code>DataModel</code>.</p> </li> <li> <p>Storing data and allowing you to reproduce previous results.</p> </li> </ul> ATTRIBUTE DESCRIPTION <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code>s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>train</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>validation</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>test</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>kwargs</code> <p>The population table used in <code>Subset</code>s other than the predefined train, validation and test subsets. You can call these subsets anything you want to, and you can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\ncontainer = getml.data.Container(my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(container.my_subset)\n</code></pre></p> <p> </p> Example <p>A <code>DataModel</code> only contains abstract data. When we fit a pipeline, we need to assign concrete data.</p> <p>This example is taken from the loans notebook . Note that in the notebook the high level <code>StarSchema</code> implementation is used. For demonstration purposes we are proceeding now with the low level implementation.</p> <p><pre><code># The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# We now have abstract placeholders on something\n# called \"population\", \"meta\", \"order\" and \"trans\".\n# But how do we assign concrete data? By using\n# a container.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. Their aliases need\n# to match the names of the placeholders in the\n# data model.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# When we call 'train', the container\n# will return the train set and the\n# peripheral tables.\nmy_pipeline.fit(container.train)\n\n# Same for 'test'\nmy_pipeline.score(container.test)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\ncontainer = getml.data.Container(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# container.train and container.test\n# work just like above.\n</code></pre> <p>Containers can also be used for storage and reproducing your results. A recommended pattern is to assign 'baseline roles' to your data frames and then using a <code>View</code> to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which container you have used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def __init__(\n    self,\n    population: Optional[Union[DataFrame, View]] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]],\n):\n    if population is not None and not isinstance(population, (DataFrame, View)):\n        raise TypeError(\n            \"'population' must be a getml.DataFrame or a getml.data.View, got \"\n            + type(population).__name__\n            + \".\"\n        )\n\n    if peripheral is not None and not _is_typed_dict(\n        peripheral, str, [DataFrame, View]\n    ):\n        raise TypeError(\n            \"'peripheral' must be a dict \"\n            + \"of getml.DataFrames or getml.data.Views.\"\n        )\n\n    if split is not None and not isinstance(\n        split, (StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'split' must be StringColumn or a StringColumnView, got \"\n            + type(split).__name__\n            + \".\"\n        )\n\n    if not isinstance(deep_copy, bool):\n        raise TypeError(\n            \"'deep_copy' must be a bool, got \" + type(split).__name__ + \".\"\n        )\n\n    exclusive = (population is not None) ^ (\n        len(_make_subsets_from_kwargs(train, validation, test, **kwargs)) != 0\n    )\n\n    if not exclusive:\n        raise ValueError(\n            \"'population' and 'train', 'validation', 'test' as well as \"\n            + \"other subsets signified by kwargs are mutually exclusive. \"\n            + \"You have to pass \"\n            + \"either 'population' or some subsets, but you cannot pass both.\"\n        )\n\n    if population is None and split is not None:\n        raise ValueError(\n            \"'split's are used for splitting population DataFrames.\"\n            \"Hence, if you supply 'split', you also have to supply \"\n            \"a population.\"\n        )\n\n    if population is not None and split is None:\n        logger.warning(\n            \"You have passed a population table without passing 'split'. \"\n            \"You can access the entire set to pass to your pipeline \"\n            \"using the .full attribute.\"\n        )\n        split = from_value(\"full\")\n\n    self._id = _make_id()\n\n    self._population = population\n    self._peripheral = peripheral or {}\n    self._split = split\n    self._deep_copy = deep_copy\n\n    self._subsets = (\n        _make_subsets_from_split(population, split)\n        if split is not None\n        else _make_subsets_from_kwargs(train, validation, test, **kwargs)\n    )\n\n    if split is None and not _is_typed_dict(self._subsets, str, [DataFrame, View]):\n        raise TypeError(\n            \"'train', 'validation', 'test' and all other subsets must be either a \"\n            \"getml.DataFrame or a getml.data.View.\"\n        )\n\n    if deep_copy:\n        self._population = _deep_copy(self._population, self._id)\n        self._peripheral = {\n            k: _deep_copy(v, self._id) for (k, v) in self._peripheral.items()\n        }\n        self._subsets = {\n            k: _deep_copy(v, self._id) for (k, v) in self._subsets.items()\n        }\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n\n    self._frozen_time = None\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.add","title":"add","text":"<pre><code>add(*args, **kwargs)\n</code></pre> <p>Adds new peripheral data frames or views.</p> Source code in <code>getml/data/container.py</code> <pre><code>def add(self, *args, **kwargs):\n    \"\"\"\n    Adds new peripheral data frames or views.\n    \"\"\"\n    wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n    if wrong_type:\n        raise TypeError(\n            \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n        )\n\n    wrong_type = [\n        k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n    ]\n\n    if wrong_type:\n        raise TypeError(\n            \"You must pass getml.DataFrames or getml.data.Views, \"\n            f\"but the following arguments were neither: {wrong_type!r}.\"\n        )\n\n    kwargs = {**{item.name: item for item in args}, **kwargs}\n\n    if self._frozen_time is not None:\n        raise ValueError(\n            f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n        )\n\n    if self._deep_copy:\n        kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n    self._peripheral = {**self._peripheral, **kwargs}\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.freeze","title":"freeze","text":"<pre><code>freeze()\n</code></pre> <p>Freezes the container, so that changes are no longer possible.</p> <p>This is required before you can extract data when <code>deep_copy=True</code>. The idea of <code>deep_copy</code> is to ensure that you can always retrace and reproduce your results. That is why the container needs to be immutable before it can be used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def freeze(self):\n    \"\"\"\n    Freezes the container, so that changes are no longer possible.\n\n    This is required before you can extract data when `deep_copy=True`. The idea of\n    `deep_copy` is to ensure that you can always retrace and reproduce your results.\n    That is why the container needs to be immutable before it can be\n    used.\n    \"\"\"\n    self.sync()\n    self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Saves the Container to disk.</p> Source code in <code>getml/data/container.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves the Container to disk.\n    \"\"\"\n\n    cmd = dict()\n    cmd[\"type_\"] = \"DataContainer.save\"\n    cmd[\"name_\"] = self._id\n\n    cmd[\"container_\"] = self._getml_deserialize()\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.sync","title":"sync","text":"<pre><code>sync()\n</code></pre> <p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/container.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    if self._frozen_time is not None:\n        raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; Dict[str, DataFrame]\n</code></pre> <p>TODO</p> Source code in <code>getml/data/container.py</code> <pre><code>def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    TODO\n    \"\"\"\n    subsets = (\n        {name: df.to_pandas() for name, df in self._subsets.items()}\n        if self._subsets\n        else {}\n    )\n    peripherals = (\n        {name: df.to_pandas() for name, df in self.peripheral.items()}\n        if self.peripheral\n        else {}\n    )\n    if subsets or peripherals:\n        return {**subsets, **peripherals}\n\n    raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/data_frame/","title":"DataFrame","text":""},{"location":"reference/data/data_frame/#getml.data.DataFrame","title":"getml.data.DataFrame","text":"<pre><code>DataFrame(\n    name: str,\n    roles: Union[dict[str, List[str]], Roles] = None,\n)\n</code></pre> <p>Handler for the data stored in the getML engine.</p> <p>The <code>DataFrame</code> class represents a data frame object in the getML engine but does not contain any actual data itself. To create such a data frame object, fill it with data via the Python API, and to retrieve a handler for it, you can use one of the <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code> class methods. The Importing Data section in the user guide explains the particularities of each of those flavors of the unified import interface.</p> <p>If the data frame object is already present in the engine - either in memory as a temporary object or on disk when <code>save</code> was called earlier -, the <code>load_data_frame</code> function will create a new handler without altering the underlying data. For more information about the lifecycle of the data in the getML engine and its synchronization with the Python API please see the corresponding User Guide.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Unique identifier used to link the handler with the underlying data frame object in the engine.</p> <p> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> </p> Example <p>Creating a new data frame object in the getML engine and importing data is done by one the class functions <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p><pre><code>random = numpy.random.RandomState(7263)\n\ntable = pandas.DataFrame()\ntable['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\ntable['join_key'] = numpy.arange(1000)\ntable['time_stamp'] = random.rand(1000)\ntable['target'] = random.rand(1000)\n\ndf_table = getml.DataFrame.from_pandas(table, name = 'table')\n</code></pre> In addition to creating a new data frame object in the getML engine and filling it with all the content of <code>table</code>, the <code>from_pandas</code> function also returns a <code>DataFrame</code> handler to the underlying data.</p> <p>You don't have to create the data frame objects anew for each session. You can use their <code>save</code> method to write them to disk, the <code>list_data_frames</code> function to list all available objects in the engine, and <code>load_data_frame</code> to create a <code>DataFrame</code> handler for a data set already present in the getML engine (see User Guide for details).</p> <pre><code>df_table.save()\n\ngetml.data.list_data_frames()\n\ndf_table_reloaded = getml.data.load_data_frame('table')\n</code></pre> Note <p>Although the Python API does not store the actual data itself, you can use the <code>to_csv</code>, <code>to_db</code>, <code>to_json</code>, and <code>to_pandas</code> methods to retrieve them.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def __init__(self, name: str, roles: Union[dict[str, List[str]], Roles] = None):\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    vars(self)[\"name\"] = name\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    roles = roles or dict()\n\n    if not isinstance(roles, dict):\n        raise TypeError(\"'roles' must be dict or a getml.data.Roles object\")\n\n    for key, val in roles.items():\n        if key not in self._possible_keys:\n            msg = \"'{}' is not a proper role and will be ignored\\n\"\n            msg += \"Possible roles are: {}\"\n            raise ValueError(msg.format(key, self._possible_keys))\n        if not _is_typed_list(val, str):\n            raise TypeError(\n                \"'{}' must be None, an empty list, or a list of str.\".format(key)\n            )\n\n    # ------------------------------------------------------------\n\n    join_keys = roles.get(\"join_key\", [])\n    time_stamps = roles.get(\"time_stamp\", [])\n    categorical = roles.get(\"categorical\", [])\n    numerical = roles.get(\"numerical\", [])\n    targets = roles.get(\"target\", [])\n    text = roles.get(\"text\", [])\n    unused_floats = roles.get(\"unused_float\", [])\n    unused_strings = roles.get(\"unused_string\", [])\n\n    # ------------------------------------------------------------\n\n    vars(self)[\"_categorical_columns\"] = [\n        StringColumn(name=cname, role=roles_.categorical, df_name=self.name)\n        for cname in categorical\n    ]\n\n    vars(self)[\"_join_key_columns\"] = [\n        StringColumn(name=cname, role=roles_.join_key, df_name=self.name)\n        for cname in join_keys\n    ]\n\n    vars(self)[\"_numerical_columns\"] = [\n        FloatColumn(name=cname, role=roles_.numerical, df_name=self.name)\n        for cname in numerical\n    ]\n\n    vars(self)[\"_target_columns\"] = [\n        FloatColumn(name=cname, role=roles_.target, df_name=self.name)\n        for cname in targets\n    ]\n\n    vars(self)[\"_text_columns\"] = [\n        StringColumn(name=cname, role=roles_.text, df_name=self.name)\n        for cname in text\n    ]\n\n    vars(self)[\"_time_stamp_columns\"] = [\n        FloatColumn(name=cname, role=roles_.time_stamp, df_name=self.name)\n        for cname in time_stamps\n    ]\n\n    vars(self)[\"_unused_float_columns\"] = [\n        FloatColumn(name=cname, role=roles_.unused_float, df_name=self.name)\n        for cname in unused_floats\n    ]\n\n    vars(self)[\"_unused_string_columns\"] = [\n        StringColumn(name=cname, role=roles_.unused_string, df_name=self.name)\n        for cname in unused_strings\n    ]\n\n    # ------------------------------------------------------------\n\n    self._check_duplicates()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.colnames","title":"colnames  <code>property</code>","text":"<pre><code>colnames: List[str]\n</code></pre> <p>List of the names of all columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Alias for <code>colnames</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.last_change","title":"last_change  <code>property</code>","text":"<pre><code>last_change: str\n</code></pre> <p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.memory_usage","title":"memory_usage  <code>property</code>","text":"<pre><code>memory_usage\n</code></pre> <p>Convenience wrapper that returns the memory usage in MB.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.roles","title":"roles  <code>property</code>","text":"<pre><code>roles\n</code></pre> <p>The roles of the columns included in this DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.rowid","title":"rowid  <code>property</code>","text":"<pre><code>rowid\n</code></pre> <p>The rowids for this data frame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>A tuple containing the number of rows and columns of the DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.add","title":"add","text":"<pre><code>add(\n    col: Union[StringColumn, FloatColumn, array],\n    name: str,\n    role: Optional[str] = None,\n    subroles: Optional[Union[str, List[str]]] = None,\n    unit: str = \"\",\n    time_formats: Optional[List[str]] = None,\n)\n</code></pre> <p>Adds a column to the current <code>DataFrame</code>.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column or numpy.ndarray to be added.</p> <p> TYPE: <code>Union[StringColumn, FloatColumn, array]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[str, List[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def add(\n    self,\n    col: Union[StringColumn, FloatColumn, np.array],\n    name: str,\n    role: Optional[str] = None,\n    subroles: Optional[Union[str, List[str]]] = None,\n    unit: str = \"\",\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        col:\n            The column or numpy.ndarray to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n\n    if isinstance(col, np.ndarray):\n        self._add_numpy_array(col, name, role, subroles, unit)\n        return\n\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n\n    is_string = isinstance(col, (StringColumnView, StringColumn))\n\n    if is_string:\n        self._add_categorical_column(col, name, role, subroles, unit)\n    else:\n        self._add_column(col, name, role, subroles, unit)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.copy","title":"copy","text":"<pre><code>copy(name: str) -&gt; DataFrame\n</code></pre> <p>Creates a deep copy of the data frame under a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new data frame.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A handle to the deep copy.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def copy(self, name: str) -&gt; \"DataFrame\":\n    \"\"\"\n    Creates a deep copy of the data frame under a new name.\n\n    Args:\n        name:\n            The name of the new data frame.\n\n    Returns:\n            A handle to the deep copy.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Permanently deletes the data frame. <code>delete</code> first unloads the data frame from memory and then deletes it from disk.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Permanently deletes the data frame. `delete` first unloads the data frame\n    from memory and then deletes it from disk.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    self._delete()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.drop","title":"drop","text":"<pre><code>drop(\n    cols: Union[\n        FloatColumn,\n        StringColumn,\n        str,\n        List[Union[FloatColumn, StringColumn, str]],\n    ]\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that has one or several columns removed.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[FloatColumn, StringColumn, str, List[Union[FloatColumn, StringColumn, str]]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new <code>View</code> object with the specified columns removed.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def drop(\n    self,\n    cols: Union[\n        FloatColumn, StringColumn, str, List[Union[FloatColumn, StringColumn, str]]\n    ],\n) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n    Returns:\n        A new [`View`][getml.data.View] object with the specified columns removed.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not _is_typed_list(names, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=names)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.freeze","title":"freeze","text":"<pre><code>freeze()\n</code></pre> <p>Freezes the data frame.</p> <p>After you have frozen the data frame, the data frame is immutable and in-place operations are no longer possible. However, you can still create views. In other words, operations like <code>set_role</code> are no longer possible, but operations like <code>with_role</code> are.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def freeze(self):\n    \"\"\"Freezes the data frame.\n\n    After you have frozen the data frame, the data frame is immutable\n    and in-place operations are no longer possible. However, you can\n    still create views. In other words, operations like\n    [`set_role`][getml.DataFrame.set_role] are no longer possible,\n    but operations like [`with_role`][getml.DataFrame.with_role] are.\n    \"\"\"\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.freeze\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(\n    table: Table,\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from an Arrow Table.</p> <p>This is one of the fastest way to get data into the getML engine.</p> PARAMETER DESCRIPTION <code>table</code> <p>The table to be read.</p> <p> TYPE: <code>Table</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_arrow(\n    cls,\n    table: pa.Table,\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from an Arrow Table.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        table:\n            The table to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_arrow(table)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_arrow(table=table, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    fnames: List[str],\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from CSV files.</p> <p>The getML engine will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>CSV file paths to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>If True, when fnames are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Note <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the current working directory. You can import their data into the getML engine using. <pre><code>df_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"'\n    )\n\n# However, the CSV format lacks type safety. If you want to\n# build a reliable pipeline, it is a good idea\n# to hard-code the roles:\n\nroles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n    )\n\n# If you think that typing out all the roles by hand is too\n# cumbersome, you can use a dry run:\n\nroles = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True\n)\n</code></pre></p> <p>This will return the roles dictionary it would have used. You can now hard-code this.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    fnames: List[str],\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    verbose: bool = True,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files.\n\n    The getML engine will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        fnames:\n            CSV file paths to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames: The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        verbose:\n            If True, when fnames are urls, the filenames are\n            printed to stdout during the download.\n\n    Returns:\n            Handler of the underlying data.\n\n    Note:\n        It is assumed that the first line of each CSV file\n        contains a header with the column names.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the current working directory. You can\n        import their data into the getML engine using.\n        ```python\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"'\n            )\n\n        # However, the CSV format lacks type safety. If you want to\n        # build a reliable pipeline, it is a good idea\n        # to hard-code the roles:\n\n        roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n            )\n\n        # If you think that typing out all the roles by hand is too\n        # cumbersome, you can use a dry run:\n\n        roles = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            dry=True\n        )\n        ```\n\n        This will return the roles dictionary it would have used. You\n        can now hard-code this.\n\n    \"\"\"\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    fnames = _retrieve_urls(fnames, verbose=verbose)\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_csv(\n            fnames=fnames,\n            num_lines_sniffed=int(num_lines_sniffed),\n            quotechar=quotechar,\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_csv(\n        fnames=fnames,\n        append=False,\n        quotechar=quotechar,\n        sep=sep,\n        num_lines_read=num_lines_read,\n        skip=skip,\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_db","title":"from_db  <code>classmethod</code>","text":"<pre><code>from_db(\n    table_name: str,\n    name: Optional[str] = None,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a table in a database.</p> <p>It will construct a data frame object in the engine, fill it with the data read from table <code>table_name</code> in the connected database (see <code>database</code>), and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be read.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created. If not passed, then the table_name will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Example <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    port=3306,\n    dbname=\"financial\",\n    user=\"guest\",\n    password=\"relational\"\n)\n\nloan = getml.DataFrame.from_db(\n    table_name='loan', name='data_frame_loan')\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_db(\n    cls,\n    table_name: str,\n    name: Optional[str] = None,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a table in a database.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from table `table_name` in the connected\n    database (see [`database`][getml.database]), and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        table_name:\n            Name of the table to be read.\n\n        name:\n            Name of the data frame to be created. If not passed,\n            then the *table_name* will be used.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection, the engine\n            will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n\n    Example:\n        ```python\n        getml.database.connect_mysql(\n            host=\"db.relational-data.org\",\n            port=3306,\n            dbname=\"financial\",\n            user=\"guest\",\n            password=\"relational\"\n        )\n\n        loan = getml.DataFrame.from_db(\n            table_name='loan', name='data_frame_loan')\n        ```\n    \"\"\"\n\n    # -------------------------------------------\n\n    name = name or table_name\n\n    # -------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\n            \"'roles' must be a getml.data.Roles object, a dict or None.\"\n        )\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # -------------------------------------------\n\n    conn = conn or database.Connection()\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_db(table_name, conn)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    data: Dict[str, Union[List[float], List[str]]],\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a new DataFrame from a dict</p> PARAMETER DESCRIPTION <code>data</code> <p>The dict containing the data. The data should be in the following format: <pre><code>data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n</code></pre></p> <p> TYPE: <code>Dict[str, Union[List[float], List[str]]]</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    data: Dict[str, Union[List[float], List[str]]],\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a new DataFrame from a dict\n\n    Args:\n        data:\n            The dict containing the data.\n            The data should be in the following format:\n            ```python\n            data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n            ```\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(data, dict):\n        raise TypeError(\"'data' must be dict.\")\n\n    return cls.from_arrow(\n        table=pa.Table.from_pydict(data),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    json_str: str,\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a new DataFrame from a JSON string.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the JSON string, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>json_str</code> <p>The JSON string containing the data. The json_str should be in the following format: <pre><code>json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n</code></pre></p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_str: str,\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a new DataFrame from a JSON string.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the JSON string, and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        json_str:\n            The JSON string containing the data.\n            The json_str should be in the following format:\n            ```python\n            json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n            ```\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be str.\")\n\n    return cls.from_dict(\n        data=json.loads(json_str),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_pandas","title":"from_pandas  <code>classmethod</code>","text":"<pre><code>from_pandas(\n    pandas_df: DataFrame,\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a <code>pandas.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pandas.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>pandas_df</code> <p>The table to be read.</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code> roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n          getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pandas(\n    cls,\n    pandas_df: pd.DataFrame,\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pandas.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        pandas_df:\n            The table to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n             roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                      getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    pandas_df_modified = _modify_pandas_columns(pandas_df)\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_pandas(pandas_df_modified)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pandas(pandas_df=pandas_df_modified, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_parquet","title":"from_parquet  <code>classmethod</code>","text":"<pre><code>from_parquet(\n    fname: str,\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n)\n</code></pre> <p>Create a DataFrame from parquet files.</p> <p>This is one of the fastest way to get data into the getML engine.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The path of the parquet file to be read.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_parquet(\n    cls,\n    fname: str,\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n):\n    \"\"\"Create a DataFrame from parquet files.\n\n    This is one of the fastest way to get data into the\n    getML engine.\n\n    Args:\n        fname:\n            The path of the parquet file to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_parquet(fname)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_parquet(fname=fname, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_pyspark","title":"from_pyspark  <code>classmethod</code>","text":"<pre><code>from_pyspark(\n    spark_df: DataFrame,\n    name: str,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a <code>pyspark.sql.DataFrame</code>.</p> <p>It will construct a data frame object in the engine, fill it with the data read from the <code>pyspark.sql.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>spark_df</code> <p>The table to be read.</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre></p> <p>Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pyspark(\n    cls,\n    spark_df: \"pyspark.sql.DataFrame\",\n    name: str,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n    It will construct a data frame object in the engine, fill it\n    with the data read from the `pyspark.sql.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        spark_df:\n            The table to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        head = spark_df.limit(2).toPandas()\n\n        sniffed_roles = _sniff_pandas(head)\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pyspark(spark_df=spark_df, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_s3","title":"from_s3  <code>classmethod</code>","text":"<pre><code>from_s3(\n    bucket: str,\n    keys: List[str],\n    region: str,\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    roles: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from CSV files located in an S3 bucket.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\ndata_frame_expd = data.DataFrame.from_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY DATA FRAME\",\n    sep=';'\n)\n</code></pre></p> <p>You can also set the access credential as environment variables before you launch the getML engine.</p> <p>Also refer to the documentation on <code>from_csv</code> for further information on overriding the CSV sniffer for greater type safety.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_s3(\n    cls,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    roles: Optional[Union[dict[str, List[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        name:\n            Name of the data frame to be created.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n            Handler of the underlying data.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        data_frame_expd = data.DataFrame.from_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY DATA FRAME\",\n            sep=';'\n        )\n        ```\n\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n        Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n        for further information on overriding the CSV sniffer for greater\n        type safety.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    roles = roles.to_dict() if isinstance(roles, Roles) else roles\n\n    if roles is None or not ignore:\n        sniffed_roles = _sniff_s3(\n            bucket=bucket,\n            keys=keys,\n            region=region,\n            num_lines_sniffed=int(num_lines_sniffed),\n            sep=sep,\n            skip=int(skip),\n            colnames=colnames,\n        )\n\n        if roles is None:\n            roles = sniffed_roles\n        else:\n            roles = _update_sniffed_roles(sniffed_roles, roles)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_s3(\n        bucket=bucket,\n        keys=keys,\n        region=region,\n        append=False,\n        sep=sep,\n        num_lines_read=int(num_lines_read),\n        skip=int(skip),\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_view","title":"from_view  <code>classmethod</code>","text":"<pre><code>from_view(\n    view: View, name: str, dry: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Create a DataFrame from a <code>View</code>.</p> <p>This classmethod will construct a data frame object in the engine, fill it with the data read from the <code>View</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>view</code> <p>The view from which we want to read the data.</p> <p> TYPE: <code>View</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>dry</code> <p>If set to True, then the data will not actually be read. Instead, the method will only return the roles it would have used. This can be used to hard-code roles when setting up a pipeline.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_view(\n    cls,\n    view: View,\n    name: str,\n    dry: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n    This classmethod will construct a data\n    frame object in the engine, fill it with the data read from\n    the [`View`][getml.data.View], and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        view:\n            The view from which we want to read the data.\n\n        name:\n            Name of the data frame to be created.\n\n        dry:\n            If set to True, then the data\n            will not actually be read. Instead, the method will only\n            return the roles it would have used. This can be used\n            to hard-code roles when setting up a pipeline.\n\n    Returns:\n            Handler of the underlying data.\n\n\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if dry:\n        return view.roles\n\n    data_frame = cls(name)\n\n    # ------------------------------------------------------------\n\n    return data_frame.read_view(view=view, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.load","title":"load","text":"<pre><code>load() -&gt; DataFrame\n</code></pre> <p>Loads saved data from disk.</p> <p>The data frame object holding the same name as the current <code>DataFrame</code> instance will be loaded from disk into the getML engine and updates the current handler using <code>refresh</code>.</p> Example <p>First, we have to create and import data sets. <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\ngetml.data.list_data_frames()\n</code></pre></p> <p>In the output of <code>list_data_frames</code> we can find our underlying data frame object 'test' listed under the 'in_memory' key (it was created and imported by <code>make_numerical</code>). This means the getML engine does only hold it in memory (RAM) yet, and we still have to <code>save</code> it to disk in order to <code>load</code> it again or to prevent any loss of information between different sessions. <pre><code>d.save()\ngetml.data.list_data_frames()\nd2 = getml.DataFrame(name = 'test').load()\n</code></pre></p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Updated handle the underlying data frame in the getML</p> <code>DataFrame</code> <p>engine.</p> Note <p>When invoking <code>load</code> all changes of the underlying data frame object that took place after the last call to the <code>save</code> method will be lost. Thus, this method  enables you to undo changes applied to the <code>DataFrame</code>. <pre><code>d, _ = getml.datasets.make_numerical()\nd.save()\n\n# Accidental change we want to undo\nd.rm('column_01')\n\nd.load()\n</code></pre> If <code>save</code> hasn't been called on the current instance yet, or it wasn't stored to disk in a previous session, <code>load</code> will throw an exception</p> <pre><code>File or directory '../projects/X/data/Y/' not found!\n</code></pre> <p>Alternatively, <code>load_data_frame</code> offers an easier way of creating <code>DataFrame</code> handlers to data in the getML engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def load(self) -&gt; \"DataFrame\":\n    \"\"\"Loads saved data from disk.\n\n    The data frame object holding the same name as the current\n    [`DataFrame`][getml.DataFrame] instance will be loaded from\n    disk into the getML engine and updates the current handler\n    using [`refresh`][getml.DataFrame.refresh].\n\n    Example:\n        First, we have to create and import data sets.\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        getml.data.list_data_frames()\n        ```\n\n        In the output of [`list_data_frames`][getml.data.list_data_frames] we\n        can find our underlying data frame object 'test' listed\n        under the 'in_memory' key (it was created and imported by\n        [`make_numerical`][getml.datasets.make_numerical]). This means the\n        getML engine does only hold it in memory (RAM) yet, and we\n        still have to [`save`][getml.DataFrame.save] it to\n        disk in order to [`load`][getml.DataFrame.load] it\n        again or to prevent any loss of information between\n        different sessions.\n        ```python\n        d.save()\n        getml.data.list_data_frames()\n        d2 = getml.DataFrame(name = 'test').load()\n        ```\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    Note:\n        When invoking [`load`][getml.DataFrame.load] all\n        changes of the underlying data frame object that took\n        place after the last call to the\n        [`save`][getml.DataFrame.save] method will be\n        lost. Thus, this method  enables you to undo changes\n        applied to the [`DataFrame`][getml.DataFrame].\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        d.save()\n\n        # Accidental change we want to undo\n        d.rm('column_01')\n\n        d.load()\n        ```\n        If [`save`][getml.DataFrame.save] hasn't been called\n        on the current instance yet, or it wasn't stored to disk in\n        a previous session, [`load`][getml.DataFrame.load]\n        will throw an exception\n\n            File or directory '../projects/X/data/Y/' not found!\n\n        Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n        offers an easier way of creating\n        [`DataFrame`][getml.DataFrame] handlers to data in the\n        getML engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.load\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.nbytes","title":"nbytes","text":"<pre><code>nbytes() -&gt; uint64\n</code></pre> <p>Size of the data stored in the underlying data frame in the getML engine.</p> RETURNS DESCRIPTION <code>uint64</code> <p>Size of the underlying object in bytes.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nbytes(self) -&gt; np.uint64:\n    \"\"\"Size of the data stored in the underlying data frame in the getML\n    engine.\n\n    Returns:\n            Size of the underlying object in bytes.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nbytes\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nbytes = comm.recv_string(sock)\n\n    return np.uint64(nbytes)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.ncols","title":"ncols","text":"<pre><code>ncols() -&gt; int\n</code></pre> <p>Number of columns in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def ncols(self) -&gt; int:\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.nrows","title":"nrows","text":"<pre><code>nrows() -&gt; int\n</code></pre> <p>Number of rows in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of rows</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nrows(self) -&gt; int:\n    \"\"\"\n    Number of rows in the current instance.\n\n    Returns:\n            Overall number of rows\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nrows\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        nrows = comm.recv_string(sock)\n\n    return int(nrows)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_arrow","title":"read_arrow","text":"<pre><code>read_arrow(table: Table, append: bool = False) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pyarrow.Table</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>table</code>.</p> PARAMETER DESCRIPTION <code>table</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> <p> TYPE: <code>Table</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Current instance.</p> Note <p>For columns containing <code>pandas.Timestamp</code> there can be small inconsistencies in the order of microseconds when sending the data to the getML engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_arrow(self, table: pa.Table, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pyarrow.Table`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `table`.\n\n    Args:\n        table:\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Current instance.\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        be small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(table, pa.Table):\n        raise TypeError(\"'table' must be of type pyarrow.Table.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_arrow\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    with comm.send_and_get_socket(cmd) as sock:\n        with sock.makefile(mode=\"wb\") as sink:\n            batches = table.to_batches()\n            with pa.ipc.new_stream(sink, table.schema) as writer:\n                for batch in batches:\n                    writer.write_batch(batch)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    fnames: List[str],\n    append: bool = False,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n    verbose: bool = True,\n) -&gt; DataFrame\n</code></pre> <p>Read CSV files.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>CSV file paths to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_csv(\n    self,\n    fnames: List[str],\n    append: bool = False,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n    verbose: bool = True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read CSV files.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        fnames:\n            CSV file paths to be read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        verbose:\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n    Returns:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_csv(...).\"\"\"\n        )\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\n            \"\"\"'fnames' must be a list containing at\n            least one path to a CSV file\"\"\"\n        )\n\n    fnames_ = _retrieve_urls(fnames, verbose)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fnames_\"] = fnames_\n\n    cmd[\"append_\"] = append\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"time_formats_\"] = time_formats\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_json","title":"read_json","text":"<pre><code>read_json(\n    json_str: str,\n    append: bool = False,\n    time_formats: Optional[List[str]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from JSON</p> <p>Fills the data frame with data from a JSON string.</p> <p>Args:</p> <pre><code>json_str:\n    The JSON string containing the data.\n\nappend:\n    If a data frame object holding the same ``name`` is\n    already present in the getML, should the content of\n    `json_str` be appended or replace the existing data?\n\ntime_formats:\n    The list of formats tried when parsing time stamps.\n    The formats are allowed to contain the following\n    special characters:\n\n    * %w - abbreviated weekday (Mon, Tue, ...)\n    * %W - full weekday (Monday, Tuesday, ...)\n    * %b - abbreviated month (Jan, Feb, ...)\n    * %B - full month (January, February, ...)\n    * %d - zero-padded day of month (01 .. 31)\n    * %e - day of month (1 .. 31)\n    * %f - space-padded day of month ( 1 .. 31)\n    * %m - zero-padded month (01 .. 12)\n    * %n - month (1 .. 12)\n    * %o - space-padded month ( 1 .. 12)\n    * %y - year without century (70)\n    * %Y - year with century (1970)\n    * %H - hour (00 .. 23)\n    * %h - hour (00 .. 12)\n    * %a - am/pm\n    * %A - AM/PM\n    * %M - minute (00 .. 59)\n    * %S - second (00 .. 59)\n    * %s - seconds and microseconds (equivalent to %S.%F)\n    * %i - millisecond (000 .. 999)\n    * %c - centisecond (0 .. 9)\n    * %F - fractional seconds/microseconds (000000 - 999999)\n    * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n    * %Z - time zone differential in RFC format (GMT or +NNNN)\n    * %% - percent sign\n</code></pre> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Note <p>This does not support NaN values. If you want support for NaN, use <code>from_json</code> instead.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_json(\n    self,\n    json_str: str,\n    append: bool = False,\n    time_formats: Optional[List[str]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Fill from JSON\n\n    Fills the data frame with data from a JSON string.\n\n    Args:\n\n        json_str:\n            The JSON string containing the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `json_str` be appended or replace the existing data?\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n            Handler of the underlying data.\n\n    Note:\n        This does not support NaN values. If you want support for NaN,\n        use [`from_json`][getml.DataFrame.from_json] instead.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_json(...).\"\"\"\n        )\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\n            \"\"\"'time_formats' must be a list of strings\n            containing at least one time format\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.from_json\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n    cmd[\"time_formats_\"] = time_formats\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, json_str)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(\n    fname: str, append: bool = False, verbose: bool = True\n) -&gt; DataFrame\n</code></pre> <p>Read a parquet file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The filepath of the parquet file to be read.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_parquet(\n    self,\n    fname: str,\n    append: bool = False,\n    verbose: bool = True,\n) -&gt; \"DataFrame\":\n    \"\"\"Read a parquet file.\n\n    Args:\n        fname:\n            The filepath of the parquet file to be read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        verbose:\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than\n            zero columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_parquet(...).\"\"\"\n        )\n\n    fname_ = _retrieve_urls([fname], verbose)[0]\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_parquet\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"append_\"] = append\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_s3","title":"read_s3","text":"<pre><code>read_s3(\n    bucket: str,\n    keys: List[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Read CSV files from an S3 bucket.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_s3(\n    self,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    time_formats: Optional[List[str]] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Read CSV files from an S3 bucket.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        sep:\n            The separator used for separating fields.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n            Handler of the underlying data.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if not isinstance(keys, list):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_s3(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"skip_\"] = skip\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_view","title":"read_view","text":"<pre><code>read_view(view: View, append: bool = False) -&gt; DataFrame\n</code></pre> <p>Read the data from a <code>View</code>.</p> PARAMETER DESCRIPTION <code>view</code> <p>The view to read.</p> <p> TYPE: <code>View</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_view(\n    self,\n    view: View,\n    append: bool = False,\n) -&gt; \"DataFrame\":\n    \"\"\"Read the data from a [`View`][getml.data.View].\n\n    Args:\n        view:\n            The view to read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n    Returns:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    view.check()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_view\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = view._getml_deserialize()\n\n    cmd[\"append_\"] = append\n\n    comm.send(cmd)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_db","title":"read_db","text":"<pre><code>read_db(\n    table_name: str,\n    append: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from Database.</p> <p>The DataFrame will be filled from a table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Table from which we want to retrieve the data.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of <code>table_name</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_db(\n    self, table_name: str, append: bool = False, conn: Optional[Connection] = None\n) -&gt; \"DataFrame\":\n    \"\"\"\n    Fill from Database.\n\n    The DataFrame will be filled from a table in the database.\n\n    Args:\n        table_name:\n            Table from which we want to retrieve the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `table_name` be appended or replace the existing data?\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_db\"\n    cmd[\"name_\"] = self.name\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_pandas","title":"read_pandas","text":"<pre><code>read_pandas(\n    pandas_df: DataFrame, append: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pandas.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> PARAMETER DESCRIPTION <code>pandas_df</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> <p> TYPE: <code>DataFrame</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> <p>Note:     For columns containing <code>pandas.Timestamp</code> there can     occur small inconsistencies in the order of microseconds     when sending the data to the getML engine. This is due to     the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pandas.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        pandas_df:\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Handler of the underlying data.\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        occur small inconsistencies in the order of microseconds\n        when sending the data to the getML engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    table = pa.Table.from_pandas(_modify_pandas_columns(pandas_df))\n\n    return self.read_arrow(table, append=append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_pyspark","title":"read_pyspark","text":"<pre><code>read_pyspark(\n    spark_df: DataFrame, append: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pyspark.sql.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML engine with <code>pandas_df</code>.</p> PARAMETER DESCRIPTION <code>spark_df</code> <p>Data the underlying data frame object in the getML engine should obtain.</p> <p> TYPE: <code>DataFrame</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pyspark(\n    self, spark_df: \"pyspark.sql.DataFrame\", append: bool = False\n) -&gt; \"DataFrame\":\n    \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML engine with `pandas_df`.\n\n    Args:\n        spark_df:\n            Data the underlying data frame object in the getML\n            engine should obtain.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    temp_dir = _retrieve_temp_dir()\n    os.makedirs(temp_dir, exist_ok=True)\n    path = os.path.join(temp_dir, self.name)\n    spark_df.write.mode(\"overwrite\").parquet(path)\n\n    filepaths = [\n        os.path.join(path, filepath)\n        for filepath in os.listdir(path)\n        if filepath[-8:] == \".parquet\"\n    ]\n\n    for i, filepath in enumerate(filepaths):\n        self.read_parquet(filepath, append or i &gt; 0)\n\n    shutil.rmtree(path)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_query","title":"read_query","text":"<pre><code>read_query(\n    query: str,\n    append: Optional[bool] = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from query</p> <p>Fills the data frame with data from a table in the database.</p> PARAMETER DESCRIPTION <code>query</code> <p>The query used to retrieve the data.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_query(\n    self,\n    query: str,\n    append: Optional[bool] = False,\n    conn: Optional[Connection] = None,\n) -&gt; \"DataFrame\":\n    \"\"\"Fill from query\n\n    Fills the data frame with data from a table in the database.\n\n    Args:\n        query:\n            The query used to retrieve the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML engine, should the content in\n            `query` be appended or replace the existing data?\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_query\"\n    cmd[\"name_\"] = self.name\n    cmd[\"query_\"] = query\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; DataFrame\n</code></pre> <p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> <p>This method can be used to avoid encoding conflicts. Note that <code>load</code> as well as several other methods automatically call <code>refresh</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Updated handle the underlying data frame in the getML</p> <code>DataFrame</code> <p>engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def refresh(self) -&gt; \"DataFrame\":\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    This method can be used to avoid encoding conflicts. Note that\n    [`load`][getml.DataFrame.load] as well as several other\n    methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.refresh\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    roles = json.loads(msg)\n\n    self.__init__(name=self.name, roles=roles)  # type: ignore\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.remove_subroles","title":"remove_subroles","text":"<pre><code>remove_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ]\n)\n</code></pre> <p>Removes all <code>subroles</code> from one or more columns.</p> PARAMETER DESCRIPTION <code>columns</code> <p>The columns or the names thereof.</p> <p> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_subroles(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n):\n    \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n    Args:\n        columns:\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_subroles(name, append=False, subroles=[])\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.remove_unit","title":"remove_unit","text":"<pre><code>remove_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ]\n)\n</code></pre> <p>Removes the unit from one or more columns.</p> PARAMETER DESCRIPTION <code>columns</code> <p>The columns or the names thereof.</p> <p> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_unit(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n):\n    \"\"\"Removes the unit from one or more columns.\n\n    Args:\n        columns:\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_unit(name, \"\")\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.save","title":"save","text":"<pre><code>save() -&gt; DataFrame\n</code></pre> <p>Writes the underlying data in the getML engine to disk.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def save(self) -&gt; \"DataFrame\":\n    \"\"\"Writes the underlying data in the getML engine to disk.\n\n    Returns:\n            The current instance.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.save\"\n    cmd[\"name_\"] = self.name\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_role","title":"set_role","text":"<pre><code>set_role(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n)\n</code></pre> <p>Assigns a new role to one or more columns.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret Time Stamps. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names of the columns.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Example <p><pre><code>data_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\ndf = getml.DataFrame.from_dict(data_df, \"animal_elections\")\ndf.set_role(['animal'], getml.data.roles.categorical)\ndf.set_role(['votes'], getml.data.roles.numerical)\ndf.set_role(\n    ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\ndf\n</code></pre> <pre><code>| date                        | animal      | votes     |\n| time stamp                  | categorical | numerical |\n---------------------------------------------------------\n| 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n| 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n| 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_role(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Assigns a new role to one or more columns.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols:\n            The columns or the names of the columns.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    Example:\n        ```python\n        data_df = dict(\n            animal=[\"hawk\", \"parrot\", \"goose\"],\n            votes=[12341, 5127, 65311],\n            date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n        df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n        df.set_role(['animal'], getml.data.roles.categorical)\n        df.set_role(['votes'], getml.data.roles.numerical)\n        df.set_role(\n            ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n        df\n        ```\n        ```\n        | date                        | animal      | votes     |\n        | time stamp                  | categorical | numerical |\n        ---------------------------------------------------------\n        | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n        | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n        | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n        ```\n    \"\"\"\n    # ------------------------------------------------------------\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # ------------------------------------------------------------\n\n    names = _handle_cols(cols)\n\n    if not isinstance(role, str):\n        raise TypeError(\"'role' must be str.\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    # ------------------------------------------------------------\n\n    for nname in names:\n        if nname not in self.colnames:\n            raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n    if role not in self._possible_keys:\n        raise ValueError(\n            \"'role' must be one of the following values: \"\n            + str(self._possible_keys)\n        )\n\n    # ------------------------------------------------------------\n\n    for name in names:\n        if self[name].role != role:\n            self._set_role(name, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_subroles","title":"set_subroles","text":"<pre><code>set_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    subroles: Optional[Union[str, List[str]]] = None,\n    append: Optional[bool] = True,\n)\n</code></pre> <p>Assigns one or several new <code>subroles</code> to one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>subroles</code> <p>The subroles to be assigned. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[str, List[str]]]</code> DEFAULT: <code>None</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_subroles(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    subroles: Optional[Union[str, List[str]]] = None,\n    append: Optional[bool] = True,\n):\n    \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        subroles:\n            The subroles to be assigned.\n            Must be from [`subroles`][getml.data.subroles].\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if isinstance(subroles, str):\n        subroles = [subroles]\n\n    if not _is_non_empty_typed_list(subroles, str):\n        raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be a bool.\")\n\n    for name in names:\n        self._set_subroles(name, append, subroles)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_unit","title":"set_unit","text":"<pre><code>set_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    unit: str,\n    comparison_only: bool = False,\n)\n</code></pre> <p>Assigns a new unit to one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_unit(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    unit: str,\n    comparison_only: bool = False,\n):\n    \"\"\"Assigns a new unit to one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not isinstance(unit, str):\n        raise TypeError(\"Parameter 'unit' must be a str.\")\n\n    if comparison_only:\n        unit += COMPARISON_ONLY\n\n    for name in names:\n        self._set_unit(name, unit)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Creates a <code>pyarrow.Table</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> RETURNS DESCRIPTION <code>Table</code> <p>Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n            Pyarrow equivalent of the current instance including its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = 0,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname:\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_db","title":"to_db","text":"<pre><code>to_db(table_name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Writes the underlying data into a newly created table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name:\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or database.Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    cmd = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_db\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_html","title":"to_html","text":"<pre><code>to_html(max_rows: int = 10)\n</code></pre> <p>Represents the data frame in HTML format, optimized for an iPython notebook.</p> PARAMETER DESCRIPTION <code>max_rows</code> <p>The maximum number of rows to be displayed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_html(self, max_rows: int = 10):\n    \"\"\"\n    Represents the data frame in HTML format, optimized for an\n    iPython notebook.\n\n    Args:\n        max_rows:\n            The maximum number of rows to be displayed.\n    \"\"\"\n\n    if not _exists_in_memory(self.name):\n        return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n    formatted = self._format()\n    formatted.max_rows = max_rows\n\n    footer = self._collect_footer_data()\n\n    return formatted._render_html(footer=footer)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Creates a <code>pandas.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs <code>pandas.DataFrame</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas equivalent of the current instance including</p> <code>DataFrame</code> <p>its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    `pandas.DataFrame`.\n\n    Returns:\n            Pandas equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_parquet","title":"to_parquet","text":"<pre><code>to_parquet(\n    fname: str,\n    compression: Literal[\n        \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    ] = \"snappy\",\n)\n</code></pre> <p>Writes the underlying data into a newly created parquet file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>compression</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <p> TYPE: <code>Literal['brotli', 'gzip', 'lz4', 'snappy', 'zstd']</code> DEFAULT: <code>'snappy'</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_parquet(\n    self,\n    fname: str,\n    compression: Literal[\"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"] = \"snappy\",\n):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname:\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression:\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(name: Optional[str] = None) -&gt; Placeholder\n</code></pre> <p>Generates a <code>Placeholder</code> from the current <code>DataFrame</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current data frame.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Placeholder</code> <p>A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_placeholder(self, name: Optional[str] = None) -&gt; Placeholder:\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        name:\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current data frame.\n\n    Returns:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_pyspark","title":"to_pyspark","text":"<pre><code>to_pyspark(\n    spark: SparkSession, name: Optional[str] = None\n) -&gt; DataFrame\n</code></pre> <p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> PARAMETER DESCRIPTION <code>spark</code> <p>The pyspark session in which you want to create the data frame.</p> <p> TYPE: <code>SparkSession</code> </p> <code>name</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pyspark(\n    self, spark: \"pyspark.sql.SparkSession\", name: Optional[str] = None\n) -&gt; \"pyspark.sql.DataFrame\":\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark:\n            The pyspark session in which you want to\n            create the data frame.\n\n        name:\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n            Pyspark equivalent of the current instance including its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_s3","title":"to_s3","text":"<pre><code>to_s3(\n    bucket: str,\n    key: str,\n    region: str,\n    sep: Optional[str] = \",\",\n    batch_size: Optional[int] = 50000,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file located in an S3 bucket. Note:     Note that S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>50000</code> </p> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_df.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: Optional[str] = \",\",\n    batch_size: Optional[int] = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        key:\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_df.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.unload","title":"unload","text":"<pre><code>unload()\n</code></pre> <p>Unloads the data frame from memory.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads the data frame from memory.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    self._delete(mem_only=True)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.where","title":"where","text":"<pre><code>where(\n    index: Union[\n        Integral,\n        slice,\n        BooleanColumnView,\n        FloatColumnView,\n        FloatColumn,\n    ]\n) -&gt; View\n</code></pre> <p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> PARAMETER DESCRIPTION <code>index</code> <p>Indicates the rows you want to select.</p> <p> TYPE: <code>Union[Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new <code>View</code> containing the selected rows.</p> Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def where(\n    self,\n    index: Union[\n        numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n    ],\n) -&gt; View:\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index:\n            Indicates the rows you want to select.\n\n    Returns:\n            A new [`View`][getml.data.View] containing the selected rows.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(int(index), int(index) + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, _ = _make_default_slice(index, len(self))\n        selector = arange(start, stop, index.step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[str] = None,\n    subroles: Optional[Union[str, List[str]]] = None,\n    unit: Optional[str] = \"\",\n    time_formats: Optional[List[str]] = None,\n)\n</code></pre> <p>Returns a new <code>View</code> that contains an additional column.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column to be added.</p> <p> TYPE: <code>Union[bool, str, float, int, datetime64, FloatColumn, FloatColumnView, StringColumn, StringColumnView, BooleanColumnView]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>getml.data.roles</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[str, List[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_column(\n    self,\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        np.datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[str] = None,\n    subroles: Optional[Union[str, List[str]]] = None,\n    unit: Optional[str] = \"\",\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col:\n            The column to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from `getml.data.roles`.\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_name","title":"with_name","text":"<pre><code>with_name(name: str) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new view.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the new name.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_name(self, name: str) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name:\n            The name of the new view.\n\n    Returns:\n        A new view with the new name.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_role","title":"with_role","text":"<pre><code>with_role(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n)\n</code></pre> <p>Returns a new <code>View</code> with modified roles.</p> <p>The difference between <code>with_role</code> and <code>set_role</code> is that <code>with_role</code> returns a view that is lazily evaluated when needed whereas <code>set_role</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_role</code> are preferable.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_role(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    The difference between [`with_role`][getml.DataFrame.with_role] and\n    [`set_role`][getml.DataFrame.set_role] is that\n    [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n    evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to\n            parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, cols, role, time_formats)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_subroles","title":"with_subroles","text":"<pre><code>with_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    subroles: Union[str, List[str]],\n    append: bool = True,\n)\n</code></pre> <p>Returns a new view with one or several new subroles on one or more columns.</p> <p>The difference between <code>with_subroles</code> and <code>set_subroles</code> is that <code>with_subroles</code> returns a view that is lazily evaluated when needed whereas <code>set_subroles</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_subroles</code> are preferable.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>subroles</code> <p>The subroles to be assigned.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_subroles(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    subroles: Union[str, List[str]],\n    append: bool = True,\n):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n    [`set_subroles`][getml.DataFrame.set_subroles] is that\n    [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n    evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        subroles:\n            The subroles to be assigned.\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, cols, subroles, append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_unit","title":"with_unit","text":"<pre><code>with_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    unit: str,\n    comparison_only: bool = False,\n)\n</code></pre> <p>Returns a view that contains a new unit on one or more columns.</p> <p>The difference between <code>with_unit</code> and <code>set_unit</code> is that <code>with_unit</code> returns a view that is lazily evaluated when needed whereas <code>set_unit</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_unit</code> are preferable.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will also set the <code>compare</code> subrole. The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_unit(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    unit: str,\n    comparison_only: bool = False,\n):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    The difference between [`with_unit`][getml.DataFrame.with_unit] and\n    [`set_unit`][getml.DataFrame.set_unit] is that\n    [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n    evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will also set the\n            [`compare`][getml.data.subroles.only.compare] subrole. The feature\n            learning algorithms and the feature selectors will interpret this\n            accordingly.\n    \"\"\"\n    return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/data_model/","title":"DataModel","text":""},{"location":"reference/data/data_model/#getml.data.DataModel","title":"getml.data.DataModel","text":"<pre><code>DataModel(population: Union[Placeholder, str])\n</code></pre> <p>Abstract representation of the relationship between tables.</p> <p>You might also want to refer to <code>Placeholder</code>.</p> ATTRIBUTE DESCRIPTION <code>population</code> <p>The placeholder representing the population table, which defines the statistical population and contains the targets.</p> <p> </p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n    )\n</code></pre></p> <p>Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre>     If you want to check out a real-world example where this     is necessary, refer to the     CORA notebook.</p> Source code in <code>getml/data/data_model.py</code> <pre><code>def __init__(self, population: Union[Placeholder, str]):\n    if isinstance(population, str):\n        population = Placeholder(population)\n\n    if not isinstance(population, Placeholder):\n        raise TypeError(\n            \"'population' must be a getml.data.Placeholder or a str, got \"\n            + type(population).__name__\n            + \".\"\n        )\n\n    self.population = population\n\n    self.peripheral = {}\n</code></pre>"},{"location":"reference/data/data_model/#getml.data.DataModel.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>A list of the names of all tables contained in the DataModel.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of names.</p>"},{"location":"reference/data/data_model/#getml.data.DataModel.add","title":"add","text":"<pre><code>add(*placeholders: Placeholder)\n</code></pre> <p>Adds peripheral placeholders to the data model.</p> PARAMETER DESCRIPTION <code>placeholders</code> <p>The placeholder or placeholders you would like to add.</p> <p> TYPE: <code>Placeholder</code> DEFAULT: <code>()</code> </p> Source code in <code>getml/data/data_model.py</code> <pre><code>def add(self, *placeholders: Placeholder):\n    \"\"\"\n    Adds peripheral placeholders to the data model.\n\n    Args:\n        placeholders:\n            The placeholder or placeholders you would like to add.\n    \"\"\"\n\n    def to_list(elem):\n        return elem if isinstance(elem, list) else [elem]\n\n    # We want to be 100% sure that all handles are unique,\n    # so we need deepcopy.\n    placeholders_dc = [\n        deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n    ]\n\n    if not _is_typed_list(placeholders_dc, Placeholder):\n        raise TypeError(\n            \"'placeholders' must consist of getml.data.Placeholders \"\n            + \"or lists thereof.\"\n        )\n\n    for placeholder in placeholders_dc:\n        self._add(placeholder)\n</code></pre>"},{"location":"reference/data/load_container/","title":"Load container","text":""},{"location":"reference/data/load_container/#getml.data.load_container","title":"getml.data.load_container","text":"<p>Loads a container.</p>"},{"location":"reference/data/load_container/#getml.data.load_container.load_container","title":"load_container","text":"<pre><code>load_container(container_id: str) -&gt; Container\n</code></pre> <p>Loads a container and all associated data frames from disk.</p> PARAMETER DESCRIPTION <code>container_id</code> <p>The id of the container you would like to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Container</code> <p>The container with the given id.</p> Source code in <code>getml/data/load_container.py</code> <pre><code>def load_container(container_id: str) -&gt; Container:\n    \"\"\"\n    Loads a container and all associated data frames from disk.\n\n    Args:\n        container_id:\n            The id of the container you would like to load.\n\n    Returns:\n        The container with the given id.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataContainer.load\"\n    cmd[\"name_\"] = container_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    cmd = json.loads(json_str)\n\n    population = _load_view(cmd[\"population_\"]) if \"population_\" in cmd else None\n\n    peripheral = {k: _load_view(v) for (k, v) in cmd[\"peripheral_\"].items()}\n\n    subsets = {k: _load_view(v) for (k, v) in cmd[\"subsets_\"].items()}\n\n    split = _parse(cmd[\"split_\"]) if \"split_\" in cmd else None\n\n    deep_copy = cmd[\"deep_copy_\"]\n    frozen_time = cmd[\"frozen_time_\"] if \"frozen_time_\" in cmd else None\n    last_change = cmd[\"last_change_\"]\n\n    container = Container(\n        population=population, peripheral=peripheral, deep_copy=deep_copy, **subsets\n    )\n\n    container._id = container_id\n    container._frozen_time = frozen_time\n    container._split = split\n    container._last_change = last_change\n\n    return container\n</code></pre>"},{"location":"reference/data/placeholder/","title":"Placeholder","text":""},{"location":"reference/data/placeholder/#getml.data.Placeholder","title":"getml.data.Placeholder","text":"<pre><code>Placeholder(\n    name: str,\n    roles: Optional[\n        Union[Roles, Dict[str, List[str]]]\n    ] = None,\n)\n</code></pre> <p>Abstract representation of tables and their relations.</p> <p>This class is an abstract representation of the <code>DataFrame</code> or <code>View</code>. However, it does not contain any actual data.</p> <p>You might also want to refer to <code>DataModel</code>.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name used for this placeholder. This name will appear in the generated SQL code.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>The roles of the columns in this placeholder. If you pass a dictionary, the keys must be the column names and the values must be lists of roles. If you pass a <code>Roles</code> object, it will be used as is.</p> <p> </p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre> If you want to check out a real-world example where this is necessary, refer to the CORA notebook .</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def __init__(\n    self, name: str, roles: Optional[Union[Roles, Dict[str, List[str]]]] = None\n):\n    self._name = name\n\n    if roles is None:\n        self._roles: Roles = Roles()\n    elif isinstance(roles, dict):\n        self._roles = Roles(**roles)\n    else:\n        self._roles = roles\n\n    self.joins: List[Join] = []\n    self.parent = None\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.join","title":"join","text":"<pre><code>join(\n    right: Placeholder,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>Joins another to placeholder to this placeholder.</p> PARAMETER DESCRIPTION <code>right</code> <p>The placeholder you would like to join.</p> <p> TYPE: <code>Placeholder</code> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> TYPE: <code>OnType</code> DEFAULT: <code>None</code> </p> <code>time_stamps</code> <p>The time stamps used to limit the join.</p> <p> TYPE: <code>TimeStampsType</code> DEFAULT: <code>None</code> </p> <code>relationship</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>many_to_many</code> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right that serves as an upper limit on the join.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def join(\n    self,\n    right: \"Placeholder\",\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins another to placeholder to this placeholder.\n\n    Args:\n        right:\n            The placeholder you would like to join.\n\n        on:\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps:\n            The time stamps used to limit the join.\n\n        relationship:\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory:\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon:\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets:\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp:\n            Name of a time stamp in *right* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right, type(self)):\n        msg = (\n            \"'right' must be a getml.data.Placeholder. \"\n            + \"You can create a placeholder by calling .to_placeholder() \"\n            + \"on DataFrames or Views.\"\n        )\n        raise TypeError(msg)\n\n    if self in right.to_list():\n        raise ValueError(\n            \"Cicular references to other placeholders are not allowed.\"\n        )\n\n    if isinstance(on, str):\n        on = (on, on)\n\n    if isinstance(time_stamps, str):\n        time_stamps = (time_stamps, time_stamps)\n\n    keys = (\n        list(zip(*on))\n        if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n        else on\n    )\n\n    for i, ph in enumerate([self, right]):\n        if ph.roles.join_key and keys:\n            not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n            if not_a_join_key:\n                raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n        if ph.roles.time_stamp and time_stamps:\n            if time_stamps[i] not in ph.roles.time_stamp:\n                raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n    if lagged_targets and horizon in (0.0, None):\n        raise ValueError(\n            \"If you allow lagged targets, then you must also set a \"\n            + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n        )\n\n    if horizon not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'horizon' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    if memory not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'memory' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    join = Join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    if any(join == existing for existing in self.joins):\n        raise ValueError(\n            \"A join with the following set of parameters already exists on \"\n            f\"the placeholder {self.name!r}:\"\n            f\"\\n\\n{join}\\n\\n\"\n            \"Redundant joins are not allowed.\"\n        )\n\n    self.joins.append(join)\n    right.parent = self  # type: ignore\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.to_list","title":"to_list","text":"<pre><code>to_list()\n</code></pre> <p>Returns a list of this placeholder and all of its descendants.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list of this placeholder and all of its descendants.\n    \"\"\"\n    return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Expresses this placeholder and all of its descendants as a dictionary.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses this placeholder and all of its descendants as a dictionary.\n    \"\"\"\n    phs = {}\n    for ph in self.to_list():\n        key = ph.name\n        if ph.children:\n            i = 2\n            while key in phs:\n                key = f\"{ph.name}{i}\"\n                i += 1\n        phs[key] = ph\n    return phs\n</code></pre>"},{"location":"reference/data/relationship/","title":"relationship","text":""},{"location":"reference/data/relationship/#getml.data.relationship","title":"getml.data.relationship","text":"<p>Marks the relationship between joins in <code>Placeholder</code></p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_many","title":"many_to_many  <code>module-attribute</code>","text":"<pre><code>many_to_many: ManyToMany = 'many-to-many'\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_one","title":"many_to_one  <code>module-attribute</code>","text":"<pre><code>many_to_one: ManyToONE = 'many-to-one'\n</code></pre> <p>Used for many-to-one relationships.</p> <p>If two tables are guaranteed to be in a many-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked many-to-one, but the assumption is violated, the pipeline will raise an exception.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_many","title":"one_to_many  <code>module-attribute</code>","text":"<pre><code>one_to_many: OneToMany = 'one-to-many'\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_one","title":"one_to_one  <code>module-attribute</code>","text":"<pre><code>one_to_one: OneToOne = 'one-to-one'\n</code></pre> <p>Used for one-to-one relationships.</p> <p>If two tables are guaranteed to be in a one-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked one-to-one, but the assumption is violated, the pipeline will raise an exception. If you are unsure whether you want to use many_to_one or one_to_one, user many_to_one.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.propositionalization","title":"propositionalization  <code>module-attribute</code>","text":"<pre><code>propositionalization: Propositionalization = (\n    \"propositionalization\"\n)\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>The flag means that you want a propositionalization algorithm to be used for this particular join. This is recommended when there are very many matches within the join and normal algorithms would take too long.</p>"},{"location":"reference/data/roles/","title":"roles","text":""},{"location":"reference/data/roles/#getml.data.roles","title":"getml.data.roles","text":"<p>A role determines if and how <code>columns</code> are handled during the construction of the <code>DataModel</code> and used by the feature learning algorithm (see <code>feature_learning</code>).</p> Example <pre><code>data_frame = getml.DataFrame.from_db(\n    name=name,\n    table_name=table_name,\n    conn=conn\n)\n\ndata_frame.set_role(\n    [\"store_nbr\", \"item_nbr\"], getml.data.roles.join_key)\ndata_frame.set_role(\"date\", getml.data.roles.time_stamp)\ndata_frame.set_role(\"units\", getml.data.roles.target)\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.categorical","title":"categorical  <code>module-attribute</code>","text":"<pre><code>categorical = 'categorical'\n</code></pre> <p>Marks categorical columns.</p> <p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integer instead of strings in your provided data set.</p>"},{"location":"reference/data/roles/#getml.data.roles.join_key","title":"join_key  <code>module-attribute</code>","text":"<pre><code>join_key = 'join_key'\n</code></pre> <p>Marks join keys.</p> <p>Role required to establish a relation between two <code>Placeholder</code>, the abstract representation of the <code>DataFrame</code>, by using the <code>join</code> method. Please refer to the chapter Data Model for details.</p> <p>The content of this column is allowed to contain NULL values. But beware, columns with NULL in their join keys won't be matched to anything, not even to NULL in other join keys.</p> <p><code>columns</code> of this role will not be handled by the feature learning algorithm.</p>"},{"location":"reference/data/roles/#getml.data.roles.numerical","title":"numerical  <code>module-attribute</code>","text":"<pre><code>numerical = 'numerical'\n</code></pre> <p>Marks numerical columns.</p> <p>This role tells the getML engine to include the associated <code>FloatColumn</code> during feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch.</p>"},{"location":"reference/data/roles/#getml.data.roles.target","title":"target  <code>module-attribute</code>","text":"<pre><code>target = 'target'\n</code></pre> <p>Marks the column(s) we would like to predict.</p> <p>The associated <code>columns</code> contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>lagged_target</code> in <code>join</code>). But they are such an important part of the analysis that the population table is required to contain at least one of them (refer to Data Model Tables).</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>.</p>"},{"location":"reference/data/roles/#getml.data.roles.text","title":"text  <code>module-attribute</code>","text":"<pre><code>text = 'text'\n</code></pre> <p>Marks text columns.</p> <p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering. Unlike categorical columns, text columns can not be used as a whole. Instead, the feature learners have to apply basic text mining techniques before they are able to use them.</p>"},{"location":"reference/data/roles/#getml.data.roles.time_stamp","title":"time_stamp  <code>module-attribute</code>","text":"<pre><code>time_stamp = 'time_stamp'\n</code></pre> <p>Marks time stamps.</p> <p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps: <pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre></p> <p>This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a <code>StringColumn</code>, you need to specify the format of this string. You can do so by using the <code>time_formats</code> argument of <code>set_role</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p>If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL.</p> Example <p><pre><code>data_df = dict(\n        date1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\n        date2=['1971-01-01', '1971-01-02', '1971-01-03'],\n        date3=['1|1|71', '1|2|71', '1|3|71'],\n    )\ndf = getml.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n</code></pre> <pre><code>| date1                       | date2                       | date3                       |\n| time stamp                  | time stamp                  | time stamp                  |\n-------------------------------------------------------------------------------------------\n| 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n| 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n| 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre></p> Note <p>getML time stamps are actually floats expressing the number of seconds since UNIX time (1970-01-01T00:00:00).</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_float","title":"unused_float  <code>module-attribute</code>","text":"<pre><code>unused_float = 'unused_float'\n</code></pre> <p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_string","title":"unused_string  <code>module-attribute</code>","text":"<pre><code>unused_string = 'unused_string'\n</code></pre> <p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles_obj/","title":"Roles","text":"getml.data.Roles"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles","title":"Roles  <code>dataclass</code>","text":"<pre><code>Roles(\n    categorical: List[str] = list(),\n    join_key: List[str] = list(),\n    numerical: List[str] = list(),\n    target: List[str] = list(),\n    text: List[str] = list(),\n    time_stamp: List[str] = list(),\n    unused_float: List[str] = list(),\n    unused_string: List[str] = list(),\n)\n</code></pre> <p>Roles can be passed to <code>DataFrame</code> to predefine the roles assigned to certain columns.</p> ATTRIBUTE DESCRIPTION <code>categorical</code> <p>Names of the categorical columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>join_key</code> <p>Names of the join key columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>numerical</code> <p>Names of the numerical columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>target</code> <p>Names of the target columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>text</code> <p>Names of the text columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>time_stamp</code> <p>Names of the time stamp columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>unused_float</code> <p>Names of the unused float columns.</p> <p> TYPE: <code>List[str]</code> </p> <code>unused_string</code> <p>Names of the unused string columns.</p> <p> TYPE: <code>List[str]</code> </p> Example <pre><code>roles = getml.data.Roles(\n    categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n)\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>The name of all columns contained in the roles object.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of all columns.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.unused","title":"unused  <code>property</code>","text":"<pre><code>unused: List[str]\n</code></pre> <p>Names of all unused columns (unused_float + unused_string).</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of column names that are categorized as unused, combining both float and string types.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.infer","title":"infer","text":"<pre><code>infer(colname: str) -&gt; str\n</code></pre> <p>Infers the role of a column.</p> PARAMETER DESCRIPTION <code>colname</code> <p>The name of the column to be inferred.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The role of the column as a string.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def infer(self, colname: str) -&gt; str:\n    \"\"\"\n    Infers the role of a column.\n\n    Args:\n        colname:\n            The name of the column to be inferred.\n\n    Returns:\n        The role of the column as a string.\n    \"\"\"\n    for role in self:\n        if colname in self[role]:\n            return role\n    raise ValueError(\"Column named '\" + colname + \"' not found.\")\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, List[str]]\n</code></pre> <p>Expresses the roles object as a dictionary.</p> RETURNS DESCRIPTION <code>Dict[str, List[str]]</code> <p>A dictionary where keys are role names and values are lists of column names.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Expresses the roles object as a dictionary.\n\n    Returns:\n        A dictionary where keys are role names and values are lists of column names.\n    \"\"\"\n    return {role: self[role] for role in self}\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles_obj.Roles.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[str]\n</code></pre> <p>Returns a list containing the roles, without the corresponding columns names.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list where each element is a role name, repeated by the number of columns in that role.</p> Source code in <code>getml/data/roles_obj.py</code> <pre><code>def to_list(self) -&gt; List[str]:\n    \"\"\"\n    Returns a list containing the roles, without the corresponding\n    columns names.\n\n    Returns:\n        A list where each element is a role name, repeated by the number of columns in that role.\n    \"\"\"\n    return [r for role in self for r in [role] * len(self[role])]\n</code></pre>"},{"location":"reference/data/split/","title":"split","text":""},{"location":"reference/data/split/#getml.data.split","title":"getml.data.split","text":"<p>Helps you split data into a training, testing, validation or other sets.</p> <p>Examples:</p> <p>Split at random:</p> <p><pre><code>split = getml.data.split.random(\n    train=0.8, test=0.1, validation=0.1\n)\n\ntrain_set = data_frame[split=='train']\nvalidation_set = data_frame[split=='validation']\ntest_set = data_frame[split=='test']\n</code></pre> Split over time:</p> <pre><code>validation_begin = getml.data.time.datetime(2010, 1, 1)\ntest_begin = getml.data.time.datetime(2011, 1, 1)\n\nsplit = getml.data.split.time(\n    population=data_frame,\n    time_stamp=\"ds\",\n    test=test_begin,\n    validation=validation_begin\n)\n\n# Contains all data before 2010-01-01 (not included)\ntrain_set = data_frame[split=='train']\n\n# Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\nvalidation_set = data_frame[split=='validation']\n\n# Contains all data after 2011-01-01 (included)\ntest_set = data_frame[split=='test']\n</code></pre>"},{"location":"reference/data/split/#getml.data.split.concat.concat","title":"concat","text":"<pre><code>concat(\n    name: str, **kwargs: DataFrame\n) -&gt; Tuple[DataFrame, StringColumnView]\n</code></pre> <p>Concatenates several data frames into and produces a split column that keeps track of their origin.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the data frame you would like to create.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>The data frames you would like to concat with the name in which they should appear in the split column.</p> <p> TYPE: <code>DataFrame</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, StringColumnView]</code> <p>A tuple containing the concatenated data frame and the split column.</p> Example <p>A common use case for this functionality are <code>TimeSeries</code>: <pre><code>data_train = getml.DataFrame.from_pandas(\n    datatraining_pandas, name='data_train')\n\ndata_validate = getml.DataFrame.from_pandas(\n    datatest_pandas, name='data_validate')\n\ndata_test = getml.DataFrame.from_pandas(\n    datatest2_pandas, name='data_test')\n\npopulation, split = getml.data.split.concat(\n    \"population\", train=data_train, validate=data_validate, test=data_test)\n\n...\n\ntime_series = getml.data.TimeSeries(\n    population=population, split=split)\n\nmy_pipeline.fit(time_series.train)\n</code></pre></p> Source code in <code>getml/data/split/concat.py</code> <pre><code>def concat(name: str, **kwargs: DataFrame) -&gt; Tuple[DataFrame, StringColumnView]:\n    \"\"\"\n    Concatenates several data frames into and produces a split\n    column that keeps track of their origin.\n\n    Args:\n        name:\n            The name of the data frame you would like to create.\n\n        kwargs:\n            The data frames you would like\n            to concat with the name in which they should appear\n            in the split column.\n\n    Returns:\n        A tuple containing the concatenated data frame and the split column.\n\n    Example:\n        A common use case for this functionality are [`TimeSeries`][getml.data.TimeSeries]:\n        ```python\n        data_train = getml.DataFrame.from_pandas(\n            datatraining_pandas, name='data_train')\n\n        data_validate = getml.DataFrame.from_pandas(\n            datatest_pandas, name='data_validate')\n\n        data_test = getml.DataFrame.from_pandas(\n            datatest2_pandas, name='data_test')\n\n        population, split = getml.data.split.concat(\n            \"population\", train=data_train, validate=data_validate, test=data_test)\n\n        ...\n\n        time_series = getml.data.TimeSeries(\n            population=population, split=split)\n\n        my_pipeline.fit(time_series.train)\n        ```\n    \"\"\"\n\n    if not _is_non_empty_typed_list(list(kwargs.values()), [DataFrame, View]):\n        raise ValueError(\n            \"'kwargs' must be non-empty and contain getml.DataFrames \"\n            + \"or getml.data.Views.\"\n        )\n\n    names = list(kwargs.keys())\n\n    first = kwargs[names[0]]\n\n    population = first.copy(name) if isinstance(first, DataFrame) else first.to_df(name)\n\n    split = from_value(names[0])\n\n    assert isinstance(split, StringColumnView), \"Should be a StringColumnView\"\n\n    for new_df_name in names[1:]:\n        split = split.update(rowid() &gt; population.nrows(), new_df_name)  # type: ignore\n        population = _concat(name, [population, kwargs[new_df_name]])\n\n    return population, split[: population.nrows()]  # type: ignore\n</code></pre>"},{"location":"reference/data/split/#getml.data.split.random.random","title":"random","text":"<pre><code>random(\n    seed: int = 5849,\n    train: float = 0.8,\n    test: float = 0.2,\n    validation: float = 0,\n    **kwargs: float\n) -&gt; StringColumnView\n</code></pre> <p>Returns a <code>StringColumnView</code> that can be used to randomly divide data into training, testing, validation or other sets.</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> <code>train</code> <p>The share of random samples assigned to the training set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>validation</code> <p>The share of random samples assigned to the validation set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>test</code> <p>The share of random samples assigned to the test set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>kwargs</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <p> TYPE: <code>float</code> DEFAULT: <code>{}</code> </p> Example <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.1, validation=0.05, other=0.05\n)\n\ntrain_set = data_frame[split=='train']\nvalidation_set = data_frame[split=='validation']\ntest_set = data_frame[split=='test']\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/random.py</code> <pre><code>def random(\n    seed: int=5849, train: float=0.8, test: float=0.2, validation: float=0, **kwargs: float\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that\n    can be used to randomly divide data into training, testing,\n    validation or other sets.\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n        train:\n            The share of random samples assigned to\n            the training set.\n\n        validation:\n            The share of random samples assigned to\n            the validation set.\n\n        test:\n            The share of random samples assigned to\n            the test set.\n\n        kwargs:\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    Example:\n        ```python\n        split = getml.data.split.random(\n            train=0.8, test=0.1, validation=0.05, other=0.05\n        )\n\n        train_set = data_frame[split=='train']\n        validation_set = data_frame[split=='validation']\n        test_set = data_frame[split=='test']\n        other_set = data_frame[split=='other']\n        ```\n\n    \"\"\"\n\n    values = np.asarray([train, validation, test] + list(kwargs.values()))\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    if np.abs(np.sum(values) - 1.0) &gt; 0.0001:\n        raise ValueError(\n            \"'train', 'validation', 'test' and all other sets must add up to 1, \"\n            + \"but add up to \"\n            + str(np.sum(values))\n            + \".\"\n        )\n\n    upper_bounds = np.cumsum(values)\n    lower_bounds = upper_bounds - values\n\n    names = [\"train\", \"validation\", \"test\"] + list(kwargs.keys())\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            (random_col(seed=seed) &gt;= lower_bounds[i])  # type: ignore\n            &amp; (random_col(seed=seed) &lt; upper_bounds[i]),\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/split/#getml.data.split.time.time","title":"time","text":"<pre><code>time(\n    population: DataFrame,\n    time_stamp: Union[str, FloatColumn, FloatColumnView],\n    validation: Optional[TimeStampType] = None,\n    test: Optional[TimeStampType] = None,\n    **kwargs: TimeStampType\n) -&gt; StringColumnView\n</code></pre> <p>Returns a <code>StringColumnView</code> that can be used to divide data into training, testing, validation or other sets.</p> <p>The arguments are <code>key=value</code> pairs of names (<code>key</code>) and starting points (<code>value</code>). The starting point defines the left endpoint of the subset. Intervals are left closed and right open, such that \\([value, next value)\\).  The (unnamed) subset left from the first named starting point, i.e.  \\([0, first value)\\), is always considered to be the training set.</p> PARAMETER DESCRIPTION <code>population</code> <p>The population table you would like to split.</p> <p> TYPE: <code>DataFrame</code> </p> <code>time_stamp</code> <p>The name of the time stamp column in the population table you want to use. Ideally, the role of said column would be <code>time_stamp</code>. If you want to split on the rowid, then pass \"rowid\" to <code>time_stamp</code>.</p> <p> TYPE: <code>Union[str, FloatColumn, FloatColumnView]</code> </p> <code>validation</code> <p>The start date of the validation set.</p> <p> TYPE: <code>Optional[TimeStampType]</code> DEFAULT: <code>None</code> </p> <code>test</code> <p>The start date of the test set.</p> <p> TYPE: <code>Optional[TimeStampType]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <p> TYPE: <code>TimeStampType</code> DEFAULT: <code>{}</code> </p> Example <pre><code>validation_begin = getml.data.time.datetime(2010, 1, 1)\ntest_begin = getml.data.time.datetime(2011, 1, 1)\nother_begin = getml.data.time.datetime(2012, 1, 1)\n\nsplit = getml.data.split.time(\n    population=data_frame,\n    time_stamp=\"ds\",\n    test=test_begin,\n    validation=validation_begin,\n    other=other_begin\n)\n\n# Contains all data before 2010-01-01 (not included)\ntrain_set = data_frame[split=='train']\n\n# Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\nvalidation_set = data_frame[split=='validation']\n\n# Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\ntest_set = data_frame[split=='test']\n\n# Contains all data after 2012-01-01 (included)\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/time.py</code> <pre><code>def time(\n    population: DataFrame,\n    time_stamp: Union[str, FloatColumn, FloatColumnView],\n    validation: Optional[TimeStampType] = None,\n    test: Optional[TimeStampType] = None,\n    **kwargs: TimeStampType\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that can be used to divide\n    data into training, testing, validation or other sets.\n\n    The arguments are\n    `key=value` pairs of names (`key`) and starting points (`value`).\n    The starting point defines the left endpoint of the subset. Intervals are left\n    closed and right open, such that $[value, next value)$.  The (unnamed) subset\n    left from the first named starting point, i.e.  $[0, first value)$, is always\n    considered to be the training set.\n\n    Args:\n        population:\n            The population table you would like to split.\n\n        time_stamp:\n            The name of the time stamp column in the population table\n            you want to use. Ideally, the role of said column would be\n            [`time_stamp`][getml.data.roles.time_stamp]. If you want to split on the rowid,\n            then pass \"rowid\" to `time_stamp`.\n\n        validation:\n            The start date of the validation set.\n\n        test:\n            The start date of the test set.\n\n        kwargs:\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    Example:\n        ```python\n        validation_begin = getml.data.time.datetime(2010, 1, 1)\n        test_begin = getml.data.time.datetime(2011, 1, 1)\n        other_begin = getml.data.time.datetime(2012, 1, 1)\n\n        split = getml.data.split.time(\n            population=data_frame,\n            time_stamp=\"ds\",\n            test=test_begin,\n            validation=validation_begin,\n            other=other_begin\n        )\n\n        # Contains all data before 2010-01-01 (not included)\n        train_set = data_frame[split=='train']\n\n        # Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\n        validation_set = data_frame[split=='validation']\n\n        # Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\n        test_set = data_frame[split=='test']\n\n        # Contains all data after 2012-01-01 (included)\n        other_set = data_frame[split=='other']\n        ```\n    \"\"\"\n    if not isinstance(population, (DataFrame, View)):\n        raise ValueError(\"'population' must be a DataFrame or a View.\")\n\n    if not isinstance(time_stamp, (str, FloatColumn, FloatColumnView)):\n        raise ValueError(\n            \"'time_stamp' must be a string, a FloatColumn, or a FloatColumnView.\"\n        )\n\n    if not test and not validation and not kwargs:\n        raise ValueError(\"You have to supply at least one starting point.\")\n\n    defaults: Dict[str, Optional[TimeStampType]] = {\n        \"test\": test,\n        \"validation\": validation,\n    }\n\n    sets = {name: value for name, value in defaults.items() if value is not None}\n\n    sets.update({**kwargs})\n\n    values = np.asarray(list(sets.values()))\n    index = np.argsort(values)\n    values = values[index]\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    names = np.asarray(list(sets.keys()))\n    names = names[index]\n\n    if isinstance(time_stamp, str):\n        time_stamp_col = (\n            population[time_stamp] if time_stamp != \"rowid\" else population.rowid\n        )\n    else:\n        time_stamp_col = time_stamp\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            time_stamp_col &gt;= values[i],\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/star_schema/","title":"StarSchema","text":""},{"location":"reference/data/star_schema/#getml.data.StarSchema","title":"getml.data.StarSchema","text":"<pre><code>StarSchema(\n    population: Optional[Union[DataFrame, View]] = None,\n    alias: Optional[str] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]]\n)\n</code></pre> <p>A StarSchema is a simplifying abstraction that can be used for machine learning problems that can be organized in a simple star schema.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model.</p> <p>The class is designed using composition  - it is neither <code>Container</code> nor <code>DataModel</code>, but has both of them.</p> <p>This means that you can always fall back to the more flexible methods using <code>Container</code> and <code>DataModel</code> by directly accessing the attributes <code>container</code> and <code>data_model</code>.</p> ATTRIBUTE DESCRIPTION <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>alias</code> <p>The alias to be used for the population table. This is required, if population is a <code>View</code>.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>join</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>train</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>validation</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>test</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>kwargs</code> <p>The population table used in <code>Subset</code> s other than the predefined train, validation and test subsets. You can call these subsets anything you want to and can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p>Example:     <pre><code># Pass the subset.\nstar_schema = getml.data.StarSchema(\n    my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(star_schema.my_subset)\n</code></pre></p> <p> </p> Example <p>Note that this example is taken from the loans notebook.</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code> and <code>Pipeline</code>.</p> <p><pre><code># First, we insert our data.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# StarSchema, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new)\n\ncontainer.add(\n    trans=trans_new,\n    order=order_new,\n    meta=meta_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# star_schema.train and star_schema.test\n# work just like above.\n</code></pre> Source code in <code>getml/data/star_schema.py</code> <pre><code>def __init__(\n    self,\n    population: Optional[Union[DataFrame, View]] = None,\n    alias: Optional[str] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]]\n):\n    if (population is None or isinstance(population, View)) and alias is None:\n        raise ValueError(\n            \"If 'population' is None or a getml.data.View, you must set an alias.\"\n        )\n\n    self._alias = alias or population.name\n\n    self._container = Container(\n        population=population,\n        peripheral=peripheral,\n        split=split,\n        deep_copy=deep_copy,\n        train=train,\n        validation=validation,\n        test=test,\n        **kwargs,\n    )\n\n    def get_placeholder():\n        if population is not None:\n            return population.to_placeholder(alias)\n        if train is not None:\n            return train.to_placeholder(alias)\n        if validation is not None:\n            return validation.to_placeholder(alias)\n        if test is not None:\n            return test.to_placeholder(alias)\n        assert (\n            len(kwargs) &gt; 0\n        ), \"This should have been checked by Container.__init__.\"\n        return kwargs[list(kwargs.keys())[0]].to_placeholder(alias)\n\n    self._data_model = DataModel(get_placeholder())\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.container","title":"container  <code>property</code>","text":"<pre><code>container: Container\n</code></pre> <p>The underlying <code>Container</code>.</p> RETURNS DESCRIPTION <code>Container</code> <p>The underlying container.</p>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.data_model","title":"data_model  <code>property</code>","text":"<pre><code>data_model: DataModel\n</code></pre> <p>The underlying <code>DataModel</code>.</p> RETURNS DESCRIPTION <code>DataModel</code> <p>The underlying data model.</p>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.join","title":"join","text":"<pre><code>join(\n    right_df: Union[DataFrame, View],\n    alias: Optional[str] = None,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>Joins a <code>DataFrame</code> or <code>View</code> to the population table.</p> <p>In a <code>StarSchema</code> or <code>TimeSeries</code>, all joins take place on the population table. If you want to create more complex data models, use <code>DataModel</code> instead.</p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered:</p> <pre><code>star_schema = getml.data.StarSchema(\n    population=population_table, split=split)\n\nstar_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> <p>If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> <p>You can join over more than one join key:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n    )\n</code></pre> <p>You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up.</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> <p>Please also refer to <code>relationship</code>.</p> PARAMETER DESCRIPTION <code>right_df</code> <p>The data frame or view you would like to join.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>alias</code> <p>The name as which you want right_df to be referred to in the generated SQL code.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> TYPE: <code>OnType</code> DEFAULT: <code>None</code> </p> <code>time_stamps</code> <p>The time stamps used to limit the join.</p> <p> TYPE: <code>TimeStampsType</code> DEFAULT: <code>None</code> </p> <code>relationship</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>many_to_many</code> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def join(\n    self,\n    right_df: Union[DataFrame, View],\n    alias: Optional[str] = None,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n    to the population table.\n\n    In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n    all joins take place on the population table. If you want to create more\n    complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n    Example:\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n\n        ```python\n        star_schema = getml.data.StarSchema(\n            population=population_table, split=split)\n\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n\n        You can join over more than one join key:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n            )\n        ```\n\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n\n        Please also refer to [`relationship`][getml.data.relationship].\n\n    Args:\n        right_df:\n            The data frame or view you would like to join.\n\n        alias:\n            The name as which you want *right_df* to be referred to in\n            the generated SQL code.\n\n        on:\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps:\n            The time stamps used to limit the join.\n\n        relationship:\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory:\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon:\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets:\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp:\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right_df, (DataFrame, View)):\n        raise TypeError(\n            f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n        )\n\n    if isinstance(right_df, View):\n        if alias is None:\n            raise ValueError(\n                \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                \"as a peripheral table.\"\n            )\n\n    def modify_join_keys(on):\n        if isinstance(on, list):\n            return [modify_join_keys(jk) for jk in on]\n\n        if isinstance(on, (str, StringColumn)):\n            on = (on, on)\n\n        if on is not None and on:\n            on = tuple(\n                jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n            )\n\n        return on\n\n    def modify_time_stamps(time_stamps):\n        if isinstance(time_stamps, (str, FloatColumn)):\n            time_stamps = (time_stamps, time_stamps)\n\n        if time_stamps is not None:\n            time_stamps = tuple(\n                time_stamp.name\n                if isinstance(time_stamp, FloatColumn)\n                else time_stamp\n                for time_stamp in time_stamps\n            )\n\n        return time_stamps\n\n    on = modify_join_keys(on)\n\n    time_stamps = modify_time_stamps(time_stamps)\n\n    upper_time_stamp = (\n        upper_time_stamp.name\n        if isinstance(upper_time_stamp, FloatColumn)\n        else upper_time_stamp\n    )\n\n    right = right_df.to_placeholder(alias)\n\n    self.data_model.population.join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    alias = alias or right_df.name\n\n    self.container.add(**{alias: right_df})\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.sync","title":"sync","text":"<pre><code>sync()\n</code></pre> <p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    self.container.sync()\n</code></pre>"},{"location":"reference/data/subroles/","title":"subroles","text":""},{"location":"reference/data/subroles/#getml.data.subroles","title":"getml.data.subroles","text":"<p>Subroles allow for more fine-granular control of how certain columns will be used by the pipeline.</p> <p>A column can have no subrole, one subrole or several subroles.</p> Example <pre><code># The Relboost feature learning algorithm will\n# ignore this column.\nmy_data_frame.set_subroles(\n    \"my_column\", getml.data.subroles.exclude.relboost)\n\n# The Substring preprocessor will be applied to this column.\n# But other preprocessors, feature learners or predictors\n# are not excluded from using it as well.\nmy_data_frame.set_subroles(\n    \"ucc\", getml.data.subroles.include.substring)\n\n# Only the EmailDomain preprocessor will be applied\n# to \"emails\". All other preprocessors, feature learners,\n# feature selectors and predictors will ignore this column.\nmy_data_frame.set_subroles(\"emails\", getml.data.subroles.only.email)\n</code></pre>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude","title":"exclude","text":"<p>Columns marked with a subrole in this submodule will not be used for the specified purpose.</p> Example <pre><code># The Relboost feature learning algorithm will\n# ignore this column.\nmy_data_frame.set_subroles(\n    \"my_column\", getml.data.subroles.exclude.relboost)\n</code></pre>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.category_trimmer","title":"category_trimmer  <code>module-attribute</code>","text":"<pre><code>category_trimmer = 'exclude category trimmer'\n</code></pre> <p>The <code>CategoryTrimmer</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.fastprop","title":"fastprop  <code>module-attribute</code>","text":"<pre><code>fastprop = 'exclude fastprop'\n</code></pre> <p><code>FastProp</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.feature_learners","title":"feature_learners  <code>module-attribute</code>","text":"<pre><code>feature_learners = 'exclude feature learners'\n</code></pre> <p>All feature learners (<code>feature_learning</code>) will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.imputation","title":"imputation  <code>module-attribute</code>","text":"<pre><code>imputation = 'exclude imputation'\n</code></pre> <p>The <code>Imputation</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.mapping","title":"mapping  <code>module-attribute</code>","text":"<pre><code>mapping = 'exclude mapping'\n</code></pre> <p>The <code>Mapping</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.multirel","title":"multirel  <code>module-attribute</code>","text":"<pre><code>multirel = 'exclude multirel'\n</code></pre> <p><code>Multirel</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.predictors","title":"predictors  <code>module-attribute</code>","text":"<pre><code>predictors = 'exclude predictors'\n</code></pre> <p>All <code>predictors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.preprocessors","title":"preprocessors  <code>module-attribute</code>","text":"<pre><code>preprocessors = 'exclude preprocessors'\n</code></pre> <p>All <code>preprocessors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.relboost","title":"relboost  <code>module-attribute</code>","text":"<pre><code>relboost = 'exclude relboost'\n</code></pre> <p><code>Relboost</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.relmt","title":"relmt  <code>module-attribute</code>","text":"<pre><code>relmt = 'exclude relmt'\n</code></pre> <p><code>RelMT</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.seasonal","title":"seasonal  <code>module-attribute</code>","text":"<pre><code>seasonal = 'exclude seasonal'\n</code></pre> <p>The <code>Seasonal</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.text_field_splitter","title":"text_field_splitter  <code>module-attribute</code>","text":"<pre><code>text_field_splitter = 'exclude text field splitter'\n</code></pre> <p>The <code>TextFieldSplitter</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.include","title":"include","text":"<p>Columns marked with a subrole in this submodule will be used for the specified purpose without excluding other purposes.</p> Example <pre><code># The Substring preprocessor will be applied to this column.\n# But other preprocessors, feature learners or predictors\n# are not excluded from using it as well.\nmy_data_frame.set_subroles(\n    \"ucc\", getml.data.subroles.include.substring)\n</code></pre>"},{"location":"reference/data/subroles/#getml.data.subroles.include.email","title":"email  <code>module-attribute</code>","text":"<pre><code>email = 'include email'\n</code></pre> <p>A column with this subrole will be used for the <code>EmailDomain</code> preprocessor.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.include.substring","title":"substring  <code>module-attribute</code>","text":"<pre><code>substring = 'include substring'\n</code></pre> <p>A column with this subrole will be used for the <code>Substring</code> preprocessor.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.only","title":"only","text":"<p>Columns marked with a subrole in this submodule will only be used for the specified purpose and nothing else.</p> Example <pre><code># Only the EmailDomain preprocessor will be applied\n# to \"emails\". All other preprocessors, feature learners,\n# feature selectors and predictors will ignore this column.\nmy_data_frame.set_subroles(\"emails\", getml.data.subroles.only.email)\n</code></pre>"},{"location":"reference/data/subroles/#getml.data.subroles.only.email","title":"email  <code>module-attribute</code>","text":"<pre><code>email = 'only email'\n</code></pre> <p>A column with this subrole will only be used for the <code>EmailDomain</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.only.substring","title":"substring  <code>module-attribute</code>","text":"<pre><code>substring = 'only substring'\n</code></pre> <p>A column with this subrole will only be used for the <code>Substring</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/data/subset/","title":"Subset","text":""},{"location":"reference/data/subset/#getml.data.Subset","title":"getml.data.Subset  <code>dataclass</code>","text":"<pre><code>Subset(\n    container_id: str,\n    peripheral: Dict[str, Union[DataFrame, View]],\n    population: Union[DataFrame, View],\n)\n</code></pre> <p>A Subset consists of a population table and one or several peripheral tables.</p> <p>It is passed by a <code>Container</code>, <code>StarSchema</code> and <code>TimeSeries</code> to the <code>Pipeline</code>.</p> ATTRIBUTE DESCRIPTION <code>container_id</code> <p>The ID of the container the subset belongs to.</p> <p> TYPE: <code>str</code> </p> <code>peripheral</code> <p>A dictionary containing the peripheral tables.</p> <p> TYPE: <code>Dict[str, Union[DataFrame, View]]</code> </p> <code>population</code> <p>The population table.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> Example <pre><code>container = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# train and test are Subsets.\n# They contain population_train\n# and population_test respectively,\n# as well as their peripheral tables\n# meta, order and trans.\nmy_pipeline.fit(container.train)\n\nmy_pipeline.score(container.test)\n</code></pre>"},{"location":"reference/data/time/","title":"time","text":""},{"location":"reference/data/time/#getml.data.time","title":"getml.data.time","text":"<p>Convenience functions for the handling of time stamps.</p> <p>In getML, time stamps are always expressed as a floating point value. This float measures the number of seconds since UNIX time (January 1, 1970, 00:00:00). Smaller units of time are expressed as fractions of a second.</p> <p>To make this a bit easier to handle, this module contains simple convenience functions that express other time units in terms of seconds.</p>"},{"location":"reference/data/time/#getml.data.time.seconds","title":"seconds","text":"<pre><code>seconds(num: float) -&gt; float\n</code></pre> <p>Returns the number of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of seconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num</p> Source code in <code>getml/data/time.py</code> <pre><code>def seconds(num: float) -&gt; float:\n    \"\"\"\n    Returns the number of seconds.\n\n    Args:\n        num:\n            The number of seconds.\n\n    Returns:\n        *num*\n    \"\"\"\n    return num\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.minutes","title":"minutes","text":"<pre><code>minutes(num: float) -&gt; float\n</code></pre> <p>Expresses num minutes in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of minutes.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num minutes expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def minutes(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* minutes in terms of seconds.\n\n    Args:\n        num:\n            The number of minutes.\n\n    Returns:\n        *num* minutes expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.hours","title":"hours","text":"<pre><code>hours(num: float) -&gt; float\n</code></pre> <p>Expresses num hours in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of hours.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num hours expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def hours(num: float)  -&gt; float:\n    \"\"\"\n    Expresses *num* hours in terms of seconds.\n\n    Args:\n        num:\n            The number of hours.\n\n    Returns:\n        *num* hours expressed in terms of seconds.\n    \"\"\"\n    return minutes(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.days","title":"days","text":"<pre><code>days(num: float) -&gt; float\n</code></pre> <p>Expresses num days in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of days.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num days expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def days(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* days in terms of seconds.\n\n    Args:\n        num:\n            The number of days.\n\n    Returns:\n        *num* days expressed in terms of seconds.\n    \"\"\"\n    return hours(num) * 24.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.weeks","title":"weeks","text":"<pre><code>weeks(num: float) -&gt; float\n</code></pre> <p>Expresses num weeks in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of weeks.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num weeks expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def weeks(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* weeks in terms of seconds.\n\n    Args:\n        num:\n            The number of weeks.\n\n    Returns:\n        *num* weeks expressed in terms of seconds.\n    \"\"\"\n    return days(num) * 7.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.milliseconds","title":"milliseconds","text":"<pre><code>milliseconds(num: float) -&gt; float\n</code></pre> <p>Expresses num milliseconds in terms of fractions of a second.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of milliseconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num milliseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def milliseconds(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* milliseconds in terms of fractions of a second.\n\n    Args:\n        num:\n            The number of milliseconds.\n\n    Returns:\n        *num* milliseconds expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.microseconds","title":"microseconds","text":"<pre><code>microseconds(num: float) -&gt; float\n</code></pre> <p>Expresses num microseconds in terms of fractions of a second.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of microseconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num microseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def microseconds(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* microseconds in terms of fractions of a second.\n\n    Args:\n        num:\n            The number of microseconds.\n\n    Returns:\n        *num* microseconds expressed in terms of seconds.\n    \"\"\"\n    return milliseconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.datetime","title":"datetime","text":"<pre><code>datetime(\n    year: int,\n    month: int,\n    day: int,\n    hour: int = 0,\n    minute: int = 0,\n    second: int = 0,\n    microsecond: int = 0,\n) -&gt; float\n</code></pre> <p>Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).</p> PARAMETER DESCRIPTION <code>year</code> <p>Year component of the date.</p> <p> TYPE: <code>int</code> </p> <code>month</code> <p>Month component of the date.</p> <p> TYPE: <code>int</code> </p> <code>day</code> <p>Day component of the date.</p> <p> TYPE: <code>int</code> </p> <code>hour</code> <p>Hour component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>minute</code> <p>Minute component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>second</code> <p>Second component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>microsecond</code> <p>Microsecond component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The number of seconds since UNIX time (January 1, 1970, 00:00:00).</p> Source code in <code>getml/data/time.py</code> <pre><code>def datetime(year: int, month: int, day: int, hour: int=0, minute: int=0, second: int=0, microsecond: int=0) -&gt; float:\n    \"\"\"\n    Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).\n\n    Args:\n        year:\n            Year component of the date.\n\n        month:\n            Month component of the date.\n\n        day:\n            Day component of the date.\n\n        hour:\n            Hour component of the date.\n\n        minute:\n            Minute component of the date.\n\n        second:\n            Second component of the date.\n\n        microsecond:\n            Microsecond component of the date.\n\n    Returns:\n        The number of seconds since UNIX time (January 1, 1970, 00:00:00).\n    \"\"\"\n    return dt.datetime(\n        year,\n        month,\n        day,\n        hour,\n        minute,\n        second,\n        microsecond,\n        tzinfo=dt.timezone.utc,\n    ).timestamp()\n</code></pre>"},{"location":"reference/data/time_series/","title":"TimeSeries","text":""},{"location":"reference/data/time_series/#getml.data.TimeSeries","title":"getml.data.TimeSeries","text":"<pre><code>TimeSeries(\n    population: Union[DataFrame, View],\n    time_stamps: str,\n    alias: Optional[str] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    on: OnType = None,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>StarSchema</code></p> <p>A TimeSeries is a simplifying abstraction that can be used for machine learning problems on time series data.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model. It also abstracts away the need for self joins.</p> ATTRIBUTE DESCRIPTION <code>time_stamps</code> <p>The time stamps used to limit the self-join.</p> <p> </p> <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>alias</code> <p>The alias to be used for the population table. If it isn't set, the 'population' will be used as the alias. To explicitly set an alias for the peripheral table, use <code>with_name</code>.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Provide the value in seconds, alternatively use  the convenience functions from <code>time</code>.</p> <p> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Provide the value in seconds, alternatively use the convenience functions from <code>time</code>.</p> <p> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <p> </p> Example <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Source code in <code>getml/data/time_series.py</code> <pre><code>def __init__(\n    self,\n    population: Union[DataFrame, View],\n    time_stamps: str,\n    alias: Optional[str] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    on: OnType = None,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str]=None,\n):\n\n    if not isinstance(population, (DataFrame, View)):\n        raise TypeError(\n            \"'population' must be a getml.DataFrame or a getml.data.View\"\n        )\n\n    if isinstance(time_stamps, FloatColumn):\n        time_stamps = time_stamps.name\n\n    if isinstance(time_stamps, FloatColumnView):\n        if \"rowid\" in _finditems(\"operator_\", time_stamps.cmd):\n            time_stamps = \"rowid\"\n\n    population = (\n        population.with_column(\n            population.rowid, name=\"rowid\", role=time_stamp\n        ).with_unit(names=\"rowid\", unit=\"rowid\", comparison_only=True)\n        if time_stamps == \"rowid\"\n        else population\n    )\n\n    alias = \"population\" if alias is None else alias\n\n    super().__init__(\n        population=population,\n        alias=alias,\n        peripheral=peripheral,\n        split=split,\n        deep_copy=deep_copy,\n    )\n\n    self.on = on\n    self.time_stamps = time_stamps\n    self.memory = memory\n    self.horizon = horizon\n    self.lagged_targets = lagged_targets\n    self.upper_time_stamp = upper_time_stamp\n\n    if not isinstance(on, list):\n        on = [on]\n\n    for o in on:\n        self._add_joins(o)\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.on","title":"on  <code>instance-attribute</code>","text":"<pre><code>on = on\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.time_stamps","title":"time_stamps  <code>instance-attribute</code>","text":"<pre><code>time_stamps = time_stamps\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory = memory\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.horizon","title":"horizon  <code>instance-attribute</code>","text":"<pre><code>horizon = horizon\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.lagged_targets","title":"lagged_targets  <code>instance-attribute</code>","text":"<pre><code>lagged_targets = lagged_targets\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.upper_time_stamp","title":"upper_time_stamp  <code>instance-attribute</code>","text":"<pre><code>upper_time_stamp = upper_time_stamp\n</code></pre>"},{"location":"reference/data/view/","title":"View","text":""},{"location":"reference/data/view/#getml.data.View","title":"getml.data.View","text":"<pre><code>View(\n    base: Union[DataFrame, View],\n    name: Optional[str] = None,\n    subselection: Optional[\n        Union[\n            BooleanColumnView, FloatColumn, FloatColumnView\n        ]\n    ] = None,\n    added: Optional[Dict] = None,\n    dropped: Optional[List[str]] = None,\n)\n</code></pre> <p>A view is a lazily evaluated, immutable representation of a <code>DataFrame</code>.</p> <p>There are important differences between a <code>DataFrame</code> and a view:</p> <ul> <li> <p>Views are lazily evaluated. That means that views do not   contain any data themselves. Instead, they just refer to   an underlying data frame. If the underlying data frame changes,   so will the view (but such behavior will result in a warning).</p> </li> <li> <p>Views are immutable. In-place operations on a view are not   possible. Any operation on a view will result in a new view.</p> </li> <li> <p>Views have no direct representation on the getML engine, and   therefore they do not need to have an identifying name.</p> </li> </ul> ATTRIBUTE DESCRIPTION <code>base</code> <p>A data frame or view used as the basis for this view.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>name</code> <p>The name assigned to this view.</p> <p> TYPE: <code>str</code> </p> <code>subselection</code> <p>Indicates which rows we would like to keep.</p> <p> TYPE: <code>Union[BooleanColumnView, FloatColumn, FloatColumnView]</code> </p> <code>added</code> <p>A dictionary that describes a new column that has been added to the view.</p> <p> TYPE: <code>Dict</code> </p> <code>dropped</code> <p>A list of columns that have been dropped.</p> <p> TYPE: <code>List[str]</code> </p> Example <p>You hardly ever directly create views. Instead, it is more likely that you will encounter them as a result of some operation on a <code>DataFrame</code>:</p> <p><pre><code># Creates a view on the first 100 lines\nview1 = data_frame[:100]\n\n# Creates a view without some columns.\nview2 = data_frame.drop([\"col1\", \"col2\"])\n\n# Creates a view in which some roles are reassigned.\nview3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n</code></pre> A recommended pattern is to assign 'baseline roles' to your data frames and then using views to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which <code>Container</code> you have used.</p> Source code in <code>getml/data/view.py</code> <pre><code>def __init__(\n    self,\n    base: Union[\"DataFrame\", \"View\"],\n    name: Optional[str] = None,\n    subselection: Optional[Union[\n        BooleanColumnView, FloatColumn, FloatColumnView\n    ]] = None,\n    added: Optional[Dict]=None,\n    dropped: Optional[List[str]] = None,\n):\n    self._added = added\n    self._base = deepcopy(base)\n    self._dropped = dropped or []\n    self._name = name\n    self._subselection = subselection\n\n    self._initial_timestamp: str = (\n        self._base._initial_timestamp\n        if isinstance(self._base, View)\n        else self._base.last_change\n    )\n\n    self._base.refresh()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.added","title":"added  <code>property</code>","text":"<pre><code>added: Dict\n</code></pre> <p>The column that has been added to the view.</p> RETURNS DESCRIPTION <code>Dict</code> <p>The column that has been added to the view.</p>"},{"location":"reference/data/view/#getml.data.View.base","title":"base  <code>property</code>","text":"<pre><code>base: Union[DataFrame, View]\n</code></pre> <p>The basis on which the view is created. Must be a <code>DataFrame</code> or a <code>View</code>.</p> RETURNS DESCRIPTION <code>Union[DataFrame, View]</code> <p>The basis on which the view is created.</p>"},{"location":"reference/data/view/#getml.data.View.colnames","title":"colnames  <code>property</code>","text":"<pre><code>colnames: List[str]\n</code></pre> <p>List of the names of all columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.View.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Alias for <code>colnames</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.View.dropped","title":"dropped  <code>property</code>","text":"<pre><code>dropped: List[str]\n</code></pre> <p>The names of the columns that has been dropped.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of the columns that has been dropped.</p>"},{"location":"reference/data/view/#getml.data.View.last_change","title":"last_change  <code>property</code>","text":"<pre><code>last_change: str\n</code></pre> <p>A string describing the last time this data frame has been changed.</p> RETURNS DESCRIPTION <code>str</code> <p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/view/#getml.data.View.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the view. If no name is explicitly set, the name will be identical to the name of the base.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the view.</p>"},{"location":"reference/data/view/#getml.data.View.roles","title":"roles  <code>property</code>","text":"<pre><code>roles: Roles\n</code></pre> <p>The roles of the columns included in this View.</p> RETURNS DESCRIPTION <code>Roles</code> <p>The roles of the columns included in this View.</p>"},{"location":"reference/data/view/#getml.data.View.rowid","title":"rowid  <code>property</code>","text":"<pre><code>rowid: List[int]\n</code></pre> <p>The rowids for this view.</p> RETURNS DESCRIPTION <code>List[int]</code> <p>The rowids for this view.</p>"},{"location":"reference/data/view/#getml.data.View.subselection","title":"subselection  <code>property</code>","text":"<pre><code>subselection: Union[\n    BooleanColumnView, FloatColumn, FloatColumnView\n]\n</code></pre> <p>The subselection that is applied to this view.</p> RETURNS DESCRIPTION <code>Union[BooleanColumnView, FloatColumn, FloatColumnView]</code> <p>The subselection that is applied to this view.</p>"},{"location":"reference/data/view/#getml.data.View.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: Tuple[Union[int, str], int]\n</code></pre> <p>A tuple containing the number of rows and columns of the View.</p>"},{"location":"reference/data/view/#getml.data.View.check","title":"check","text":"<pre><code>check()\n</code></pre> <p>Checks whether the underlying data frame has been changed after the creation of the view.</p> Source code in <code>getml/data/view.py</code> <pre><code>def check(self):\n    \"\"\"\n    Checks whether the underlying data frame has been changed\n    after the creation of the view.\n    \"\"\"\n    last_change = self.last_change\n    if last_change != self.__dict__[\"_initial_timestamp\"]:\n        logger.warning(\n            \"The data frame underlying view '\"\n            + self.name\n            + \"' was last changed at \"\n            + last_change\n            + \", which was after the creation of the view. \"\n            + \"This might lead to unexpected results. You might \"\n            + \"want to recreate the view. (Views are lazily \"\n            + \"evaluated, so recreating them is a very \"\n            + \"inexpensive operation).\"\n        )\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.drop","title":"drop","text":"<pre><code>drop(cols: Union[str, List[str]]) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that has one or several columns removed.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The names of the columns to be dropped.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the specified columns removed.</p> Source code in <code>getml/data/view.py</code> <pre><code>def drop(self, cols: Union[str, List[str]]) -&gt; \"View\":\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols:\n            The names of the columns to be dropped.\n\n    Returns:\n            A new view with the specified columns removed.\n    \"\"\"\n    if isinstance(cols, str):\n        cols = [cols]\n\n    if not _is_typed_list(cols, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=cols)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.ncols","title":"ncols","text":"<pre><code>ncols() -&gt; int\n</code></pre> <p>Number of columns in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/view.py</code> <pre><code>def ncols(self) -&gt; int:\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.nrows","title":"nrows","text":"<pre><code>nrows(force: bool = False) -&gt; Union[int, str]\n</code></pre> <p>Returns the number of rows in the current instance.</p> PARAMETER DESCRIPTION <code>force</code> <p>If the number of rows is unknown, do you want to force the engine to calculate it anyway? This is a relatively expensive operation, therefore you might not necessarily want this.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[int, str]</code> <p>The number of rows in the current instance.</p> Source code in <code>getml/data/view.py</code> <pre><code>def nrows(self, force: bool=False) -&gt; Union[int, str]:\n    \"\"\"\n    Returns the number of rows in the current instance.\n\n    Args:\n        force:\n            If the number of rows is unknown,\n            do you want to force the engine to calculate it anyway?\n            This is a relatively expensive operation, therefore\n            you might not necessarily want this.\n\n    Returns:\n            The number of rows in the current instance.\n    \"\"\"\n\n    self.refresh()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.get_nrows\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n    cmd[\"force_\"] = force\n\n    with comm.send_and_get_socket(cmd) as sock:\n        json_str = comm.recv_string(sock)\n\n    if json_str[0] != \"{\":\n        comm.engine_exception_handler(json_str)\n\n    result = json.loads(json_str)\n\n    if \"recordsTotal\" in result:\n        return int(result[\"recordsTotal\"])\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; View\n</code></pre> <p>Aligns meta-information of the current instance with the corresponding data frame in the getML engine.</p> RETURNS DESCRIPTION <code>View</code> <p>Updated handle the underlying data frame in the getML</p> <code>View</code> <p>engine.</p> Source code in <code>getml/data/view.py</code> <pre><code>def refresh(self) -&gt; \"View\":\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML engine.\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n            engine.\n\n    \"\"\"\n    self._base = self.__dict__[\"_base\"].refresh()\n    return self\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Creates a <code>pyarrow.Table</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyarrow.Table</code>.</p> RETURNS DESCRIPTION <code>Table</code> <p>Pyarrow equivalent of the current instance including</p> <code>Table</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_arrow(self) -&gt; pyarrow.Table:\n    \"\"\"Creates a `pyarrow.Table` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a JSON string.</p> RETURNS DESCRIPTION <code>str</code> <p>JSON string of the current instance including its</p> <code>str</code> <p>underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a JSON string.\n\n    Returns:\n            JSON string of the current instance including its\n            underlying data.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = 0,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>getml/data/view.py</code> <pre><code>def to_csv(\n    self, fname: str, quotechar: str = '\"', sep: str = \",\", batch_size: int = 0\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname:\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    fname_ = os.path.abspath(fname)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_csv\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"fname_\"] = fname_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_db","title":"to_db","text":"<pre><code>to_db(table_name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Writes the underlying data into a newly created table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/view.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name:\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    if not isinstance(conn, Connection):\n        raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_db\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"table_name_\"] = table_name\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Creates a <code>pandas.DataFrame</code> from the view.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pandas.DataFrame</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas equivalent of the current instance including</p> <code>DataFrame</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a `pandas.DataFrame` from the view.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pandas.DataFrame`.\n\n    Returns:\n            Pandas equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return _to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(name: Optional[str] = None) -&gt; Placeholder\n</code></pre> <p>Generates a <code>Placeholder</code> from the current <code>View</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current view.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Placeholder</code> <p>A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_placeholder(self, name: Optional[str] = None) -&gt; Placeholder:\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`View`][getml.data.View].\n\n    Args:\n        name:\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current view.\n\n    Returns:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_parquet","title":"to_parquet","text":"<pre><code>to_parquet(\n    fname: str,\n    compression: Literal[\n        \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    ] = \"snappy\",\n)\n</code></pre> <p>Writes the underlying data into a newly created parquet file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>compression</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <p> TYPE: <code>Literal['brotli', 'gzip', 'lz4', 'snappy', 'zstd']</code> DEFAULT: <code>'snappy'</code> </p> Source code in <code>getml/data/view.py</code> <pre><code>def to_parquet(self, fname: str, compression: Literal[\"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"]=\"snappy\"):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname:\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression:\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    _to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_pyspark","title":"to_pyspark","text":"<pre><code>to_pyspark(\n    spark: SparkSession, name: Optional[str] = None\n) -&gt; DataFrame\n</code></pre> <p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> PARAMETER DESCRIPTION <code>spark</code> <p>The pyspark session in which you want to create the data frame.</p> <p> TYPE: <code>SparkSession</code> </p> <code>name</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pyspark equivalent of the current instance including</p> <code>DataFrame</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pyspark(self, spark: \"pyspark.sql.SparkSession\", name: Optional[str]=None) -&gt; \"pyspark.sql.DataFrame\":\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark:\n            The pyspark session in which you want to\n            create the data frame.\n\n        name:\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_s3","title":"to_s3","text":"<pre><code>to_s3(\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file located in an S3 bucket.</p> Note <p>S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50000</code> </p> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_view.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/view.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n\n    Note:\n        S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        key:\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    Example:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_view.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.where","title":"where","text":"<pre><code>where(\n    index: Optional[\n        Union[\n            BooleanColumnView, FloatColumn, FloatColumnView\n        ]\n    ]\n) -&gt; View\n</code></pre> <p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> PARAMETER DESCRIPTION <code>index</code> <p>Boolean column indicating the rows you want to select.</p> <p> TYPE: <code>Optional[Union[BooleanColumnView, FloatColumn, FloatColumnView]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view containing only the rows that satisfy the condition.</p> Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/view.py</code> <pre><code>def where(self, index: Optional[Union[BooleanColumnView, FloatColumn, FloatColumnView]]) -&gt; \"View\":\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index:\n            Boolean column indicating the rows you want to select.\n\n    Returns:\n        A new view containing only the rows that satisfy the condition.\n\n    Example:\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n    \"\"\"\n    if isinstance(index, numbers.Integral):\n        index = index if int(index) &gt; 0 else len(self) + index\n        selector = arange(index, index + 1)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, slice):\n        start, stop, step = _make_default_slice(index, len(self))\n        selector = arange(start, stop, step)\n        return View(base=self, subselection=selector)\n\n    if isinstance(index, (BooleanColumnView, FloatColumn, FloatColumnView)):\n        return View(base=self, subselection=index)\n\n    raise TypeError(\"Unsupported type for a subselection: \" + type(index).__name__)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_column","title":"with_column","text":"<pre><code>with_column(\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[\n        Union[dict[str, List[str]], Roles]\n    ] = None,\n    unit: Optional[str] = \"\",\n    subroles: Optional[Union[str, List[str]]] = None,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that contains an additional column.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column to be added.</p> <p> TYPE: <code>Union[bool, str, float, int, datetime64, FloatColumn, FloatColumnView, StringColumn, StringColumnView, BooleanColumnView]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <p> TYPE: <code>Optional[Union[dict[str, List[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[str, List[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view containing the additional column.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_column(\n    self,\n    col:Union[\n    bool,\n    str,\n    float,\n    int,\n    np.datetime64,\n    FloatColumn,\n    FloatColumnView,\n    StringColumn,\n    StringColumnView,\n    BooleanColumnView,\n], name: str, role: Optional[Union[dict[str, List[str]], Roles]] =None, unit: Optional[str]=\"\", subroles: Optional[Union[str, List[str]]] =None, time_formats: Optional[List[str]] = None,\n) -&gt; \"View\":\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col:\n            The column to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        A new view containing the additional column.\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_name","title":"with_name","text":"<pre><code>with_name(name: str) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new view.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the new name.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_name(self, name: str) -&gt; \"View\":\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n\n    Returns:\n        A new view with the new name.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_role","title":"with_role","text":"<pre><code>with_role(\n    names: Union[str, List[str]],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with modified roles.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the modified roles.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_role(self, names: Union[str, List[str]], role: str, time_formats: Optional[List[str]]=None) -&gt; \"View\":\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        names:\n            The name or names of the column.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    Returns:\n        A new view with the modified roles.\n    \"\"\"\n    return _with_role(self, names, role, time_formats)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_subroles","title":"with_subroles","text":"<pre><code>with_subroles(\n    names: Union[str, List[str]],\n    subroles: Union[str, List[str]],\n    append: bool = True,\n)\n</code></pre> <p>Returns a new view with one or several new subroles on one or more columns.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>subroles</code> <p>The subroles to be assigned.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <p>A new view with the modified subroles.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_subroles(self, names: Union[str, List[str]], subroles: Union[str, List[str]], append: bool=True):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    Args:\n        names:\n            The name or names of the column.\n\n        subroles:\n            The subroles to be assigned.\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n\n    Returns:\n        A new view with the modified subroles.\n    \"\"\"\n    return _with_subroles(self, names, subroles, append)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_unit","title":"with_unit","text":"<pre><code>with_unit(\n    names: Union[str, List[str]],\n    unit: str,\n    comparison_only: bool = False,\n) -&gt; View\n</code></pre> <p>Returns a view that contains a new unit on one or more columns.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will append \", comparison only\" to the unit. The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the modified unit.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_unit(self, names: Union[str, List[str]], unit: str, comparison_only: bool=False) -&gt; \"View\":\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    Args:\n        names:\n            The name or names of the column.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will append \", comparison only\" to the unit.\n            The feature learning algorithms and the feature selectors will\n            interpret this accordingly.\n\n    Returns:\n        A new view with the modified unit.\n    \"\"\"\n    return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/database/","title":"Index","text":""},{"location":"reference/database/#getml.database","title":"getml.database","text":"<p>This module contains communication routines to access various databases.</p> <p>The <code>connect_bigquery</code>, <code>connect_hana</code>, <code>connect_greenplum</code>, <code>connect_mariadb</code>, <code>connect_mysql</code>, <code>connect_postgres</code>, and <code>connect_sqlite3</code> functions establish a connection between a database and the getML engine. During the data import using either the <code>read_db</code> or <code>read_query</code> methods of a <code>DataFrame</code> instance or the corresponding <code>from_db</code> class method all data will be directly loaded from the database into the engine without ever passing the Python interpreter.</p> <p>In addition, several auxiliary functions that might be handy during the analysis and interaction with the database are provided.</p>"},{"location":"reference/database/#getml.database.Connection","title":"Connection","text":"<pre><code>Connection(conn_id: str = 'default')\n</code></pre> <p>A handle to a database connection on the getML engine.</p> ATTRIBUTE DESCRIPTION <code>conn_id</code> <p>The name you want to use to reference the connection. You can call it anything you want to. If a database connection with the same conn_id already exists, that connection will be removed automatically and the new connection will take its place. The default conn_id is \"default\", which refers to the default connection. If you do not explicitly pass a connection handle to any function that relates to a database, the default connection will be used automatically.</p> <p> </p> Source code in <code>getml/database/connection.py</code> <pre><code>def __init__(self, conn_id: str = \"default\"):\n    self.conn_id = conn_id\n</code></pre>"},{"location":"reference/database/#getml.database.connect_bigquery.connect_bigquery","title":"connect_bigquery","text":"<pre><code>connect_bigquery(\n    database_id: str,\n    project_id: str,\n    google_application_credentials: Union[str, Path],\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new BigQuery database connection.</p> PARAMETER DESCRIPTION <code>database_id</code> <p>The ID of the database to connect to.</p> <p> TYPE: <code>str</code> </p> <code>project_id</code> <p>The ID of the project to connect to.</p> <p> TYPE: <code>str</code> </p> <code>google_application_credentials</code> <p>The path of the Google application credentials. (Must be located on the machine hosting the getML engine).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> Note <p>Not supported in the getML community edition.</p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Source code in <code>getml/database/connect_bigquery.py</code> <pre><code>def connect_bigquery(\n    database_id: str,\n    project_id: str,\n    google_application_credentials: Union[str, Path],\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new BigQuery database connection.\n\n    Args:\n        database_id:\n            The ID of the database to connect to.\n\n        project_id:\n            The ID of the project to connect to.\n\n        google_application_credentials:\n            The path of the Google application credentials.\n            (Must be located on the machine hosting the getML engine).\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        Not supported in the getML community edition.\n\n    Returns:\n        The connection object.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"database_id_\"] = database_id\n    cmd[\"project_id_\"] = project_id\n    cmd[\"google_application_credentials_\"] = os.path.abspath(\n        str(google_application_credentials)\n    )\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"bigquery\"\n\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The API expects a password, but in this case there is none\n        comm.send_string(sock, \"\")\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_greenplum.connect_greenplum","title":"connect_greenplum","text":"<pre><code>connect_greenplum(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    hostaddr: str,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new Greenplum database connection.</p> <p>But first, make sure your database is running, and you can reach it from your command line.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>hostaddr</code> <p>IP address of the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the Greenplum database.</p> <p>The default port used by Greenplum is 5432.</p> <p>If you do not know, which port to use, type the following into your Greenplum client:</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>5432</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> <p>Note:     By selecting an existing table of your database in     <code>from_db</code> function, you can create     a new <code>DataFrame</code> containing all its data.     Alternatively you can use the     <code>read_db</code> and     <code>read_query</code> methods to replace     the content of the current <code>DataFrame</code>     instance or append further rows based on either a table or a     specific query.</p> <pre><code>You can also write your results back into the Greenplum\ndatabase. By passing the name for the destination table to\n[`transform`][getml.Pipeline.transform], the features\ngenerated from your raw data will be written back. Passing\nthem into [`predict`][getml.Pipeline.predict], instead,\nmakes predictions of the target variables to new, unseen data\nand stores the result into the corresponding table.\n</code></pre> Source code in <code>getml/database/connect_greenplum.py</code> <pre><code>def connect_greenplum(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    hostaddr: str,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"Creates a new Greenplum database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from your command line.\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the Greenplum database.\n\n        password:\n            Password with which to log into the Greenplum database.\n\n        host:\n            Host of the Greenplum database.\n\n        hostaddr:\n            IP address of the Greenplum database.\n\n        port:\n            Port of the Greenplum database.\n\n            The default port used by Greenplum is 5432.\n\n            If you do not know, which port to use, type the following into your\n            Greenplum client:\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the Greenplum\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"greenplum\"\n\n    cmd[\"host_\"] = host\n    cmd[\"hostaddr_\"] = hostaddr\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_hana.connect_hana","title":"connect_hana","text":"<pre><code>connect_hana(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 39017,\n    default_schema: Optional[str] = \"public\",\n    ping_interval: int = 0,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new HANA database connection.</p> PARAMETER DESCRIPTION <code>user</code> <p>Username with which to log into the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the database.</p> <p> TYPE: <code>int</code> DEFAULT: <code>39017</code> </p> <code>default_schema</code> <p>The schema within the database you want to connect use unless another schema is explicitly set.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'public'</code> </p> <code>ping_interval</code> <p>The interval at which you want to ping the database, in seconds. Set to 0 for no pings at all.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> Note <p>Not supported in the getML community edition.</p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Source code in <code>getml/database/connect_hana.py</code> <pre><code>def connect_hana(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 39017,\n    default_schema: Optional[str] = \"public\",\n    ping_interval: int = 0,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new HANA database connection.\n\n    Args:\n        user:\n            Username with which to log into the HANA database.\n\n        password:\n            Password with which to log into the HANA database.\n\n        host:\n            Host of the HANA database.\n\n        port:\n            Port of the database.\n\n        default_schema:\n            The schema within the database you want to connect\n            use unless another schema is explicitly set.\n\n        ping_interval:\n            The interval at which you want to ping the database,\n            in seconds. Set to 0 for no pings at all.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Note:\n        Not supported in the getML community edition.\n\n    Returns:\n        The connection object.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"sap_hana\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"default_schema_\"] = default_schema\n    cmd[\"user_\"] = user\n    cmd[\"ping_interval_\"] = ping_interval\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_mariadb.connect_mariadb","title":"connect_mariadb","text":"<pre><code>connect_mariadb(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new MariaDB database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the MariaDB database.</p> <p>The default port for MariaDB is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your MariaDB client.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3306</code> </p> <code>unix_socket</code> <p>The UNIX socket used to connect to the MariaDB database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your MariaDB client.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'/var/run/mysqld/mysqld.sock'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MariaDB database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mariadb.py</code> <pre><code>def connect_mariadb(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new MariaDB database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the MariaDB database.\n\n        password:\n            Password with which to log into the MariaDB database.\n\n        host:\n            Host of the MariaDB database.\n\n        port:\n            Port of the MariaDB database.\n\n            The default port for MariaDB is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your MariaDB client.\n\n        unix_socket:\n            The UNIX socket used to connect to the MariaDB database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your MariaDB client.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MariaDB\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions\n        of the target variables to new, unseen data and stores the result into\n        the corresponding table.\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_mysql.connect_mysql","title":"connect_mysql","text":"<pre><code>connect_mysql(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new MySQL database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the MySQL database.</p> <p>The default port for MySQL is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your mysql client.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3306</code> </p> <code>unix_socket</code> <p>The UNIX socket used to connect to the MySQL database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your mysql client.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'/var/run/mysqld/mysqld.sock'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MySQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mysql.py</code> <pre><code>def connect_mysql(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new MySQL database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the MySQL database.\n\n        password:\n            Password with which to log into the MySQL database.\n\n        host:\n            Host of the MySQL database.\n\n        port:\n            Port of the MySQL database.\n\n            The default port for MySQL is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your mysql client.\n\n        unix_socket:\n            The UNIX socket used to connect to the MySQL database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your mysql client.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MySQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_odbc.connect_odbc","title":"connect_odbc","text":"<pre><code>connect_odbc(\n    server_name: str,\n    user: str = \"\",\n    password: str = \"\",\n    escape_chars: str = '\"',\n    double_precision: str = \"DOUBLE PRECISION\",\n    integer: str = \"INTEGER\",\n    text: str = \"TEXT\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new ODBC database connection.</p> <p>ODBC is standardized format that can be used to connect to almost any database.</p> <p>Before you use the ODBC connector, make sure the database is up and running and that the appropriate ODBC drivers are installed.</p> PARAMETER DESCRIPTION <code>server_name</code> <p>The server name, as referenced in your .obdc.ini file.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>password</code> <p>Password with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>escape_chars</code> <p>ODBC drivers are supposed to support escaping table and column names using '\"' characters irrespective of the syntax in the target database. Unfortunately, not all ODBC drivers follow this standard. This is why some tuning might be necessary.</p> <p>The escape_chars value behaves as follows:</p> <ul> <li> <p>If you pass an empty string, schema, table and column names will not   be escaped at all. This is not a problem unless some table   or column names are identical to SQL keywords.</p> </li> <li> <p>If you pass a single character, schema, table and column names will   be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)   or <code>TABLE_NAME</code>.<code>COLUMN_NAME</code> (MySQL/MariaDB style).</p> </li> <li> <p>If you pass two characters, table, column and schema names will be   enveloped between these to characters. For instance, if you pass \"[]\",   the produced queries look as follows:   [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).</p> </li> <li> <p>If you pass more than two characters, the engine will throw an exception.</p> </li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>double_precision</code> <p>The keyword used for double precision columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'DOUBLE PRECISION'</code> </p> <code>integer</code> <p>The keyword used for integer columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'INTEGER'</code> </p> <code>text</code> <p>The keyword used for text columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/database/connect_odbc.py</code> <pre><code>def connect_odbc(\n    server_name: str,\n    user: str = \"\",\n    password: str = \"\",\n    escape_chars: str = '\"',\n    double_precision: str = \"DOUBLE PRECISION\",\n    integer: str = \"INTEGER\",\n    text: str = \"TEXT\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new ODBC database connection.\n\n    ODBC is standardized format that can be used to connect to almost any\n    database.\n\n    Before you use the ODBC connector, make sure the database is up and\n    running and that the appropriate ODBC drivers are installed.\n\n    Args:\n        server_name:\n            The server name, as referenced in your .obdc.ini file.\n\n        user:\n            Username with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        password:\n            Password with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        escape_chars:\n            ODBC drivers are supposed to support\n            escaping table and column names using '\"' characters irrespective of the\n            syntax in the target database. Unfortunately, not all ODBC drivers\n            follow this standard. This is why some\n            tuning might be necessary.\n\n            The escape_chars value behaves as follows:\n\n            * If you pass an empty string, schema, table and column names will not\n              be escaped at all. This is not a problem unless some table\n              or column names are identical to SQL keywords.\n\n            * If you pass a single character, schema, table and column names will\n              be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)\n              or `TABLE_NAME`.`COLUMN_NAME` (MySQL/MariaDB style).\n\n            * If you pass two characters, table, column and schema names will be\n              enveloped between these to characters. For instance, if you pass \"[]\",\n              the produced queries look as follows:\n              [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).\n\n            * If you pass more than two characters, the engine will throw an exception.\n\n        double_precision:\n            The keyword used for double precision columns.\n\n        integer:\n            The keyword used for integer columns.\n\n        text:\n            The keyword used for text columns.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"odbc\"\n\n    cmd[\"server_name_\"] = server_name\n    cmd[\"user_\"] = user\n    cmd[\"escape_chars_\"] = escape_chars\n    cmd[\"double_precision_\"] = double_precision\n    cmd[\"integer_\"] = integer\n    cmd[\"text_\"] = text\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_postgres.connect_postgres","title":"connect_postgres","text":"<pre><code>connect_postgres(\n    dbname: str,\n    user: str,\n    password: str,\n    host: Optional[str] = None,\n    hostaddr: Optional[str] = None,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new PostgreSQL database connection.</p> <p>But first, make sure your database is running, and you can reach it from via your command line.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the PostgreSQL database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the PostgreSQL database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the PostgreSQL database.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>hostaddr</code> <p>IP address of the PostgreSQL database. This should be in the standard IPv4 address format, e.g., 172.28.40.9. If your machine supports IPv6, you can also use those addresses. TCP/IP communication is always used when a nonempty string is specified for this parameter.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>port</code> <p>Port of the PostgreSQL database.</p> <p>The default port used by PostgreSQL is 5432.</p> <p>If you do not know, which port to use, type the following into your PostgreSQL client</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>5432</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the PostgreSQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_postgres.py</code> <pre><code>def connect_postgres(\n    dbname: str,\n    user: str,\n    password: str,\n    host: Optional[str] = None,\n    hostaddr: Optional[str] = None,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new PostgreSQL database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from via your command line.\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the PostgreSQL database.\n\n        password:\n            Password with which to log into the PostgreSQL database.\n\n        host:\n            Host of the PostgreSQL database.\n\n        hostaddr:\n            IP address of the PostgreSQL database.\n            This should be in the standard IPv4 address format, e.g., 172.28.40.9.\n            If your machine supports IPv6, you can also use those addresses.\n            TCP/IP communication is always used when a nonempty string is specified\n            for this parameter.\n\n        port:\n            Port of the PostgreSQL database.\n\n            The default port used by PostgreSQL is 5432.\n\n            If you do not know, which port to use, type the following into your\n            PostgreSQL client\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the PostgreSQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"postgres\"\n\n    if host is not None:\n        cmd[\"host_\"] = host\n\n    if hostaddr is not None:\n        cmd[\"hostaddr_\"] = hostaddr\n\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.connect_sqlite3.connect_sqlite3","title":"connect_sqlite3","text":"<pre><code>connect_sqlite3(\n    name: str = \":memory:\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new SQLite3 database connection.</p> <p>SQLite3 is a popular in-memory database. It is faster than distributed databases, like PostgreSQL, but less stable under massive parallel access, consumes more memory and requires all contained data sets to be loaded into memory, which might fill up too much of your RAM, especially for large data sets.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the sqlite3 file.  If the file does not exist, it will be created. Set to \":memory:\" for a purely in-memory SQLite3 database.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':memory:'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The new SQLite3 database connection.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the SQLite3 database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_sqlite3.py</code> <pre><code>def connect_sqlite3(\n    name: str = \":memory:\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"Creates a new SQLite3 database connection.\n\n    SQLite3 is a popular in-memory database. It is faster than\n    distributed databases, like PostgreSQL, but less stable under massive\n    parallel access, consumes more memory and requires all contained\n    data sets to be loaded into memory, which might fill up too much\n    of your RAM, especially for large data sets.\n\n    Args:\n        name:\n            Name of the sqlite3 file.  If the file does not exist, it\n            will be created. Set to \":memory:\" for a purely in-memory SQLite3\n            database.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The new SQLite3 database connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the SQLite3\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # We want to use the absolute path, unless it is a pure\n    # in-memory version.\n    name_ = name\n\n    if name_ != \":memory:\":\n        name_ = os.path.abspath(name)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name_\n    cmd[\"type_\"] = \"Database.new\"\n\n    cmd[\"db_\"] = \"sqlite3\"\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is usually sent separately,\n        # so it doesn't\n        # end up in the logs. However, Sqlite3 does not\n        # need a password, so we just send a dummy.\n        comm.send_string(sock, \"none\")\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/#getml.database.copy_table.copy_table","title":"copy_table","text":"<pre><code>copy_table(\n    source_conn: Connection,\n    target_conn: Connection,\n    source_table: str,\n    target_table: Optional[str] = None,\n)\n</code></pre> <p>Copies a table from one database connection to another.</p> PARAMETER DESCRIPTION <code>source_conn</code> <p>The database connection to be copied from.</p> <p> TYPE: <code>Connection</code> </p> <code>target_conn</code> <p>The database connection to be copied to.</p> <p> TYPE: <code>Connection</code> </p> <code>source_table</code> <p>The name of the table in the source connection.</p> <p> TYPE: <code>str</code> </p> <code>target_table</code> <p>The name of the table in the target connection. If you do not explicitly pass a target_table, the name will be identical to the source_table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Example <p>A frequent use case for this function is to copy data from a data source into sqlite. This is a good idea, because sqlite is faster than most standard, ACID-compliant databases, and also you want to avoid messing up a productive environment.</p> <p>It is important to explicitly pass conn_id, otherwise the source connection will be closed when you create the target connection. What you pass as conn_id is entirely up to you, as long as the conn_id of the source and the target are different. It can be any name you see fit.</p> <pre><code>source_conn = getml.database.connect_odbc(\n    \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\ntarget_conn = getml.database.connect_sqlite3(\n    \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\ndata.database.copy_table(\n        source_conn=source_conn,\n        target_conn=target_conn,\n        source_table=\"MY-TABLE\"\n)\n</code></pre> Source code in <code>getml/database/copy_table.py</code> <pre><code>def copy_table(\n    source_conn: Connection,\n    target_conn: Connection,\n    source_table: str,\n    target_table: Optional[str] = None,\n):\n    \"\"\"\n    Copies a table from one database connection to another.\n\n    Args:\n        source_conn:\n            The database connection to be copied from.\n\n        target_conn:\n            The database connection to be copied to.\n\n        source_table:\n            The name of the table in the source connection.\n\n        target_table:\n            The name of the table in the target\n            connection. If you do not explicitly pass a target_table, the\n            name will be identical to the source_table.\n\n    Example:\n        A frequent use case for this function is to copy data from a data source into\n        sqlite. This is a good idea, because sqlite is faster than most standard,\n        ACID-compliant databases, and also you want to avoid messing up a productive\n        environment.\n\n        It is important to explicitly pass conn_id, otherwise the source connection\n        will be closed\n        when you create the target connection. What you pass as conn_id is entirely\n        up to you,\n        as long as the conn_id of the source and the target are different. It can\n        be any name you see fit.\n\n        ```python\n        source_conn = getml.database.connect_odbc(\n            \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\n        target_conn = getml.database.connect_sqlite3(\n            \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\n        data.database.copy_table(\n                source_conn=source_conn,\n                target_conn=target_conn,\n                source_table=\"MY-TABLE\"\n        )\n        ```\n\n    \"\"\"\n\n    # -------------------------------------------\n\n    target_table = target_table or source_table\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.copy_table\"\n\n    cmd[\"source_conn_id_\"] = source_conn.conn_id\n    cmd[\"target_conn_id_\"] = target_conn.conn_id\n    cmd[\"source_table_\"] = source_table\n    cmd[\"target_table_\"] = target_table\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/#getml.database.drop_table.drop_table","title":"drop_table","text":"<pre><code>drop_table(name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Drops a table from the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>The table to be dropped.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/database/drop_table.py</code> <pre><code>def drop_table(name: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Drops a table from the database.\n\n    Args:\n        name:\n            The table to be dropped.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.drop_table\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/#getml.database.execute.execute","title":"execute","text":"<pre><code>execute(query: str, conn: Optional[Connection] = None)\n</code></pre> <p>Executes an SQL query on the database.</p> <p>Please note that this is not meant to return results. If you want to get results, use database.get(...) instead.</p> PARAMETER DESCRIPTION <code>query</code> <p>The SQL query to be executed.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/database/execute.py</code> <pre><code>def execute(query: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Executes an SQL query on the database.\n\n    Please note that this is not meant to return results. If you want to\n    get results, use database.get(...) instead.\n\n    Args:\n        query:\n            The SQL query to be executed.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.execute\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/database/#getml.database.get.get","title":"get","text":"<pre><code>get(\n    query: str, conn: Optional[Connection] = None\n) -&gt; DataFrame\n</code></pre> <p>Executes an SQL query on the database and returns the result as a pandas dataframe.</p> PARAMETER DESCRIPTION <code>query</code> <p>The SQL query to be executed.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The result of the query as a pandas dataframe.</p> Source code in <code>getml/database/get.py</code> <pre><code>def get(query: str, conn: Optional[Connection] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Executes an SQL query on the database and returns the result as\n    a pandas dataframe.\n\n    Args:\n        query:\n            The SQL query to be executed.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        The result of the query as a pandas dataframe.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.get\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return pd.read_json(json_str)\n</code></pre>"},{"location":"reference/database/#getml.database.get_colnames.get_colnames","title":"get_colnames","text":"<pre><code>get_colnames(\n    name: str, conn: Optional[Connection] = None\n) -&gt; list[str]\n</code></pre> <p>Lists the colnames of a table held in the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the table in the database.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[str]</code> <p>A list of strings containing the names of the columns in the table.</p> Source code in <code>getml/database/get_colnames.py</code> <pre><code>def get_colnames(name: str, conn: Optional[Connection] = None) -&gt; list[str]:\n    \"\"\"\n    Lists the colnames of a table held in the database.\n\n    Args:\n        name:\n            The name of the table in the database.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        A list of strings containing the names of the columns in the table.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.get_colnames\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return arr\n</code></pre>"},{"location":"reference/database/#getml.database.list_connections.list_connections","title":"list_connections","text":"<pre><code>list_connections() -&gt; List[Connection]\n</code></pre> <p>Returns a list handles to all connections that are currently active on the engine.</p> RETURNS DESCRIPTION <code>List[Connection]</code> <p>A list of Connection objects.</p> Source code in <code>getml/database/list_connections.py</code> <pre><code>def list_connections() -&gt; List[Connection]:\n    \"\"\"\n    Returns a list handles to all connections\n    that are currently active on the engine.\n\n    Returns:\n        A list of Connection objects.\n    \"\"\"\n\n    cmd: Dict[Any, str] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.list_connections\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return [Connection(elem) for elem in arr]\n</code></pre>"},{"location":"reference/database/#getml.database.list_tables.list_tables","title":"list_tables","text":"<pre><code>list_tables(conn: Optional[Connection] = None) -&gt; List[str]\n</code></pre> <p>Lists all tables and views currently held in the database.</p> PARAMETER DESCRIPTION <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of strings containing the names of the tables and views.</p> Source code in <code>getml/database/list_tables.py</code> <pre><code>def list_tables(conn: Optional[Connection] = None) -&gt; List[str]:\n    \"\"\"\n    Lists all tables and views currently held in the database.\n\n    Args:\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        A list of strings containing the names of the tables and views.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.list_tables\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return json.loads(comm.recv_string(sock))\n</code></pre>"},{"location":"reference/database/#getml.database.read_csv.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None\n</code></pre> <p>Reads a CSV file into the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>quotechar</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv . You can import their data into the database using the following commands:</p> <pre><code>stmt = data.database.sniff_csv(\n        fnames=[\"file1.csv\", \"file2.csv\"],\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\ndata.database.read_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY_TABLE\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/database/read_csv.py</code> <pre><code>def read_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None:\n    \"\"\"\n    Reads a CSV file into the database.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        fnames:\n            The list of CSV file names to be read.\n\n        quotechar:\n            The character used to wrap strings. Default:`\"`\n\n        sep\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* . You can import their data into the database\n        using the following commands:\n\n        ```python\n        stmt = data.database.sniff_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        data.database.read_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY_TABLE\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    fnames_ = _retrieve_urls(fnames)\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.read_csv\"\n\n    cmd[\"fnames_\"] = fnames_\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/#getml.database.read_s3.read_s3","title":"read_s3","text":"<pre><code>read_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None\n</code></pre> <p>Reads a list of CSV files located in an S3 bucket.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The separator used for separating fields. Default:<code>,</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\ndata.database.read_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY_TABLE\",\n    sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML engine.</p> Source code in <code>getml/database/read_s3.py</code> <pre><code>def read_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None:\n    \"\"\"\n    Reads a list of CSV files located in an S3 bucket.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        data.database.read_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY_TABLE\",\n            sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n    \"\"\"\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.read_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/#getml.database.sniff_csv.sniff_csv","title":"sniff_csv","text":"<pre><code>sniff_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of CSV files.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>quotechar</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields. Default:<code>,</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/database/sniff_csv.py</code> <pre><code>def sniff_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of CSV files.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        fnames:\n            The list of CSV file names to be read.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        quotechar:\n            The character used to wrap strings. Default:`\"`\n\n        sep:\n            The separator used for separating fields. Default:`,`\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    fnames_ = _retrieve_urls(fnames)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.sniff_csv\"\n\n    cmd[\"fnames_\"] = fnames_\n    cmd[\"num_lines_sniffed_\"] = num_lines_sniffed\n    cmd[\"quotechar_\"] = quotechar\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return comm.recv_string(sock)\n</code></pre>"},{"location":"reference/database/#getml.database.sniff_s3.sniff_s3","title":"sniff_s3","text":"<pre><code>sniff_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    num_lines_sniffed: int = 1000,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of CSV files located in an S3 bucket.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML engine.</p> Source code in <code>getml/database/sniff_s3.py</code> <pre><code>def sniff_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    num_lines_sniffed: int = 1000,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of CSV files located in an S3 bucket.\n\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        sep:\n            The character used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the engine will use the default connection.\n\n    Returns:\n        Appropriate `CREATE TABLE` statement.\n\n    Example:\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML engine.\n\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.sniff_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_sniffed_\"] = num_lines_sniffed\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            sock.close()\n            comm.engine_exception_handler(msg)\n        return comm.recv_string(sock)\n</code></pre>"},{"location":"reference/datasets/","title":"Index","text":""},{"location":"reference/datasets/#getml.datasets","title":"getml.datasets","text":"<p>The <code>datasets</code> module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</p>"},{"location":"reference/datasets/#getml.datasets.load_air_pollution","title":"load_air_pollution","text":"<pre><code>load_air_pollution(\n    roles: bool = True, as_pandas: bool = False\n) -&gt; DataFrameT\n</code></pre> <p>Regression dataset on air pollution in Beijing, China</p> <p>The dataset consists of a single table split into train and test sets around 2014-01-01.</p> <p>The original publication is: Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather impact, APEC and winter heating. Proceedings of the Royal Society A, 471, 20150257.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrameT</code> <p>A DataFrame holding the data described above.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>air_pollution</code></li> </ul> Example <p><pre><code>air_pollution = getml.datasets.load_air_pollution()\ntype(air_pollution)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-demo .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_air_pollution(\n    roles: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT:\n    \"\"\"\n    Regression dataset on air pollution in Beijing, China\n\n    The dataset consists of a single table split into train and test sets\n    around 2014-01-01.\n\n    The original publication is:\n    Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and\n    Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather\n    impact, APEC and winter heating. Proceedings of the Royal Society A, 471,\n    20150257.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        roles:\n            Return data with roles set\n\n    Returns:\n            A DataFrame holding the data described above.\n\n                The following DataFrames are returned:\n\n                    * `air_pollution`\n\n    Example:\n        ```python\n        air_pollution = getml.datasets.load_air_pollution()\n        type(air_pollution)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-demo\n        ](https://github.com/getml/getml-demo/blob/master/air_pollution.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n\n    \"\"\"\n\n    ds_name = \"air_pollution\"\n\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_atherosclerosis","title":"load_atherosclerosis","text":"<pre><code>load_atherosclerosis(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on the lethality of atherosclerosis</p> <p>The atherosclerosis dataset is a medical dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository) . It contains information from a longitudinal study on 1417 middle-aged men observed over the course of 20 years. After preprocessing, it consists of 2 tables with 76 and 66 columns:</p> <ul> <li> <p><code>population</code>: Data on the study's participants</p> </li> <li> <p><code>contr</code>: Data on control dates</p> </li> </ul> <p>The population table is split into a training (70%), a testing (15%) set and a validation (15%) set.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population</code></li> <li><code>contr</code></li> </ul> Example <p><pre><code>population, contr = getml.datasets.load_atherosclerosis()\ntype(population)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_atherosclerosis(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on the lethality of atherosclerosis\n\n    The atherosclerosis dataset is a medical dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)\n    ](https://relational-data.org/dataset/Atherosclerosis). It contains\n    information from a longitudinal study on 1417 middle-aged men observed over\n    the course of 20 years. After preprocessing, it consists of 2 tables with 76\n    and 66 columns:\n\n    * `population`: Data on the study's participants\n\n    * `contr`: Data on control dates\n\n    The population table is split into a training (70%), a testing (15%) set and a\n    validation (15%) set.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame` s\n\n        roles:\n            Return data with roles set\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n            Tuple containing (sorted alphabetically by `df.name`) the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True) or\n                if `as_dict` is `True`: Dictionary containing the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True). The keys correspond to the name of the DataFrame on the\n                [`engine`][getml.engine].\n\n                The following DataFrames are returned:\n\n                - `population`\n                - `contr`\n\n    Example:\n        ```python\n        population, contr = getml.datasets.load_atherosclerosis()\n        type(population)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/atherosclerosis.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"atherosclerosis\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_biodegradability","title":"load_biodegradability","text":"<pre><code>load_biodegradability(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Regression dataset on molecule weight prediction</p> <p>The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy). The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE).</p> <p>The original publication is: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878</p> <p>The dataset was collected through the Relational Dataset Repository (former CTU Prague Relational Learning Repository)</p> <p>It contains information on 1309 molecules with 6166 bonds. It consists of 5 tables.</p> <p>The population table is split into a training (50 %) and a testing (25%) and validation (25%) sets.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>molecule</code></li> <li><code>atom</code></li> <li><code>bond</code></li> <li><code>gmember</code></li> <li><code>group</code></li> </ul> Example <pre><code>biodegradability = getml.datasets.load_biodegradability(as_dict=True)\ntype(biodegradability[\"molecule_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_biodegradability(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Regression dataset on molecule weight prediction\n\n    The QSAR biodegradation dataset was built in the Milano Chemometrics and\n    QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy).\n    The data have been used to develop QSAR (Quantitative Structure Activity\n    Relationships) models for the study of the relationships between chemical\n    structure and biodegradation of molecules. Biodegradation experimental\n    values of 1055 chemicals were collected from the webpage of the National\n    Institute of Technology and Evaluation of Japan (NITE).\n\n    The original publication is:\n    Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V.\n    (2013). Quantitative Structure - Activity Relationship models for ready\n    biodegradability of chemicals. Journal of Chemical Information and Modeling,\n    53, 867-878\n\n    The dataset was collected through the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Biodegradability)\n\n    It contains information on 1309 molecules with 6166 bonds. It consists of 5\n    tables.\n\n    The population table is split into a training (50 %) and a testing (25%) and\n    validation (25%) sets.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        roles:\n            Return data with roles set\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n            Tuple containing (sorted alphabetically by `df.name`) the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True) or if `as_dict` is `True`: Dictionary containing the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True). The keys correspond to the name of the DataFrame on the\n                [`engine`][getml.engine].\n\n                The following DataFrames are returned:\n\n                * `molecule`\n                * `atom`\n                * `bond`\n                * `gmember`\n                * `group`\n\n    Example:\n        ```python\n        biodegradability = getml.datasets.load_biodegradability(as_dict=True)\n        type(biodegradability[\"molecule_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"biodegradability\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_consumer_expenditures","title":"load_consumer_expenditures","text":"<pre><code>load_consumer_expenditures(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on consumer expenditures</p> <p>The Consumer Expenditure Data Set is a public domain data set provided by the American Bureau of Labor Statistics. It includes the diary entries, where American consumers are asked to keep record of the products they have purchased each month.</p> <p>We use this dataset to classify whether an item was purchased as a gift or not.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population</code></li> <li><code>expd</code></li> <li><code>fmld</code></li> <li><code>memd</code></li> </ul> Example <p><pre><code>ce = getml.datasets.load_consumer_expenditures(as_dict=True)\ntype(ce[\"expd\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_consumer_expenditures(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on consumer expenditures\n\n    The Consumer Expenditure Data Set is a public domain data set provided by\n    the [American Bureau of Labor Statistics](https://www.bls.gov/cex/pumd.htm).\n    It includes the diary entries, where American consumers are asked to keep\n    record of the products they have purchased each month.\n\n    We use this dataset to classify whether an item was purchased as a gift or not.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n            Tuple containing (sorted alphabetically by `df.name`) the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True) or\n                if `as_dict` is `True`: Dictionary containing the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True). The keys correspond to the name of the DataFrame on the\n                [`engine`][getml.engine].\n\n                The following DataFrames are returned:\n\n                * `population`\n                * `expd`\n                * `fmld`\n                * `memd`\n\n    Example:\n        ```python\n        ce = getml.datasets.load_consumer_expenditures(as_dict=True)\n        type(ce[\"expd\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/consumer_expenditures.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float].\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"consumer_expenditures\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_interstate94","title":"load_interstate94","text":"<pre><code>load_interstate94(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT\n</code></pre> <p>Regression dataset on traffic volume prediction</p> <p>The interstate94 dataset is a multivariate time series containing the hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is based on data provided by the MN Department of Transportation . Some additional data preparation done by John Hogue. The dataset features some particular interesting characteristics common for time series, which classical models may struggle to appropriately deal with. Such characteristics are:</p> <ul> <li>High frequency (hourly)</li> <li>Dependence on irregular events (holidays)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrameT</code> <p>A DataFrame holding the data described above.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>traffic</code></li> </ul> Example <p><pre><code>traffic = getml.datasets.load_interstate94()\ntype(traffic)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the interstate94 dataset including all necessary preprocessing steps please refer to getml-examples.</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_interstate94(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT:\n    \"\"\"\n    Regression dataset on traffic volume prediction\n\n    The interstate94 dataset is a multivariate time series containing the\n    hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is\n    based on data provided by the [MN Department of Transportation\n    ](https://www.dot.state.mn.us/). Some additional data preparation done by\n    [John Hogue](https://github.com/dreyco676/Anomaly_Detection_A_to_Z/). The\n    dataset features some particular interesting characteristics common for\n    time series, which classical models may struggle to appropriately deal\n    with. Such characteristics are:\n\n    * High frequency (hourly)\n    * Dependence on irregular events (holidays)\n    * Strong and overlapping cycles (daily, weekly)\n    * Anomalies\n    * Multiple seasonalities\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n    Returns:\n            A DataFrame holding the data described above.\n\n                The following DataFrames are returned:\n\n                * `traffic`\n\n    Example:\n        ```python\n        traffic = getml.datasets.load_interstate94()\n        type(traffic)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the interstate94 dataset including all necessary\n        preprocessing steps please refer to [getml-examples](https://github.com/getml/getml-demo/blob/master/interstate94.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"interstate94\"\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_loans","title":"load_loans","text":"<pre><code>load_loans(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on loan default</p> <p>The loans dataset is based on a financial dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository).</p> <p>The original publication is: Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.</p> <p>The dataset contains information on 606 successful and 76 unsuccessful loans. After some preprocessing it contains 5 tables</p> <ul> <li> <p><code>account</code>: Information about the borrower(s) of a given loan.</p> </li> <li> <p><code>loan</code>: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)</p> </li> <li> <p><code>meta</code>: Meta information about the obligor, such as gender and geo-information</p> </li> <li> <p><code>order</code>: Information about permanent orders, debited payments and account balances.</p> </li> <li> <p><code>trans</code>: Information about transactions and accounts balances.</p> </li> </ul> <p>The population table is split into a training and a testing set at 80% of the main population.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>account</code></li> <li><code>loan</code></li> <li><code>meta</code></li> <li><code>order</code></li> <li><code>trans</code></li> </ul> Example <p><pre><code>loans = getml.datasets.load_loans(as_dict=True)\ntype(loans[\"population_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the loans dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_loans(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on loan default\n\n    The loans dataset is based on a financial dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Financial).\n\n    The original publication is:\n    Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.\n\n    The dataset contains information on 606 successful and 76 unsuccessful\n    loans. After some preprocessing it contains 5 tables\n\n    * `account`: Information about the borrower(s) of a given loan.\n\n    * `loan`: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)\n\n    * `meta`: Meta information about the obligor, such as gender and geo-information\n\n    * `order`: Information about permanent orders, debited payments and account balances.\n\n    * `trans`: Information about transactions and accounts balances.\n\n    The population table is split into a training and a testing set at 80% of the main population.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n            Tuple containing (sorted alphabetically by `df.name`) the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True) or\n                if `as_dict` is `True`: Dictionary containing the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True). The keys correspond to the name of the DataFrame on the\n                [`engine`][getml.engine].\n\n                The following DataFrames are returned:\n\n                * `account`\n                * `loan`\n                * `meta`\n                * `order`\n                * `trans`\n\n    Example:\n        ```python\n        loans = getml.datasets.load_loans(as_dict=True)\n        type(loans[\"population_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the loans dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/loans.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"loans\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.load_occupancy","title":"load_occupancy","text":"<pre><code>load_occupancy(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on occupancy detection</p> <p>The occupancy detection dataset is a very simple multivariate time series from the UCI Machine Learning Repository . It is a binary classification problem. The task is to predict room occupancy from Temperature, Humidity, Light and CO2.</p> <p>The original publication is: Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Energy and Buildings, 112, 28-39.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population_train</code></li> <li><code>population_test</code></li> <li><code>population_validation</code></li> </ul> Example <p><pre><code>population_train, population_test, _ = getml.datasets.load_occupancy()\ntype(occupancy_train)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_occupancy(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on occupancy detection\n\n    The occupancy detection dataset is a very simple multivariate time series\n    from the [UCI Machine Learning Repository\n    ](https://archive.ics.uci.edu/dataset/357/occupancy+detection). It is a\n    binary classification problem. The task is to predict room occupancy\n    from Temperature, Humidity, Light and CO2.\n\n    The original publication is:\n    Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an\n    office room from light, temperature, humidity and CO2 measurements using\n    statistical learning models. Energy and Buildings, 112, 28-39.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        as_pandas:\n            Return data as `pandas.DataFrame` s\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n            Tuple containing (sorted alphabetically by `df.name`) the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True) or\n                if `as_dict` is `True`: Dictionary containing the data as\n                [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n                is True). The keys correspond to the name of the DataFrame on the\n                [`engine`][getml.engine].\n\n                The following DataFrames are returned:\n\n                * `population_train`\n                * `population_test`\n                * `population_validation`\n\n    Example:\n        ```python\n        population_train, population_test, _ = getml.datasets.load_occupancy()\n        type(occupancy_train)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/occupancy.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"occupancy\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_categorical","title":"make_categorical","text":"<pre><code>make_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9. The SQL definition of the target variable read like this</li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_row_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p><code>aggregations</code> used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9. The SQL definition of the target variable read like this\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_row_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [`aggregations`][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(0, 10, n_rows_peripheral).astype(str)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] != \"1\")\n        &amp; (temp[\"column_01\"] != \"2\")\n        &amp; (temp[\"column_01\"] != \"9\")\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"categorical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"categorical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_discrete","title":"make_discrete","text":"<pre><code>make_discrete(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random integer between -10 and 10</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the minimum value greater than 0   in the peripheral table for which   <code>time_stamp_peripheral &lt; time_stamp_population</code>   and the join key matches <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 &gt; 0 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n         t1.time_stamp;\n</code></pre></li> </ul> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_row_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_discrete(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random integer between -10 and 10\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the minimum value greater than 0\n      in the peripheral table for which\n      ``time_stamp_peripheral &lt; time_stamp_population``\n      and the join key matches\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 &gt; 0 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n             t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_row_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(-11, 11, n_rows_peripheral)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] &gt; 0.0)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"discrete_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"discrete_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_numerical","title":"make_numerical","text":"<pre><code>make_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_row_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_row_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"time_stamp_peripheral\"] &gt;= temp[\"time_stamp_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_same_units_categorical","title":"make_same_units_categorical","text":"<pre><code>make_same_units_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9</li> </ul> <pre><code>SELECT aggregation( column_02 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 == t2.column_01 )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_row_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9\n\n    ```sql\n    SELECT aggregation( column_02 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 == t2.column_01 )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_row_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        (random.rand(n_rows_population) * 10.0).astype(np.int32).astype(str)\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        (random.rand(n_rows_peripheral) * 10.0).astype(np.int32).astype(str)\n    )\n    peripheral_table[\"column_02\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] == temp[\"column_01_population\"])\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_02\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_02\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    population_name = (\n        population_name\n        or \"make_same_units_categorical_population__\" + str(random_state)\n    )\n\n    peripheral_name = (\n        peripheral_name\n        or \"make_same_units_categorical_peripheral__\" + str(random_state)\n    )\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"numerical\": [\"column_02\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_same_units_numerical","title":"make_same_units_numerical","text":"<pre><code>make_same_units_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_row_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_row_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        random.rand(n_rows_population) * 2.0 - 1.0\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        random.rand(n_rows_peripheral) * 2.0 - 1.0\n    )\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] &gt; temp[\"column_01_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = (\n        temp[[\"column_01_peripheral\", \"join_key\"]]\n        .groupby([\"join_key\"], as_index=False)\n        .count()\n    )\n\n    temp = temp.rename(index=str, columns={\"column_01_peripheral\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"same_unit_numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"same_unit_numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.make_snowflake","title":"make_snowflake","text":"<pre><code>make_snowflake(\n    n_rows_population: int = 500,\n    n_rows_peripheral1: int = 5000,\n    n_rows_peripheral2: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name1: str = \"\",\n    peripheral_name2: str = \"\",\n    aggregation1: str = aggregations.Sum,\n    aggregation2: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and two peripheral tables.</p> <p>The first peripheral table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>join_key2</code>: unique integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The second peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key2</code>: random integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable as defined by the SQL block below:</li> </ul> <pre><code>SELECT aggregation1( feature_1_1 )\nFROM POPULATION t1\nLEFT JOIN (\n    SELECT aggregation2( t4.column_01 ) AS feature_1_1\n    FROM PERIPHERAL t3\n    LEFT JOIN PERIPHERAL2 t4\n    ON t3.join_key2 = t4.join_key2\n    WHERE (\n       ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n    ) AND t4.time_stamp &lt;= t3.time_stamp\n    GROUP BY t3.join_key,\n         t3.time_stamp\n) t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral1</code> <p>Number of rows in the first peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5000</code> </p> <code>n_rows_peripheral2</code> <p>Number of rows in the second peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name1</code> <p>Name assigned to the <code>DataFrame</code> holding the first peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_1_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name2</code> <p>Name assigned to the <code>DataFrame</code> holding the second peripheral table. If set to a name already existing on the getML engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_2_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation1</code> <p>aggregations used to generate the 'target' column in the first peripheral table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Sum</code> </p> <code>aggregation2</code> <p>aggregations used to generate the 'target' column in the second peripheral table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>Count</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> <li>peripheral_2 (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_snowflake(\n    n_rows_population: int = 500,\n    n_rows_peripheral1: int = 5000,\n    n_rows_peripheral2: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name1: str = \"\",\n    peripheral_name2: str = \"\",\n    aggregation1: str = aggregations.Sum,\n    aggregation2: str = aggregations.Count,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and two peripheral tables.\n\n    The first peripheral table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `join_key2`: unique integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The second peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key2`: random integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable as defined by the SQL block below:\n\n    ```sql\n    SELECT aggregation1( feature_1_1 )\n    FROM POPULATION t1\n    LEFT JOIN (\n        SELECT aggregation2( t4.column_01 ) AS feature_1_1\n        FROM PERIPHERAL t3\n        LEFT JOIN PERIPHERAL2 t4\n        ON t3.join_key2 = t4.join_key2\n        WHERE (\n           ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n        ) AND t4.time_stamp &lt;= t3.time_stamp\n        GROUP BY t3.join_key,\n             t3.time_stamp\n    ) t2\n    ON t1.join_key = t2.join_key\n    WHERE t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral1:\n            Number of rows in the first peripheral table.\n\n        n_rows_peripheral2:\n            Number of rows in the second peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `snowflake_population_` and the seed of the random\n            number generator.\n\n        peripheral_name1:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the first\n            peripheral table. If set to a name already existing on the\n            getML engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_1_` and the seed of the\n            random number generator.\n\n        peripheral_name2:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the second\n            peripheral table. If set to a name already existing on the\n            getML engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_2_` and the seed of the\n            random number generator.\n\n        aggregation1:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the first peripheral table.\n\n        aggregation2:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the second peripheral table.\n\n    Returns:\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n                * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n                * peripheral_2 ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral1) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral1)\n    ]\n    peripheral_table[\"join_key2\"] = range(n_rows_peripheral1)\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral1)\n\n    peripheral_table2 = pd.DataFrame()\n    peripheral_table2[\"column_01\"] = random.rand(n_rows_peripheral2) * 2.0 - 1.0\n    peripheral_table2[\"join_key2\"] = [\n        int(float(n_rows_peripheral1) * random.rand(1)[0])\n        for i in range(n_rows_peripheral2)\n    ]\n    peripheral_table2[\"time_stamp_peripheral2\"] = random.rand(n_rows_peripheral2)\n\n    # ----------------\n    # Merge peripheral_table with peripheral_table2\n\n    temp = peripheral_table2.merge(\n        peripheral_table[[\"join_key2\", \"time_stamp_peripheral\"]],\n        how=\"left\",\n        on=\"join_key2\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral2\"] &lt;= temp[\"time_stamp_peripheral\"])\n        &amp; (temp[\"time_stamp_peripheral2\"] &gt;= temp[\"time_stamp_peripheral\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation2, \"column_01\", \"join_key2\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"temporary\"})\n\n    peripheral_table = peripheral_table.merge(temp, how=\"left\", on=\"join_key2\")\n\n    del temp\n\n    # Replace NaN with 0.0\n    peripheral_table[\"temporary\"] = [\n        0.0 if val != val else val for val in peripheral_table[\"temporary\"]\n    ]\n\n    # ----------------\n    # Merge population_table with peripheral_table\n\n    temp2 = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp2 = temp2[(temp2[\"time_stamp_peripheral\"] &lt;= temp2[\"time_stamp_population\"])]\n\n    # Define the aggregation\n    temp2 = _aggregate(temp2, aggregation1, \"temporary\", \"join_key\")\n\n    temp2 = temp2.rename(index=str, columns={\"temporary\": \"targets\"})\n\n    population_table = population_table.merge(temp2, how=\"left\", on=\"join_key\")\n\n    del temp2\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # Remove temporary column.\n    del peripheral_table[\"temporary\"]\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    peripheral_table2 = peripheral_table2.rename(\n        index=str, columns={\"time_stamp_peripheral2\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"snowflake_population_\" + str(random_state)\n    if not peripheral_name1:\n        peripheral_name1 = \"snowflake_peripheral_1_\" + str(random_state)\n    if not peripheral_name2:\n        peripheral_name2 = \"snowflake_peripheral_2_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name1,\n        roles={\n            \"join_key\": [\"join_key\", \"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    peripheral_on_engine2 = data.DataFrame(\n        name=peripheral_name2,\n        roles={\n            \"join_key\": [\"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table2)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine, peripheral_on_engine2\n</code></pre>"},{"location":"reference/datasets/#getml.datasets.base.DataFrameT","title":"DataFrameT  <code>module-attribute</code>","text":"<pre><code>DataFrameT = Union[DataFrame, DataFrame]\n</code></pre> <p>DataFrame types for builtin demonstration datasets</p>"},{"location":"reference/engine/","title":"Index","text":""},{"location":"reference/engine/#getml.engine","title":"getml.engine","text":"<p>This module is a collection of utility functions for the overall communication and the session management of the getML engine.</p> <p>In order to log into the engine, you have to open your favorite internet browser and enter http://localhost:1709 in the navigation bar. This tells it to connect to a local TCP socket at port 1709 opened by the getML monitor. This will only be possible from within the same device!</p> Example <p>First of all, you need to start the getML engine. Next, you need to create a new project or load an existing one.</p> <p><pre><code>getml.engine.list_projects()\ngetml.engine.set_project('test')\n</code></pre> After doing all calculations for today you can shut down the getML engine.</p> <pre><code>print(getml.engine.is_alive())\ngetml.engine.shutdown()\n</code></pre> Note <p>The Python process and the getML engine must be located on the same machine. If you intend to run the engine on a remote host, make sure to start your Python session on that device as well. Also, when using SSH sessions, make sure to start Python using <code>python &amp;</code> followed by <code>disown</code> or using <code>nohup python</code>. This ensures the Python process and all the script it has to run won't be killed the moment your remote connection becomes unstable, and you are able to recover them later on (see <code>remote_access</code>).</p> <p>All data frame objects and models in the getML engine are bundled in projects. When loading an existing project, the current memory of the engine will be flushed and all changes applied to <code>DataFrame</code> instances after calling their <code>save</code> method will be lost. Afterwards, all <code>Pipeline</code> will be loaded into memory automatically. The data frame objects will not be loaded automatically since they consume significantly more memory than the pipelines. They can be loaded manually using <code>load_data_frame</code> or <code>load</code>.</p> <p>The getML engine reflects the separation of data into individual projects on the level of the filesystem too. All data belonging to a single project is stored in a dedicated folder in the 'projects' directory located in '.getML' in your home folder. These projects can be copied and shared between different platforms and architectures without any loss of information. However, you must copy the entire project and not just individual data frames or pipelines.</p>"},{"location":"reference/engine/#getml.engine.is_alive","title":"is_alive  <code>module-attribute</code>","text":"<pre><code>is_alive = is_engine_alive\n</code></pre>"},{"location":"reference/engine/#getml.engine.delete_project","title":"delete_project","text":"<pre><code>delete_project(name: str)\n</code></pre> <p>Deletes a project.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of your project.</p> <p> TYPE: <code>str</code> </p> Note <p>All data and models contained in the project directory will be permanently lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def delete_project(name: str):\n    \"\"\"Deletes a project.\n\n    Args:\n        name:\n            Name of your project.\n\n    Note:\n        All data and models contained in the project directory will be\n        permanently lost.\n\n    \"\"\"\n    _delete_project(name)\n</code></pre>"},{"location":"reference/engine/#getml.engine.is_engine_alive","title":"is_engine_alive","text":"<pre><code>is_engine_alive() -&gt; bool\n</code></pre> <p>Checks if the getML engine is running.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the getML engine is running and ready to accept commands and False otherwise.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def is_engine_alive() -&gt; bool:\n    \"\"\"Checks if the getML engine is running.\n\n    Returns:\n            True if the getML engine is running and ready to accept\n                commands and False otherwise.\n\n    \"\"\"\n\n    cmd: Dict[str, str] = {}\n    cmd[\"type_\"] = \"is_alive\"\n    cmd[\"name_\"] = \"\"\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", comm.port))\n    except ConnectionRefusedError:\n        return False\n\n    comm.send_string(sock, json.dumps(cmd))\n\n    sock.close()\n\n    return True\n</code></pre>"},{"location":"reference/engine/#getml.engine.list_projects","title":"list_projects","text":"<pre><code>list_projects() -&gt; list[str]\n</code></pre> <p>List all projects on the getML engine.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>Lists the name of all the projects.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_projects() -&gt; list[str]:\n    \"\"\"\n    List all projects on the getML engine.\n\n    Returns:\n            Lists the name of all the projects.\n    \"\"\"\n    return _list_projects_impl(running_only=False)\n</code></pre>"},{"location":"reference/engine/#getml.engine.list_running_projects","title":"list_running_projects","text":"<pre><code>list_running_projects() -&gt; list[str]\n</code></pre> <p>List all projects on the getML engine that are currently running.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>Lists the name of all the projects currently running.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_running_projects() -&gt; list[str]:\n    \"\"\"\n    List all projects on the getML engine that are currently running.\n\n    Returns:\n        Lists the name of all the projects currently running.\n    \"\"\"\n    return _list_projects_impl(running_only=True)\n</code></pre>"},{"location":"reference/engine/#getml.engine.set_project","title":"set_project","text":"<pre><code>set_project(name: str)\n</code></pre> <p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the engine, a new one will be created.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def set_project(name: str):\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the engine, a new one will\n    be created.\n\n    Args:\n           name: Name of the new project.\n    \"\"\"\n    _set_project(name)\n</code></pre>"},{"location":"reference/engine/#getml.engine.shutdown","title":"shutdown","text":"<pre><code>shutdown()\n</code></pre> <p>Shuts down the getML engine.</p> Note <p>All changes applied to the <code>DataFrame</code> after calling their <code>save</code> method will be lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def shutdown():\n    \"\"\"Shuts down the getML engine.\n\n    Note:\n        All changes applied to the [`DataFrame`][getml.DataFrame]\n        after calling their [`save`][getml.DataFrame.save]\n        method will be lost.\n\n    \"\"\"\n    _shutdown()\n</code></pre>"},{"location":"reference/engine/#getml.engine.suspend_project","title":"suspend_project","text":"<pre><code>suspend_project(name: str)\n</code></pre> <p>Suspends a project that is currently running.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of your project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def suspend_project(name: str):\n    \"\"\"Suspends a project that is currently running.\n\n    Args:\n        name:\n            Name of your project.\n    \"\"\"\n    _suspend_project(name)\n</code></pre>"},{"location":"reference/feature_learning/","title":"Index","text":"<p>On this page we go into details of community vs enterprise features and provide some infos on the feature learner from within this sub package.</p>"},{"location":"reference/feature_learning/#getml.feature_learning","title":"getml.feature_learning","text":"<p>This module contains relational learning algorithms to learn features from relational data or time series.</p> Note <p>All feature learners need to be passed to <code>Pipeline</code>.</p>"},{"location":"reference/feature_learning/aggregations/","title":"aggregations","text":""},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations","title":"getml.feature_learning.aggregations","text":"<p>This module contains all possible aggregations to be used with <code>Multirel</code>, <code>FastProp</code>, <code>Mapping</code>.</p> <p>Refer to the feature learning section in the user guide for details about how these aggregations are used in the context of feature learning.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.AVG","title":"AVG  <code>module-attribute</code>","text":"<pre><code>AVG: Final[Avg] = 'AVG'\n</code></pre> <p>Average value of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT","title":"COUNT  <code>module-attribute</code>","text":"<pre><code>COUNT: Final[Count] = 'COUNT'\n</code></pre> <p>Number of rows in a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT_DISTINCT_OVER_COUNT","title":"COUNT_DISTINCT_OVER_COUNT  <code>module-attribute</code>","text":"<pre><code>COUNT_DISTINCT_OVER_COUNT: Final[CountDistinctOverCount] = (\n    \"COUNT DISTINCT OVER COUNT\"\n)\n</code></pre> <p>COUNT DISTINCT divided by COUNT. Please note that this aggregation is not  supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT_MINUS_COUNT_DISTINCT","title":"COUNT_MINUS_COUNT_DISTINCT  <code>module-attribute</code>","text":"<pre><code>COUNT_MINUS_COUNT_DISTINCT: Final[\n    CountMinusCountDistinct\n] = \"COUNT MINUS COUNT DISTINCT\"\n</code></pre> <p>Counts minus counts distinct. Substracts COUNT DISTINCT from COUNT.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1S","title":"EWMA_1S  <code>module-attribute</code>","text":"<pre><code>EWMA_1S: Final[EWMA_1s] = 'EWMA_1S'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1M","title":"EWMA_1M  <code>module-attribute</code>","text":"<pre><code>EWMA_1M: Final[EWMA_1m] = 'EWMA_1M'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 minute. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1H","title":"EWMA_1H  <code>module-attribute</code>","text":"<pre><code>EWMA_1H: Final[EWMA_1h] = 'EWMA_1H'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 hour. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1D","title":"EWMA_1D  <code>module-attribute</code>","text":"<pre><code>EWMA_1D: Final[EWMA_1d] = 'EWMA_1D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 day. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_7D","title":"EWMA_7D  <code>module-attribute</code>","text":"<pre><code>EWMA_7D: Final[EWMA_7d] = 'EWMA_7D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 7 days. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_30D","title":"EWMA_30D  <code>module-attribute</code>","text":"<pre><code>EWMA_30D: Final[EWMA_30d] = 'EWMA_30D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 30 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_90D","title":"EWMA_90D  <code>module-attribute</code>","text":"<pre><code>EWMA_90D: Final[EWMA_90d] = 'EWMA_90D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 90 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_365D","title":"EWMA_365D  <code>module-attribute</code>","text":"<pre><code>EWMA_365D: Final[EWMA_365d] = 'EWMA_365D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 365 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1S","title":"EWMA_TREND_1S  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1S: Final[EWMA_TREND_1s] = 'EWMA_TREND_1S'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1M","title":"EWMA_TREND_1M  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1M: Final[EWMA_TREND_1m] = 'EWMA_TREND_1M'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 minute. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1H","title":"EWMA_TREND_1H  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1H: Final[EWMA_TREND_1h] = 'EWMA_TREND_1H'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 hour. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1D","title":"EWMA_TREND_1D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1D: Final[EWMA_TREND_1d] = 'EWMA_TREND_1D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 day. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_7D","title":"EWMA_TREND_7D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_7D: Final[EWMA_TREND_7d] = 'EWMA_TREND_7D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 7 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_30D","title":"EWMA_TREND_30D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_30D: Final[EWMA_TREND_30d] = 'EWMA_TREND_30D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 30 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_90D","title":"EWMA_TREND_90D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_90D: Final[EWMA_TREND_90d] = 'EWMA_TREND_90D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 90 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_365D","title":"EWMA_TREND_365D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_365D: Final[EWMA_TREND_365d] = 'EWMA_TREND_365D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 365 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.FIRST","title":"FIRST  <code>module-attribute</code>","text":"<pre><code>FIRST: Final[First] = 'FIRST'\n</code></pre> <p>First value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.KURTOSIS","title":"KURTOSIS  <code>module-attribute</code>","text":"<pre><code>KURTOSIS: Final[Kurtosis] = 'KURTOSIS'\n</code></pre> <p>The kurtosis of a given column. Please note that this aggregation is not supported  by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.LAST","title":"LAST  <code>module-attribute</code>","text":"<pre><code>LAST: Final[Last] = 'LAST'\n</code></pre> <p>Last value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MAX","title":"MAX  <code>module-attribute</code>","text":"<pre><code>MAX: Final[Max] = 'MAX'\n</code></pre> <p>Largest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MEDIAN","title":"MEDIAN  <code>module-attribute</code>","text":"<pre><code>MEDIAN: Final[Median] = 'MEDIAN'\n</code></pre> <p>Median of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MIN","title":"MIN  <code>module-attribute</code>","text":"<pre><code>MIN: Final[Min] = 'MIN'\n</code></pre> <p>Smallest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MODE","title":"MODE  <code>module-attribute</code>","text":"<pre><code>MODE: Final[Mode] = 'MODE'\n</code></pre> <p>Most frequent value of a given column. Please note that this aggregation is not  supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NUM_MAX","title":"NUM_MAX  <code>module-attribute</code>","text":"<pre><code>NUM_MAX: Final[NumMax] = 'NUM MAX'\n</code></pre> <p>The number of times we observe the maximum value. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NUM_MIN","title":"NUM_MIN  <code>module-attribute</code>","text":"<pre><code>NUM_MIN: Final[NumMin] = 'NUM MIN'\n</code></pre> <p>The number of times we observe the minimum value. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_1","title":"Q_1  <code>module-attribute</code>","text":"<pre><code>Q_1: Final[Q1] = 'Q1'\n</code></pre> <p>The 1%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_5","title":"Q_5  <code>module-attribute</code>","text":"<pre><code>Q_5: Final[Q5] = 'Q5'\n</code></pre> <p>The 5%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_10","title":"Q_10  <code>module-attribute</code>","text":"<pre><code>Q_10: Final[Q10] = 'Q10'\n</code></pre> <p>The 10%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_25","title":"Q_25  <code>module-attribute</code>","text":"<pre><code>Q_25: Final[Q25] = 'Q25'\n</code></pre> <p>The 25%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_75","title":"Q_75  <code>module-attribute</code>","text":"<pre><code>Q_75: Final[Q75] = 'Q75'\n</code></pre> <p>The 75%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_90","title":"Q_90  <code>module-attribute</code>","text":"<pre><code>Q_90: Final[Q90] = 'Q90'\n</code></pre> <p>The 90%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_95","title":"Q_95  <code>module-attribute</code>","text":"<pre><code>Q_95: Final[Q95] = 'Q95'\n</code></pre> <p>The 95%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_99","title":"Q_99  <code>module-attribute</code>","text":"<pre><code>Q_99: Final[Q99] = 'Q99'\n</code></pre> <p>The 99%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.SKEW","title":"SKEW  <code>module-attribute</code>","text":"<pre><code>SKEW: Final[Skew] = 'SKEW'\n</code></pre> <p>Skewness of a given column. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.STDDEV","title":"STDDEV  <code>module-attribute</code>","text":"<pre><code>STDDEV: Final[Stddev] = 'STDDEV'\n</code></pre> <p>Standard deviation of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.SUM","title":"SUM  <code>module-attribute</code>","text":"<pre><code>SUM: Final[Sum] = 'SUM'\n</code></pre> <p>Total sum of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_FIRST_MAXIMUM","title":"TIME_SINCE_FIRST_MAXIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_FIRST_MAXIMUM: Final[TimeSinceFirstMaximum] = (\n    \"TIME SINCE FIRST MAXIMUM\"\n)\n</code></pre> <p>The time difference between the first time we see the maximum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_FIRST_MINIMUM","title":"TIME_SINCE_FIRST_MINIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_FIRST_MINIMUM: Final[TimeSinceFirstMinimum] = (\n    \"TIME SINCE FIRST MINIMUM\"\n)\n</code></pre> <p>The time difference between the first time we see the minimum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_LAST_MAXIMUM","title":"TIME_SINCE_LAST_MAXIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_LAST_MAXIMUM: Final[TimeSinceLastMaximum] = (\n    \"TIME SINCE LAST MAXIMUM\"\n)\n</code></pre> <p>The time difference between the last time we see the maximum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_LAST_MINIMUM","title":"TIME_SINCE_LAST_MINIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_LAST_MINIMUM: Final[TimeSinceLastMinimum] = (\n    \"TIME SINCE LAST MINIMUM\"\n)\n</code></pre> <p>The time difference between the last time we see the minimum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TREND","title":"TREND  <code>module-attribute</code>","text":"<pre><code>TREND: Final[Trend] = 'TREND'\n</code></pre> <p>Extracts a linear trend from a variable over time and extrapolates this trend to  the current time stamp. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.VAR","title":"VAR  <code>module-attribute</code>","text":"<pre><code>VAR: Final[Var] = 'VAR'\n</code></pre> <p>Statistical variance of a given numerical column. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.VARIATION_COEFFICIENT","title":"VARIATION_COEFFICIENT  <code>module-attribute</code>","text":"<pre><code>VARIATION_COEFFICIENT: Final[VariationCoefficient] = (\n    \"VARIATION COEFFICIENT\"\n)\n</code></pre> <p>VAR divided by MEAN. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MultirelAggregations","title":"MultirelAggregations  <code>module-attribute</code>","text":"<pre><code>MultirelAggregations = Union[\n    Avg,\n    Count,\n    CountDistinct,\n    CountMinusCountDistinct,\n    First,\n    Last,\n    Max,\n    Median,\n    Min,\n    Stddev,\n    Sum,\n    Var,\n]\n</code></pre> <p>Union of all Multirel aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.AdditionalFastPropAggregations","title":"AdditionalFastPropAggregations  <code>module-attribute</code>","text":"<pre><code>AdditionalFastPropAggregations = Union[\n    CountDistinctOverCount,\n    EWMA_1s,\n    EWMA_1m,\n    EWMA_1h,\n    EWMA_1d,\n    EWMA_7d,\n    EWMA_30d,\n    EWMA_90d,\n    EWMA_365d,\n    EWMA_TREND_1s,\n    EWMA_TREND_1m,\n    EWMA_TREND_1h,\n    EWMA_TREND_1d,\n    EWMA_TREND_7d,\n    EWMA_TREND_30d,\n    EWMA_TREND_90d,\n    EWMA_TREND_365d,\n    Kurtosis,\n    Mode,\n    NumMax,\n    NumMin,\n    Q1,\n    Q5,\n    Q10,\n    Q25,\n    Q75,\n    Q90,\n    Q95,\n    Q99,\n    Skew,\n    TimeSinceFirstMaximum,\n    TimeSinceFirstMinimum,\n    TimeSinceLastMaximum,\n    TimeSinceLastMinimum,\n    Trend,\n    VariationCoefficient,\n]\n</code></pre> <p>Union of all FastProp aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MappingAggregations","title":"MappingAggregations  <code>module-attribute</code>","text":"<pre><code>MappingAggregations = Union[\n    Avg,\n    Count,\n    CountDistinct,\n    CountDistinctOverCount,\n    CountMinusCountDistinct,\n    Kurtosis,\n    Max,\n    Median,\n    Min,\n    Mode,\n    NumMax,\n    NumMin,\n    Q1,\n    Q5,\n    Q10,\n    Q25,\n    Q75,\n    Q90,\n    Q95,\n    Q99,\n    Skew,\n    Stddev,\n    Sum,\n    Var,\n    VariationCoefficient,\n]\n</code></pre> <p>Union of all Mapping aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Aggregations","title":"Aggregations  <code>module-attribute</code>","text":"<pre><code>Aggregations = Union[\n    MultirelAggregations, AdditionalFastPropAggregations\n]\n</code></pre> <p>Union of all possible aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.FASTPROP","title":"FASTPROP  <code>module-attribute</code>","text":"<pre><code>FASTPROP = AggregationsSets(\n    all=AGGREGATIONS,\n    default=frozenset(\n        [\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            FIRST,\n            LAST,\n            MAX,\n            MEDIAN,\n            MIN,\n            MODE,\n            STDDEV,\n            SUM,\n            TREND,\n        ]\n    ),\n    minimal=frozenset([AVG, COUNT, MAX, MIN, SUM]),\n)\n</code></pre> <p>Set of default aggregations for <code>FastProp</code>.  <code>All</code> contains all aggregations supported by FastProp, <code>Default</code> contains the subset  of reasonable default aggregations, <code>Minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MULTIREL","title":"MULTIREL  <code>module-attribute</code>","text":"<pre><code>MULTIREL = AggregationsSets(\n    all=MULTIREL_AGGREGATIONS,\n    default=frozenset(\n        [\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            FIRST,\n            LAST,\n            MAX,\n            MEDIAN,\n            MIN,\n            MODE,\n            STDDEV,\n            SUM,\n            TREND,\n        ]\n    ),\n    minimal=frozenset([AVG, COUNT, MAX, MIN, SUM]),\n)\n</code></pre> <p>Set of default aggregations for <code>Multirel</code>.  <code>All</code> contains all aggregations supported by Multirel, <code>Default</code> contains the subset  of reasonable default aggregations, <code>Minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MAPPING","title":"MAPPING  <code>module-attribute</code>","text":"<pre><code>MAPPING = AggregationsSets(\n    all=MAPPING_AGGREGATIONS,\n    default=frozenset([AVG]),\n    minimal=frozenset([AVG]),\n)\n</code></pre> <p>Set of default aggregations for <code>Mapping</code>. <code>All</code>  contains all aggregations supported by the mapping preprocessor. <code>Default</code> and  <code>Minimal</code> are identical and include only the AVG aggregation, which is the  recommended setting for classification problems.</p>"},{"location":"reference/feature_learning/fastboost/","title":"Fastboost","text":""},{"location":"reference/feature_learning/fastboost/#getml.feature_learning.Fastboost","title":"getml.feature_learning.Fastboost  <code>dataclass</code>","text":"<pre><code>Fastboost(\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLoss, SquareLoss]\n    ] = None,\n    max_depth: int = 5,\n    min_child_weights: float = 1.0,\n    num_features: int = 100,\n    num_threads: int = 1,\n    reg_lambda: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Fastboost</code> automates feature learning for relational data and time series. The algorithm used is slightly simpler than <code>Relboost</code> and much faster.</p> PARAMETER DESCRIPTION <code>gamma</code> <p>During the training of Fastboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>min_child_weights</code> <p>Determines the minimum sum of the weights a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since Fastboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>subsample</code> <p>Fastboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Fastboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, the number of samples drawn will be identical to the size of the population table. When set to 0.0, there will be no sampling at all. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/feature_learning/fastprop/","title":"FastProp","text":""},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.FastProp","title":"getml.feature_learning.FastProp  <code>dataclass</code>","text":"<pre><code>FastProp(\n    aggregation: Iterable[Aggregations] = FASTPROP.default,\n    delta_t: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLoss, SquareLoss]\n    ] = None,\n    max_lag: int = 0,\n    min_df: int = 30,\n    n_most_frequent: int = 0,\n    num_features: int = 200,\n    num_threads: int = 0,\n    sampling_factor: float = 1.0,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Generates simple features based on propositionalization.</p> <p><code>FastProp</code> generates simple and easily interpretable features for relational data and time series. It is based on a propositionalization approach and has been optimized for speed and memory efficiency. <code>FastProp</code> generates a large number of features and selects the most relevant ones based on the pair-wise correlation with the target(s).</p> <p>It is recommended to combine <code>FastProp</code> with the <code>Mapping</code> and <code>Seasonal</code> preprocessors, which can drastically improve predictive accuracy.</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: FastProp.</p> PARAMETER DESCRIPTION <code>agg_sets</code> <p>Available aggregation sets for the FastProp feature learner.</p> <p> </p> <code>aggregation</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <p> TYPE: <code>Iterable[Aggregations]</code> DEFAULT: <code>default</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables. Please note that you must also pass a value to max_lag.</p> <p>For more information please refer to Data Model Time Series. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> DEFAULT: <code>None</code> </p> <code>max_lag</code> <p>Maximum number of steps taken into the past to form lag variables. The step size is determined by delta_t. Please note that you must also pass a value to delta_t.</p> <p>For more information please refer to Time Series. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>n_most_frequent</code> <p><code>FastProp</code> can find the N most frequent categories in a categorical column and derive features from them. The parameter determines how many categories should be used. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sampling_factor</code> <p>FastProp uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 2,000 samples are drawn from the population table. If the population table contains less than 2,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p>"},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.FastProp.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>FastProp</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params:\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`FastProp`][getml.feature_learning.FastProp].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/loss_functions/","title":"loss_functions","text":""},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions","title":"getml.feature_learning.loss_functions","text":"<p>Loss functions used by the feature learning algorithms.</p> <p>The getML Python API contains two different loss functions. We recommend using <code>SQUARELOSS</code> for regression problems and <code>CROSSENTROPYLOSS</code> for classification problems.</p> <p>Please note that these loss functions will only be used by the feature learning algorithms and not by the <code>predictors</code>.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.CrossEntropyLoss","title":"CrossEntropyLoss  <code>module-attribute</code>","text":"<pre><code>CrossEntropyLoss = Literal['CrossEntropyLoss']\n</code></pre> <p>Type of the cross entropy loss function.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.CROSSENTROPYLOSS","title":"CROSSENTROPYLOSS  <code>module-attribute</code>","text":"<pre><code>CROSSENTROPYLOSS: Final[CrossEntropyLoss] = (\n    \"CrossEntropyLoss\"\n)\n</code></pre> <p>The cross entropy between two probability distributions \\(p(x)\\) and \\(q(x)\\) is a combination of the information contained in \\(p(x)\\) and the additional information stored in \\(q(x)\\) with respect to \\(p(x)\\). In technical terms: it is the entropy of \\(p(x)\\) plus the Kullback-Leibler divergence - a distance in probability space - from \\(q(x)\\) to \\(p(x)\\).</p> \\[ H(p,q) = H(p) + D_{KL}(p||q) \\] <p>For discrete probability distributions the cross entropy loss can be calculated by</p> \\[ H(p,q) = - \\sum_{x \\in X} p(x) \\log q(x) \\] <p>and for continuous probability distributions by</p> \\[ H(p,q) = - \\int_{X} p(x) \\log q(x) dx \\] <p>with \\(X\\) being the support of the samples and \\(p(x)\\) and \\(q(x)\\) being two discrete or continuous probability distributions over \\(X\\).</p> Note <p>Recommended loss function for classification problems.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.SquareLoss","title":"SquareLoss  <code>module-attribute</code>","text":"<pre><code>SquareLoss = Literal['SquareLoss']\n</code></pre> <p>Type of the square loss function.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.SQUARELOSS","title":"SQUARELOSS  <code>module-attribute</code>","text":"<pre><code>SQUARELOSS: Final[SquareLoss] = 'SquareLoss'\n</code></pre> <p>The Square loss (aka mean squared error (MSE)) measures the loss by calculating the average of all squared deviations of the predictions \\(\\hat{y}\\) from the observed (given) outcomes \\(y\\). Depending on the context this measure is also known as mean squared error (MSE) or mean squared deviation (MSD).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2  \\] <p>with \\(n\\) being the number of samples, \\(y\\) the observed outcome, and \\(\\hat{y}\\) the estimate.</p> Note <p>Recommended loss function for regression problems.</p>"},{"location":"reference/feature_learning/multirel/","title":"Multirel","text":""},{"location":"reference/feature_learning/multirel/#getml.feature_learning.Multirel","title":"getml.feature_learning.Multirel  <code>dataclass</code>","text":"<pre><code>Multirel(\n    aggregation: Iterable[Aggregations] = MULTIREL.default,\n    allow_sets: bool = True,\n    delta_t: float = 0.0,\n    grid_factor: float = 1.0,\n    loss_function: Optional[\n        Union[CrossEntropyLoss, SquareLoss]\n    ] = None,\n    max_length: int = 4,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 100,\n    num_subfeatures: int = 5,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    regularization: float = 0.01,\n    round_robin: bool = False,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    share_aggregations: float = 0.0,\n    share_conditions: float = 1.0,\n    shrinkage: float = 0.0,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Multi-Relational Decision Tree Learning.</p> <p><code>Multirel</code> automates feature learning for relational data and time series. It is based on an efficient variation of the Multi-Relational Decision Tree Learning (MRDTL).</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: Multirel.</p> PARAMETER DESCRIPTION <code>agg_sets</code> <p>Available aggregation sets for the Multirel feature learner.</p> <p> </p> <code>aggregation</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be from <code>aggregations</code>.</p> <p> TYPE: <code>Iterable[Aggregations]</code> DEFAULT: <code>default</code> </p> <code>allow_sets</code> <p>Multirel can summarize different categories into sets for producing conditions. When expressed as SQL statements these sets might look like this: <pre><code>t2.category IN ( 'value_1', 'value_2', ... )\n</code></pre> This can be very powerful, but it can also produce features that are hard to read and might be prone to overfitting when the <code>sampling_factor</code> is too low.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information please refer to Time Series in the User Guide. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>grid_factor</code> <p>Multirel will try a grid of critical values for your numerical features. A higher <code>grid_factor</code> will lead to a larger number of critical values being considered. This can increase the training time, but also lead to more accurate features. Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> DEFAULT: <code>None</code> </p> <code>max_length</code> <p>The maximum length a subcondition might have. Multirel will create conditions in the form <pre><code>(condition 1.1 AND condition 1.2 AND condition 1.3 )\nOR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n...\n</code></pre> Using this parameter you can set the maximum number of conditions allowed in the brackets. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>regularization</code> <p>Most important regularization parameter for the quality of the features produced by Multirel. Higher values will lead to less complex features and less danger of overfitting. A <code>regularization</code> of 1.0 is very strong and allows no conditions. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>round_robin</code> <p>If True, the Multirel picks a different <code>aggregation</code> every time a new feature is generated.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_factor</code> <p>Multirel uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>share_aggregations</code> <p>Every time a new feature is generated, the <code>aggregation</code> will be taken from a random subsample of possible aggregations and values to be aggregated. This parameter determines the size of that subsample. Only relevant when <code>round_robin</code> is False. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>share_conditions</code> <p>Every time a new column is tested for applying conditions, it might be skipped at random. This parameter determines the probability that a column will not be skipped. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>shrinkage</code> <p>Since Multirel works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. Higher values will lead to more danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/feature_learning/multirel/#getml.feature_learning.Multirel.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relboost/","title":"Relboost","text":""},{"location":"reference/feature_learning/relboost/#getml.feature_learning.Relboost","title":"getml.feature_learning.Relboost  <code>dataclass</code>","text":"<pre><code>Relboost(\n    allow_null_weights: bool = False,\n    delta_t: float = 0.0,\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLoss, SquareLoss]\n    ] = None,\n    max_depth: int = 3,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 100,\n    num_subfeatures: int = 100,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    reg_lambda: float = 0.0,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Relboost</code> automates feature learning for relational data and time series. It is based on a generalization of the XGBoost algorithm to relational data, hence the name.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: Relboost.</p> PARAMETER DESCRIPTION <code>allow_null_weights</code> <p>Whether you want to allow <code>Relboost</code> to set weights to NULL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Time Series in the User Guide. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>gamma</code> <p>During the training of Relboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [\\(0\\), \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sampling_factor</code> <p>Relboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Relboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since Relboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/feature_learning/relboost/#getml.feature_learning.Relboost.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relmt/","title":"RelMT","text":""},{"location":"reference/feature_learning/relmt/#getml.feature_learning.RelMT","title":"getml.feature_learning.RelMT  <code>dataclass</code>","text":"<pre><code>RelMT(\n    allow_avg: bool = True,\n    delta_t: float = 0.0,\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLoss, SquareLoss]\n    ] = None,\n    max_depth: int = 2,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 30,\n    num_subfeatures: int = 30,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    reg_lambda: float = 0.0,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on relational linear model trees.</p> <p><code>RelMT</code> automates feature learning for relational data and time series. It is based on a generalization of linear model trees to relational data, hence the name. A linear model tree is a decision tree with linear models on its leaves.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: RelMT.</p> PARAMETER DESCRIPTION <code>allow_avg</code> <p>Whether to allow an AVG aggregation. Particularly for time series problems, AVG aggregations are not necessary and you can save some time by taking them out.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Time Series in the User Guide. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>gamma</code> <p>During the training of RelMT, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLoss, SquareLoss]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML engine. Range: [-\\(\\infty\\), \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>RelMT</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sampling_factor</code> <p>RelMT uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time RelMT generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since RelMT works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to more danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/feature_learning/relmt/#getml.feature_learning.RelMT.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/hyperopt/","title":"Index","text":""},{"location":"reference/hyperopt/#getml.hyperopt","title":"getml.hyperopt","text":"<p>Automatically find the best parameters for</p> <ul> <li><code>Multirel</code></li> <li><code>Relboost</code></li> <li><code>RelMT</code></li> <li><code>FastProp</code></li> <li><code>FastBoost</code></li> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> </ul> Example <p>The easiest way to conduct a hyperparameter optimization is to use the built-in tuning routines. Note that these tuning routines usually take a day to complete unless we use very small data sets as we do in this example.</p> <p><pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_feature_learners(\n    pipeline=base_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=tuned_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre>     If you want to define the hyperparameter space and     the tuning routing yourself, this is how you     can do that:</p> <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.scores.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.list_hyperopts","title":"list_hyperopts","text":"<pre><code>list_hyperopts() -&gt; list[str]\n</code></pre> <p>Lists all hyperparameter optimization objects present in the engine.</p> <p>Note that this function only lists hyperopts which are part of the current project. See <code>set_project</code> for changing projects.</p> <p>To subsequently load one of them, use <code>load_hyperopt</code>.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list containing the names of all hyperopts.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def list_hyperopts() -&gt; list[str]:\n    \"\"\"Lists all hyperparameter optimization objects present in the engine.\n\n    Note that this function only lists hyperopts which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects.\n\n    To subsequently load one of them, use\n    [`load_hyperopt`][getml.hyperopt.load_hyperopt.load_hyperopt].\n\n    Returns:\n        list containing the names of all hyperopts.\n\n    Note:\n        Not supported in the getML community edition.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_hyperopts\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.tune_feature_learners","title":"tune_feature_learners","text":"<pre><code>tune_feature_learners(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline\n</code></pre> <p>A high-level interface for optimizing the feature learners of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of feature learners (from <code>feature_learning</code>) of a given pipeline by breaking each feature learner's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Container</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> <code>n_iter</code> <p>The number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>num_threads</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline containing tuned versions of the feature learners.</p> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_feature_learners(\n    pipeline: Pipeline,\n    container: Container,\n    train:str=\"train\",\n    validation:str=\"validation\",\n    n_iter:int=0,\n    score:Optional[str]=None,\n    num_threads:int=0,\n) -&gt; Pipeline:\n    \"\"\"\n    A high-level interface for optimizing the feature learners of a\n    [`Pipeline`][getml.pipeline.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of feature learners\n    (from [`feature_learning`][getml.feature_learning]) of a given pipeline by breaking each\n    feature learner's hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer\n    to tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline:\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n        n_iter:\n            The number of iterations.\n\n        score:\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads:\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Returns:\n        Pipeline containing tuned versions of the feature learners.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n        ```python\n        tuned_pipeline = getml.hyperopt.tune_predictors(\n            pipeline=base_pipeline,\n            container=container)\n        ```\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_feature_learners = []\n\n    for feature_learner in pipeline.feature_learners:\n        tuned_pipeline = _tune_feature_learner(\n            feature_learner=feature_learner,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.feature_learners) == 1\n        ), \"Expected exactly one feature learner, got \" + str(\n            len(tuned_pipeline.feature_learners)\n        )\n\n        tuned_feature_learners.append(tuned_pipeline.feature_learners[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        tuned_feature_learners,\n        copy.deepcopy(pipeline.predictors),\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.tune_predictors","title":"tune_predictors","text":"<pre><code>tune_predictors(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline\n</code></pre> <p>A high-level interface for optimizing the predictors of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of predictors (from <code>getml.predictors</code>) of a given pipeline by breaking each predictor's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Container</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> <code>n_iter</code> <p>The number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>num_threads</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline containing tuned predictors.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_predictors(\n    pipeline: Pipeline,\n    container: Container,\n    train:str=\"train\",\n    validation:str=\"validation\",\n    n_iter:int=0,\n    score:Optional[str]=None,\n    num_threads:int=0,\n) -&gt; Pipeline:\n    \"\"\"\n    A high-level interface for optimizing the predictors of a\n    [`Pipeline`][getml.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of predictors (from\n    [`getml.predictors`][getml.predictors]) of a given pipeline by breaking each\n    predictor's\n    hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer to\n    tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline:\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n        n_iter:\n            The number of iterations.\n\n        score:\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads:\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Example:\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n        ```python\n        tuned_pipeline = getml.hyperopt.tune_predictors(\n            pipeline=base_pipeline,\n            container=container)\n        ```\n\n    Returns:\n        Pipeline containing tuned predictors.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_predictors = []\n\n    for predictor in pipeline.predictors:\n        tuned_pipeline = _tune_predictor(\n            predictor=predictor,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert (\n            len(tuned_pipeline.predictors) == 1\n        ), \"Expected exactly one predictor, got \" + str(len(tuned_pipeline.predictors))\n\n        tuned_predictors.append(tuned_pipeline.predictors[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        copy.deepcopy(pipeline.feature_learners),\n        tuned_predictors,\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.exists","title":"exists","text":"<pre><code>exists(name: str) -&gt; bool\n</code></pre> <p>Determines whether a hyperopt exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A boolean indicating whether a hyperopt named name exists.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"Determines whether a hyperopt exists.\n\n    Args:\n        name: The name of the hyperopt.\n\n    Returns:\n        A boolean indicating whether a hyperopt named *name* exists.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    return name in list_hyperopts()\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.delete","title":"delete","text":"<pre><code>delete(name: str) -&gt; None\n</code></pre> <p>If a hyperopt named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a hyperopt named 'name' exists, it is deleted.\n\n    Args:\n        name: The name of the hyperopt.\n    \"\"\"\n\n    if not exists(name):\n        return\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Hyperopt.delete\"\n    cmd[\"name_\"] = name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.load_hyperopt.load_hyperopt","title":"load_hyperopt","text":"<pre><code>load_hyperopt(\n    name: str,\n) -&gt; Union[\n    GaussianHyperparameterSearch,\n    LatinHypercubeSearch,\n    RandomSearch,\n]\n</code></pre> <p>Loads a hyperparameter optimization object from the getML engine into Python.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt to be loaded.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[GaussianHyperparameterSearch, LatinHypercubeSearch, RandomSearch]</code> <p>The hyperopt object.</p> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/load_hyperopt.py</code> <pre><code>def load_hyperopt(name: str) -&gt; Union[GaussianHyperparameterSearch, LatinHypercubeSearch, RandomSearch]:\n    \"\"\"Loads a hyperparameter optimization object from the getML engine into Python.\n\n    Args:\n        name:\n            The name of the hyperopt to be loaded.\n\n    Returns:\n        The hyperopt object.\n\n    Note:\n        Not supported in the getML community edition.\n    \"\"\"\n    # This will be overwritten by .refresh(...) anyway\n    dummy_pipeline = _make_dummy(\"123456\")\n\n    dummy_param_space = {\"predictors\": [{\"reg_lambda\": [0.0, 1.0]}]}\n\n    json_obj = _get_json_obj(name)\n\n    if json_obj[\"type_\"] == \"GaussianHyperparameterSearch\":\n        return GaussianHyperparameterSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"LatinHypercubeSearch\":\n        return LatinHypercubeSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"RandomSearch\":\n        return RandomSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    raise ValueError(\"Unknown type: '\" + json_obj[\"type_\"] + \"'!\")\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels","title":"kernels","text":"<p>Collection of kernel functions to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.exp","title":"exp  <code>module-attribute</code>","text":"<pre><code>exp = 'exp'\n</code></pre> <p>An exponential kernel yielding non-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.gauss","title":"gauss  <code>module-attribute</code>","text":"<pre><code>gauss = 'gauss'\n</code></pre> <p>A Gaussian kernel yielding analytic (infinitely--differentiable) sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.matern32","title":"matern32  <code>module-attribute</code>","text":"<pre><code>matern32 = 'matern32'\n</code></pre> <p>A Mat\u00e9rn 3/2 kernel yielding once-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.matern52","title":"matern52  <code>module-attribute</code>","text":"<pre><code>matern52 = 'matern52'\n</code></pre> <p>A Mat\u00e9rn 5/2 kernel yielding twice-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization","title":"optimization","text":"<p>Collection of optimization algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization.bfgs","title":"bfgs  <code>module-attribute</code>","text":"<pre><code>bfgs = 'bfgs'\n</code></pre> <p>Broyden-Fletcher-Goldbarb-Shanno optimization algorithm.</p> <p>The BFGS algorithm is a quasi-Newton method that requires the function to be differentiable.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization.nelder_mead","title":"nelder_mead  <code>module-attribute</code>","text":"<pre><code>nelder_mead = 'nelderMead'\n</code></pre> <p>Nelder-Mead optimization algorithm.</p> <p>Nelder-Mead is a direct search method that does not require functions to be differentiable.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in","title":"burn_in","text":"<p>Collection of burn-in algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in.latin_hypercube","title":"latin_hypercube  <code>module-attribute</code>","text":"<pre><code>latin_hypercube = 'latinHypercube'\n</code></pre> <p>Samples from the hyperparameter space almost randomly, but ensures that the different draws are sufficiently different from each other.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in.random","title":"random  <code>module-attribute</code>","text":"<pre><code>random = 'random'\n</code></pre> <p>Samples from the hyperparameter space at random.</p>"},{"location":"reference/hyperopt/gaussian/","title":"Gaussian Hyperparameter","text":""},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch","title":"GaussianHyperparameterSearch","text":"<pre><code>GaussianHyperparameterSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    ratio_iter: float = 0.8,\n    optimization_algorithm: str = nelder_mead,\n    optimization_burn_in_algorithm: str = latin_hypercube,\n    optimization_burn_ins: int = 500,\n    surrogate_burn_in_algorithm: str = latin_hypercube,\n    gaussian_kernel: str = matern52,\n    gaussian_optimization_burn_in_algorithm: str = latin_hypercube,\n    gaussian_optimization_algorithm: str = nelder_mead,\n    gaussian_optimization_burn_ins: int = 500,\n    gaussian_nugget: int = 50,\n    early_stopping: bool = True,\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Bayesian hyperparameter optimization using a Gaussian process.</p> <p>After a burn-in period, a Gaussian process is used to pick the most promising parameter combination to be evaluated next based on the knowledge gathered throughout previous evaluations. Assessing the quality of potential combinations will be done using the expected information (EI).</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful when constructing it since only the parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm, this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> <code>ratio_iter</code> <p>Ratio of the iterations used for the burn-in. For a <code>ratio_iter</code> of 1.0, all iterations will be spent in the burn-in period resulting in an equivalence of this class to <code>LatinHypercubeSearch</code> or <code>RandomSearch</code> - depending on <code>surrogate_burn_in_algorithm</code>. Range: [0, 1]</p> <p>As a rule of thumb at least 70 percent of the evaluations should be spent in the burn-in phase. The more comprehensive the exploration of the <code>param_space</code> during the burn-in, the less likely it is that the Gaussian process gets stuck in local minima.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>optimization_algorithm</code> <p>Determines the optimization algorithm used for the local search in the optimization of the expected information (EI). Must be from <code>optimization</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>nelder_mead</code> </p> <code>optimization_burn_in_algorithm</code> <p>Specifies the algorithm used to draw initial points in the burn-in period of the optimization of the expected information (EI). Must be from <code>burn_in</code>.</p> <p> DEFAULT: <code>latin_hypercube</code> </p> <code>optimization_burn_ins</code> <p>Number of random evaluation points used during the burn-in of the minimization of the expected information (EI). After the surrogate model - the Gaussian process - was successfully fitted to the previous parameter combination, the algorithm is able to calculate the EI for a given point. In order to get to the next combination, the EI has to be maximized over the whole parameter space. Much like the GaussianProcess itself, this requires a burn-in phase. Range: [3, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>surrogate_burn_in_algorithm</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period. Must be from <code>burn_in</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>latin_hypercube</code> </p> <code>gaussian_kernel</code> <p>Specifies the 1-dimensional kernel of the Gaussian process which will be used along each dimension of the parameter space. All the choices below will result in continuous sample paths and their main difference is the degree of smoothness of the results with 'exp' yielding the least and 'gauss' yielding the most smooth paths. Must be from <code>kernels</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>matern52</code> </p> <code>gaussian_optimization_burn_in_algorithm</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period of the optimization of the Gaussian process. Must be from <code>burn_in</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>latin_hypercube</code> </p> <code>gaussian_optimization_algorithm</code> <p>Determines the optimization algorithm used for the local search in the fitting of the Gaussian process to the previous parameter combinations. Must be from <code>optimization</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>nelder_mead</code> </p> <code>gaussian_optimization_burn_ins</code> <p>Number of random evaluation points used during the burn-in of the fitting of the Gaussian process. Range: [3, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>early_stopping</code> <p>Whether you want to apply early stopping to the predictors.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Note <p>A Gaussian hyperparameter search works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90%   of all iterations. During that burn-in phase, the hyperparameter   space is sampled more or less at random. You can control   this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a   Gaussian process on the hyperparameters with the <code>score</code> we want to   maximize or minimize as the predicted variable. Note that the   Gaussian process has hyperparameters itself, which are also optimized.   You can control this phase using <code>gaussian_kernel</code>,   <code>gaussian_optimization_algorithm</code>,   <code>gaussian_optimization_burn_in_algorithm</code> and   <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information   (EI), which is how much additional information it might get from   evaluating   a particular point in the hyperparameter space. The expected information   is to be maximized. The point in the hyperparameter space with   the maximum expected information is the next point that is actually   evaluated (meaning a new pipeline with these hyperparameters is trained).   You can control this phase using <code>optimization_algorithm</code>,   <code>optimization_burn_ins</code> and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space,   it starts evaluating hyperparameter combinations that are   particularly interesting.</p> </li> </ul> References <ul> <li>Carl Edward Rasmussen and Christopher K. I. Williams, MIT   Press, 2006 </li> <li>Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009   </li> </ul> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score:str=metrics.rmse,\n    n_iter:int=100,\n    seed:int=5483,\n    ratio_iter:float=0.80,\n    optimization_algorithm:str=nelder_mead,\n    optimization_burn_in_algorithm:str=latin_hypercube,\n    optimization_burn_ins:int=500,\n    surrogate_burn_in_algorithm:str=latin_hypercube,\n    gaussian_kernel:str=matern52,\n    gaussian_optimization_burn_in_algorithm:str=latin_hypercube,\n    gaussian_optimization_algorithm:str=nelder_mead,\n    gaussian_optimization_burn_ins:int=500,\n    gaussian_nugget:int=50,\n    early_stopping:bool=True,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        ratio_iter=ratio_iter,\n        optimization_algorithm=optimization_algorithm,\n        optimization_burn_in_algorithm=optimization_burn_in_algorithm,\n        optimization_burn_ins=optimization_burn_ins,\n        surrogate_burn_in_algorithm=surrogate_burn_in_algorithm,\n        gaussian_kernel=gaussian_kernel,\n        gaussian_optimization_algorithm=gaussian_optimization_algorithm,\n        gaussian_optimization_burn_in_algorithm=gaussian_optimization_burn_in_algorithm,\n        gaussian_optimization_burn_ins=gaussian_optimization_burn_ins,\n        gaussian_nugget=gaussian_nugget,\n        early_stopping=early_stopping,\n    )\n\n    self._type = \"GaussianHyperparameterSearch\"\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; \"_Hyperopt\":\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\n        \"population_validation_df_\"\n    ] = population_table_validation._getml_deserialize()\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.time()\n        msg = comm.log(sock)\n        end = time.time()\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; \"_Hyperopt\":\n    \"\"\"Reloads the hyperparameter optimization from the engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/latin/","title":"Latin Hypercube","text":""},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch","title":"LatinHypercubeSearch","text":"<pre><code>LatinHypercubeSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Latin hypercube sampling of the hyperparameters.</p> <p>Uses a multidimensional, uniform cumulative distribution function to draw the random numbers from. For drawing <code>n_iter</code> samples, the distribution will be divided in <code>n_iter</code>*<code>n_iter</code> hypercubes of equal size (<code>n_iter</code> per dimension). <code>n_iter</code> of them will be selected in such a way only one per dimension is used and an independent and identically-distributed (iid) random number is drawn within the boundaries of the hypercube.</p> <p>A latin hypercube search can be seen as a compromise between a grid search, which iterates through the entire hyperparameter space, and a random search, which draws completely random samples from the hyperparameter space.</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a LatinHypercubeSearch around the reference model\n\nlatin_search = hyperopt.LatinHypercubeSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nlatin_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score:str=metrics.rmse,\n    n_iter:int=100,\n    seed:int=5483,\n    **kwargs,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        **kwargs,\n    )\n\n    self._type = \"LatinHypercubeSearch\"\n\n    self.surrogate_burn_in_algorithm = latin_hypercube\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; \"_Hyperopt\":\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\n        \"population_validation_df_\"\n    ] = population_table_validation._getml_deserialize()\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.time()\n        msg = comm.log(sock)\n        end = time.time()\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; \"_Hyperopt\":\n    \"\"\"Reloads the hyperparameter optimization from the engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != latin_hypercube:\n        raise ValueError(\n            \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n        )\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/random/","title":"Random","text":""},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch","title":"RandomSearch","text":"<pre><code>RandomSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Uniformly distributed sampling of the hyperparameters.</p> <p>During every iteration, a new set of hyperparameters is chosen at random by uniformly drawing a random value in between the lower and upper bound for each dimension of <code>param_space</code> independently.</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <p><pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.Count,\n        aggregations.Sum\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a RandomSearch around the reference model\n\nrandom_search = hyperopt.RandomSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nrandom_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score:str=metrics.rmse,\n    n_iter:int=100,\n    seed:int=5483,\n    **kwargs,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        **kwargs,\n    )\n\n    self._type = \"RandomSearch\"\n\n    self.surrogate_burn_in_algorithm = random\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; \"_Hyperopt\":\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\n        \"population_validation_df_\"\n    ] = population_table_validation._getml_deserialize()\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.time()\n        msg = comm.log(sock)\n        end = time.time()\n\n    if msg != \"Success!\":\n        comm.engine_exception_handler(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; \"_Hyperopt\":\n    \"\"\"Reloads the hyperparameter optimization from the engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != random:\n        raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/pipeline/","title":"Index","text":""},{"location":"reference/pipeline/#getml.pipeline","title":"getml.pipeline","text":"<p>Contains handlers for all steps involved in a data science project after data preparation:</p> <ul> <li>Automated feature learning</li> <li>Automated feature selection</li> <li>Training and evaluation of machine learning (ML) algorithms</li> <li>Deployment of the fitted models</li> </ul> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>: Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <p><pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the <code>__repr__()</code> method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.delete","title":"delete","text":"<pre><code>delete(name: str) -&gt; None\n</code></pre> <p>If a pipeline named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the pipeline.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a pipeline named 'name' exists, it is deleted.\n\n    Args:\n        name:\n            Name of the pipeline.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        _make_dummy(name).delete()\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.exists","title":"exists","text":"<pre><code>exists(name: str) -&gt; bool\n</code></pre> <p>Returns true if a pipeline named 'name' exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the pipeline.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the pipeline exists, False otherwise.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"\n    Returns true if a pipeline named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n\n    Returns:\n            True if the pipeline exists, False otherwise.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_pipelines = list_pipelines()\n\n    return name in all_pipelines\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.list_pipelines","title":"list_pipelines","text":"<pre><code>list_pipelines() -&gt; List[str]\n</code></pre> <p>Lists all pipelines present in the engine.</p> <p>Note that this function only lists pipelines which are part of the current project. See <code>set_project</code> for changing projects and <code>pipelines</code> for more details about the lifecycles of the pipelines.</p> <p>To subsequently load one of them, use <code>load</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names of all pipelines.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def list_pipelines() -&gt; List[str]:\n    \"\"\"Lists all pipelines present in the engine.\n\n    Note that this function only lists pipelines which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects and [`pipelines`][getml.pipeline] for more details about\n    the lifecycles of the pipelines.\n\n    To subsequently load one of them, use\n    [`load`][getml.pipeline.load].\n\n    Returns:\n        List containing the names of all pipelines.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"list_pipelines\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.load","title":"load","text":"<pre><code>load(name: str) -&gt; Pipeline\n</code></pre> <p>Loads a pipeline from the getML engine into Python.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the pipeline to be loaded.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline that is a handler for the pipeline signified by name.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def load(name: str) -&gt; Pipeline:\n    \"\"\"Loads a pipeline from the getML engine into Python.\n\n    Args:\n        name: The name of the pipeline to be loaded.\n\n    Returns:\n        Pipeline that is a handler for the pipeline signified by name.\n    \"\"\"\n\n    return _make_dummy(name).refresh()\n</code></pre>"},{"location":"reference/pipeline/column/","title":"Column","text":""},{"location":"reference/pipeline/column/#getml.pipeline.column.Column","title":"getml.pipeline.column.Column  <code>dataclass</code>","text":"<pre><code>Column(\n    index: int,\n    name: str,\n    marker: str,\n    table: str,\n    target: str,\n    importance: float = np.nan,\n)\n</code></pre> <p>Dataclass that holds data about a single column.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of the column.</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>The name of the column.</p> <p> TYPE: <code>str</code> </p> <code>marker</code> <p>The marker of the column.</p> <p> TYPE: <code>str</code> </p> <code>table</code> <p>The table the column is from.</p> <p> TYPE: <code>str</code> </p> <code>target</code> <p>The target the column is associated with.</p> <p> TYPE: <code>str</code> </p> <code>importance</code> <p>The importance of the column.</p> <p> TYPE: <code>float</code> DEFAULT: <code>nan</code> </p>"},{"location":"reference/pipeline/columns/","title":"Columns","text":""},{"location":"reference/pipeline/columns/#getml.pipeline.Columns","title":"getml.pipeline.Columns","text":"<pre><code>Columns(\n    pipeline: str,\n    targets: Sequence[str],\n    peripheral: Sequence[Placeholder],\n    data: Optional[Sequence[Column]] = None,\n)\n</code></pre> <p>Container which holds a pipeline's columns. These include the columns for which importance can be calculated, such as the ones with <code>roles</code> as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Columns can be accessed by name, index or with a NumPy array. The container supports slicing and is sort- and filterable. Further, the container holds global methods to request columns' importances and apply a column selection to data frames provided to the pipeline.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The id of the pipeline.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The names of the targets used for this pipeline.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>peripheral</code> <p>The abstract representation of peripheral tables used for this pipeline.</p> <p> TYPE: <code>Sequence[Placeholder]</code> </p> <code>data</code> <p>The columns to be stored in the container. If not provided, they are obtained from the engine.</p> <p> TYPE: <code>Optional[Sequence[Column]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_columns = my_pipeline.columns\n\nfirst_column = my_pipeline.columns[0]\n\nall_but_last_10_columns = my_pipeline.columns[:-10]\n\nimportant_columns = [column for column in my_pipeline.columns if\ncolumn.importance &gt; 0.1]\n\nnames, importances = my_pipeline.columns.importances()\n\n# Drops all categorical and numerical columns that are not # in the\ntop 20%. new_container = my_pipeline.columns.select(\n    container, share_selected_columns=0.2,\n)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def __init__(\n    self,\n    pipeline: str,\n    targets: Sequence[str],\n    peripheral: Sequence[Placeholder],\n    data: Optional[Sequence[Column]] = None,\n) -&gt; None:\n    if not isinstance(pipeline, str):\n        raise ValueError(\"'pipeline' must be a str.\")\n\n    if not _is_typed_list(targets, str):\n        raise TypeError(\"'targets' must be a list of str.\")\n\n    self.pipeline = pipeline\n\n    self.targets = targets\n\n    self.peripheral = peripheral\n\n    self.peripheral_names = [p.name for p in self.peripheral]\n\n    if data is not None:\n        self.data = data\n    else:\n        self._load_columns()\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Column], bool]) -&gt; Columns\n</code></pre> <p>Filters the columns container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Column], bool]</code> </p> RETURNS DESCRIPTION <code>Columns</code> <p>A container of filtered Columns.</p> Example <pre><code>important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\nperipheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n    \"\"\"\n    Filters the columns container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        A container of filtered Columns.\n\n    Example:\n        ```python\n        important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n        peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    columns_filtered = [column for column in self.data if conditional(column)]\n    return self._make_columns(columns_filtered)\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the column importances.</p> <p>Column importances extend the idea of column importances to the columns originally inserted into the pipeline. Each column is assigned an importance value that measures its contribution to the predictive performance. All columns importances add up to 1.</p> <p>The importances can be calculated for columns with <code>roles</code> such as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the columns.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the column importances.\n\n    Column importances extend the idea of column importances\n    to the columns originally inserted into the pipeline.\n    Each column is assigned an importance value that measures\n    its contribution to the predictive performance. All\n    columns importances add up to 1.\n\n    The importances can be calculated for columns with\n    [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the columns.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    descriptions, importances = self._get_column_importances(\n        target_num=target_num, sort=sort\n    )\n\n    # ------------------------------------------------------------\n\n    names = np.asarray(\n        [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n    )\n\n    # ------------------------------------------------------------\n\n    return names, importances\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.select","title":"select","text":"<pre><code>select(\n    container: Union[Container, StarSchema, TimeSeries],\n    share_selected_columns: float = 0.5,\n) -&gt; Container\n</code></pre> <p>Returns a new data container with all insufficiently important columns dropped.</p> PARAMETER DESCRIPTION <code>container</code> <p>The container containing the data you want to use.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>share_selected_columns</code> <p>The share of columns to keep. Must be between 0.0 and 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>Container</code> <p>A new container with the columns dropped.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def select(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    share_selected_columns: float = 0.5,\n) -&gt; Container:\n    \"\"\"\n    Returns a new data container with all insufficiently important columns dropped.\n\n    Args:\n        container:\n            The container containing the data you want to use.\n\n        share_selected_columns: The share of columns\n            to keep. Must be between 0.0 and 1.0.\n\n    Returns:\n        A new container with the columns dropped.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        data = self.select(\n            container.container, share_selected_columns=share_selected_columns\n        )\n        new_container = deepcopy(container)\n        new_container._container = data\n        return new_container\n\n    # ------------------------------------------------------------\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a getml.data.Container, \"\n            + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n        )\n\n    if not isinstance(share_selected_columns, numbers.Real):\n        raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n    if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n        raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n    # ------------------------------------------------------------\n\n    descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n    # ------------------------------------------------------------\n\n    num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n    keep_columns = descriptions[:num_keep]\n\n    # ------------------------------------------------------------\n\n    subsets = {\n        k: _drop(v, keep_columns, k, POPULATION)\n        for (k, v) in container.subsets.items()\n    }\n\n    peripheral = {\n        k: _drop(v, keep_columns, k, PERIPHERAL)\n        for (k, v) in container.peripheral.items()\n    }\n\n    # ------------------------------------------------------------\n\n    new_container = Container(**subsets)\n    new_container.add(**peripheral)\n    new_container.freeze()\n\n    # ------------------------------------------------------------\n\n    return new_container\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns\n</code></pre> <p>Sorts the Columns container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - table(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Column], Any]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Columns</code> <p>A container of sorted columns.</p> Example <pre><code>by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns:\n    \"\"\"\n    Sorts the Columns container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - table(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted columns.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        columns_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_columns(columns_sorted)\n\n    if by is None:\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        columns_sorted.sort(key=lambda column: column.target)\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"tables?$\", string=by):\n        columns_sorted = sorted(\n            self.data,\n            key=lambda column: column.table,\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.importance, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the columns in a pandas data frame.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n    names, markers, tables, importances, targets = (\n        self._pivot(field)\n        for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n    )\n\n    data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n    data_frame[\"name\"] = names\n\n    data_frame[\"marker\"] = markers\n\n    data_frame[\"table\"] = tables\n\n    data_frame[\"importance\"] = importances\n\n    data_frame[\"target\"] = targets\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/dialect/","title":"dialect","text":""},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect","title":"getml.pipeline.dialect","text":"<p>SQL dialects that can be used for the generated code.</p> <p>One way to productionize a <code>Pipeline</code> is to transpile its features to production-ready SQL code. This SQL code can be run on standard cloud infrastructure. Please also refer to <code>SQLCode</code>.</p> Example <pre><code>sql_code = my_pipeline.features.to_sql(\n    getml.pipeline.dialect.spark_sql)\n\n# Creates a folder called \"my_pipeline\"\n# which contains the SQL scripts.\nsql_code.save(\"my_pipeline\")\n</code></pre>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.bigquery","title":"bigquery  <code>module-attribute</code>","text":"<pre><code>bigquery = _all_dialects[0]\n</code></pre> <p>BigQuery is a proprietary database system used by the Google Cloud.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.human_readable_sql","title":"human_readable_sql  <code>module-attribute</code>","text":"<pre><code>human_readable_sql = _all_dialects[1]\n</code></pre> <p>SQL that is not meant to be executed, but for interpretation by humans.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.mysql","title":"mysql  <code>module-attribute</code>","text":"<pre><code>mysql = _all_dialects[2]\n</code></pre> <p>MySQL and its fork MariaDB are among the most popular open-source database systems.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.postgres","title":"postgres  <code>module-attribute</code>","text":"<pre><code>postgres = _all_dialects[3]\n</code></pre> <p>The PostgreSQL or postgres dialect is a popular SQL dialect used by PostgreSQL and its many derivatives like Redshift or Greenplum.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.spark_sql","title":"spark_sql  <code>module-attribute</code>","text":"<pre><code>spark_sql = _all_dialects[4]\n</code></pre> <p>Spark SQL is the SQL dialect used by Apache Spark.</p> <p>Apache Spark is an open-source, distributed, in-memory engine for large-scale data processing and a popular choice for producutionizing machine learning pipelines.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.sqlite3","title":"sqlite3  <code>module-attribute</code>","text":"<pre><code>sqlite3 = _all_dialects[5]\n</code></pre> <p>SQLite3 is a light-weight and widely used database system.</p> <p>It is recommended for live prediction systems or when the amount of data handled is unlikely to be too large.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.tsql","title":"tsql  <code>module-attribute</code>","text":"<pre><code>tsql = _all_dialects[6]\n</code></pre> <p>TSQL or Transact-SQL is the dialect used by most Microsoft databases.</p> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/pipeline/feature/","title":"Feature","text":""},{"location":"reference/pipeline/feature/#getml.pipeline.feature","title":"getml.pipeline.feature","text":"<p>Custom representing a sole feature.</p>"},{"location":"reference/pipeline/feature/#getml.pipeline.feature.Feature","title":"Feature  <code>dataclass</code>","text":"<pre><code>Feature(\n    index: int,\n    name: str,\n    pipeline: str,\n    target: str,\n    targets: Sequence[str],\n    importance: float,\n    correlation: float,\n    sql: SQLString,\n)\n</code></pre> <p>Dataclass that holds data about a single feature.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of the feature.</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>The name of the feature.</p> <p> TYPE: <code>str</code> </p> <code>pipeline</code> <p>The pipeline the feature is from.</p> <p> TYPE: <code>str</code> </p> <code>target</code> <p>The target the feature is associated with.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The targets the feature is associated with.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>importance</code> <p>The importance of the feature.</p> <p> TYPE: <code>float</code> </p> <code>correlation</code> <p>The correlation of the feature with the target.</p> <p> TYPE: <code>float</code> </p> <code>sql</code> <p>The SQL code of the feature.</p> <p> TYPE: <code>SQLString</code> </p>"},{"location":"reference/pipeline/features/","title":"Features","text":""},{"location":"reference/pipeline/features/#getml.pipeline.Features","title":"getml.pipeline.Features","text":"<pre><code>Features(\n    pipeline: str,\n    targets: Sequence[str],\n    data: Optional[Sequence[Feature]] = None,\n)\n</code></pre> <p>Container which holds a pipeline's features. Features can be accessed by name, index or with a numpy array. The container supports slicing and is sort- and filterable.</p> <p>Further, the container holds global methods to request features' importances, correlations and their respective transpiled sql representation.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The name of the pipeline the features are associated with.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The targets the features are associated with.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>data</code> <p>The features to be stored in the container.</p> <p> TYPE: <code>Optional[Sequence[Feature]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_features = my_pipeline.features\n\nfirst_feature = my_pipeline.features[0]\n\nsecond_feature = my_pipeline.features[\"feature_1_2\"]\n\nall_but_last_10_features = my_pipeline.features[:-10]\n\nimportant_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\nnames, importances = my_pipeline.features.importances()\n\nnames, correlations = my_pipeline.features.correlations()\n\nsql_code = my_pipeline.features.to_sql()\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def __init__(\n    self,\n    pipeline: str,\n    targets: Sequence[str],\n    data: Optional[Sequence[Feature]] = None,\n) -&gt; None:\n    if not isinstance(pipeline, str):\n        raise ValueError(\"'pipeline' must be a str.\")\n\n    if not _is_typed_list(targets, str):\n        raise TypeError(\"'targets' must be a list of str.\")\n\n    self.pipeline = pipeline\n\n    self.targets = targets\n\n    if data is None:\n        self.data = self._load_features()\n\n    else:\n        self.data = list(data)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.correlation","title":"correlation  <code>property</code>","text":"<pre><code>correlation: List[float]\n</code></pre> <p>Holds the correlations of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[float]</code> <p>List containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.importance","title":"importance  <code>property</code>","text":"<pre><code>importance: List[float]\n</code></pre> <p>Holds the correlations of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[float]</code> <p>List containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.name","title":"name  <code>property</code>","text":"<pre><code>name: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.correlations","title":"correlations","text":"<pre><code>correlations(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the feature correlations, as displayed in the getML monitor.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains the correlations with the target.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def correlations(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature correlations,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains the correlations with the target.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n    assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n    if hasattr(self, \"data\"):\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(correlations)\n            ]\n        )\n\n        names = names[indices]\n        correlations = correlations[indices]\n\n    if not sort:\n        return names, correlations\n\n    indices = np.argsort(np.abs(correlations))[::-1]\n\n    return (names[indices], correlations[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Feature], bool]) -&gt; Features\n</code></pre> <p>Filters the Features container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Feature], bool]</code> </p> RETURNS DESCRIPTION <code>Features</code> <p>A container of filtered Features.</p> Example <pre><code>important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\ncorrelated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n    \"\"\"\n     Filters the Features container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered Features.\n\n    Example:\n        ```python\n        important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n        correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n        ```\n    \"\"\"\n    features_filtered = [feature for feature in self.data if conditional(feature)]\n    return Features(self.pipeline, self.targets, data=features_filtered)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the feature importances, as displayed in the getML monitor.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature importances,\n    as displayed in the getML monitor.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains their importances. By definition, all importances add up to 1.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_importances\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    importances = np.asarray(json_obj[\"feature_importances_\"])\n\n    if hasattr(self, \"data\"):\n        assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(importances)\n            ]\n        )\n\n        names = names[indices]\n        importances = importances[indices]\n\n    if not sort:\n        return names, importances\n\n    assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[[Feature], Union[float, int, str]]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features\n</code></pre> <p>Sorts the Features container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - correlation(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Feature], Union[float, int, str]]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> Return <p>A container of sorted Features.</p> Example <pre><code>by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\nby_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[\n            [Feature],\n            Union[\n                float,\n                int,\n                str,\n            ],\n        ]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features:\n    \"\"\"\n    Sorts the Features container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - correlation(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted Features.\n\n    Example:\n        ```python\n        by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n        by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        features_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_features(features_sorted)\n\n    else:\n        if by is None:\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.index, reverse=reverse\n            )\n            features_sorted.sort(key=lambda feature: feature.target)\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.name, reverse=reverse\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"correlations?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: abs(feature.correlation),\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: feature.importance,\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the features in a pandas data frame.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas data frame containing the features' names, importances, correlations, and SQL code.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the features in a pandas data frame.\n\n    Returns:\n        A pandas data frame containing the features' names, importances, correlations, and SQL code.\n    \"\"\"\n\n    return self._to_pandas()\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.to_sql","title":"to_sql","text":"<pre><code>to_sql(\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode\n</code></pre> <p>Returns SQL statements visualizing the features.</p> PARAMETER DESCRIPTION <code>targets</code> <p>Whether you want to include the target columns in the main table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>subfeatures</code> <p>Whether you want to include the code for the subfeatures of a snowflake schema.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dialect</code> <p>The SQL dialect to use. Must be from <code>dialect</code>. Please note that not all dialects are supported in the getML community edition.</p> <p> TYPE: <code>str</code> DEFAULT: <code>sqlite3</code> </p> <code>schema</code> <p>The schema in which to wrap all generated tables and indices. None for no schema. Not applicable to all dialects. For the BigQuery and MySQL dialects, the schema is identical to the database ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>nchar_categorical</code> <p>The maximum number of characters used in the VARCHAR for categorical columns. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>nchar_join_key</code> <p>The maximum number of characters used in the VARCHAR for join keys. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>nchar_text</code> <p>The maximum number of characters used in the VARCHAR for text columns. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4096</code> </p> <code>size_threshold</code> <p>The maximum number of characters to display in a single feature. Displaying extremely complicated features can crash your iPython notebook or lead to unexpectedly high memory consumption, which is why a reasonable upper limit is advantageous. Set to None for no upper limit.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>50000</code> </p> RETURNS DESCRIPTION <code>SQLCode</code> <p>Object representing the features.</p> <p>Examples:</p> <pre><code>my_pipeline.features.to_sql()\n</code></pre> Note <p>Only fitted pipelines (<code>fit</code>) can hold trained features which can be returned as SQL statements.</p> Note <p>The getML community edition only supports transpilation to human-readable SQL. Passing 'sqlite3' will also produce human-readable SQL.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_sql(\n    self,\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode:\n    \"\"\"\n    Returns SQL statements visualizing the features.\n\n    Args:\n        targets:\n            Whether you want to include the target columns\n            in the main table.\n\n        subfeatures:\n            Whether you want to include the code for the\n            subfeatures of a snowflake schema.\n\n        dialect:\n            The SQL dialect to use. Must be from\n            [`dialect`][getml.pipeline.dialect]. Please\n            note that not all dialects are supported\n            in the getML community edition.\n\n        schema:\n            The schema in which to wrap all generated tables and\n            indices. None for no schema. Not applicable to all dialects.\n            For the BigQuery and MySQL dialects, the schema is identical\n            to the database ID.\n\n        nchar_categorical:\n            The maximum number of characters used in the\n            VARCHAR for categorical columns. Not applicable\n            to all dialects.\n\n        nchar_join_key:\n            The maximum number of characters used in the\n            VARCHAR for join keys. Not applicable\n            to all dialects.\n\n        nchar_text:\n            The maximum number of characters used in the\n            VARCHAR for text columns. Not applicable\n            to all dialects.\n\n        size_threshold:\n            The maximum number of characters to display\n            in a single feature. Displaying extremely\n            complicated features can crash your iPython\n            notebook or lead to unexpectedly high memory\n            consumption, which is why a reasonable\n            upper limit is advantageous. Set to None\n            for no upper limit.\n\n    Returns:\n            Object representing the features.\n\n    Examples:\n        ```python\n        my_pipeline.features.to_sql()\n        ```\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can hold trained\n        features which can be returned as SQL statements.\n\n    Note:\n        The getML community edition only supports\n        transpilation to human-readable SQL. Passing\n        'sqlite3' will also produce human-readable SQL.\n\n    \"\"\"\n\n    if not isinstance(targets, bool):\n        raise TypeError(\"'targets' must be a bool!\")\n\n    if not isinstance(subfeatures, bool):\n        raise TypeError(\"'subfeatures' must be a bool!\")\n\n    if not isinstance(dialect, str):\n        raise TypeError(\"'dialect' must be a string!\")\n\n    if not isinstance(nchar_categorical, int):\n        raise TypeError(\"'nchar_categorical' must be an int!\")\n\n    if not isinstance(nchar_join_key, int):\n        raise TypeError(\"'nchar_join_key' must be an int!\")\n\n    if not isinstance(nchar_text, int):\n        raise TypeError(\"'nchar_text' must be an int!\")\n\n    if dialect not in _all_dialects:\n        raise ValueError(\n            \"'dialect' must from getml.pipeline.dialect, \"\n            + \"meaning that is must be one of the following: \"\n            + str(_all_dialects)\n            + \".\"\n        )\n\n    if size_threshold is not None and not isinstance(size_threshold, int):\n        raise TypeError(\"'size_threshold' must be an int or None!\")\n\n    if size_threshold is not None and size_threshold &lt;= 0:\n        raise ValueError(\"'size_threshold' must be a positive number!\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.to_sql\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"targets_\"] = targets\n    cmd[\"subfeatures_\"] = subfeatures\n    cmd[\"dialect_\"] = dialect\n    cmd[\"schema_\"] = schema or \"\"\n    cmd[\"nchar_categorical_\"] = nchar_categorical\n    cmd[\"nchar_join_key_\"] = nchar_join_key\n    cmd[\"nchar_text_\"] = nchar_text\n\n    if size_threshold is not None:\n        cmd[\"size_threshold_\"] = size_threshold\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        sql = comm.recv_string(sock)\n\n    return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/metadata/","title":"Metadata","text":""},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata","title":"getml.pipeline.metadata","text":"<p>Contains the metadata related to the data frames that were originally passed to .fit(...).</p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata","title":"Metadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to a data frame that were originally passed to .fit(...).</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the data frame.</p> <p> </p> <code>roles</code> <p>The roles of the columns in the data frame.</p> <p> </p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata.roles","title":"roles  <code>instance-attribute</code>","text":"<pre><code>roles: Roles\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata","title":"AllMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to all the data frames that were originally passed to .fit(...).</p> PARAMETER DESCRIPTION <code>peripheral</code> <p>The metadata of the peripheral tables.</p> <p> </p> <code>population</code> <p>The metadata of the population table.</p> <p> </p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata.peripheral","title":"peripheral  <code>instance-attribute</code>","text":"<pre><code>peripheral: List[Metadata]\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata.population","title":"population  <code>instance-attribute</code>","text":"<pre><code>population: Metadata\n</code></pre>"},{"location":"reference/pipeline/metrics/","title":"metrics","text":""},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics","title":"getml.pipeline.metrics","text":"<p>Signifies different scoring methods.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.auc","title":"auc  <code>module-attribute</code>","text":"<pre><code>auc = _all_metrics[0]\n</code></pre> <p>Area under the curve - refers to the area under the receiver operating characteristic (ROC) curve.</p> <p>Used for classification problems.</p> <p>When handling a classification problem, the ROC curve maps the relationship between two conflicting goals:</p> <p>On the hand, we want a high true positive rate. The true positive rate, sometimes referred to as recall, measures the share of true positive predictions over all positives:</p> \\[ TPR = \\frac{number \\; of \\; true \\; positives}{number \\; of \\; all \\; positives} \\] <p>In other words, we want our classification algorithm to \"catch\" as many positives as possible.</p> <p>On the other hand, we also want a low false positive rate (FPR). The false positive rate measures the share of false positives over all negatives.</p> \\[ FPR = \\frac{number \\; of \\; false \\; positives}{number \\; of \\; all \\; negatives} \\] <p>In other words, we want as few \"false alarms\" as possible.</p> <p>However, unless we have a perfect classifier, these two goals conflict with each other.</p> <p>The ROC curve maps the TPR against the FPR. We now measure the area under said curve (AUC). A higher AUC implies that the trade-off between TPR and FPR is more beneficial. A perfect model would have an AUC of 1. An AUC of 0.5 implies that the model has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.accuracy","title":"accuracy  <code>module-attribute</code>","text":"<pre><code>accuracy = _all_metrics[1]\n</code></pre> <p>Accuracy - measures the share of accurate predictions as of total samples in the testing set.</p> <p>Used for classification problems.</p> \\[ accuracy = \\frac{number \\; of \\; correct \\; predictions}{number \\; of \\; all \\; predictions} \\] <p>The number of correct predictions depends on the threshold used: For instance, we could interpret all predictions for which the probability is greater than 0.5 as a positive and all others as a negative. But we do not have to use a threshold of 0.5 - we might as well use any other threshold. Which threshold we choose will impact the calculated accuracy.</p> <p>When calculating the accuracy, the value returned is the accuracy returned by the best threshold.</p> <p>Even though accuracy is the most intuitive way to measure a classification algorithm, it can also be very misleading when the samples are very skewed. For instance, if only 2% of the samples are positive, a predictor that always predicts negative outcomes will have an accuracy of 98%. This sounds very good to the layman, but the predictor in this example actually has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.cross_entropy","title":"cross_entropy  <code>module-attribute</code>","text":"<pre><code>cross_entropy = _all_metrics[2]\n</code></pre> <p>Cross entropy, also referred to as log-loss, is a measure of the likelihood of the classification model.</p> <p>Used for classification problems.</p> <p>Mathematically speaking, cross-entropy for a binary classification problem is defined as follows:</p> \\[ cross \\; entropy = - \\frac{1}{N} \\sum_{i}^{N} (y_i \\log p_i + (1 - y_i) \\log(1 - p_i), \\] <p>where \\(p_i\\) is the probability of a positive outcome as predicted by the classification algorithm and \\(y_i\\) is the target value, which is 1 for a positive outcome and 0 otherwise.</p> <p>There are several ways to justify the use of cross entropy to evaluate classification algorithms. But the most intuitive way is to think of it as a measure of likelihood. When we have a classification algorithm that gives us probabilities, we would like to know how likely it is that we observe a particular state of the world given the probabilities.</p> <p>We can calculate this likelihood as follows:</p> \\[ likelihood = \\prod_{i}^{N} (p_i^{y_i} * (1 - p_i)^{1 - y_i}). \\] <p>(Recall that \\(y_i\\) can only be 0 or 1.)</p> <p>If we take the logarithm of the likelihood as defined above, divide by \\(N\\) and then multiply by <code>-1</code> (because we want lower to mean better and 0 to mean perfect), the outcome will be cross entropy.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.mae","title":"mae  <code>module-attribute</code>","text":"<pre><code>mae = _all_metrics[3]\n</code></pre> <p>Mean Absolute Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ MAE = \\frac{\\sum_{i=1}^n | \\mathbf{y}_i - \\mathbf{\\hat{y}}_i |}{n}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rmse","title":"rmse  <code>module-attribute</code>","text":"<pre><code>rmse = _all_metrics[4]\n</code></pre> <p>Root Mean Squared Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^n ( \\mathbf{y}_i - \\mathbf{\\hat{y}}_i )^2}{n}}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rsquared","title":"rsquared  <code>module-attribute</code>","text":"<pre><code>rsquared = _all_metrics[5]\n</code></pre> <p>\\(R^{2}\\) - squared correlation coefficient between predictions and targets.</p> <p>Used for regression problems.</p> <p>\\(R^{2}\\) is defined as follows:</p> \\[ R^{2} = \\frac{(\\sum_{i=1}^n ( y_i - \\bar{y_i} ) *  ( \\hat{y_i} - \\bar{\\hat{y_i}} ))^2 }{\\sum_{i=1}^n ( y_i - \\bar{y_i} )^2 \\sum_{i=1}^n ( \\hat{y_i} - \\bar{\\hat{y_i}} )^2 }, \\] <p>where \\(y_i\\) are the true values, \\(\\hat{y_i}\\) are the predictions and \\(\\bar{...}\\) denotes the mean operator.</p> <p>An \\(R^{2}\\) of 1 implies perfect correlation between the predictions and the targets and an \\(R^{2}\\) of 0 implies no correlation at all.</p>"},{"location":"reference/pipeline/pipeline/","title":"Pipeline","text":""},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline","title":"getml.pipeline.Pipeline","text":"<pre><code>Pipeline(\n    data_model: Optional[DataModel] = None,\n    peripheral: Optional[List[Placeholder]] = None,\n    preprocessors: Optional[\n        Union[\n            CategoryTrimmer,\n            EmailDomain,\n            Imputation,\n            Mapping,\n            Seasonal,\n            Substring,\n            TextFieldSplitter,\n            List[\n                Union[\n                    CategoryTrimmer,\n                    EmailDomain,\n                    Imputation,\n                    Mapping,\n                    Seasonal,\n                    Substring,\n                    TextFieldSplitter,\n                ]\n            ],\n        ]\n    ] = None,\n    feature_learners: Optional[\n        Union[\n            Union[\n                Fastboost,\n                FastProp,\n                Multirel,\n                Relboost,\n                RelMT,\n            ],\n            List[\n                Union[\n                    Fastboost,\n                    FastProp,\n                    Multirel,\n                    Relboost,\n                    RelMT,\n                ]\n            ],\n        ]\n    ] = None,\n    feature_selectors: Optional[\n        Union[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n            ],\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    predictors: Optional[\n        Union[\n            LinearRegression,\n            LogisticRegression,\n            XGBoostClassifier,\n            XGBoostRegressor,\n            ScaleGBMClassifier,\n            ScaleGBMRegressor,\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    loss_function: Optional[str] = None,\n    tags: Optional[list[str]] = None,\n    include_categorical: bool = False,\n    share_selected_features: float = 0.5,\n)\n</code></pre> <p>A Pipeline is the main class for feature learning and prediction.</p> PARAMETER DESCRIPTION <code>data_model</code> <p>Abstract representation of the data_model, which defines the abstract relationships between the tables. Required for the feature learners.</p> <p> TYPE: <code>Optional[DataModel]</code> DEFAULT: <code>None</code> </p> <code>peripheral</code> <p>Abstract representations of the additional tables used to augment the information provided in <code>population</code>. These have to be the same objects that were <code>join</code> ed onto the <code>population</code> <code>Placeholder</code>. Their order determines the order of the peripheral <code>DataFrame</code> passed to the 'peripheral_tables' argument in <code>check</code>, <code>fit</code>, <code>predict</code>, <code>score</code>, and <code>transform</code>, if you pass the data frames as a list. If you omit the peripheral placeholders, they will be inferred from the data model and ordered alphabetically.</p> <p> TYPE: <code>Optional[List[Placeholder]]</code> DEFAULT: <code>None</code> </p> <code>preprocessors</code> <p>The preprocessor(s) to be used. Must be from <code>preprocessors</code>. A single preprocessor does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter, List[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter]]]]</code> DEFAULT: <code>None</code> </p> <code>feature_learners</code> <p>The feature learner(s) to be used. Must be from <code>feature_learning</code>. A single feature learner does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[Union[Fastboost, FastProp, Multirel, Relboost, RelMT], List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]]]]</code> DEFAULT: <code>None</code> </p> <code>feature_selectors</code> <p>Predictor(s) used to select the best features. Must be from <code>predictors</code>. A single feature selector does not have to be wrapped in a list. Make sure to also set share_selected_features.</p> <p> TYPE: <code>Optional[Union[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor], List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> DEFAULT: <code>None</code> </p> <code>predictors</code> <p>Predictor(s) used to generate the predictions. If more than one predictor is passed, the predictions generated will be averaged. Must be from <code>predictors</code>. A single predictor does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor, List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> DEFAULT: <code>None</code> </p> <code>loss_function</code> <p>The loss function to use for the feature learners.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags exist to help you organize your pipelines. You can add any tags that help you remember what you were trying to do.</p> <p> TYPE: <code>Optional[list[str]]</code> DEFAULT: <code>None</code> </p> <code>include_categorical</code> <p>Whether you want to pass categorical columns in the population table to the predictor.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>share_selected_features</code> <p>The share of features you want the feature selection to keep. When set to 0.0, then all features will be kept.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>:</p> <p>Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> <p>Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the __repr__ method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data_model: Optional[DataModel] = None,\n    peripheral: Optional[List[Placeholder]] = None,\n    preprocessors: Optional[\n        Union[\n            CategoryTrimmer,\n            EmailDomain,\n            Imputation,\n            Mapping,\n            Seasonal,\n            Substring,\n            TextFieldSplitter,\n            List[\n                Union[\n                    CategoryTrimmer,\n                    EmailDomain,\n                    Imputation,\n                    Mapping,\n                    Seasonal,\n                    Substring,\n                    TextFieldSplitter,\n                ]\n            ],\n        ],\n    ] = None,\n    feature_learners: Optional[\n        Union[\n            Union[Fastboost, FastProp, Multirel, Relboost, RelMT],\n            List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]],\n        ]\n    ] = None,\n    feature_selectors: Optional[\n        Union[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n            ],\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ],\n    ] = None,\n    predictors: Optional[\n        Union[\n            LinearRegression,\n            LogisticRegression,\n            XGBoostClassifier,\n            XGBoostRegressor,\n            ScaleGBMClassifier,\n            ScaleGBMRegressor,\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    loss_function: Optional[str] = None,\n    tags: Optional[list[str]] = None,\n    include_categorical: bool = False,\n    share_selected_features: float = 0.5,\n) -&gt; None:\n    data_model = data_model or DataModel(\"population\")\n\n    if not isinstance(data_model, DataModel):\n        raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n    peripheral = peripheral or _infer_peripheral(data_model.population)\n\n    preprocessors = preprocessors or []\n\n    feature_learners = feature_learners or []\n\n    feature_selectors = feature_selectors or []\n\n    predictors = predictors or []\n\n    tags = tags or []\n\n    if not isinstance(preprocessors, list):\n        preprocessors = [preprocessors]\n\n    if not isinstance(feature_learners, list):\n        feature_learners = [feature_learners]\n\n    if not isinstance(feature_selectors, list):\n        feature_selectors = [feature_selectors]\n\n    if not isinstance(predictors, list):\n        predictors = [predictors]\n\n    if not isinstance(peripheral, list):\n        peripheral = [peripheral]\n\n    if not isinstance(tags, list):\n        tags = [tags]\n\n    self._id: str = NOT_FITTED\n\n    self.type = \"Pipeline\"\n\n    loss_function = (\n        loss_function\n        or (\n            [fl.loss_function for fl in feature_learners if fl.loss_function]\n            or [\"SquareLoss\"]\n        )[0]\n    )\n\n    feature_learners = [\n        _handle_loss_function(fl, loss_function) for fl in feature_learners\n    ]\n\n    self.data_model = data_model\n    self.feature_learners = feature_learners\n    self.feature_selectors = feature_selectors\n    self.include_categorical = include_categorical\n    self.loss_function = loss_function\n    self.peripheral = peripheral\n    self.predictors = predictors\n    self.preprocessors = preprocessors\n    self.share_selected_features = share_selected_features\n    self.tags = Tags(tags)\n\n    self._metadata: Optional[AllMetadata] = None\n\n    self._scores: Dict[str, Any] = {}\n\n    self._targets: List[str] = []\n\n    setattr(type(self), \"_supported_params\", list(self.__dict__.keys()))\n\n    self._validate()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.accuracy","title":"accuracy  <code>property</code>","text":"<pre><code>accuracy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the accuracy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The accuracy of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.auc","title":"auc  <code>property</code>","text":"<pre><code>auc: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the auc of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The auc of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: Columns\n</code></pre> <p><code>Columns</code> object that can be used to handle information about the original columns utilized by the feature learners.</p> RETURNS DESCRIPTION <code>Columns</code> <p>The columns object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.cross_entropy","title":"cross_entropy  <code>property</code>","text":"<pre><code>cross_entropy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the cross entropy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The cross entropy of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.features","title":"features  <code>property</code>","text":"<pre><code>features: Features\n</code></pre> <p><code>Features</code> object that can be used to handle the features generated by the feature learners.</p> RETURNS DESCRIPTION <code>Features</code> <p>The features object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.fitted","title":"fitted  <code>property</code>","text":"<pre><code>fitted: bool\n</code></pre> <p>Whether the pipeline has already been fitted.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline has already been fitted.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.mae","title":"mae  <code>property</code>","text":"<pre><code>mae: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the mae of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The mae of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.plots","title":"plots  <code>property</code>","text":"<pre><code>plots: Plots\n</code></pre> <p><code>Plots</code> object that can be used to generate plots like an ROC curve or a lift curve.</p> RETURNS DESCRIPTION <code>Plots</code> <p>The plots object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>ID of the pipeline. This is used to uniquely identify the pipeline on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The ID of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.is_classification","title":"is_classification  <code>property</code>","text":"<pre><code>is_classification: bool\n</code></pre> <p>Whether the pipeline can be used for classification problems.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline can be used for classification problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.is_regression","title":"is_regression  <code>property</code>","text":"<pre><code>is_regression: bool\n</code></pre> <p>Whether the pipeline can be used for regression problems.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline can be used for regression problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: Optional[AllMetadata]\n</code></pre> <p>Contains information on the data frames that were passed to .fit(...). The roles contained therein can be directly passed to existing data frames to correctly reassign the roles of existing columns. If the pipeline has not been fitted, this is None.</p> RETURNS DESCRIPTION <code>Optional[AllMetadata]</code> <p>The metadata of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the pipeline. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The ID of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.rmse","title":"rmse  <code>property</code>","text":"<pre><code>rmse: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the rmse of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The rmse of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.rsquared","title":"rsquared  <code>property</code>","text":"<pre><code>rsquared: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the rsquared of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The rsquared of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.scores","title":"scores  <code>property</code>","text":"<pre><code>scores: Scores\n</code></pre> <p>Contains all scores generated by <code>score</code></p> RETURNS DESCRIPTION <code>Scores</code> <p>A container that holds the scores for the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.scored","title":"scored  <code>property</code>","text":"<pre><code>scored: bool\n</code></pre> <p>Whether the pipeline has been scored.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline has been scored.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.tables","title":"tables  <code>property</code>","text":"<pre><code>tables: Tables\n</code></pre> <p><code>Tables</code> object that can be used to handle information about the original tables utilized by the feature learners.</p> RETURNS DESCRIPTION <code>Tables</code> <p>The tables object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.targets","title":"targets  <code>property</code>","text":"<pre><code>targets: List[str]\n</code></pre> <p>Contains the names of the targets used for this pipeline.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of the targets.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.check","title":"check","text":"<pre><code>check(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]\n</code></pre> <p>Checks the validity of the data model.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Dict[str, Union[DataFrame, View]], Sequence[Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def check(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]:\n    \"\"\"\n    Checks the validity of the data model.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    \"\"\"\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    temp = copy.deepcopy(self)\n\n    temp._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = temp.type + \".check\"\n    cmd[\"name_\"] = temp.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        print(\"Checking data model...\")\n        msg = comm.log(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        print()\n        issues = Issues(comm.recv_issues(sock))\n        if len(issues) == 0:\n            print(\"OK.\")\n        else:\n            print(\n                f\"The pipeline check generated {len(issues.info)} \"\n                + f\"issues labeled INFO and {len(issues.warnings)} \"\n                + \"issues labeled WARNING.\"\n            )\n\n    temp.delete()\n\n    return None if len(issues) == 0 else issues\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the pipeline from the engine.</p> Note <p>Caution: You can not undo this action!</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes the pipeline from the engine.\n\n    Note:\n        Caution: You can not undo this action!\n    \"\"\"\n    self._check_whether_fitted()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".delete\"\n    cmd[\"name_\"] = self.id\n    cmd[\"mem_only_\"] = False\n\n    comm.send(cmd)\n\n    self._id = NOT_FITTED\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.deploy","title":"deploy","text":"<pre><code>deploy(deploy: bool) -&gt; None\n</code></pre> <p>Allows a fitted pipeline to be addressable via an HTTP request. See deployment for details.</p> PARAMETER DESCRIPTION <code>deploy</code> <p>If <code>True</code>, the deployment of the pipeline will be triggered.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def deploy(self, deploy: bool) -&gt; None:\n    \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n    See [deployment][deployment] for details.\n\n    Args:\n        deploy: If `True`, the deployment of the pipeline\n            will be triggered.\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if not isinstance(deploy, bool):\n        raise TypeError(\"'deploy' must be of type bool\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".deploy\"\n    cmd[\"name_\"] = self.id\n    cmd[\"deploy_\"] = deploy\n\n    comm.send(cmd)\n\n    self._save()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.fit","title":"fit","text":"<pre><code>fit(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[\n        Union[DataFrame, View, Subset]\n    ] = None,\n    check: bool = True,\n) -&gt; \"Pipeline\"\n</code></pre> <p>Trains the feature learning algorithms, feature selectors and predictors.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>validation_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable. If you are passing a subset, that subset must be derived from the same container as population_table.</p> <p>Only used for early stopping in <code>XGBoostClassifier</code> and <code>XGBoostRegressor</code>.</p> <p> TYPE: <code>Optional[Union[DataFrame, View, Subset]]</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>Whether you want to check the data model before fitting. The checks are equivalent to the checks run by <code>check</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>'Pipeline'</code> <p>The fitted pipeline.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def fit(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n    check: bool = True,\n) -&gt; \"Pipeline\":\n    \"\"\"Trains the feature learning algorithms, feature selectors\n    and predictors.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        validation_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If you are passing a subset, that subset\n            must be derived from the same container as *population_table*.\n\n            Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n            and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n        check:\n            Whether you want to check the data model before fitting. The checks are\n            equivalent to the checks run by [`check`][getml.Pipeline.check].\n\n    Returns:\n        The fitted pipeline.\n    \"\"\"\n\n    additional_tags = (\n        [\"container-\" + population_table.container_id]\n        if isinstance(population_table, data.Subset)\n        else []\n    )\n\n    if (\n        isinstance(population_table, data.Subset)\n        and isinstance(validation_table, data.Subset)\n        and validation_table.container_id != population_table.container_id\n    ):\n        raise ValueError(\n            \"The subset used for validation must be from the same container \"\n            + \"as the subset used for training.\"\n        )\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    if isinstance(validation_table, data.Subset):\n        validation_table = validation_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if check:\n        warnings = self.check(population_table, peripheral_tables)\n        if warnings:\n            print(\"To see the issues in full, run .check() on the pipeline.\")\n            print()\n\n    self._send(additional_tags)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = self.type + \".fit\"\n    cmd[\"name_\"] = self.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    if validation_table is not None:\n        cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        begin = time.time()\n\n        msg = comm.log(sock)\n\n        end = time.time()\n\n        if \"Trained\" in msg:\n            print()\n            print(msg)\n            _print_time_taken(begin, end, \"Time taken: \")\n        else:\n            comm.engine_exception_handler(msg)\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.predict","title":"predict","text":"<pre><code>predict(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[float_], None]\n</code></pre> <p>Forecasts on new, unseen data using the trained <code>predictor</code>.</p> <p>Returns the predictions generated by the pipeline based on <code>population_table</code> and <code>peripheral_tables</code> or writes them into a database named <code>table_name</code>.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>table_name</code> <p>If not an empty string, the resulting predictions will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Union[NDArray[float_], None]</code> <p>Resulting predictions provided in an array of the (number of rows in <code>population_table</code>, number of targets in <code>population_table</code>).</p> Note <p>Only fitted pipelines (<code>fit</code>) can be used for prediction.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def predict(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[np.float_], None]:\n    \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n    Returns the predictions generated by the pipeline based on\n    `population_table` and `peripheral_tables` or writes them into\n    a database named `table_name`.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        table_name:\n            If not an empty string, the resulting predictions will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be used for\n        prediction.\n\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            predict=True,\n            table_name=table_name,\n        )\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; 'Pipeline'\n</code></pre> <p>Reloads the pipeline from the engine.</p> <p>This discards all local changes you have made since the last time you called <code>fit</code>.</p> RETURNS DESCRIPTION <code>'Pipeline'</code> <p>Current instance</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def refresh(self) -&gt; \"Pipeline\":\n    \"\"\"Reloads the pipeline from the engine.\n\n    This discards all local changes you have made since the\n    last time you called [`fit`][getml.Pipeline.fit].\n\n    Returns:\n            Current instance\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".refresh\"\n    cmd[\"name_\"] = self.id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.engine_exception_handler(msg)\n\n    json_obj = json.loads(msg)\n\n    self._parse_json_obj(json_obj)\n\n    return self\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.score","title":"score","text":"<pre><code>score(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores\n</code></pre> <p>Calculates the performance of the <code>predictor</code>.</p> <p>Returns different scores calculated on <code>population_table</code> and <code>peripheral_tables</code>.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>The scores of the pipeline.</p> Note <p>Only fitted pipelines (<code>fit</code>) can be scored.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def score(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores:\n    \"\"\"Calculates the performance of the ``predictor``.\n\n    Returns different scores calculated on `population_table` and\n    `peripheral_tables`.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    Returns:\n        The scores of the pipeline.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be\n        scored.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n\n        self._transform(\n            peripheral_tables, population_table, sock, predict=True, score=True\n        )\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n\n        scores = comm.recv_string(sock)\n\n        scores = json.loads(scores)\n\n    self.refresh()\n\n    self._save()\n\n    return self.scores\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.transform","title":"transform","text":"<pre><code>transform(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[float_], None]\n</code></pre> <p>Translates new data into the trained features.</p> <p>Transforms the data passed in <code>population_table</code> and <code>peripheral_tables</code> into features, which can be inserted into machine learning models.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>df_name</code> <p>If not an empty string, the resulting features will be written into a newly created DataFrame.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>table_name</code> <p>If not an empty string, the resulting features will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, NDArray[float_], None]</code> <p>The features generated by the pipeline.</p> Example <p>By default, <code>transform</code> returns a <code>ndarray</code>: <pre><code>my_features_array = pipe.transform()\n</code></pre> You can also export your features as a <code>DataFrame</code> by providing the <code>df_name</code> argument: <pre><code>my_features_df = pipe.transform(df_name=\"my_features\")\n</code></pre> Or you can write the results directly into a database: <pre><code>getml.database.connect_odbc(...)\npipe.transform(table_name=\"MY_FEATURES\")\n</code></pre></p> Note <p>Only fitted pipelines (<code>fit</code>) can transform data into features.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def transform(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n    \"\"\"Translates new data into the trained features.\n\n    Transforms the data passed in `population_table` and\n    `peripheral_tables` into features, which can be inserted into\n    machine learning models.\n\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        df_name:\n            If not an empty string, the resulting features will be\n            written into a newly created DataFrame.\n\n        table_name:\n            If not an empty string, the resulting features will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        The features generated by the pipeline.\n\n    Example:\n        By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n        ```python\n        my_features_array = pipe.transform()\n        ```\n        You can also export your features as a [`DataFrame`][getml.DataFrame]\n        by providing the `df_name` argument:\n        ```python\n        my_features_df = pipe.transform(df_name=\"my_features\")\n        ```\n        Or you can write the results directly into a database:\n        ```python\n        getml.database.connect_odbc(...)\n        pipe.transform(table_name=\"MY_FEATURES\")\n        ```\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can transform\n        data into features.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.engine_exception_handler(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            df_name=df_name,\n            table_name=table_name,\n        )\n\n    if df_name != \"\":\n        return data.DataFrame(name=df_name).refresh()\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/plots/","title":"Plots","text":""},{"location":"reference/pipeline/plots/#getml.pipeline.Plots","title":"getml.pipeline.Plots","text":"<pre><code>Plots(name: str)\n</code></pre> <p>Custom class for handling the plots generated by the pipeline.</p> PARAMETER DESCRIPTION <code>name</code> <p>The id of the pipeline the plots are associated with.</p> <p> TYPE: <code>str</code> </p> Example <pre><code>recall, precision = my_pipeline.plots.precision_recall_curve()\nfpr, tpr = my_pipeline.plots.roc_curve()\n</code></pre> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    if not isinstance(name, str):\n        raise ValueError(\"'name' must be a str.\")\n\n    self.name = name\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.lift_curve","title":"lift_curve","text":"<pre><code>lift_curve(target_num: int = 0) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the lift curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The first array is the proportion of samples, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the lift, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the lift curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the proportion of samples, usually displayed on the x-axis.\n        The second array is the lift, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.lift_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.precision_recall_curve","title":"precision_recall_curve","text":"<pre><code>precision_recall_curve(\n    target_num: int = 0,\n) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the precision-recall curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the precision, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def precision_recall_curve(\n    self, target_num: int = 0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the precision-recall curve, as displayed in the getML\n    monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n        The second array is the precision, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.roc_curve","title":"roc_curve","text":"<pre><code>roc_curve(target_num: int = 0) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the ROC curve, as displayed in the getML monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The first array is the false positive rate, usually displayed on the x-axis.</p> <code>ndarray</code> <p>The second array is the true positive rate, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the ROC curve, as displayed in the getML monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the false positive rate, usually displayed on the x-axis.\n        The second array is the true positive rate, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.roc_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.engine_exception_handler(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/score/","title":"Score","text":""},{"location":"reference/pipeline/score/#getml.pipeline.score","title":"getml.pipeline.score","text":""},{"location":"reference/pipeline/score/#getml.pipeline.score.Score","title":"Score  <code>dataclass</code>","text":"<pre><code>Score(date_time: datetime, set_used: str, target: str)\n</code></pre> <p>               Bases: <code>ABC</code></p>"},{"location":"reference/pipeline/score/#getml.pipeline.score.ClassificationScore","title":"ClassificationScore  <code>dataclass</code>","text":"<pre><code>ClassificationScore(\n    date_time: datetime,\n    set_used: str,\n    target: str,\n    accuracy: float,\n    auc: float,\n    cross_entropy: float,\n)\n</code></pre> <p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a classification pipeline.</p> PARAMETER DESCRIPTION <code>accuracy</code> <p>The <code>accuracy</code> of the classification.</p> <p> TYPE: <code>float</code> </p> <code>auc</code> <p>The area under the curve: <code>auc</code>.</p> <p> TYPE: <code>float</code> </p> <code>cross_entropy</code> <p>The <code>cross_entropy</code>.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/pipeline/score/#getml.pipeline.score.RegressionScore","title":"RegressionScore  <code>dataclass</code>","text":"<pre><code>RegressionScore(\n    date_time: datetime,\n    set_used: str,\n    target: str,\n    mae: float,\n    rmse: float,\n    rsquared: float,\n)\n</code></pre> <p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a regression pipeline.</p> PARAMETER DESCRIPTION <code>mae</code> <p>The mean absolute error: <code>mae</code></p> <p> TYPE: <code>float</code> </p> <code>rmse</code> <p>The root mean squared error: <code>rmse</code></p> <p> TYPE: <code>float</code> </p> <code>rsquared</code> <p>The squared correlation coefficient: <code>rsquared</code></p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/pipeline/scores_container/","title":"Scores","text":""},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores","title":"getml.pipeline.Scores","text":"<pre><code>Scores(\n    data: Sequence[Score], latest: Dict[str, List[float]]\n)\n</code></pre> <p>Container which holds the history of all scores associated with a given pipeline. The container supports slicing and is sort- and filterable.</p> PARAMETER DESCRIPTION <code>data</code> <p>A list of <code>Score</code> objects.</p> <p> TYPE: <code>Sequence[Score]</code> </p> <code>latest</code> <p>A dictionary containing the latest scores for each metric.</p> <p> TYPE: <code>Dict[str, List[float]]</code> </p> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def __init__(self, data: Sequence[Score], latest: Dict[str, List[float]]) -&gt; None:\n    self._latest = latest\n\n    self.is_classification = all(\n        isinstance(score, ClassificationScore) for score in data\n    )\n\n    self.is_regression = not self.is_classification\n\n    self.data = data\n\n    self.sets_used = [score.set_used for score in data]\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.accuracy","title":"accuracy  <code>property</code>","text":"<pre><code>accuracy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>accuracy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.auc","title":"auc  <code>property</code>","text":"<pre><code>auc: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>auc</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.cross_entropy","title":"cross_entropy  <code>property</code>","text":"<pre><code>cross_entropy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>cross entropy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.mae","title":"mae  <code>property</code>","text":"<pre><code>mae: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>mae</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The mean absolute error.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.rmse","title":"rmse  <code>property</code>","text":"<pre><code>rmse: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>rmse</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The root mean squared error.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.rsquared","title":"rsquared  <code>property</code>","text":"<pre><code>rsquared: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>rsquared</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The squared correlation coefficient.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Score], bool]) -&gt; Scores\n</code></pre> <p>Filters the scores container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Score], bool]</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>A container of filtered scores.</p> Example <pre><code>from datetime import datetime, timedelta\none_week_ago = datetime.today() - timedelta(days=7)\nscores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n    \"\"\"\n    Filters the scores container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered scores.\n\n    Example:\n        ```python\n        from datetime import datetime, timedelta\n        one_week_ago = datetime.today() - timedelta(days=7)\n        scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n        ```\n    \"\"\"\n    scores_filtered = [score for score in self.data if conditional(score)]\n\n    return Scores(scores_filtered, self._latest)\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.sort","title":"sort","text":"<pre><code>sort(\n    key: Callable[[Score], Union[float, int, str]],\n    descending: bool = False,\n) -&gt; Scores\n</code></pre> <p>Sorts the scores container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable[[Score], Union[float, int, str]]</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>A container of sorted scores.</p> Example <pre><code>by_auc = pipe.scores.sort(key=lambda score: score.auc)\nmost_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def sort(\n    self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n) -&gt; Scores:\n    \"\"\"\n    Sorts the scores container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted scores.\n\n    Example:\n        ```python\n        by_auc = pipe.scores.sort(key=lambda score: score.auc)\n        most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n        ```\n    \"\"\"\n\n    scores_sorted = sorted(self.data, key=key, reverse=descending)\n    return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/sql_code/","title":"SQLCode","text":""},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode","title":"getml.pipeline.SQLCode","text":"<pre><code>SQLCode(\n    code: Sequence[Union[str, SQLString]],\n    dialect: str = sqlite3,\n)\n</code></pre> <p>Custom class for handling the SQL code of the features generated by the pipeline.</p> PARAMETER DESCRIPTION <code>code</code> <p>The SQL code of the features.</p> <p> TYPE: <code>Sequence[Union[str, SQLString]]</code> </p> <code>dialect</code> <p>The SQL dialect used in the code. Default is 'sqlite3'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>sqlite3</code> </p> Example <pre><code>sql_code = my_pipeline.features.to_sql()\n\n# You can access individual features\n# by index.\nfeature_1_1 = sql_code[0]\n\n# You can also access them by name.\nfeature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n# You can also type the name of\n# a table or column to find all\n# features related to that table\n# or column.\nfeatures = sql_code.find(\"SOME_TABLE\")\n\n# HINT: The generated SQL code always\n# escapes table and column names using\n# quotation marks. So if you want exact\n# matching, you can do this:\nfeatures = sql_code.find('\"SOME_TABLE\"')\n</code></pre> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def __init__(\n    self,\n    code: Sequence[Union[str, SQLString]],\n    dialect: str = sqlite3,\n) -&gt; None:\n    if not _is_typed_list(code, str):\n        raise TypeError(\"'code' must be a list of str.\")\n\n    self.code = [SQLString(elem) for elem in code]\n\n    self.dialect = dialect\n\n    self.tables = [\n        _edit_table_name(table_name)\n        for table_name in re.findall(_table_pattern(self.dialect), \"\".join(code))\n    ]\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.find","title":"find","text":"<pre><code>find(keyword: str) -&gt; SQLCode\n</code></pre> <p>Returns the SQLCode for all features containing the keyword.</p> PARAMETER DESCRIPTION <code>keyword</code> <p>The keyword to be found.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SQLCode</code> <p>The SQL code for all features containing the keyword.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def find(self, keyword: str) -&gt; SQLCode:\n    \"\"\"\n    Returns the SQLCode for all features\n    containing the keyword.\n\n    Args:\n        keyword: The keyword to be found.\n\n    Returns:\n        The SQL code for all features containing the keyword.\n    \"\"\"\n    if not isinstance(keyword, str):\n        raise TypeError(\"'keyword' must be a str.\")\n\n    return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.save","title":"save","text":"<pre><code>save(\n    fname: str, split: bool = True, remove: bool = False\n) -&gt; None\n</code></pre> <p>Saves the SQL code to a file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the file or folder (if <code>split==True</code>) in which you want to save the features.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>If True, the code will be split into multiple files, one for each feature and saved into a folder <code>fname</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove</code> <p>If True, the existing SQL files in <code>fname</code> folder generated previously with the save method will be removed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n    \"\"\"\n    Saves the SQL code to a file.\n\n    Args:\n        fname:\n            The name of the file or folder (if `split==True`)\n            in which you want to save the features.\n\n        split:\n            If True, the code will be split into multiple files, one for\n            each feature and saved into a folder `fname`.\n\n        remove:\n            If True, the existing SQL files in `fname` folder generated\n            previously with the save method will be removed.\n    \"\"\"\n    if not split:\n        with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(self))\n        return\n\n    directory = Path(fname)\n\n    if directory.exists():\n        iter_dir = os.listdir(fname)\n\n        pattern = \"^\\d{4}.*\\_.*\\.sql$\"\n\n        exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n        if not remove and exist_files_path:\n            print(f\"The following files already exist in the directory ({fname}):\")\n            for fp in np.sort(exist_files_path):\n                print(fp)\n            print(\"Please set 'remove=True' to remove them.\")\n            return\n\n        if remove and exist_files_path:\n            for fp in exist_files_path:\n                os.remove(fname + \"/\" + fp)\n\n    directory.mkdir(exist_ok=True)\n\n    for index, code in enumerate(self.code, 1):\n        match = re.search(_table_pattern(self.dialect), str(code))\n        name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n        name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n        file_path = directory / f\"{index:04d}_{name}.sql\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(code))\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Returns a raw string representation of the SQL code.</p> RETURNS DESCRIPTION <code>str</code> <p>A raw string representation of the SQL code.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"\n    Returns a raw string representation of the SQL code.\n\n    Returns:\n        A raw string representation of the SQL code.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/pipeline/sql_string/","title":"Sql string","text":""},{"location":"reference/pipeline/sql_string/#getml.pipeline.sql_string","title":"getml.pipeline.sql_string","text":"<p>Custom str type that holds SQL Source code.</p>"},{"location":"reference/pipeline/sql_string/#getml.pipeline.sql_string.SQLString","title":"SQLString","text":"<p>               Bases: <code>str</code></p> <p>A custom string type that handles the representation of SQL code strings.</p>"},{"location":"reference/pipeline/table/","title":"Table","text":""},{"location":"reference/pipeline/table/#getml.pipeline.table.Table","title":"getml.pipeline.table.Table  <code>dataclass</code>","text":"<pre><code>Table(\n    name: str, importance: float, target: str, marker: str\n)\n</code></pre> <p>A dataclass that holds data about a single table.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the table.</p> <p> TYPE: <code>str</code> </p> <code>importance</code> <p>The importance of the table.</p> <p> TYPE: <code>float</code> </p> <code>target</code> <p>The target of the table.</p> <p> TYPE: <code>str</code> </p> <code>marker</code> <p>The marker of the table.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/pipeline/tables/","title":"Tables","text":""},{"location":"reference/pipeline/tables/#getml.pipeline.Tables","title":"getml.pipeline.Tables","text":"<pre><code>Tables(\n    targets: Sequence[str],\n    columns: Columns,\n    data: Optional[Sequence[Table]] = None,\n)\n</code></pre> <p>This container holds a pipeline's tables. These tables are build from the columns for which importances can be calculated. The motivation behind this container is to determine which tables are more important than others.</p> <p>Tables can be accessed by name, index or with a NumPy array. The container supports slicing and can be sorted and filtered. Further, the container holds global methods to request tables' importances.</p> PARAMETER DESCRIPTION <code>targets</code> <p>The targets associated with the pipeline.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>columns</code> <p>The columns with which the tables are built.</p> <p> TYPE: <code>Columns</code> </p> <code>data</code> <p>A list of <code>Table</code> objects.</p> <p> TYPE: <code>Optional[Sequence[Table]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_tables = my_pipeline.tables\nfirst_table = my_pipeline.tables[0]\nall_but_last_10_tables = my_pipeline.tables[:-10]\nimportant_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\nnames, importances = my_pipeline.tables.importances()\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def __init__(\n    self,\n    targets: Sequence[str],\n    columns: Columns,\n    data: Optional[Sequence[Table]] = None,\n) -&gt; None:\n    self._targets = targets\n    self._columns = columns\n\n    if data is not None:\n        self.data = data\n\n    else:\n        self._load_tables()\n\n    if not (targets and columns) and not data:\n        raise ValueError(\n            \"Missing required arguments. Either provide `targets` &amp; \"\n            \"`columns` or else provide `data`.\"\n        )\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s tables.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.targets","title":"targets  <code>property</code>","text":"<pre><code>targets: list[str]\n</code></pre> <p>Holds the targets of a <code>Pipeline</code>'s tables.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Table], bool]) -&gt; Tables\n</code></pre> <p>Filters the tables container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Table], bool]</code> </p> RETURNS DESCRIPTION <code>Tables</code> <p>A container of filtered tables.</p> Example <pre><code>important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\nperipheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n    \"\"\"\n    Filters the tables container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered tables.\n\n    Example:\n        ```python\n        important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n        peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    tables_filtered = [table for table in self.data if conditional(table)]\n    return self._make_tables(tables_filtered)\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the importances of tables.</p> <p>Table importances are calculated by summing up the importances of the columns belonging to the tables. Each column is assigned an importance value that measures its contribution to the predictive performance. For each target, the importances add up to 1.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the tables.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the importances of tables.\n\n    Table importances are calculated by summing up the importances of the\n    columns belonging to the tables. Each column is assigned an importance\n    value that measures its contribution to the predictive performance. For\n    each target, the importances add up to 1.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances. (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the tables.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    target_name = self._targets[target_num]\n\n    names = np.empty(0, dtype=str)\n    importances = np.empty(0, dtype=float)\n\n    for table in self.data:\n        if table.target == target_name:\n            names = np.append(names, table.name)\n            importances = np.append(importances, table.importance)\n\n    if not sort:\n        return names, importances\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables\n</code></pre> <p>Sorts the Tables container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Table], Any]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tables</code> <p>A container of sorted tables.</p> Example <pre><code>by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables:\n    \"\"\"\n    Sorts the Tables container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted tables.\n\n    Example:\n        ```python\n        by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        tables_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_tables(tables_sorted)\n\n    if by is None:\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        tables_sorted.sort(key=lambda table: table.target)\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.importance, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the tables in a pandas DataFrame.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame containing the tables' names, importances, targets and markers.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the tables in a pandas DataFrame.\n\n    Returns:\n        A pandas DataFrame containing the tables' names, importances, targets and markers.\n    \"\"\"\n\n    data_frame = pd.DataFrame()\n\n    for i, table in enumerate(self.data):\n        data_frame.loc[i, \"name\"] = table.name\n        data_frame.loc[i, \"importance\"] = table.importance\n        data_frame.loc[i, \"target\"] = table.target\n        data_frame.loc[i, \"marker\"] = table.marker\n\n    return data_frame\n</code></pre>"},{"location":"reference/predictors/","title":"Index","text":""},{"location":"reference/predictors/#getml.predictors","title":"getml.predictors","text":"<p>This module contains machine learning algorithms to learn and predict on the generated features.</p> <p>The predictor classes defined in this module serve two purposes. First, a predictor can be used as a <code>feature_selector</code> in <code>Pipeline</code> to only select the best features generated during the automated feature learning and to get rid of any redundancies. Second, by using it as a <code>predictor</code>, it will be trained on the features of the supplied data set and used to predict to unknown results. Every time a new data set is passed to the <code>predict</code> method of one of the models, the raw relational data is interpreted in the data model, which was provided during the construction of the model, transformed into features using the trained feature learning algorithm, and, finally, its target will be predicted using the trained predictor.</p> <p>The algorithms can be grouped according to their finesse and whether you want to use them for a classification or regression problem.</p> simple sophisticated regression <code>LinearRegression</code> <code>XGBoostRegressor</code> classification <code>LogisticRegression</code> <code>XGBoostClassifier</code> Note <p>All predictors need to be passed to <code>Pipeline</code>.</p>"},{"location":"reference/predictors/linear_regression/","title":"LinearRegression","text":""},{"location":"reference/predictors/linear_regression/#getml.predictors.LinearRegression","title":"getml.predictors.LinearRegression  <code>dataclass</code>","text":"<pre><code>LinearRegression(\n    learning_rate: float = 0.9, reg_lambda: float = 1e-10\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for regression problems.</p> <p>Learns a simple linear relationship using ordinary least squares (OLS) regression:</p> \\[ \\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ... \\] <p>The weights are optimized by minimizing the squared loss of the predictions \\(\\hat{y}\\) w.r.t. the target \\(y\\).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2 \\] <p>Linear regressions can be trained arithmetically or numerically. Training arithmetically is more accurate, but suffers worse scalability.</p> <p>If you decide to pass categorical features to the <code>LinearRegression</code>, it will be trained numerically. Otherwise, it will be trained arithmetically.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>The learning rate used for training numerically (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>reg_lambda</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p>"},{"location":"reference/predictors/linear_regression/#getml.predictors.LinearRegression.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If nothing is passed, the default parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>l = getml.predictors.LinearRegression()\nl.learning_rate = 8.1\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>def validate(self, params: Optional[dict]=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If nothing is passed,\n            the default parameters will be validated.\n\n    Example:\n        ```python\n        l = getml.predictors.LinearRegression()\n        l.learning_rate = 8.1\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/logistic_regression/","title":"LogisticRegression","text":""},{"location":"reference/predictors/logistic_regression/#getml.predictors.LogisticRegression","title":"getml.predictors.LogisticRegression  <code>dataclass</code>","text":"<pre><code>LogisticRegression(\n    learning_rate: float = 0.9, reg_lambda: float = 1e-10\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for classification problems.</p> <p>Learns a simple linear relationship using the sigmoid function:</p> \\[ \\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...) \\] <p>\\(\\sigma\\) denotes the sigmoid function:</p> \\[ \\sigma(z) = \\frac{1}{1 + exp(-z)} \\] <p>The weights are optimized by minimizing the cross entropy loss of the predictions \\(\\hat{y}\\) w.r.t. the targets \\(y\\).</p> \\[ L(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y}) \\] <p>Logistic regressions are always trained numerically.</p> <p>If you decide to pass categorical features: <code>annotating_roles_categorical</code> to the <code>LogisticRegression</code>, it will be trained using the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm. Otherwise, it will be trained using adaptive moments (Adam). BFGS is more accurate, but less scalable than Adam.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>The learning rate used for the Adaptive Moments algorithm (only relevant when categorical features are included). Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>reg_lambda</code> <p>L2 regularization parameter. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p>"},{"location":"reference/predictors/logistic_regression/#getml.predictors.LogisticRegression.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>l = getml.predictors.LogisticRegression()\nl.learning_rate = 20\nl.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>def validate(self, params: Optional[dict]=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Examples:\n        ```python\n        l = getml.predictors.LogisticRegression()\n        l.learning_rate = 20\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_classifier/","title":"ScaleGBMClassifier","text":""},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.ScaleGBMClassifier","title":"getml.predictors.ScaleGBMClassifier  <code>dataclass</code>","text":"<pre><code>ScaleGBMClassifier(\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    goss_a: float = 1.0,\n    goss_b: float = 0.0,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"binary:logistic\"\n    ] = \"binary:logistic\",\n    reg_lambda: float = 1.0,\n    seed: int = 5843,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting classifier that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>goss_a</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>goss_b</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>binary:logistic</code></li> </ul> <p> TYPE: <code>Literal['binary:logistic']</code> DEFAULT: <code>'binary:logistic'</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5843</code> </p>"},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.ScaleGBMClassifier.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>def validate(self, params: Optional[dict]=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_regressor/","title":"ScaleGBMRegressor","text":""},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.ScaleGBMRegressor","title":"getml.predictors.ScaleGBMRegressor  <code>dataclass</code>","text":"<pre><code>ScaleGBMRegressor(\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    goss_a: float = 1.0,\n    goss_b: float = 0.0,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:squarederror\"\n    ] = \"reg:squarederror\",\n    reg_lambda: float = 1.0,\n    seed: int = 5843,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting regressor that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>The regressor implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>goss_a</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>goss_b</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> </ul> <p> TYPE: <code>Literal['reg:squarederror']</code> DEFAULT: <code>'reg:squarederror'</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5843</code> </p>"},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.ScaleGBMRegressor.validate","title":"validate","text":"<pre><code>validate(params=None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/xgboost_classifier/","title":"XGBoostClassifier","text":""},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.XGBoostClassifier","title":"getml.predictors.XGBoostClassifier  <code>dataclass</code>","text":"<pre><code>XGBoostClassifier(\n    booster: str = \"gbtree\",\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    learning_rate: float = 0.1,\n    max_delta_step: float = 0.0,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    external_memory: bool = False,\n    normalize_type: str = \"tree\",\n    num_parallel_tree: int = 1,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:logistic\", \"binary:logistic\", \"binary:logitraw\"\n    ] = \"binary:logistic\",\n    one_drop: bool = False,\n    rate_drop: float = 0.0,\n    reg_alpha: float = 0.0,\n    reg_lambda: float = 1.0,\n    sample_type: str = \"uniform\",\n    silent: bool = True,\n    skip_drop: float = 0.0,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting classifier based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>booster</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on 'dart'.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'gbtree'</code> </p> <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_delta_step</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>external_memory</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>normalize_type</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p>'tree': a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p>'forest': a new tree has the same weight as a the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tree'</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:logistic</code></li> <li><code>binary:logistic</code></li> <li><code>binary:logitraw</code></li> </ul> <p> TYPE: <code>Literal['reg:logistic', 'binary:logistic', 'binary:logitraw']</code> DEFAULT: <code>'binary:logistic'</code> </p> <code>one_drop</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rate_drop</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_alpha</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>sample_type</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'uniform'</code> </p> <code>silent</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip_drop</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>subsample</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.XGBoostClassifier.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>x = getml.predictors.XGBoostClassifier()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>def validate(self, params: Optional[dict]=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostClassifier()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\n        \"reg:logistic\",\n        \"binary:logistic\",\n        \"binary:logitraw\",\n    ]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostClassifier\n                             are 'reg:logistic', 'binary:logistic',\n                             and 'binary:logitraw'\"\"\"\n        )\n</code></pre>"},{"location":"reference/predictors/xgboost_regressor/","title":"XGBoostRegressor","text":""},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.XGBoostRegressor","title":"getml.predictors.XGBoostRegressor  <code>dataclass</code>","text":"<pre><code>XGBoostRegressor(\n    booster: str = \"gbtree\",\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    external_memory: bool = False,\n    gamma: float = 0.0,\n    learning_rate: float = 0.1,\n    max_delta_step: float = 0.0,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    normalize_type: str = \"tree\",\n    num_parallel_tree: int = 1,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"\n    ] = \"reg:squarederror\",\n    one_drop: bool = False,\n    rate_drop: float = 0.0,\n    reg_alpha: float = 0.0,\n    reg_lambda: float = 1.0,\n    sample_type: str = \"uniform\",\n    silent: bool = True,\n    skip_drop: float = 0.0,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting regressor based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>booster</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on <code>dart</code>.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'gbtree'</code> </p> <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>external_memory</code> <p>When the in_memory flag of the engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the engine is set to True, (the default value), XGBoost will never use external memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_delta_step</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\))</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>normalize_type</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p><code>tree</code>: a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p><code>forest</code>: a new tree has the same weight as the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tree'</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> <li><code>reg:tweedie</code></li> <li>'reg:linear'</li> </ul> <p> TYPE: <code>Literal['reg:squarederror', 'reg:tweedie', 'reg:linear']</code> DEFAULT: <code>'reg:squarederror'</code> </p> <code>one_drop</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rate_drop</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_alpha</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>sample_type</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'uniform'</code> </p> <code>silent</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip_drop</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>subsample</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.XGBoostRegressor.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>x = getml.predictors.XGBoostRegressor()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the __init__ constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML engine.</p> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>def validate(self, params: Optional[dict]=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Example:\n        ```python\n        x = getml.predictors.XGBoostRegressor()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the \\_\\_init\\_\\_ constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostRegressor\n                             are 'reg:squarederror', 'reg:tweedie',\n                             and 'reg:linear'\"\"\"\n        )\n</code></pre>"},{"location":"reference/preprocessors/","title":"Index","text":""},{"location":"reference/preprocessors/#getml.preprocessors","title":"getml.preprocessors","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/#getml.preprocessors.CategoryTrimmer","title":"CategoryTrimmer  <code>dataclass</code>","text":"<pre><code>CategoryTrimmer(\n    max_num_categories: int = 999, min_freq: int = 30\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>Reduces the cardinality of high-cardinality categorical columns.</p> PARAMETER DESCRIPTION <code>max_num_categories</code> <p>The maximum cardinality allowed. If the cardinality is higher than that only the most frequent categories will be kept, all others will be trimmed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>999</code> </p> <code>min_freq</code> <p>The minimum frequency required for a category to be included.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> Example <pre><code>category_trimmer = getml.preprocessors.CategoryTrimmer()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[category_trimmer],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.EmailDomain","title":"EmailDomain  <code>dataclass</code>","text":"<pre><code>EmailDomain()\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The EmailDomain preprocessor extracts the domain from e-mail addresses.</p> <p>For instance, if the e-mail address is 'some.guy@domain.com', the preprocessor will automatically extract '@domain.com'.</p> <p>The preprocessor will be applied to all <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.email</code> or <code>only.email</code>.</p> <p>It is recommended that you assign <code>only.email</code>, because it is unlikely that the e-mail address itself is interesting.</p> Example <pre><code>my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\ndomain = getml.preprocessors.EmailDomain()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[domain],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Imputation","title":"Imputation  <code>dataclass</code>","text":"<pre><code>Imputation(add_dummies: bool = False)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Imputation preprocessor replaces all NULL values in numerical columns with the mean of the remaining columns.</p> <p>Optionally, it can additionally add a dummy column that signifies whether the original value was imputed.</p> PARAMETER DESCRIPTION <code>add_dummies</code> <p>Whether you want to add dummy variables that signify whether the original value was imputed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>imputation = getml.preprocessors.Imputation()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[imputation],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Mapping","title":"Mapping  <code>dataclass</code>","text":"<pre><code>Mapping(\n    aggregation: Iterable[Aggregations] = MAPPING.default,\n    min_freq: int = 30,\n    multithreading: bool = True,\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>A mapping preprocessor maps categorical values, discrete values and individual words in a text field to numerical values. These numerical values are retrieved by aggregating targets in the relational neighbourhood.</p> <p>You are particularly encouraged to use the mapping preprocessor in combination with <code>FastProp</code>.</p> <p>Refer to the User guide for more information.</p> PARAMETER DESCRIPTION <code>agg_sets</code> <p>Available aggregation sets for the mapping preprocessor.</p> <p> </p> <code>aggregation</code> <p>The aggregation function to use over the targets.</p> <p>Must be from <code>aggregations</code>.</p> <p> TYPE: <code>Iterable[Aggregations]</code> DEFAULT: <code>default</code> </p> <code>min_freq</code> <p>The minimum number of targets required for a value to be included in the mapping. Range: [0, \\(\\infty\\)]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>multithreading</code> <p>Whether you want to apply multithreading.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <pre><code>mapping = getml.preprocessors.Mapping()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[mapping],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre> Note <p>Not supported in the getML community edition.</p>"},{"location":"reference/preprocessors/#getml.preprocessors.Seasonal","title":"Seasonal  <code>dataclass</code>","text":"<pre><code>Seasonal(\n    disable_year: bool = False,\n    disable_month: bool = False,\n    disable_weekday: bool = False,\n    disable_hour: bool = False,\n    disable_minute: bool = False,\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Seasonal preprocessor extracts seasonal data from time stamps.</p> <p>The preprocessor automatically iterates through all time stamps in any data frame and extracts seasonal parameters.</p> <p>These include:</p> <ul> <li>year</li> <li>month</li> <li>weekday</li> <li>hour</li> <li>minute</li> </ul> <p>The algorithm also evaluates the potential usefulness of any extracted seasonal parameter. Parameters that are unlikely to be useful are not included.</p> PARAMETER DESCRIPTION <code>disable_year</code> <p>Prevents the Seasonal preprocessor from extracting the year from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_month</code> <p>Prevents the Seasonal preprocessor from extracting the month from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_weekday</code> <p>Prevents the Seasonal preprocessor from extracting the weekday from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_hour</code> <p>Prevents the Seasonal preprocessor from extracting the hour from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_minute</code> <p>Prevents the Seasonal preprocessor from extracting the minute from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>seasonal = getml.preprocessors.Seasonal()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[seasonal],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Substring","title":"Substring  <code>dataclass</code>","text":"<pre><code>Substring(begin: int, length: int, unit: str = '')\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Substring preprocessor extracts substrings from categorical columns and unused string columns.</p> <p>The preprocessor will be applied to all <code>categorical</code> and <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.substring</code> or <code>only.substring</code>.</p> <p>To further limit the scope of a substring preprocessor, you can also assign a unit.</p> PARAMETER DESCRIPTION <code>begin</code> <p>Index of the beginning of the substring (starting from 0).</p> <p> TYPE: <code>int</code> </p> <code>length</code> <p>The length of the substring.</p> <p> TYPE: <code>int</code> </p> <code>unit</code> <p>The unit of all columns to which the preprocessor should be applied. These columns must also have the subrole substring.</p> <p>If it is left empty, then the preprocessor will be applied to all columns with the subrole <code>include.substring</code> or <code>only.substring</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Example <pre><code>my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\nmy_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\nmy_df.set_unit(\"col2\", \"substr14\")\n\n# Will be applied to col1 and col2\nsubstr13 = getml.preprocessors.Substring(0, 3)\n\n# Will only be applied to col2\nsubstr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[substr13],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.TextFieldSplitter","title":"TextFieldSplitter  <code>dataclass</code>","text":"<pre><code>TextFieldSplitter()\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>A TextFieldSplitter splits columns with role <code>text</code> into relational bag-of-words representations to allow the feature learners to learn patterns based on the prescence of certain words within the text fields.</p> <p>Text fields will be split on a whitespace or any of the following characters:</p> <p><pre><code>; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n</code></pre> Refer to the User Guide for more information.</p> Example <pre><code>text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[text_field_splitter],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/project/","title":"Index","text":""},{"location":"reference/project/#getml.project","title":"getml.project","text":"<p>This module helps you handle your current project.</p>"},{"location":"reference/project/#getml.project.attrs.load","title":"load","text":"<pre><code>load(bundle: str, name: Optional[str] = None) -&gt; None\n</code></pre> <p>Loads a project from a bundle and connects to it.</p> PARAMETER DESCRIPTION <code>bundle</code> <p>The <code>.getml</code> bundle file to load.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>A name for the project contained in the bundle. If None, the name will be extracted from the bundle.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef load(bundle: str, name: Optional[str]=None) -&gt; None:\n    \"\"\"\n    Loads a project from a bundle and connects to it.\n\n    Args:\n        bundle: The `.getml` bundle file to load.\n\n        name: A name for the project contained in the bundle.\n          If None, the name will be extracted from the bundle.\n    \"\"\"\n    return comm._load_project(bundle, name)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the currently connected project. All related pipelines, data frames and hyperopts will be irretrievably deleted.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef delete() -&gt; None:\n    \"\"\"\n    Deletes the currently connected project. All related pipelines,\n    data frames and hyperopts will be irretrievably deleted.\n    \"\"\"\n    comm._delete_project(_name())\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.restart","title":"restart","text":"<pre><code>restart() -&gt; None\n</code></pre> <p>Suspends and then relaunches the currently connected project. This will kill all jobs currently running on that process.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef restart() -&gt; None:\n    \"\"\"\n    Suspends and then relaunches the currently connected project.\n    This will kill all jobs currently running on that process.\n    \"\"\"\n    comm._set_project(_name(), restart=True)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.save","title":"save","text":"<pre><code>save(\n    filename: Optional[str] = None,\n    target_dir: Optional[str] = None,\n    replace: bool = True,\n) -&gt; None\n</code></pre> <p>Saves the currently connected project to disk.</p> PARAMETER DESCRIPTION <code>filename</code> <p>The name of the <code>.getml</code> bundle file</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>target_dir</code> <p>the directory to save the bundle to. If None, the current working directory is used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>replace</code> <p>Whether to replace an existing bundle.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef save(filename: Optional[str]=None, target_dir: Optional[str]=None, replace: bool=True) -&gt; None:\n    \"\"\"\n    Saves the currently connected project to disk.\n\n    Args:\n        filename: The name of the `.getml` bundle file\n\n        target_dir: the directory to save the bundle to.\n          If None, the current working directory is used.\n\n        replace: Whether to replace an existing bundle.\n    \"\"\"\n    return comm._save_project(_name(), filename, target_dir, replace)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.suspend","title":"suspend","text":"<pre><code>suspend() -&gt; None\n</code></pre> <p>Suspends the currently connected project.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef suspend() -&gt; None:\n    \"\"\"\n    Suspends the currently connected project.\n    \"\"\"\n    return comm._suspend_project(_name())\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.switch","title":"switch","text":"<pre><code>switch(name: str) -&gt; None\n</code></pre> <p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the engine, a new one will be created. See the User guide for more information.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef switch(name: str) -&gt; None:\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the engine, a new one will\n    be created. See the [User guide][project-management] for more\n    information.\n\n    Args:\n        name: Name of the new project.\n    \"\"\"\n    comm._set_project(name)\n</code></pre>"},{"location":"reference/project/#data_frames","title":"data_frames","text":"<p><pre><code>getml.project.data_frames\n</code></pre> An instance of getml.project.DataFrames.</p>"},{"location":"reference/project/#hyperopts","title":"hyperopts","text":"<p><pre><code>getml.project.hyperopts\n</code></pre> An instance of getml.project.Hyperopts.</p>"},{"location":"reference/project/#pipelines","title":"pipelines","text":"<p><pre><code>getml.project.pipelines\n</code></pre> An instance of getml.project.Pipelines.</p>"},{"location":"reference/project/#name","title":"name","text":"<p><pre><code>getml.project.name\n</code></pre> Holds the name of the current project.</p>"},{"location":"reference/project/data_frames/","title":"DataFrames","text":""},{"location":"reference/project/data_frames/#getml.project.DataFrames","title":"getml.project.DataFrames","text":"<pre><code>DataFrames(data=None)\n</code></pre> <p>Container which holds all data frames associated with the running project that are currently stored in memory. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def __init__(self, data=None):\n    self._in_memory = list_data_frames()[\"in_memory\"]\n    self._on_disk = list_data_frames()[\"on_disk\"]\n\n    if data is None:\n        self.data = [load_data_frame(name) for name in self._in_memory]\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.in_memory","title":"in_memory  <code>property</code>","text":"<pre><code>in_memory: list[str]\n</code></pre> <p>Returns the names of all data frames currently in memory.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>The names of all data frames currently in memory.</p>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.on_disk","title":"on_disk  <code>property</code>","text":"<pre><code>on_disk: list[str]\n</code></pre> <p>Returns the names of all data frames stored in the project folder.</p>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes all data frames in the current project.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes all data frames in the current project.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; DataFrames\n</code></pre> <p>Filters the data frames container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>DataFrames</code> <p>A container of filtered data frames.</p> Example <pre><code>big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; \"DataFrames\":\n    \"\"\"\n    Filters the data frames container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered data frames.\n\n    Example:\n        ```python\n        big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n        ```\n    \"\"\"\n\n    dfs_filtered = [df for df in self.data if conditional(df)]\n    return DataFrames(data=dfs_filtered)\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Loads all data frames stored in the project folder to memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Loads all data frames stored in the project folder to memory.\n    \"\"\"\n\n    for df in self.on_disk:\n        if df not in self.in_memory:\n            self.data.append(load_data_frame(df))\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.retrieve","title":"retrieve","text":"<pre><code>retrieve()\n</code></pre> <p>Retrieve a dict of all data frames in memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def retrieve(self):\n    \"\"\"\n    Retrieve a dict of all data frames in memory.\n    \"\"\"\n\n    return {df.name: df for df in self.data}\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Saves all data frames currently in memory to disk.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"\n    Saves all data frames currently in memory to disk.\n    \"\"\"\n\n    for df in self.data:\n        df.save()\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False) -&gt; DataFrames\n</code></pre> <p>Sorts the data frames container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Return <p>A container of sorted data frames.</p> Example <pre><code>by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def sort(self, key: Callable, descending: bool=False) -&gt; \"DataFrames\":\n    \"\"\"\n    Sorts the data frames container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Return:\n            A container of sorted data frames.\n\n    Example:\n        ```python\n        by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n        ```\n    \"\"\"\n\n    dfs_sorted = sorted(self.data, key=key, reverse=descending)\n    return DataFrames(data=dfs_sorted)\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.unload","title":"unload","text":"<pre><code>unload() -&gt; None\n</code></pre> <p>Unloads all data frames in the current project from memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def unload(self) -&gt; None:\n    \"\"\"\n    Unloads all data frames in the current project from memory.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/hyperopts/","title":"Hyperopts","text":""},{"location":"reference/project/hyperopts/#getml.project.Hyperopts","title":"getml.project.Hyperopts","text":"<pre><code>Hyperopts(data=None)\n</code></pre> <p>Container which holds all hyperopts associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def __init__(self, data=None):\n    self.ids = list_hyperopts()\n\n    if data is None:\n        self.data = [load_hyperopt(id) for id in self.ids]\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/hyperopts/#getml.project.Hyperopts.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; Hyperopts\n</code></pre> <p>Filters the hyperopts container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Hyperopts</code> <p>A container of filtered hyperopts.</p> Example <pre><code>gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; \"Hyperopts\":\n    \"\"\"\n    Filters the hyperopts container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered hyperopts.\n\n    Example:\n        ```python\n        gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n        ```\n    \"\"\"\n    hyperopts_filtered = [\n        hyperopt for hyperopt in self.data if conditional(hyperopt)\n    ]\n    return Hyperopts(data=hyperopts_filtered)\n</code></pre>"},{"location":"reference/project/hyperopts/#getml.project.Hyperopts.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False) -&gt; Hyperopts\n</code></pre> <p>Sorts the hyperopts container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Hyperopts</code> <p>A container of sorted hyperopts.</p> Example <pre><code>by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def sort(self, key: Callable, descending: bool=False) -&gt; \"Hyperopts\":\n    \"\"\"\n    Sorts the hyperopts container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted hyperopts.\n\n    Example:\n        ```python\n        by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n        ```\n    \"\"\"\n    hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n    return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/pipelines/","title":"Pipelines","text":""},{"location":"reference/project/pipelines/#getml.project.Pipelines","title":"getml.project.Pipelines","text":"<pre><code>Pipelines(data=None)\n</code></pre> <p>Container which holds all pipelines associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Example <p>Show the first 10 pipelines belonging to the current project: <pre><code>getml.project.pipelines[:10]\n</code></pre> You can use nested list comprehensions to retrieve a scoring history for your project: <pre><code>import matplotlib.pyplot as plt\n\nhyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                      for score in pipe.scores[\"data_test\"]\n                      if \"hyperopt\" in pipe.tags]\n\nfig, ax = plt.subplots()\nax.bar(*zip(*hyperopt_scores))\n</code></pre></p> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def __init__(self, data=None):\n    self.ids = list_pipelines()\n\n    if data is None:\n        self.data = _refresh_all()\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/pipelines/#getml.project.Pipelines.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False)\n</code></pre> <p>Sorts the pipelines container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>A container of sorted pipelines.</p> Example <pre><code>by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\nby_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def sort(self, key: Callable, descending: bool=False):\n    \"\"\"\n    Sorts the pipelines container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted pipelines.\n\n    Example:\n        ```python\n        by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n        by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n        ```\n    \"\"\"\n    pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n    return Pipelines(data=pipelines_sorted)\n</code></pre>"},{"location":"reference/project/pipelines/#getml.project.Pipelines.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; Pipelines\n</code></pre> <p>Filters the pipelines container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Pipelines</code> <p>A container of filtered pipelines.</p> Example <pre><code>pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\naccurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; \"Pipelines\":\n    \"\"\"\n    Filters the pipelines container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered pipelines.\n\n    Example:\n        ```python\n        pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n        accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n        ```\n    \"\"\"\n    pipelines_filtered = [\n        pipeline for pipeline in self.data if conditional(pipeline)\n    ]\n\n    return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/sqlite3/","title":"Index","text":""},{"location":"reference/sqlite3/#getml.sqlite3","title":"getml.sqlite3","text":"<p>This module contains wrappers around sqlite3 and related utility functions, which enable you to productionize pipelines using only sqlite3 and Python, fully based on open-source code.</p> <p>This requires SQLite version 3.33.0 or above. To check the sqlite3 version of your Python distribution, do the following:</p> <pre><code>import sqlite3\nsqlite3.sqlite_version\n</code></pre> Example <p>For our example we will assume that you want to productionize the CORA project.</p> <p>First, we want to transpile the features into SQL code, like this:</p> <p><pre><code># Set targets to False, if you want an inference pipeline.\npipe1.features.to_sql(targets=True).save(\"cora\")\n</code></pre> This transpiles the features learned by pipe1 into a set of SQLite3 scripts ready to be executed. These scripts are contained in a folder called \"cora\".</p> <p>We also assume that you have the three tables needed for the CORA project in the form of pandas.DataFrames (other data sources are possible).</p> <p>We want to create a new sqlite3 connection and then read in the data: <pre><code>conn = getml.sqlite3.connect(\"cora.db\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"cites\", data_frame=cites, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"content\", data_frame=content, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"paper\", data_frame=paper, if_exists=\"replace\")\n</code></pre> Now we can execute the scripts we have just created: <pre><code>conn = getml.sqlite3.execute(conn, \"cora\")\n</code></pre> The transpiled pipeline will always create a table called \"FEATURES\", which contain the features. Here is how we retrieve them: <pre><code>features = getml.sqlite3.to_pandas(conn, \"FEATURES\")\n</code></pre> Now you have created your features in a pandas DataFrame ready to be inserted into your favorite machine learning library.</p> <p>To build stable data science pipelines, it is often a good idea to ensure type safety by hard-coding your table schema. You can use the sniff... methods to do that: <pre><code>getml.sqlite3.sniff_pandas(\"cites\", cites)\n</code></pre> This will generate SQLite3 code that creates the \"cites\" table. You can hard-code that into your pipeline. This will ensure that the data always have the correct types, avoiding awkward problems in the future.</p>"},{"location":"reference/sqlite3/#getml.sqlite3.connect.connect","title":"connect","text":"<pre><code>connect(database: str) -&gt; Connection\n</code></pre> <p>Generates a new sqlite3 connection.</p> <p>This connection contains all customized aggregations and transformation functions needed to execute the SQL pipeline generated by getML. Other than that it behaves just like a normal sqlite3 connection from the Python standard library.</p> PARAMETER DESCRIPTION <code>database</code> <p>Filename of the database. Use ':memory:' to create an in-memory database.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>A new sqlite3 connection with all custom functions and aggregations registered.</p> Source code in <code>getml/sqlite3/connect.py</code> <pre><code>def connect(database: str) -&gt; sqlite3.Connection:\n    \"\"\"\n    Generates a new sqlite3 connection.\n\n    This connection contains all customized aggregations\n    and transformation functions needed to execute the\n    SQL pipeline generated by getML. Other than that\n    it behaves just like a normal sqlite3 connection from\n    the Python standard library.\n\n    Args:\n        database:\n            Filename of the database. Use ':memory:' to\n            create an in-memory database.\n\n    Returns:\n            A new sqlite3 connection with all custom\n                functions and aggregations registered.\n    \"\"\"\n\n    if not isinstance(database, str):\n        raise TypeError(\"'database' must be of type str\")\n\n    if sqlite3.sqlite_version &lt; \"3.33.0\":\n        raise ValueError(\n            \"getML requires SQLite version 3.33.0 or above. Found version \"\n            + sqlite3.sqlite_version\n            + \". Please upgrade Python and/or the Python sqlite3 package.\"\n        )\n\n    conn = sqlite3.connect(database)\n\n    conn.create_function(\"contains\", 2, _contains)\n    conn.create_function(\"email_domain\", 1, _email_domain)\n    conn.create_function(\"get_word\", 2, _get_word)\n    conn.create_function(\"num_words\", 1, _num_words)\n\n    conn.create_aggregate(\"COUNT_ABOVE_MEAN\", 1, _CountAboveMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_BELOW_MEAN\", 1, _CountBelowMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_DISTINCT_OVER_COUNT\", 1, _CountDistinctOverCount)  # type: ignore\n    conn.create_aggregate(\"EWMA_1S\", 2, _EWMA1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_1M\", 2, _EWMA1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_1H\", 2, _EWMA1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_1D\", 2, _EWMA1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_7D\", 2, _EWMA7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_30D\", 2, _EWMA30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_90D\", 2, _EWMA90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_365D\", 2, _EWMA365D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1S\", 2, _EWMATrend1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1M\", 2, _EWMATrend1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1H\", 2, _EWMATrend1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1D\", 2, _EWMATrend1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_7D\", 2, _EWMATrend7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_30D\", 2, _EWMATrend30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_90D\", 2, _EWMATrend90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_365D\", 2, _EWMATrend365D)  # type: ignore\n    conn.create_aggregate(\"FIRST\", 2, _First)  # type: ignore\n    conn.create_aggregate(\"KURTOSIS\", 1, _Kurtosis)\n    conn.create_aggregate(\"LAST\", 2, _Last)  # type: ignore\n    conn.create_aggregate(\"MEDIAN\", 1, _Median)\n    conn.create_aggregate(\"MODE\", 1, _Mode)\n    conn.create_aggregate(\"NUM_MAX\", 1, _NumMax)  # type: ignore\n    conn.create_aggregate(\"NUM_MIN\", 1, _NumMin)  # type: ignore\n    conn.create_aggregate(\"Q1\", 1, _Q1)  # type: ignore\n    conn.create_aggregate(\"Q5\", 1, _Q5)  # type: ignore\n    conn.create_aggregate(\"Q10\", 1, _Q10)  # type: ignore\n    conn.create_aggregate(\"Q25\", 1, _Q25)  # type: ignore\n    conn.create_aggregate(\"Q75\", 1, _Q75)  # type: ignore\n    conn.create_aggregate(\"Q90\", 1, _Q90)  # type: ignore\n    conn.create_aggregate(\"Q95\", 1, _Q95)  # type: ignore\n    conn.create_aggregate(\"Q99\", 1, _Q99)  # type: ignore\n    conn.create_aggregate(\"SKEW\", 1, _Skew)\n    conn.create_aggregate(\"STDDEV\", 1, _Stddev)\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MAXIMUM\", 2, _TimeSinceFirstMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MINIMUM\", 2, _TimeSinceFirstMinimum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MAXIMUM\", 2, _TimeSinceLastMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MINIMUM\", 2, _TimeSinceLastMinimum)  # type: ignore\n    conn.create_aggregate(\"TREND\", 2, _Trend)  # type: ignore\n    conn.create_aggregate(\"VAR\", 1, _Var)\n    conn.create_aggregate(\"VARIATION_COEFFICIENT\", 1, _VariationCoefficient)\n\n    return conn\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.execute.execute","title":"execute","text":"<pre><code>execute(conn: Connection, fname: str) -&gt; None\n</code></pre> <p>Executes an SQL script or several SQL scripts on SQLite3.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>fname</code> <p>The names of the SQL script or a folder containing SQL scripts. If you decide to pass a folder, the SQL scripts must have the ending '.sql'.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/sqlite3/execute.py</code> <pre><code>def execute(conn: sqlite3.Connection, fname: str) -&gt; None:\n    \"\"\"\n    Executes an SQL script or several SQL scripts on SQLite3.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        fname:\n            The names of the SQL script or a folder containing SQL scripts.\n            If you decide to pass a folder, the SQL scripts must have the ending '.sql'.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    # ------------------------------------------------------------\n\n    # Store temporary object in-memory.\n    conn.execute(\"PRAGMA temp_store=2;\")\n\n    if os.path.isdir(fname):\n        scripts = _retrieve_scripts(fname, \".sql\")\n        for script in scripts:\n            execute(conn, script)\n        return\n\n    _log(\"Executing \" + fname + \"...\")\n\n    with open(fname, \"rt\", encoding=\"utf-8\") as sqlfile:\n        queries = sqlfile.read().split(\";\")\n\n    for query in queries:\n        conn.execute(query + \";\")\n\n    conn.commit()\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_csv.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    conn: Connection,\n    fnames: Union[str, list[str]],\n    table_name: str,\n    header: bool = True,\n    if_exists: str = \"append\",\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; None\n</code></pre> <p>Reads a list of CSV files and writes them into an sqlite3 table.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>fnames</code> <p>The names of the CSV files.</p> <p> TYPE: <code>Union[str, list[str]]</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>header</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>quotechar</code> <p>The string escape character.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>if_exists</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values to the existing table.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'append'</code> </p> <code>sep</code> <p>The field separator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>The number of lines to skip (before a possible header)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/sqlite3/read_csv.py</code> <pre><code>def read_csv(\n    conn: sqlite3.Connection,\n    fnames: Union[str, list[str]],\n    table_name: str,\n    header: bool=True,\n    if_exists: str=\"append\",\n    quotechar: str='\"',\n    sep: str=\",\",\n    skip: int=0,\n    colnames: Optional[List[str]]=None,\n) -&gt; None:\n    \"\"\"\n    Reads a list of CSV files and writes them into an sqlite3 table.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        fnames:\n            The names of the CSV files.\n\n        table_name:\n            The name of the table to write to.\n\n        header:\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        quotechar:\n            The string escape character.\n\n        if_exists:\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values to the existing table.\n\n        sep:\n            The field separator.\n\n        skip:\n            The number of lines to skip (before a possible header)\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    schema = sniff_csv(\n        fnames=fnames,\n        table_name=table_name,\n        header=header,\n        quotechar=quotechar,\n        sep=sep,\n        skip=skip,\n        colnames=colnames,\n    )\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    for fname in fnames:\n        _log(\"Loading '\" + fname + \"' into '\" + table_name + \"'...\")\n        data = _read_csv_file(fname, sep, quotechar, header and not colnames, skip)\n        read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_list.read_list","title":"read_list","text":"<pre><code>read_list(\n    conn: Connection, table_name: str, data: List[List[Any]]\n) -&gt; None\n</code></pre> <p>Reads data into an sqlite3 table.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>The data to insert into the table. Every list represents one row to be read into the table.</p> <p> TYPE: <code>List[List[Any]]</code> </p> Source code in <code>getml/sqlite3/read_list.py</code> <pre><code>def read_list(conn: sqlite3.Connection, table_name: str, data: List[List[Any]]) -&gt; None:\n    \"\"\"\n    Reads data into an sqlite3 table.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        table_name:\n            The name of the table to write to.\n\n        data:\n            The data to insert into the table.\n            Every list represents one row to be read into the table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(data, list):\n        raise TypeError(\"'data' must be a list of lists\")\n\n    # ------------------------------------------------------------\n\n    ncols = _get_num_columns(conn, table_name)\n    old_length = len(data)\n    data = [line for line in data if len(line) == ncols]\n    placeholders = \"(\" + \",\".join([\"?\"] * ncols) + \")\"\n    query = 'INSERT INTO \"' + table_name + '\" VALUES ' + placeholders\n    conn.executemany(query, data)\n    conn.commit()\n    _log(\n        \"Read \"\n        + str(len(data))\n        + \" lines. \"\n        + str(old_length - len(data))\n        + \" invalid lines.\"\n    )\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_pandas.read_pandas","title":"read_pandas","text":"<pre><code>read_pandas(\n    conn: Connection,\n    table_name: str,\n    data_frame: DataFrame,\n    if_exists: Literal[\n        \"fail\", \"replace\", \"append\"\n    ] = \"append\",\n) -&gt; None\n</code></pre> <p>Loads a pandas.DataFrame into SQLite3.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>data_frame</code> <p>The pandas.DataFrame to read into the table. The column names must match the column names of the target table in the SQLite3 database, but their order is not important.</p> <p> TYPE: <code>DataFrame</code> </p> <code>if_exists</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values into the existing table.</li> </ul> <p> TYPE: <code>Literal['fail', 'replace', 'append']</code> DEFAULT: <code>'append'</code> </p> Source code in <code>getml/sqlite3/read_pandas.py</code> <pre><code>def read_pandas(conn: sqlite3.Connection, table_name: str, data_frame: pd.DataFrame, if_exists: Literal['fail', 'replace', 'append']=\"append\") -&gt; None:\n    \"\"\"\n    Loads a pandas.DataFrame into SQLite3.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        table_name:\n            The name of the table to write to.\n\n        data_frame:\n            The pandas.DataFrame to read\n            into the table. The column names must match the column\n            names of the target table in the SQLite3 database, but\n            their order is not important.\n\n        if_exists:\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values into the existing table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    # ------------------------------------------------------------\n\n    _log(\"Loading pandas.DataFrame into '\" + table_name + \"'...\")\n\n    schema = sniff_pandas(table_name, data_frame)\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    colnames = _get_colnames(conn, table_name)\n    data = data_frame[colnames].values.tolist()\n    data = [\n        [\n            field\n            if isinstance(field, (numbers.Number, str)) or field is None\n            else str(field)\n            for field in row\n        ]\n        for row in data\n    ]\n    read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.sniff_csv.sniff_csv","title":"sniff_csv","text":"<pre><code>sniff_csv(\n    fnames: Union[str, list[str]],\n    table_name: str,\n    header: bool = True,\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of csv files.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, list[str]]</code> </p> <code>table_name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>header</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_csv.py</code> <pre><code>def sniff_csv(\n    fnames: Union[str, list[str]],\n    table_name: str,\n    header: bool=True,\n    num_lines_sniffed: int=1000,\n    quotechar: str='\"',\n    sep: str=\",\",\n    skip: int=0,\n    colnames: Optional[List[str]]=None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of csv files.\n\n    Args:\n        fnames:\n            The list of CSV file names to be read.\n\n        table_name:\n            Name of the table in which the data is to be inserted.\n\n        header:\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n\n    Returns:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(num_lines_sniffed, int):\n        raise TypeError(\"'num_lines_sniffed' must be a int\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    header_lines = 0 if header and not colnames else None\n\n    def read(fname):\n        return pd.read_csv(\n            fname,\n            nrows=num_lines_sniffed,\n            header=header_lines,\n            sep=sep,\n            quotechar=quotechar,\n            skiprows=skip,\n            names=colnames,\n        )\n\n    data_frames = [read(fname) for fname in fnames]\n\n    merged = pd.concat(data_frames, join=\"inner\")\n\n    return sniff_pandas(table_name, merged)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.sniff_pandas.sniff_pandas","title":"sniff_pandas","text":"<pre><code>sniff_pandas(table_name: str, data_frame: DataFrame) -&gt; str\n</code></pre> <p>Sniffs a pandas data frame.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>data_frame</code> <p>The pandas.DataFrame to read into the table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_pandas.py</code> <pre><code>def sniff_pandas(table_name: str, data_frame: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Sniffs a pandas data frame.\n\n    Args:\n        table_name:\n            Name of the table in which the data is to be inserted.\n\n        data_frame:\n            The pandas.DataFrame to read into the table.\n\n    Returns:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    # ------------------------------------------------------------\n\n    colnames = data_frame.columns\n    coltypes = data_frame.dtypes\n\n    sql_types: Dict[str, List[str]] = {\"INTEGER\": [], \"REAL\": [], \"TEXT\": []}\n\n    for cname, ctype in zip(colnames, coltypes):\n        if _is_int_type(ctype):\n            sql_types[\"INTEGER\"].append(cname)\n            continue\n        if _is_numerical_type(ctype):\n            sql_types[\"REAL\"].append(cname)\n        else:\n            sql_types[\"TEXT\"].append(cname)\n\n    return _generate_schema(table_name, sql_types)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.to_list.to_list","title":"to_list","text":"<pre><code>to_list(\n    conn: Connection, query: str\n) -&gt; tuple[list[str], list[list]]\n</code></pre> <p>Transforms a query or table into a list of lists. Returns a tuple which contains the column names and the actual data.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>query</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>tuple[list[str], list[list]]</code> <p>The column names and the data as a list of lists.</p> Source code in <code>getml/sqlite3/to_list.py</code> <pre><code>def to_list(conn: sqlite3.Connection, query: str) -&gt; tuple[list[str], list[list]]:\n    \"\"\"\n    Transforms a query or table into a list of lists. Returns\n    a tuple which contains the column names and the actual data.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        query:\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n\n    Returns:\n            The column names and the data as a list of lists.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    query = _handle_query(query)\n    cursor = conn.execute(query)\n    colnames = [description[0] for description in cursor.description]\n    data = cursor.fetchall()\n    return colnames, data\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.to_pandas.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(conn: Connection, query: str) -&gt; DataFrame\n</code></pre> <p>Returns a table as a pandas.DataFrame.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>query</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The table as a pandas.DataFrame.</p> Source code in <code>getml/sqlite3/to_pandas.py</code> <pre><code>def to_pandas(conn: sqlite3.Connection, query: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a table as a pandas.DataFrame.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        query:\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n\n    Returns:\n            The table as a pandas.DataFrame.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    colnames, data = to_list(conn, query)\n    data_frame = pd.DataFrame(data)\n    data_frame.columns = colnames\n    return data_frame\n</code></pre>"},{"location":"user_guide/","title":"Index","text":""},{"location":"user_guide/#user-guide_1","title":"User Guide","text":"<p>placeholder for toc: https://docs.getml.com/latest/user_guide/index.html#user-guide</p>"},{"location":"user_guide/annotating_data/annotating_data/","title":"Annotating data","text":""},{"location":"user_guide/annotating_data/annotating_data/#annotating-data_1","title":"Annotating data","text":"<p>After you have imported your data into the getML engine, there is one more step to undertake before you can start learning features: You need to assign a role to each column. Why is that?</p> <p>First, the general structure of the individual data frames is needed to construct the relational data model. This is done by assigning the roles join key and time stamp. The former defines the columns that are used to join different data frames, the latter ensures that only rows in a reasonable time frame are taken into account (otherwise there might be data leaks).</p> <p>Second, you need to tell the feature learning algorithm how to interpret the individual columns for it to construct sophisticated features. That is why we need the roles numerical, categorical, and target. You can also assign units to each column in a Data Frame.</p> <p>This chapter contains detailed information on the individual roles and units.</p>"},{"location":"user_guide/annotating_data/annotating_data/#in-short","title":"In short","text":"<p>When building the data model, you should keep the following things in mind:</p> <ul> <li>Every <code>DataFrame</code> in a data model needs to have at least one column (<code>columns</code>) with the role join key.</li> <li>The role time stamp has to be used to prevent data leaks (refer to data model time series for details).</li> </ul> <p>When learning features, please keep the following things in mind:</p> <ul> <li>Only <code>columns</code> with roles of categorical, numerical, and time stamp will be used by the feature learning algorithm for aggregations or conditions, unless you explicitly tell it to aggregate target columns as well (refer to <code>allow_lagged_target</code> in <code>join()</code>).</li> <li>Columns are only compared with each other if they have the same unit.</li> <li>If you want to make sure that a column is only used for comparison, you can set <code>comparison_only</code> (refer to annotating units). Time stamps are automatically set to <code>comparison_only</code>.</li> </ul> <p></p>"},{"location":"user_guide/annotating_data/annotating_data/#roles","title":"Roles","text":"<p>Roles determine if and how <code>columns</code> are handled during the construction of the data model and how they are interpreted by the feature learning algorithm. The following roles are available in getML:</p> Role Class Included in FL algorithm <code>categorical</code> <code>StringColumn</code> yes <code>numerical</code> <code>FloatColumn</code> yes <code>text</code> <code>StringColumn</code> yes <code>time_stamp</code> <code>FloatColumn</code> yes <code>join_key</code> <code>StringColumn</code> no <code>target</code> <code>FloatColumn</code> not by default <code>unused_float</code> <code>FloatColumn</code> no <code>unused_string</code> <code>StringColumn</code> no <p>When constructing a <code>DataFrame</code> via the class methods <code>from_csv</code>, <code>from_pandas</code>, <code>from_db</code>, and <code>from_json</code>, all <code>columns</code> will have either the role unused float or unused string . Unused columns will be ignored by the feature learning and machine learning (ML) algorithms.</p> <pre><code>import pandas as pd\ndata_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    weight=[12.14, 12.6, 11.92],\n    animal_id=[123, 512, 671],\n    date=[\"2019-05-02\", \"2019-02-28\", \"2018-12-24\"]\n)\npandas_df = pd.DataFrame(data=data_df)\ngetml_df = getml.data.DataFrame.from_pandas(pandas_df, name='animal elections')\n\ngetml_df\n# Output:\n# | votes        | weight       | animal_id    | animal        | date          |\n# | unused float | unused float | unused float | unused string | unused string |\n# ------------------------------------------------------------------------------\n# | 12341        | 12.14        | 123          | hawk          | 2019-05-02    |\n# | 5127         | 12.6         | 512          | parrot        | 2019-02-28    |\n# | 65311        | 11.92        | 671          | goose         | 2018-12-24    |\n</code></pre> <p>To make use of the imported data, you have to tell getML how you intend to use each column by assigning a role (<code>roles</code>). This is done by using the <code>set_role</code> method of the <code>DataFrame</code>. Each column must have exactly one role. If you wish to use a column in two different roles, you have to add it twice and assign each copy a different role.</p> <p><pre><code>getml_df.set_role(['animal_id'], getml.data.roles.join_key)\ngetml_df.set_role(['animal'], getml.data.roles.categorical)\ngetml_df.set_role(['votes', 'weight'], getml.data.roles.numerical)\ngetml_df.set_role(['date'], getml.data.roles.time_stamp)    \ngetml_df\n# Output:\n# | date                        | animal_id | animal      | votes     | weight    |\n# | time stamp                  | join key  | categorical | numerical | numerical |\n# ---------------------------------------------------------------------------------\n# | 2019-05-02T00:00:00.000000Z | 123       | hawk        | 12341     | 12.14     |\n# | 2019-02-28T00:00:00.000000Z | 512       | parrot      | 5127      | 12.6      |\n# | 2018-12-24T00:00:00.000000Z | 671       | goose       | 65311     | 11.92     |\n</code></pre> When assigning new roles to existing columns, you might notice that some of these calls are completed in an instance while others might take a considerable amount of time. What's happening here? A column's role also determines its type.  When you set a new role, an implicit type conversion might take place.</p>"},{"location":"user_guide/annotating_data/annotating_data/#a-note-on-reproducibility-and-efficiency","title":"A note on reproducibility and efficiency","text":"<p>When building a stable pipeline you want to deploy in a productive environment, the flexible default behavior of the import interface might be more of an obstacle. For instance, CSV files are not type-safe. A column that was interpreted as a float column for one set of files might be interpreted as a string column for a different set of files. This obviously has implications for the stability of your pipeline. Therefore, it might be a good idea to hard-code column roles.</p> <p>In the getML Python API, you can bypass the default deduction of the role of each column by providing a dictionary mapping each column name to a role in the import interface.</p> <p><pre><code>roles = {\"categorical\": [\"colname1\", \"colname2\"], \"target\": [\"colname3\"]}\ndf_expd = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles,\n    ignore=True\n)\n</code></pre> If the <code>ignore</code> argument is set to <code>True</code>, any columns missing in the dictionary won't be imported at all.</p> <p>If you feel that writing the roles by hand is too tedious, you can use <code>dry</code>: If you call the import interface while setting the <code>dry</code> argument to <code>True</code>, no data is read. Instead, the default roles of all columns will be returned as a dictionary. You can store, alter, and hard-code this dictionary into your stable pipeline.</p> <pre><code>roles = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True                                     \n)\n</code></pre> <p>Even if your data source is type safe, setting roles is still a good idea because it is also more efficient. Using <code>set_role()</code> creates a deep copy of the original column and might perform an implicit type conversion. If you already know where you want your data to end up, it might be a good idea to set roles in advance. </p>"},{"location":"user_guide/annotating_data/annotating_data/#join-key","title":"Join key","text":"<p>Join keys are required to establish a relation between two <code>DataFrame</code> objects. Please refer to the data models for details.</p> <p>The content of this column is allowed to contain NULL values. NULL values won't be matched to anything, not even to NULL values in other join keys.</p> <p><code>columns</code> of this role will not be aggregated by the feature learning algorithm or used for conditions. </p>"},{"location":"user_guide/annotating_data/annotating_data/#time-stamp","title":"Time stamp","text":"<p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps:</p> <p><pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre> This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a  <code>StringColumn</code>,  you need to specify the format of this string. You can do so by using  the <code>time_formats</code> argument of <code>set_role()</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign </li> </ul> <p>If none of the formats works, the getML engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL. <pre><code>data_df = dict(\ndate1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\ndate2=['1971-01-01', '1971-01-02', '1971-01-03'],\ndate3=['1|1|71', '1|2|71', '1|3|71'],\n)\ndf = getml.data.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n# | date1                       | date2                       | date3                       |\n# | time stamp                  | time stamp                  | time stamp                  |\n# -------------------------------------------------------------------------------------------\n# | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n# | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n# | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre></p> <p>Note</p> <p>getML time stamps are actually floats expressing the number of seconds since  UNIX time (1970-01-01T00:00:00).</p> <p></p>"},{"location":"user_guide/annotating_data/annotating_data/#target","title":"Target","text":"<p>The associated <code>columns</code> contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>allow_lagged_target</code> in <code>join()</code>). However, they are such an important part of the analysis that the population table is required to contain at least one of them (refer to data model tables).</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>. </p>"},{"location":"user_guide/annotating_data/annotating_data/#numerical","title":"Numerical","text":"<p>This role tells the getML engine to include the associated <code>FloatColumn</code> during the feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch. </p>"},{"location":"user_guide/annotating_data/annotating_data/#categorical","title":"Categorical","text":"<p>This role tells the getML engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integers instead of strings. </p>"},{"location":"user_guide/annotating_data/annotating_data/#text","title":"Text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. For more information on getML's handling of text fields, refer to the Preprocessing section </p>"},{"location":"user_guide/annotating_data/annotating_data/#unused_float","title":"Unused_float","text":"<p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/annotating_data/annotating_data/#unused_string","title":"Unused_string","text":"<p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/annotating_data/annotating_data/#units","title":"Units","text":"<p>By default, all columns of role categorical or numerical will only be compared to fixed values.</p> <p><pre><code>...\nWHERE numerical_column &gt; some_value \nOR categorical_column == 'some string'\n...\n</code></pre> If you want the feature learning algorithms to compare these columns with each other (like in the snippet below),  you have to explicitly set a unit. </p> <p><pre><code>...\nWHERE numerical_column1 - numerical_column2 &gt; some_value\nOR categorical_column1 != categorical_column2\n...\n</code></pre> Using <code>set_unit()</code> you can set the unit of a column to an arbitrary, non-empty string. If it matches the string of another column, both of them will be compared by the getML engine. Please note that a column can not have more than one unit.</p> <p>There are occasions where only a pairwise comparison of columns but not a comparison with fixed values is useful. To cope with this problem, you can set the <code>comparison_only</code> flag in <code>set_unit()</code>.</p> <p>Note</p> <p>Note that time stamps are used for comparison only by default. The feature  learning algorithm will not compare them to a fixed date, because it is very unlikely that such a feature would perform well out-of-sample.</p>"},{"location":"user_guide/data_model/data_model/","title":"Data model","text":""},{"location":"user_guide/data_model/data_model/#data-model_1","title":"Data model","text":"<p>Defining the data model is a crucial step before training one of getML's <code>Pipeline</code>s. You typically deal with this step after having imported your data and specified the roles of each column.</p> <p>When working with getML, the raw data usually comes in the form of relational data. That means the information relevant for a prediction is spread over several tables. The data model is the definition of the relationships between all of them.</p> <p>Most relational machine learning problems can be represented in the form of a star schema, in which case you can use the <code>StarSchema</code> abstraction. If your data set is a time series, you can use the <code>TimeSeries</code> abstraction. </p>"},{"location":"user_guide/data_model/data_model/#tables","title":"Tables","text":"<p>When defining the data model, we distinguish between a population table and one or more peripheral tables. In the context of this tutorial, we will use the term \"table\" as a catch-all for <code>DataFrame</code>s and <code>View</code>s. </p>"},{"location":"user_guide/data_model/data_model/#the-population-table","title":"The population table","text":"<p>The population table is the main table of the analysis. It defines the statistical  population of your machine learning problem and contains the target variable(s), which we want to predict. Furthermore, the table usually also contains one or more columns with the role join_key. These are keys used to establish a relationship \u2013 also called joins \u2013 with one or more peripheral tables.</p> <p>The example below contains the population table of a customer churn analysis. The target variable is <code>churn</code> \u2013 whether a person stops using the services and products of a company. It also contains the information whether or not a given customer has churned after a certain reference date. The join key <code>customer_id</code> is used to establish relations with a peripheral table. Additionally, the date the customer joined the company is contained in the column <code>date_joined</code>, which we have assigned the role time_stamp.</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#peripheral-tables","title":"Peripheral tables","text":"<p>Peripheral tables contain additional information relevant for the prediction of the target variable in the population table. Each of them is related to the latter (or another peripheral table, refer to the snowflake schema) via a join_key.</p> <p>The images below represent two peripheral tables that could be used for our customer churn analysis. One table represents complaints a customer made with a certain agent, and the other represents the transactions the customer made using their account.</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#placeholders","title":"Placeholders","text":"<p>In getML, <code>Placeholder</code>s are used to construct the <code>DataModel</code>. They are abstract representations of <code>DataFrame</code>s or <code>View</code>s and the relationships among each other, but do not contain any data themselves.</p> <p>The idea behind the placeholder concept is that they allow constructing an abstract data model without any reference to an actual data set. This data model serves as input for the <code>Pipeline</code>. Later on, the <code>feature_learning</code> algorithms can be trained and applied on any data set that follows this data model.</p> <p>More information on how to construct placeholders and build a data model can be found in the API documentation for <code>Placeholder</code> and <code>DataModel</code>. </p>"},{"location":"user_guide/data_model/data_model/#joins","title":"Joins","text":"<p>Joins are used to establish relationships between placeholders. To join two placeholders, the data frames used to derive them should both have at least one join_key. The joining itself is done using the <code>join()</code> method.</p> <p>All columns corresponding to time stamps have to be given the role time_stamp, and one of them in both the population and peripheral table is usually passed to the <code>join()</code> method. This approach ensures that no information from the future is considered during training by including only those rows of the peripheral table in the join operation for which the time stamp of the corresponding row in the population table is either the same or more recent.</p> <p></p>"},{"location":"user_guide/data_model/data_model/#data-schemata","title":"Data schemata","text":"<p>After creating placeholders for all data frames in an analysis, we are ready to create the actual data schema. A data schema is a certain way of assembling population and peripheral tables.</p>"},{"location":"user_guide/data_model/data_model/#the-star-schema","title":"The star schema","text":"<p>The <code>StarSchema</code> is the simplest way of establishing relations between the population and the peripheral tables, sufficient for the majority of data science projects.</p> <p>In the star schema, the population table is surrounded by any number of peripheral tables, all joined via a certain join key. However, no joins between peripheral tables are allowed.</p> <p>Because this is a very popular schema in many machine learning problems on relational data, getML contains a special class for these sorts of problems: <code>StarSchema</code>.</p> <p>The population table and two peripheral tables introduced in Tables can be arranged in a star schema like this:</p> <p></p> <p></p>"},{"location":"user_guide/data_model/data_model/#the-snowflake-schema","title":"The snowflake schema","text":"<p>In some cases, the star schema is not enough to represent the complexity of a data set. This is where the snowflake schema comes in: In a snowflake schema, peripheral tables can have peripheral tables of their own.</p> <p>Assume that in the customer churn analysis shown earlier, there is an additional table containing information about the calls a certain agent made in customer service. It can be joined to the <code>COMPLAINTS</code> table using the key <code>agent_id</code>.</p> <p></p> <p>To model snowflake schemata, you need to use the <code>DataModel</code> and <code>Container</code> classes.</p> <p></p>"},{"location":"user_guide/data_model/data_model/#time-series","title":"Time series","text":"<p>Time series can be handled by a self-join. In addition, some extra parameters and considerations are required when building features based on time stamps. </p>"},{"location":"user_guide/data_model/data_model/#self-joining-a-single-table","title":"Self-joining a single table","text":"<p>If you are dealing with a classical (multivariate) time series and all your data is contained in a single table, all the concepts covered so far still apply. You just have to perform a so-called self-join by providing your table as both the population and peripheral table and join them.</p> <p>The process works as follows: Whenever a row in the population table - a single measurement - is taken, it will be combined with all the content of the peripheral table - the same time series - for which the time stamps are smaller than the one in the line we picked.</p> <p>You can also use the <code>TimeSeries</code> abstraction, which abstracts away the self-join. In this case, you do not have to think about self-joins too much.</p>"},{"location":"user_guide/data_model/data_model/#horizon-and-memory","title":"Horizon and Memory","text":"<p>Crucial concepts of time series analysis are horizon and memory. In the context of getML's time series analysis, horizon is defined as a point forecast. That means the prediction of the target variable at the point as far in the future as defined by the horizon.   </p> <p>Memory, on the other hand is the time duration into the past, that is considered when making a prediction. The memory is used to define the time window of data entering the model between the past and now. The horizon defines the point in the future that the predictions is being made for.   </p>"},{"location":"user_guide/data_model/data_model/#time_stamps-and-on","title":"<code>time_stamps</code> and <code>on</code>","text":"<p>Two parameters in the time series signature determine how the self join is carried out. The <code>time_stamps</code> parameter defines what column is the underlying time dimension to which memory and horizon are applied to. The chosen column must also be of role time_stamp.  </p> <p><code>on</code> simply provides an extra handle to control, what subset of the data is part of any given time series. For example if you have a time series of sales data, you might want to only consider the sales data of a certain product category. In this case you would specify the <code>on</code> parameter to be the column containing the product category.</p> <p>Tip</p> <p>If you assign a column to the <code>on</code> parameter, then this column will not enter the model as a predictor. If you have reason to believe that this column is relevant to the model (i.e. the actual product category), duplicate that column in advance and assign the duplicate to the <code>on</code> parameter. (see class method <code>add()</code>)</p>"},{"location":"user_guide/data_model/data_model/#lagged-target-and-horizon","title":"Lagged Target and horizon","text":"<p>Another useful parameter in time series analysis is <code>lagged_target</code>. This boolean controls whether the target variable is used as a predictor. Including the target variable as a predictor can be useful in time series analysis, when at time of prediction, the target variable up until and including now is known. In turn, this means lagged target variables are only permissible if the target variable is predicted for some when in the future. That is, the horizon must be assigned a positive value.</p>"},{"location":"user_guide/data_model/data_model/#features-based-on-time-stamps","title":"Features based on time stamps","text":"<p>The getML engine is able to automatically generate features based on aggregations over time windows. Both the length of the time window and the aggregation itself will be determined by the feature learning algorithm. The only requirement is to provide the temporal resolution your time series is sampled with in the <code>delta_t</code> parameter in any feature learning algorithm.</p>"},{"location":"user_guide/deployment/deployment/","title":"Deployment","text":"<p>The results of the feature learning and the prediction can be retrieved in different ways and formats.</p> <p>Transpiling pipelines</p> <p>Using <code>SQLCode.save()</code>, you can transpile Pipelines to SQL code, which can be used without any proprietary components.</p> <p>Returning Python objects</p> <p>Using the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods of a trained <code>Pipeline</code>, you can access both the features and the predictions as <code>numpy.ndarray</code> via the Python API.</p> <p>Writing into a database</p> <p>You can also write both features and prediction results back into a new table of the connected database by providing the <code>table_name</code> argument in the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods. Please refer to the unified import interface for information on how to connect to a database.</p> <p>Responding to a HTTP POST request</p> <p>The getML suite contains HTTP endpoints to post new data via a JSON string and retrieve either the resulting features or the predictions.</p>"},{"location":"user_guide/deployment/deployment/#batch-prediction","title":"Batch prediction","text":"<p>Batch prediction pipelines are the most common way of productionizing machine learning pipelines on relational data. These pipelines are usually set to run regularly (once a month, once a week, once a day...) to create a batch of predictions on the newest data. They are typically inserted into a Docker container and scheduled using tools like Jenkins and/or Airflow.</p> <p>If you are looking for a pure Python, 100% open-source way to productionize getML's <code>Pipeline</code>s, you can transpile all the features into sqlite3 code. sqlite3 is part of the Python standard library, and you can use getML's 100% open source and pure Python <code>sqlite3</code> which provides some useful extra functionality not included in Python's standard library.</p>"},{"location":"user_guide/deployment/deployment/#http-endpoints","title":"HTTP Endpoints","text":"<p>As soon as you have trained a pipeline, whitelisted it for external access using its  <code>deploy</code> method, and configured the getML monitor  for remote access, you can transform new data into  features or make predictions on them using these endpoints:</p> <ul> <li>Transform endpoint: <code>http://localhost:1709/transform/PIPELINE_NAME</code></li> <li>Predict endpoint: <code>http://localhost:1709/predict/PIPELINE_NAME</code></li> </ul> <p>To each of them, you must send a POST request containing the new data as a JSON string in a specific request format.</p> <p>Note</p> <p>For testing and developing purposes, you can also use the HTTP port of the monitor to query the endpoints. Note that this is only possible within the same host. The corresponding syntax is  http://localhost:1709/predict/PIPELINE_NAME</p> <p></p>"},{"location":"user_guide/deployment/deployment/#request-format","title":"Request Format","text":"<p>In all POST requests to the endpoints, a JSON string with the following syntax has to be provided in the body:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_1\": [],\n    \"column_2\": []\n  },{\n    \"column_1\": [],\n    \"column_2\": []\n  }],\n  \"population\": {\n    \"column_1\": [],\n    \"column_2\": []\n  }\n}\n</code></pre> <p>It has to have exactly two keys in the top level called <code>population</code> and <code>peripheral</code>. These will contain the new input data.</p> <p>The order of the columns is irrelevant. They will be matched according to their names. However, the order of the individual peripheral tables is very important and has to exactly match the order the corresponding <code>Placeholder</code> have been provided in the constructor of <code>pipeline</code>.</p> <p>In our example above, we could post a JSON string like this:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [0.1, 0.2, 0.3, 0.4, 0.8]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/deployment/deployment/#time-stamp-formats-in-requests","title":"Time stamp formats in requests","text":"<p>You might have noticed that the time stamps in the example above have been passed as numerical values and not as their string representations shown in the beginning. Both ways are supported by the getML monitor. But if you choose to pass the string representation, you also have to specify the particular format in order for the getML engine to interpret your data properly.</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 00:15:00\", \"2010-01-01 08:00:00\", \"2010-01-01 09:30:00\", \"2010-01-01 13:00:00\", \"2010-01-01 23:35:00\"]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 12:30:00\", \"2010-01-01 23:30:00\"]\n  },\n  \"timeFormats\": [\"%Y-%m-%d %H:%M:%S\"]\n}\n</code></pre> <p>All special characters available for specifying the format of the time stamps are listed and described in e.g. <code>read_csv()</code>.</p>"},{"location":"user_guide/deployment/deployment/#using-an-existing-dataframe","title":"Using an existing <code>DataFrame</code>","text":"<p>You can also use a <code>DataFrame</code> that already  exists on the getML engine:</p> <pre><code>{\n  \"peripheral\": [{\n    \"df\": \"peripheral_table\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/deployment/deployment/#using-data-from-a-database","title":"Using data from a database","text":"<p>You can also read the data from the connected database (see unified import interface)  by passing an arbitrary query to the <code>query</code> key:</p> <pre><code>{\n  \"peripheral\": [{\n    \"query\": \"SELECT * FROM PERIPHERAL WHERE join_key = '0';\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre> <p></p>"},{"location":"user_guide/deployment/deployment/#transform-endpoint","title":"Transform Endpoint","text":"<p>The transform endpoint returns the generated features.</p> <p>http://localhost:1709/transform/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility.</p> <p><pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/transform/PIPELINE_NAME\n</code></pre> </p>"},{"location":"user_guide/deployment/deployment/#predict-endpoint","title":"Predict Endpoint","text":"<p>When using getML as an end-to-end data science pipeline, you can use the predict endpoint to upload new, unseen data and receive the resulting predictions as a response via HTTP.</p> <p>http://localhost:1709/predict/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML monitor) for better reproducibility.</p> <pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/predict/PIPELINE_NAME\n</code></pre>"},{"location":"user_guide/feature_engineering/feature_engineering/","title":"Feature engineering","text":""},{"location":"user_guide/feature_engineering/feature_engineering/#feature-engineering_1","title":"Feature engineering","text":"<p>The deep learning revolution has enabled automated feature engineering for images and sound data. Yet, for relational data and classical time series analysis, feature engineering is still done by hand or using very simple brute force methods. Our mission is to change that.</p> <p>The automation of feature engineering on relational data and time series is at the heart of the getML software suite. There are other libraries that implement feature engineering tools on top of frameworks like <code>data.tables</code> in R, <code>pandas</code> in Python, or <code>Apache Spark</code>. In essence, they all use a brute force approach: Generate a large number of features, then use some sort of feature selection routine to pick a small subselection of them.</p> <p>getML has chosen another path: Our highly efficient feature learning algorithms produce features that are far more advanced than what manual feature engineering could achieve or what could be accomplished using simple brute force approaches.</p>"},{"location":"user_guide/feature_engineering/feature_engineering/#definition","title":"Definition","text":"<p>Feature engineering is the process of constructing variables, so-called features, from a dataset. These features are used as the input for machine learning algorithms. In most real-world datasets, the raw data is spread over multiple tables and the task is to bring these tables together and construct features based on their relationships. These features are stored in a flat feature table. In other words, feature engineering is the operation of merging and aggregating a relational data model into a flat (feature) table. From an academic point of view, most machine learning algorithms used nowadays can be classified as propositional learners. The process of creating flat attribute-value representations from relational data through simple rules or aggregation functions therefore is called propositionalization.</p> <p>Usually, feature engineering is done manually, by using brute force approaches or domain knowledge. This process is sometimes also referred to as data wrangling. In any case it is a tedious, time-consuming, and error-prone process. Manual feature engineering is often done by writing scripts in languages like Python, R, or SQL.</p> <p>Note</p> <p>Unfortunately, the term feature engineering is ambiguous. More often than not, feature engineering is meant to describe numerical transformations or encoding techniques on a single table. The definition used above assumes that the raw data comes in relational form, which is true for almost all real-world data sets.</p>"},{"location":"user_guide/feature_engineering/feature_engineering/#feature-learning-vs-propositionalization","title":"Feature learning vs. propositionalization","text":"<p>We follow academia and classify techniques that use simple, merely unconditional transformations (like aggregations) to construct flat (attribute-value) representations as propositionalization approaches, while we classify algorithms which directly learn from relational data structures as feature learning. Here, we pick up a term coined in the deep learning context, where complex relationships are equally learned from raw input data.</p> <p>getML provides a framework capable of automatically extracting useful and meaningful features from a relational data model by finding the best merge and aggregate operations. In fact, the relationships between the target and the original data is learned through one of getML's feature learning algorithms.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#design-principles","title":"Design principles","text":"<p>The general procedure for feature learning on relational data and time series using getML looks like this:</p> <p>The only required input is a relational data schema. In particular, there needs to be some sort of target variable(s), which shall be predicted. For time series, the schema would typically be a self-join. In addition to this general information on the data schema, the intended usage of the variables has to be provided by setting the roles of the corresponding columns. How to setup a data scheme is described in data model.</p> <p>Features are often of the type (illustrated in pseudo SQL-like syntax): <pre><code>COUNT the number of `transactions` within the last X `days`\n</code></pre> where \\(X\\) is some sort of fixed numerical value. getML's algorithms do identify appropriate values for \\(X\\) automatically and there is no need for you to provide them by hand.</p> <p>Features can also take the form of:</p> <p><pre><code>COUNT the number of `transactions` for which the `transaction type` is \u2018type_1\u2019 OR \u2018type_2\u2019 OR \u2019type_3\u2019 OR \u2026\n</code></pre> getML's algorithms also find appropriate conditions based on categorical data without any input from the user.</p> <p>The feature learning algorithms can handle combinations of conditions too. So, features of the form:</p> <p><pre><code>SOME_AGGREGATION( over some column ) WHERE ( condition_1 AND\ncondition_2 ) OR ( condition_3 AND condition_4 ) OR condition_5\n</code></pre> will be engineered automatically as well. Again, no input from the user is required.</p> <p>To increase transparency relating to the created features, they can be expressed in SQL code. Even though automatically generated features will always be less intuitive than hand-crafted ones and could be quite complex, we want the user to get an understanding of what is going on. </p>"},{"location":"user_guide/feature_engineering/feature_engineering/#algorithms","title":"Algorithms","text":"<p>getML contains four powerful feature learning algorithms: <code>FastProp</code>, <code>Multirel</code>, <code>Relboost</code> and <code>RelMT</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#fastprop","title":"FastProp","text":"<p><code>FastProp</code> is getML's take on propositionalization. It is a fast and efficient implementation utilizing aggregations-based operations, which transform a relational data structure to a flat table. FastProp allows for the really fast generation of a substantial number of features based on simple (unconditional) aggregations.</p> <p>A typical FastProp feature looks like this:</p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT MAX( t2.column ) AS feature_1,\n      t1.rowid AS \"rownum\"\nFROM \"population\" t1\nLEFT JOIN \"peripheral\" t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nORDER BY t2.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> You may notice that such a feature looks pretty similar to the Multirel feature below. And indeed, FastProp shares some of its <code>aggregations</code> with Multirel. FastProp features, however, are usually much simpler because they lack the complex conditions learned by getML's other algorithms (the <code>WHERE</code> statement in the SQL representation). FastProp is an excellent choice in an exploration phase of a data science project and delivers decent results out of the box in many cases. It is recommended that you combine FastProp with mappings.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#multirel","title":"Multirel","text":"<p>Simply speaking, <code>Multirel</code> is a more efficient variation of Multi-relational Decision Tree Learning (MRDTL). The core idea is to minimize redundancies in the original algorithm by incremental updates. We then combined our improved version of MRDTL with ensemble learning methods.</p> <p>MRDTL is a strain of academic literature that was particularly popular in the early 2000s. It is based on a greedy, tree-like approach:</p> <ul> <li>Define some sort of objective function that evaluates the quality of your feature as it relates to the target variable(s).</li> <li>Pick an aggregation and some column to be aggregated.</li> <li>Try out different conditions. Keep the one that generates the greatest improvement of your objective. Repeat until no improvement can be found or some sort of stopping criterion is reached.</li> </ul> <p>The reason this approach has never really taken off outside of academia is that an efficient implementation is far from trivial. Most papers on MRDTL implement the algorithm on top of an existing relational database system, like MySQL.</p> <p>The main problem with trying to implement something like this on top of an existing database is that it requires many redundant operations. Consider a feature like:</p> <p><pre><code>COUNT the number of `transactions` in the last X `days`\n</code></pre> As we iterate through different values for the threshold \\(X\\), we are forced to repeat the same operations on the same data over and over again. Tasks like this bring traditional database engines to their knees.</p> <p>The core idea of getML's Multirel algorithm is to minimize redundancies through <code>incremental updates</code>. To allow for incremental updates and maximal efficiency, we developed a database engine from scratch in C++. When we evaluate a feature like:</p> <pre><code>COUNT the number of `transactions` in the last 90 `days`\n</code></pre> <p>and</p> <pre><code>COUNT the number of `transactions` in the last 91 `days`\n</code></pre> <p>very little changes in between. Multirel only recalculates what has changed and keeps everything else untouched. Therefore, it needs two ingredients that can be incrementally updated: An objective function and the actual aggregation(s).</p> <p>Our first ingredient is an objective function that must be suited for incremental updates. When we move from 90 to 91 days, presumably only very few lines in the population table actually change. We do not need to recalculate the entire table. In practice, most commonly used objective functions are fine and this is not much of a limitation. However, there are some, like rank correlation, that cannot be used.</p> <p>The second ingredient, the aggregations, must allow for incremental updates too. This part is a bit harder, so let us elaborate: Let\u2019s say we have a match between the population table that contains our targets and another table (or a self-join). This match happens to be between the two thresholds 90 and 91 days. As we move from 90 to 91 days, we have to update our aggregation for that match. For maximum efficiency, this needs also to be done incrementally. That means we do not want to recalculate the entire aggregation for all matches that it aggregates - instead just for the one match in between the two thresholds.</p> <p>We want to also support the <code>AND</code> and <code>OR</code> combinations of conditions. Therefore, it is possible that a match was not included in the aggregation before, but becomes part of it as we move the threshold. It is also possible that the match was included in the aggregation, but now it isn\u2019t anymore.</p> <p>For an aggregation like <code>Count</code>, incremental updates are straight-forward. If the match was not included, but now it is, then increment by 1. If was included, but it isn\u2019t anymore, then decrement by 1.</p> <p>Things are more tricky for aggregations like <code>Max</code>, <code>Median</code>, or <code>CountDistinct</code>. For instance, whereas incrementing <code>Max</code> is easy, decrementing it is hard. If the match used to be included and is in fact the maximum value, we now have to find the next biggest match. And we have to find it quickly - ideally iterating through a set of thresholds should take linear time in the number of matches. To make it even more complicated, some cross-joins might result in a lot of matches, so any data structures that have non-trivial memory overhead are a no-go.</p> <p>Everything so far has shed light on how we train one feature. But in practice, we want more than one. So, how do we do that? Since we are using a tree-based algorithm anyway, we are able to harness the power of ensemble learning algorithms that have been shown to work very well with non-relational decision trees, namely bagging and gradient boosting.</p> <p>With bagging, we just sample randomly from our population table. We train a feature on that sample and then pick a different random sample to train the next feature.</p> <p>With gradient boosting, we calculate the pseudo-residuals of our previously trained features. We then train features that predict these pseudo-residuals. This procedure guarantees that new features are targeted and compensate the weaknesses of older ones.</p> <p>Transpiled to SQL, a typical feature generated by Multirel looks like this:</p> <p></p> <pre><code>CREATE TABLE FEATURE_1 AS\nSELECT COUNT( * ) AS feature_1,\n       t1.join_key,\n       t1.time_stamp\nFROM (\n     SELECT * ,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> <p>Further information can be found in the API documentation for <code>Multirel</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#relboost","title":"Relboost","text":"<p><code>Relboost</code> is a generalization of the gradient boosting algorithm. More specifically, it generalizes the xgboost implementation to relational learning.</p> <p>The main difference between Relboost and Multirel is that Multirel aggregates columns, whereas Relboost aggregates learnable weights.</p> <p>Relboost addresses a problem with Multirel that is related to computational complexity theory: In Multirel, every column can be aggregated and/or used for generating a condition. That means that the number of possible features is \\(\\mathcal{O}(n^2)\\) in the number of columns in the original tables. As a result, having twice as many columns will lead to a search space that is four times as large (in reality, it is a bit more complicated than that, but the basic point is true).</p> <p>Any computer scientist or applied mathematician will tell you that \\(\\mathcal{O}(n^2)\\) is a problem. If you have tables with many columns, it might turn out to be a problem. Of course, this issue is not specific to Multirel: It is a very fundamental problem that you would also have, if you were to write your features by hand or use brute force.</p> <p>Relboost offers a way out of this dilemma: Because Relboost aggregates learnable weights and columns will only be used for conditions, but not for aggregation. So, now the search space is \\(\\mathcal{O}(n)\\) in the number of columns in the original tables - much better.</p> <p>This might seem very theoretical, but it has considerable implications: From our experience with real-world data in various projects, we know that Relboost usually outperforms Multirel in terms of predictive accuracy and training time.</p> <p>However, these advantages come at a price: First, the features generated by Relboost are less intuitive. They are further away from what you might write by hand, even though they can still be expressed as SQL code. Second, it is more difficult to apply Relboost to multiple targets, because Relboost has to learn separate rules and weights for each target.</p> <p>Expressed as SQL code, a typical feature generated by Relboost looks like this:</p> <p></p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n     WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN 0.0\n     WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN 1.0\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> Further information can be found in the API documentation for <code>Relboost</code>.</p> <p></p>"},{"location":"user_guide/feature_engineering/feature_engineering/#relmt","title":"RelMT","text":"<p><code>RelMT</code> is a generalization of linear model trees to relational data. Linear model trees are decision trees with a linear model at each leaf, resulting in a hybrid model that combines the strengths of linear models (like interpretability or the ability to capture linear relationships) with those of tree-based algorithms (like good performance or the ability to capture nonlinear relationships).</p> <p>RelMT features are particularly well-suited for time-series applications because time series often carry autoregressive structures, which can be approximated well by linear models. Think that this month's revenue can usually be modeled particularly well as a (linear) function of last month's revenue and so on. Purely tree-based models often struggle to learn such relationships because they have to fit a piecewise-constant model by predicting the average of all observations associated with each leaf. Thus, it can require a vast amount of splits to approximate a linear relationship.</p> <p>Here is a typical RelMT feature:</p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n    WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * -122.121 + COALESCE( t2.column - 3301.156, 0.0 ) * -0.003 \n    WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * 3.654 + COALESCE( t2.column - 3301.156, 0.0 ) * -1.824 + -8.720\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> RelMT features share some characteristics with Relboost features: Compare the example feature to the Relboost feature above. Both algorithms generate splits based on a combination of conditions (the <code>WHEN</code> part of the <code>CASE WHEN</code> statement above). But while Relboost learns weights for its leaves (the <code>THEN</code> part of the <code>CASE WHEN</code> statement), RelMT learns a linear model, allowing for linear combinations between columns from the population table and columns of a certain peripheral table.</p>"},{"location":"user_guide/getml_suite/","title":"Index","text":""},{"location":"user_guide/getml_suite/#the-getml-suite","title":"The getML suite","text":"<p>The getML software consists of three fundamental components:</p> <ul> <li>Engine</li> <li>Monitor</li> <li>Python API</li> </ul> <p>The getML engine is written in C++ and is the heart of the getML suite. It holds all data, is responsible for the feature engineering and the machine learning (ML) part, and does all the heavy lifting.</p> <p>You can control the engine using the getML Python API. The API provides handlers to the objects in the engine and all functionalities necessary to do an end-to-end data science project.</p> <p>To help you explore the various data sets and ML models built during your analysis, we provide the getML monitor. The monitor is written in Go. In addition to visualization, it lets you handle the login and the account management for the getML suite.</p> <p>To get started with the getML, head over to the installation instructions.</p>"},{"location":"user_guide/getml_suite/engine/","title":"Engine","text":""},{"location":"user_guide/getml_suite/engine/#the-getml-engine","title":"The getML engine","text":"<p>The getML engine is a standalone program written in C++ that does the actual work of feature engineering and prediction.</p> <p></p>"},{"location":"user_guide/getml_suite/engine/#starting-the-engine","title":"Starting the engine","text":"<p>The engine can be started using the dedicated launcher icon or by using the getML  command line interface (CLI). For more information, check out the installation  instructions for your operating system.</p>"},{"location":"user_guide/getml_suite/engine/#shutting-down-the-engine","title":"Shutting down the engine","text":"<p>There are several ways to shut down the getML engine:</p> <ul> <li>Click the '\u23fb Shutdown' tab in the sidebar of the monitor</li> <li>Press <code>Ctrl-C</code> (if started via the command line)</li> <li>Run the getML command-line interface (CLI) (see installation) using    the <code>-stop</code>    option</li> </ul>"},{"location":"user_guide/getml_suite/engine/#logging","title":"Logging","text":"<p>The engine keeps a log about what it is currently doing.</p> <p>The easiest way to view the log is to click the '&lt;&gt; Log' tab in the sidebar of the getML monitor. The engine will also output its log to the command line when it is started using the command-line interface.</p>"},{"location":"user_guide/getml_suite/monitor/","title":"Monitor","text":""},{"location":"user_guide/getml_suite/monitor/#the-getml-monitor","title":"The getML monitor","text":"<p>The getML monitor contains information on the data imported into the engine as well as the trained pipelines and their performance. It is written in Go and compiled into a binary that is separate from the getML engine.</p>"},{"location":"user_guide/getml_suite/monitor/#accessing-the-monitor","title":"Accessing the monitor","text":"<p>The monitor is always started on the same machine as the engine. The engine and the monitor use sockets to communicate. The monitor opens an HTTP port - 1709 by default - for you to access it via your favorite internet browser. Entering the following address into the navigation bar will point your browser to the monitor:</p> <p>http://localhost:1709</p> <p>The HTTP port can only be accessed from within the host the getML suite is running on.</p> <p>The main purpose of the monitor is to help you with your data science project by providing visual feedback.</p> <p>Tip</p> <p>If you experience any issues opening the monitor, try any of these steps:</p> <ul> <li>Manually shutdown the engine and restart it: <code>getml.engine.shutdown()</code> and <code>getml.engine.launch()</code></li> <li>Kill the associated background process in the terminal and restart the engine</li> <li>Close all tabs and windows in which the monitor ran previously and try again</li> </ul>"},{"location":"user_guide/getml_suite/python_api/","title":"Python API","text":""},{"location":"user_guide/getml_suite/python_api/#the-getml-python-api","title":"The getML Python API","text":"<p>The getML Python API is shipped along with the matching version of the getML engine  and monitor in the file you can download from getml.com (see  Installation).</p> <p>The most important thing you have to keep in mind when working with the Python API is this:</p> <p>\u00a0\u00a0\u00a0\u00a0All classes in the Python API are just handles to objects living in the getML engine.</p> <p>In addition, two basic requirements need to be fulfilled to successfully use the API:</p> <ol> <li>You need a running getML engine (on the same host as your Python session) (see starting the engine)</li> <li>You need to set a project in the getML engine using <code>getml.engine.set_project()</code>.</li> </ol> <p><pre><code>import getml\ngetml.engine.set_project('test')\n</code></pre> This section provides some general information about the API and how it interacts  with the engine. For an in-depth read about its individual classes, check out the  Python API documentation.</p>"},{"location":"user_guide/getml_suite/python_api/#connecting-to-the-getml-engine","title":"Connecting to the getML engine","text":"<p>The getML Python API automatically connects to the engine with every command you execute. It establishes a socket connection to the engine through a port inferred by the time you connect to a project (or create a new project).</p>"},{"location":"user_guide/getml_suite/python_api/#session-management","title":"Session management","text":"<p>You can set the current project (see Managing projects) using <code>set_project()</code>. If no project matches the supplied name, a new one will be created. To get a list of all available projects in your engine, you can use <code>list_projects()</code> and to remove an entire project, you can use <code>delete_project()</code>. </p>"},{"location":"user_guide/getml_suite/python_api/#lifecycles-and-synchronization-between-engine-and-api","title":"Lifecycles and synchronization between engine and API","text":"<p>The most important objects are the following:</p> <ul> <li>Data frames (<code>DataFrame</code>), which act as a container for all your data.</li> <li>Pipelines (<code>Pipeline</code>), which hold the trained states of the algorithms. </li> </ul>"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-dataframe","title":"Lifecycle of a <code>DataFrame</code>","text":"<p>You can create a <code>DataFrame</code> by calling one of the class methods: <code>from_csv()</code>, <code>from_db()</code>, <code>from_json()</code>, or <code>from_pandas()</code>. These create a data frame object in the getML engine, import the provided data, and return a handler to the object as a <code>DataFrame</code> in the Python API (see Importing data).</p> <p>When you apply any method, like <code>add()</code>, the changes will be automatically reflected in both the engine and Python. Under the hood, the Python API sends a command to create a new column to the getML engine. The moment the engine is done, it informs the Python API and the latter triggers the <code>refresh()</code> method to update the Python handler.</p> <p>Data frames are never saved automatically and never loaded automatically. All unsaved changes to a <code>DataFrame</code> will be lost when restarting the engine. To save a <code>DataFrame</code>, use <code>save()</code>. You can also use batch operations like <code>save_all()</code> and <code>load_all()</code> to save or load all data frames associated with the current project. The <code>DataFrames</code> container for the current project in memory can be accessed through <code>getml.project.data_frames</code>.</p> <p>To access the container, holding all of a project's data frames:</p> <p><pre><code>getml.project.data_frames\n</code></pre> To save all data frames in memory to the project folder:</p> <pre><code>getml.project.data_frames.save_all()\n</code></pre> <p>You can subset the container to access single <code>DataFrame</code> instances. You can then call all available methods on those instances. For example, to store a single data frame to disk: <pre><code>getml.project.data_frames[0].save()\n</code></pre> If a <code>DataFrame</code> called NAME_OF_THE_DATA_FRAME is already available in memory, <code>load_data_frame()</code> will return a handle to that data frame. If no such <code>DataFrame</code> is held in memory, the function will try to load the data frame from disk and then return a handle. If that is unsuccessful, an exception is thrown.</p> <p>If you want to force the API to load the version stored on disk over the one held in memory, you can use the <code>load()</code> method as follows:</p> <pre><code>df = getml.data.DataFrame(NAME_OF_THE_DATA_FRAME).load()\n</code></pre>"},{"location":"user_guide/getml_suite/python_api/#lifecycle-of-a-pipeline","title":"Lifecycle of a <code>Pipeline</code>","text":"<p>The lifecycle of a <code>Pipeline</code> is straightforward since the getML engine automatically saves all changes made to a pipeline and automatically loads all pipelines contained in a project.</p> <p>Using the constructors, the individual pipelines are created within the Python API, where they are represented as a set of hyperparameters. The actual weights of the machine learning algorithms are only stored in the getML engine and never transferred to the Python API.</p> <p>When applying any method, like <code>fit()</code>, the changes will be automatically reflected in both the engine and the Python API.</p> <p>When using <code>set_project()</code> to load an existing project, all pipelines contained in that project will be automatically loaded into memory. You can get an overview of all pipelines associated with the current project by accessing the <code>Pipelines</code> container, accessible through <code>getml.project.pipelines</code>.</p> <p>In order to create a corresponding handle in the Python API, you can use <code>load()</code>: <pre><code>pipe = getml.pipeline.load(NAME_OF_THE_PIPELINE)\n</code></pre> The function <code>list_pipelines()</code> lists all available pipelines within a project.</p>"},{"location":"user_guide/hyperopt/hyperopt/","title":"Hyperparameter optimization","text":""},{"location":"user_guide/hyperopt/hyperopt/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>In the sections on feature engineering and predicting, we learned how to train both the feature learning algorithm and the machine learning algorithm used for prediction in the getML engine. However, there are lots of parameters involved. <code>Multirel</code>, <code>Relboost</code>, <code>RelMT</code>, <code>FastProp</code>, <code>LinearRegression</code>, <code>LogisticRegression</code>, <code>XGBoostClassifier</code>, and <code>XGBoostRegressor</code> all have their own settings. That is why you might want to use hyperparameter optimization.</p> <p>The most relevant parameters of these classes can be chosen to constitute individual dimensions of a parameter space. For each parameter, a lower and upper bound has to be provided and the hyperparameter optimization will search the space within these bounds. This will be done iteratively by drawing a specific parameter combination, overwriting the corresponding parameters in a base pipeline, and then fitting and scoring it. The algorithm used to draw from the parameter space is represented by the different classes of <code>hyperopt</code>.</p> <p>While <code>RandomSearch</code> and <code>LatinHypercubeSearch</code> are purely statistical approaches, <code>GaussianHyperparameterSearch</code> uses prior knowledge obtained from evaluations of previous parameter combinations to select the next one.</p>"},{"location":"user_guide/hyperopt/hyperopt/#tuning-routines","title":"Tuning routines","text":"<p>The easiest way to conduct a hyperparameter optimization in getML are the tuning routines <code>tune_feature_learners()</code> and <code>tune_predictors()</code>. They roughly work as follows:</p> <ul> <li> <p>They begin with a base pipeline, in which the default parameters for the feature learner or the predictor are used.</p> </li> <li> <p>They then proceed by optimizing 2 or 3 parameters at a time using a <code>GaussianHyperparameterSearch</code>. If the best pipeline outperforms the base pipeline, the best pipeline becomes the new base pipeline.</p> </li> <li> <p>Taking the base pipeline from the previous steps, the tuning routine then optimizes the next 2 or 3 hyperparameters. If the best pipeline from that hyperparameter optimization outperforms the current base pipeline, that pipeline becomes the new base pipeline.</p> </li> <li> <p>These steps are then repeated for more hyperparameters.</p> </li> </ul> <p>The following tables list the tuning recipes and hyperparameter subspaces for each step:</p>"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-predictors","title":"Tuning recipes for predictors","text":"Predictor Stage Hyperparameter Subspace <code>LinearRegression</code>; <code>LogisticRegression</code> 1 (base parameters) reg_lambda [1E-11, 100] learning_rate [0.5, 0.99] <code>XGBoostClassifier</code>; <code>XGBoostRegressor</code> 1 (base parameters) learning_rate [0.05, 0.3] 2 (tree parameters) max_depth [1, 15] min_child_weights [1, 6] gamma [0, 5] 3 (sampling parameters) colsample_bytree [0.75, 0.9] subsample [0.75, 0.9] 4 (regularization parameters) reg_alpha [0, 5] reg_lambda [0, 10]"},{"location":"user_guide/hyperopt/hyperopt/#tuning-recipes-for-feature-learners","title":"Tuning recipes for feature learners","text":"Feature Learner Stage Hyperparameter Subspace <code>FastProp</code> 1 (base parameters) num_features [50, 500] n_most_frequent [0, 20] <code>Multirel</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>Relboost</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>RelMT</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_depth [1, 8] min_num_samples [1, 500] 3 (regularization parameters) reg_lambda [0, 0.0001] <p>The advantage of the tuning routines is that they provide a convenient out-of-the-box experience for hyperparameter tuning. For most use cases, it is sufficient to tune the XGBoost predictor.</p> <p>More advanced users can rely on the more low-level hyperparameter optimization routines.</p>"},{"location":"user_guide/hyperopt/hyperopt/#random-search","title":"Random search","text":"<p>A <code>RandomSearch</code> draws random hyperparameter sets from the hyperparameter space.</p>"},{"location":"user_guide/hyperopt/hyperopt/#latin-hypercube-search","title":"Latin hypercube search","text":"<p>A <code>LatinHypercubeSearch</code> draws almost random hyperparameter sets from the hyperparameter space, but ensures that they are sufficiently different from each other.</p>"},{"location":"user_guide/hyperopt/hyperopt/#gaussian-hyperparameter-search","title":"Gaussian hyperparameter search","text":"<p>A <code>GaussianHyperparameterSearch</code> works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90% of all iterations. During that burn-in phase, the hyperparameter space is sampled more or less at random, using either a random search or a latin hypercube search. You can control this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a Gaussian process on the hyperparameters with the score we want to maximize or minimize as the predicted variable. Note that the Gaussian process has hyperparameters itself, which are also optimized. You can control this phase using <code>gaussian_kernel</code>, <code>gaussian_optimization_algorithm</code>, <code>gaussian_optimization_burn_in_algorithm</code>, and <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information (EI). The EI is a measure of how much additional information it might get from evaluating a particular point in the hyperparameter space. The expected information is to be maximized. The point in the hyperparameter space with the maximum expected information is the next point that is actually evaluated (meaning a new pipeline with these hyperparameters is trained). You can control this phase using <code>optimization_algorithm</code>, <code>optimization_burn_ins</code>, and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space, it starts evaluating hyperparameter combinations that are particularly interesting.</p> </li> </ul>"},{"location":"user_guide/importing_data/","title":"Index","text":""},{"location":"user_guide/importing_data/#importing-data_1","title":"Importing data","text":"<p>Before being able to analyze and process your data using the getML software, you have to import it into the engine. At the end of this step, you will have your data in data frame objects in the getML engine and will be ready to annotate them.</p> <p>Note</p> <p>If you have imported your data into the engine before and want to restore it, refer to Lifecycle of a DataFrame</p> <p></p>"},{"location":"user_guide/importing_data/#unified-import-interface","title":"Unified import interface","text":"<p>The getML Python API provides a unified import interface requiring similar arguments and resulting in the same output format, regardless of the data source.</p> <p>You can use one of the dedicated <code>from_csv()</code>, <code>from_pandas()</code>, <code>from_db()</code>, and <code>from_json()</code> class methods to construct a data frame object in the getML engine, fill it with the provided data, and retrieve a <code>DataFrame</code> handle in the Python API. </p> <p>If you already have a data frame object in place, you can use the <code>read_csv()</code>, <code>read_pandas()</code>, <code>read_db()</code>, or <code>read_json()</code> methods of the corresponding <code>DataFrame</code> handle to either replace its content with new data or append to it.</p> <p>All those functions also have their counterparts for exporting called <code>to_csv()</code>, <code>to_pandas()</code>, <code>to_db()</code>, and <code>to_json()</code>.</p> <p>The particularities of the individual formats will be covered in the following sections:</p> <ul> <li>CSV interface</li> <li>Pandas interface</li> <li>JSON interface</li> <li>SQLite3 interface</li> <li>MySQL interface</li> <li>MariaDB interface</li> <li>PostgreSQL interface</li> <li>Greenplum interface</li> <li>ODBC interface</li> </ul>"},{"location":"user_guide/importing_data/#data-frames","title":"Data Frames","text":"<p>The resulting <code>DataFrame</code> instance in the Python API represents a handle to the corresponding data frame object in the getML engine. The mapping between the two is done based on the name of the object, which has to be unique. Similarly, the names of  the <code>columns</code> are required to be unique within the data frame they are associated with.</p>"},{"location":"user_guide/importing_data/#handling-of-null-values","title":"Handling of NULL values","text":"<p>Unfortunately, data sources often  contain missing or corrupt data - also called NULL values. getML is able to work with missing values except for the target variable, which must not contain any NULL values (because having NULL targets does not make any sense). Please refer to the section on  join keys for details about their handling during the construction of the data model.</p> <p>During import, a NULL value is automatically inserted at all occurrences of the strings \"nan\", \"None\", \"NA\", or an empty string as well as at all occurrences of <code>None</code> and <code>NaN</code>.</p>"},{"location":"user_guide/importing_data/csv_interface/","title":"CSV interface","text":""},{"location":"user_guide/importing_data/csv_interface/#csv-interface","title":"CSV interface","text":"<p>The fastest way to import data into the getML engine is to read it directly from CSV files.</p>"},{"location":"user_guide/importing_data/csv_interface/#import-from-csv","title":"Import from CSV","text":"<p>Using the <code>from_csv()</code> class method, you can create a new <code>DataFrame</code> based on a table stored in the provided file(s). The <code>read_csv()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/csv_interface/#export-to-csv","title":"Export to CSV","text":"<p>In addition to reading data from a CSV file, you can also write an existing <code>DataFrame</code> back into one using <code>to_csv()</code>.</p>"},{"location":"user_guide/importing_data/greenplum_interface/","title":"Greenplum interface","text":""},{"location":"user_guide/importing_data/greenplum_interface/#greenplum-interface","title":"Greenplum interface","text":"<p>Greenplum is an open source database system maintained by Pivotal Software, Inc. It can be connected to the getML engine using the function <code>connect_greenplum()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.</p>"},{"location":"user_guide/importing_data/greenplum_interface/#import-from-greenplum","title":"Import from Greenplum","text":"<p>By selecting an existing table of your database in the <code>from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/greenplum_interface/#export-to-greenplum","title":"Export to Greenplum","text":"<p>You can also write your results back into the Greenplum database. By providing a name for the destination table in <code>Pipeline.transform()</code>, the features generated from your raw data will be written back. Passing it into <code>Pipeline.predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/json_interface/","title":"JSON interface","text":""},{"location":"user_guide/importing_data/json_interface/#json-interface","title":"JSON interface","text":"<p>The another convenient but slow way to import data into the getML engine via its Python API.</p>"},{"location":"user_guide/importing_data/json_interface/#import-from-json","title":"Import from JSON","text":"<p>Using the <code>from_json()</code> class method, you can create a new <code>DataFrame</code> based on a JSON string. The <code>read_json()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/json_interface/#export-to-json","title":"Export to JSON","text":"<p>In addition to reading data from a JSON string, you can also convert an existing <code>DataFrame</code> into one using <code>to_json()</code>.</p>"},{"location":"user_guide/importing_data/mariadb_interface/","title":"MariaDB interface","text":""},{"location":"user_guide/importing_data/mariadb_interface/#mariadb-interface","title":"MariaDB interface","text":"<p>MariaDB is a popular open source fork of MySQL. It can be connected to the getML engine using the function <code>connect_mariadb()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your username and password ready, and you can reach it from your command line.</p> <p>If you are unsure which port or socket your MariaDB is running on, you can start the <code>mysql</code> command line interface </p> <p><pre><code>$ mysql\n</code></pre> and use the following queries to get the required insights.</p> <pre><code>MariaDB [(none)]&gt; SELECT @@port;\n\nMariaDB [(none)]&gt; SELECT @@socket;\n</code></pre>"},{"location":"user_guide/importing_data/mariadb_interface/#import-from-mariadb","title":"Import from MariaDB","text":"<p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/mariadb_interface/#export-to-mariadb","title":"Export to MariaDB","text":"<p>You can also write your results back into the MariaDB database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/mysql_interface/","title":"MySQL interface","text":""},{"location":"user_guide/importing_data/mysql_interface/#mysql-interface","title":"MySQL interface","text":"<p>MySQL is one of the most popular databases in use today. It can be connected to the getML engine using the function <code>connect_mysql()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.</p> <p>If you are unsure which port or socket your MySQL is running on, you can start the <code>mysql</code> command line interface <pre><code>$ mysql\n</code></pre> and use the following queries to get the required insights.</p> <pre><code>&gt; SELECT @@port;\n\n&gt; SELECT @@socket;\n</code></pre>"},{"location":"user_guide/importing_data/mysql_interface/#import-from-mysql","title":"Import from MySQL","text":"<p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/mysql_interface/#export-to-mysql","title":"Export to MySQL","text":"<p>You can also write your results back into the MySQL database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/odbc_interface/","title":"ODBC interface","text":""},{"location":"user_guide/importing_data/odbc_interface/#odbc-interface","title":"ODBC interface","text":"<p>ODBC (Open Database Connectivity) is an API specification for connecting software programming language to a database, developed by Microsoft.</p> <p>In a nutshell, it works like this:</p> <ul> <li>Any database of relevance has an ODBC driver that translates calls from the ODBC API into a format the database can understand, returning the query results in a format understood by the ODBC API.</li> <li>To connect getML or other software to a database using ODBC, you first need to install the ODBC driver provided by your database vendor.</li> <li>In theory, ODBC drivers should translate queries from the SQL 99 standard into the SQL dialect, but this is often ignored in practice. Also, not all ODBC drivers support all ODBC calls.</li> </ul> <p>At getML, native APIs are preferred for connecting to relational databases. ODBC is used when native APIs are not feasible due to licensing or other restrictions, especially for connecting to proprietary databases like Oracle, Microsoft SQL Server, or IBM DB2.</p> <p>ODBC is pre-installed on modern Windows operating systems, while Linux and macOS can use open-source implementations like unixODBC and iODBC, with getML using unixODBC.</p>"},{"location":"user_guide/importing_data/odbc_interface/#an-example-microsoft-sql-server","title":"An example: Microsoft SQL Server","text":"<p>To connect to Microsoft SQL Server using ODBC:</p> <ol> <li>If you do not have a Microsoft SQL Server instance, you can download a trial or development version.</li> <li>Download the ODBC driver for SQL Server.</li> <li>Configure the ODBC driver. Many drivers provide customized scripts for this, so manual configuration might not be necessary.</li> </ol> <p>For Linux and macOS, create a <code>.odbc.ini</code> file in your home directory with the following contents:</p> <p><pre><code>[ANY-NAME-YOU-WANT]\nDriver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.5.so.2.1\nServer = 123.45.67.890\nPort = 1433\nUser = YOUR-USERNAME\nPassword = YOUR-PASSWORD\nDatabase = YOUR-DATABASE\nLanguage = us_english\nNeedODBCTypesOnly = 1\n</code></pre> On Docker, you can make appropriate changes to the Dockerfile and then rerun <code>./setup.sh</code> or <code>bash setup.sh</code>.</p> <p>You will need to set the following parameters:</p> <ul> <li>The first line is the server name or data source name. You can use this name to tell getML that this is the server you want to connect to.</li> <li>The Driver is the location of the ODBC driver you have just downloaded. The location or file name might be different on your system.</li> <li>The Server is the IP address of the server. If the server is on the same machine as getML, just write \"localhost\".</li> <li>The Port is the port on which to connect the server. The default port for SQL Server is 1433.</li> <li>User and Password are the user name and password that allow access to the server.</li> <li>The Database is the database inside the server you want to connect to.</li> </ul> <p>You can now connect getML to the database:</p> <pre><code>getml.database.connect_odbc(\n    server_name=\"ANY-NAME-YOU-WANT\",\n    user=\"YOUR-USERNAME\",\n    password=\"YOUR-PASSWORD\",\n    escape_chars=\"[]\")\n</code></pre>"},{"location":"user_guide/importing_data/odbc_interface/#important-always-pass-escape_chars","title":"Important: Always pass escape_chars","text":"<p>Earlier we mentioned that ODBC drivers are supposed to translate standard SQL queries into the specific SQL dialects, but this requirement is often ignored.</p> <p>A typical issue is escape characters, needed when the names of your schemas, tables, or columns are SQL keywords, like the loans dataset containing a table named ORDER.</p> <p>To avoid this problem, you can envelop the schema, table, and column names in escape characters.  <pre><code>SELECT \"some_column\" FROM \"SOME_SCHEMA\".\"SOME_TABLE\";\n</code></pre> getML always uses escape characters for its automatically generated queries.</p> <p>The SQL standard requires that the quotation mark (\") be used as the escape character. However, many SQL dialects do not follow this requirement, e.g., SQL Server uses \"[]\":</p> <p><pre><code>SELECT [some_column] FROM [SOME_SCHEMA].[SOME_TABLE];\n</code></pre> MySQL and MariaDB work like this: <pre><code>SELECT `some_column` FROM `SOME_SCHEMA`.`SOME_TABLE`;\n</code></pre> To avoid frustration, determine your server's escape characters and explicitly pass them to <code>connect_odbc()</code>.</p>"},{"location":"user_guide/importing_data/odbc_interface/#import-data-using-odbc","title":"Import data using ODBC","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/odbc_interface/#export-data-using-odbc","title":"Export data using ODBC","text":"<p>You can also write your results back into the PostgreSQL database. When you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/pandas_interface/","title":"Pandas interface","text":""},{"location":"user_guide/importing_data/pandas_interface/#pandas-interface","title":"Pandas interface","text":"<p>Pandas is one of the key packages used in most data science projects done in Python. The associated import interface is one of the slowest, but you can harness the good data exploration and manipulation capabilities of this Python package.</p>"},{"location":"user_guide/importing_data/pandas_interface/#import-from-pandas","title":"Import from Pandas","text":"<p>Using the <code>DataFrame.from_pandas()</code> class method, you can create a new <code>DataFrame</code> based on the provided <code>pandas.DataFrame</code>. The <code>read_pandas()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p>"},{"location":"user_guide/importing_data/pandas_interface/#export-to-pandas","title":"Export to Pandas","text":"<p>In addition to reading data from a <code>pandas.DataFrame</code>, you can also write an existing <code>DataFrame</code> back into a <code>pandas.DataFrame</code> using <code>DataFrame.to_pandas()</code>. Due to the way data is stored within the getML engine, the dtypes of the original <code>pandas.DataFrame</code> cannot be restored properly and there might be inconsistencies in the order of microseconds being introduced into timestamps.</p>"},{"location":"user_guide/importing_data/postgres_interface/","title":"PostgreSQL interface","text":""},{"location":"user_guide/importing_data/postgres_interface/#postgresql-interface","title":"PostgreSQL interface","text":"<p>PostgreSQL is a powerful and well-established open source database system. It can be connected to the getML engine using the function <code>connect_postgres()</code>. Make sure your database is running, you have the corresponding hostname, port, user name, and password ready, and you can reach it from your command line.</p>"},{"location":"user_guide/importing_data/postgres_interface/#import-from-postgresql","title":"Import from PostgreSQL","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/postgres_interface/#export-to-postgresql","title":"Export to PostgreSQL","text":"<p>You can also write your results back into the PostgreSQL database. If you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/","title":"SQLite3 interface","text":""},{"location":"user_guide/importing_data/sqlite3_interface/#sqlite3-interface","title":"SQLite3 interface","text":"<p>SQLite3 is a popular in-memory database. It is faster than classical relational databases like PostgreSQL but less stable under massive parallel access. It requires all contained datasets to be loaded into memory, which might use up too much RAM, especially for large datasets.</p> <p>As with all other databases in the unified import interface of the getML Python API, you first need to connect to it using <code>connect_sqlite3()</code>.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/#import-from-sqlite3","title":"Import from SQLite3","text":"<p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p>"},{"location":"user_guide/importing_data/sqlite3_interface/#export-to-sqlite3","title":"Export to SQLite3","text":"<p>You can also write your results back into the SQLite3 database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/predicting/predicting/","title":"Predicting","text":""},{"location":"user_guide/predicting/predicting/#predicting_1","title":"Predicting","text":"<p>Now that you know how to engineer a flat table of features, you are ready to make predictions of the target variable(s).</p>"},{"location":"user_guide/predicting/predicting/#using-getml","title":"Using getML","text":"<p>getML comes with four built-in machine learning predictors:</p> <ul> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> </ul> <p>Using one of them in your analysis is very simple. Just pass one as the <code>predictor</code> argument to either <code>Pipeline</code> on initialization. As a list, more than one predictor can be passed to the pipeline.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor,\n)\n</code></pre> <p>When you call <code>fit()</code> on a pipeline, the entire pipeline will be trained.</p> <p>Note</p> <p>The time estimation for training a pipeline is a rough estimate. Occasionally, the training time can be significantly longer than the estimate. But the pipeline never silently crashes. Given enough time, computations always finish.</p> <p>Note that <code>Pipeline</code> comes with dependency tracking. That means it can figure out on its own what has changed and what needs to be trained again.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    population=population_placeholder,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor \n)\n\npipe.fit(...)\n\npipe.predictors[0].n_estimators = 50\n\n# Only the predictor has changed,\n# so only the predictor will be refitted.\npipe.fit(...)\n</code></pre> <p>To score the performance of your prediction on a test data set, the getML models come with a <code>score()</code> method. The available metrics are documented in <code>scores</code>.</p> <p>To use a trained model, including both the trained features and the predictor, to make predictions on new, unseen data, call the <code>predict()</code> method of your model.</p>"},{"location":"user_guide/predicting/predicting/#using-external-software","title":"Using external software","text":"<p>In our experience, the most relevant contribution to making accurate predictions are the generated features. Before trying to tweak your analysis by using sophisticated prediction algorithms and tuning their hyperparameters, we recommend tuning the hyperparameters of your <code>Multirel</code> or <code>Relboost</code> instead. You can do so either by hand or using getML's automated hyperparameter optimization.</p> <p>If you wish to use external predictors, you can transform new data, which is compliant with your relational data model, to a flat feature table using the <code>transform()</code> method of your pipeline.</p>"},{"location":"user_guide/preprocessing/preprocessing/","title":"Preprocessing","text":""},{"location":"user_guide/preprocessing/preprocessing/#preprocessing_1","title":"Preprocessing","text":"<p>As preprocessing, we categorize operations on data frames that are not directly related to the relational data model. While feature learning and propositionalization deal with relational data structures and result in a single-table representation thereof, we categorize all operations that work on single tables as preprocessing. This includes numerical transformations, encoding techniques, or alternative representations.</p> <p>getML's preprocessors allow you to extract domains from email addresses (<code>EmailDomain</code>), impute missing values (<code>Imputation</code>), map categorical columns to a continuous representation (<code>Mapping</code>), extract seasonal components from time stamps (<code>Seasonal</code>), extract sub strings from string-based columns (<code>Substring</code>) and split up <code>text</code> columns (<code>TextFieldSplitter</code>). Preprocessing operations in getML are very efficient and happen really fast. In fact, most of the time you won't even notice the presence of a preprocessor in your pipeline. getML's preprocessors operate on an abstract level without polluting your original data, are evaluated lazily and their set-up requires minimal effort.</p> <p>Here is a small example that shows the <code>Seasonal</code> preprocessor in action. <pre><code>import getml\n\ngetml.project.switch(\"seasonal\")\n\ntraffic = getml.datasets.load_interstate94()\n\n# traffic explicitly holds seasonal components (hour, day, month, ...)\n# extracted from column ds; we copy traffic and delete all those components\ntraffic2 = traffic.drop([\"hour\", \"weekday\", \"day\", \"month\", \"year\"])\n\nstart_test = getml.data.time.datetime(2018, 3, 14)\n\nsplit = getml.data.split.time(\n    population=traffic,\n    test=start_test,\n    time_stamp=\"ds\",\n)\n\ntime_series1 = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\ntime_series2 = getml.data.TimeSeries(\n    population=traffic2,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_function.SquareLoss)\n\npipe1 = getml.pipeline.Pipeline(\n    data_model=time_series1.data_model,\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\npipe2 = getml.pipeline.Pipeline(\n    data_model=time_series2.data_model,\n    preprocessors=[getml.preprocessors.Seasonal()],\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\n# pipe1 includes no preprocessor but receives the data frame with the components\npipe1.fit(time_series1.train)\n\n# pipe2 includes the preprocessor; receives data w/o components\npipe2.fit(time_series2.train)\n\nmonth_based1 = pipe1.features.filter(lambda feat: \"month\" in feat.sql)\nmonth_based2 = pipe2.features.filter(\n    lambda feat: \"COUNT( DISTINCT t2.\\\"strftime('%m'\" in feat.sql\n)\n\nprint(month_based1[1].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_10\";\n# \n# CREATE TABLE \"FEATURE_1_10\" AS\n# SELECT COUNT( t2.\"month\"  ) - COUNT( DISTINCT t2.\"month\" ) AS \"feature_1_10\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n\nprint(month_based2[0].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_5\";\n# \n# CREATE TABLE \"FEATURE_1_5\" AS\n# SELECT COUNT( t2.\"strftime('%m', ds )\"  ) - COUNT( DISTINCT t2.\"strftime('%m', ds )\" ) AS \"feature_1_5\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n</code></pre></p> <p>If you compare both of the features above, you will notice they are exactly the same: <code>COUNT - COUNT(DISTINCT)</code> on the month component conditional on the time-based restrictions introduced through memory and horizon.</p> <p>Pipelines can include more than one preprocessor.</p> <p>While most of getML's preprocessors are straightforward, two of them deserve a more detailed introduction: <code>Mapping</code> and <code>TextFieldSplitter</code>. </p>"},{"location":"user_guide/preprocessing/preprocessing/#mappings","title":"Mappings","text":"<p><code>Mapping</code> s are an alternative representation for categorical columns, text columns, and (quasi-categorical) discrete-numerical columns. Each discrete value (category) of a categorical column is mapped to a continuous spectrum by calculating the average target value for the subset of all rows belonging to the respective category. For columns from peripheral tables, the average target values are propagated back by traversing the relational structure.</p> <p>Mappings are a simple and interpretable alternative representation for categorical data. By introducing a continuous representation, mappings allow getML's feature learning algorithms to apply arbitrary aggregations to categorical columns. Further, mappings enable huge gains in efficiency when learning patterns from categorical data. You can control the extent mappings are utilized by specifying the minimum number of matching rows required for categories that constitutes a mapping through the <code>min_freq</code> parameter.</p> <p>Here is an example mapping from the CORA notebook: <pre><code> DROP TABLE IF EXISTS \"CATEGORICAL_MAPPING_1_1_1\";\n CREATE TABLE \"CATEGORICAL_MAPPING_1_1_1\"(key TEXT NOT NULL PRIMARY KEY, value NUMERIC);\n INSERT INTO \"CATEGORICAL_MAPPING_1_1_1\"(key, value)\n VALUES('Case_Based', 0.7109826589595376),\n       ('Rule_Learning', 0.07368421052631578),\n       ('Reinforcement_Learning', 0.0576923076923077),\n       ('Theory', 0.0547945205479452),\n       ('Genetic_Algorithms', 0.03157894736842105),\n       ('Neural_Networks', 0.02088772845953003),\n       ('Probabilistic_Methods', 0.01293103448275862);\n</code></pre> Inspecting the actual values, it's highly likely, that this mapping stems from a feature learned by a sub learner targeting the label \"Case_Based\". In addition to the trivial case, we can see that the next closed neighboring category is the \"Rule_Learning\" category, to which 7.3 % of the papers citing the target papers are categorized. </p>"},{"location":"user_guide/preprocessing/preprocessing/#handling-of-free-form-text","title":"Handling of free form text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. Learning from <code>text</code> columns works as follows: First, for each of the <code>text</code> columns, a vocabulary is built by taking into account the feature learner's text mining specific hyperparameter <code>vocab_size</code>. If a text field contains words that belong to the vocabulary, getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either can be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. Opting for the second approach is as easy as adding the <code>TextFieldSplitter</code> to the list of <code>preprocessors</code> on your <code>Pipeline</code>. The resulting bag of words can be viewed as another one-to-many relationship within our data model where each row holding a text field is related to n peripheral rows (n is the number of words in the text field). Consider the following example, where the text field is split into a relational bag of words.</p>"},{"location":"user_guide/preprocessing/preprocessing/#one-row-of-a-table-with-a-text-field","title":"One row of a table with a text field","text":"rownum text field 52 The quick brown fox jumps over the lazy dog"},{"location":"user_guide/preprocessing/preprocessing/#the-implicit-peripheral-table-that-results-from-splitting","title":"The (implicit) peripheral table that results from splitting","text":"rownum words 52 the 52 quick 52 brown 52 fox 52 jumps 52 over 52 the 52 lazy 52 dog <p>As text fields now present another relation, getML's feature learning algorithms are able to learn structural logic from text fields' contents by applying aggregations over the resulting bag of words itself (<code>COUNT WHERE words IN ('quick', 'jumps')</code>). Further, by utilizing mappings, any aggregation applicable to a (mapped) categorical column can be applied to bag-of-words mappings as well.</p> <p>Note that the splitting of text fields can be computationally expensive. If performance suffers too much, you may resort to the default behavior by removing the <code>TextFieldSplitter</code> from your <code>Pipeline</code>.</p>"},{"location":"user_guide/project_management/project_management/","title":"Managing projects","text":""},{"location":"user_guide/project_management/project_management/#managing-projects","title":"Managing projects","text":"<p>When working with getML, all data is bundled into projects. getML's projects are managed through the <code>getml.project</code> module.</p>"},{"location":"user_guide/project_management/project_management/#the-relationship-between-projects-and-engine-processes","title":"The relationship between projects and engine processes","text":"<p>Each project is tied to a specific instance of the getML engine running as a global process (independent from your python session). In this way, it is possible to share one getML instance with multiple users to work on different projects. When switching projects through <code>getml.project.switch()</code>, the python API spawns a new process and establishes a connection to this process, while the currently loaded project remains in memory and its process is delegated to the background (until you explicitly <code>suspend()</code> the project). You can also work on multiple projects simultaneously from different python sessions. This comes in particularly handy if you use Jupyter Lab to open multiple notebooks and manage multiple python kernels simultaneously.</p> <p>To load an existing project or create a new one, you can do so from the 'Projects' view in the monitor or use the API (<code>getml.engine.set_project()</code>).</p> <p>If you want to shut down the engine process associated with the current project, you can call <code>getml.project.suspend()</code>. When you suspend the project, the memory of the engine is flushed and all unsaved changes to the data frames are lost (see lifecycles and synchronization between engine and API for details). All pipelines of the new project are automatically loaded into memory. You can retrieve all of your project's pipelines through <code>getml.project.pipelines</code>.</p> <p>Projects can be deleted by clicking the trash can icon in the 'Projects' tab of the getML monitor or by calling <code>getml.engine.delete_project()</code> (to delete a project by name) or <code>getml.project.delete()</code> (to suspend and delete the project currently loaded).</p>"},{"location":"user_guide/project_management/project_management/#managing-data-using-projects","title":"Managing data using projects","text":"<p>Every project has its own folder in <code>~/.getML/getml-VERSION/projects</code> (for Linux and macOS) in which all of its data and pipelines are stored. On Windows, the projects folder is in the same location as <code>getML.exe</code>. These folders can be easily shared between different instances of getML; even between different operating systems. However, individual pipelines or data frames cannot be simply copied to another project folder \u2013 they are tied to the project. Projects can be bundled and exported/imported.</p>"},{"location":"user_guide/project_management/project_management/#using-the-project-module-to-manage-your-project","title":"Using the project module to manage your project","text":"<p>The <code>getml.project</code> module is the entry point to your projects. From here, you can: query project-specific data (<code>getml.project.pipelines</code>, <code>getml.project.data_frames</code>, <code>getml.project.hyperopts</code>), manage the state of the current project (<code>getml.project.delete()</code>, <code>getml.project.restart()</code>, <code>getml.project.switch()</code>, <code>getml.project.suspend()</code>), and import projects from or export projects as a <code>.getml</code> bundle to disk (<code>getml.project.load()</code>, <code>getml.project.save()</code>).</p>"}]}